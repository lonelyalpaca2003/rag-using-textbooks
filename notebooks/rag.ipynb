{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "#from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pypdf\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_with_metadata_included(data_path:str):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if not filename.endswith('.pdf'):\n",
    "            continue\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        reader = pypdf.PdfReader(file_path)\n",
    "\n",
    "        if \"lecture\" in filename.lower():\n",
    "            doc_type = \"lecture\"\n",
    "\n",
    "        if 'lecture' not in filename.lower():\n",
    "            doc_type = \"textbook\"\n",
    "\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            doc = Document(text = text, \n",
    "                           metadata = {\n",
    "                               \"file_name\" : filename, \n",
    "                               \"page_num\" : page_num, \n",
    "                               \"doc_type\" : doc_type,\n",
    "                               \"course\" : \"Machine Learning\"\n",
    "                           })     \n",
    "            all_docs.append(doc)\n",
    "            \n",
    "\n",
    "    return all_docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-12 23:14:01,820 - WARNING - Ignoring wrong pointing object 173 0 (offset 0)\n",
      "2026-02-12 23:14:01,821 - WARNING - Ignoring wrong pointing object 376 0 (offset 0)\n",
      "2026-02-12 23:14:01,821 - WARNING - Ignoring wrong pointing object 393 0 (offset 0)\n",
      "2026-02-12 23:14:01,822 - WARNING - Ignoring wrong pointing object 425 0 (offset 0)\n",
      "2026-02-12 23:14:01,822 - WARNING - Ignoring wrong pointing object 427 0 (offset 0)\n",
      "2026-02-12 23:14:01,822 - WARNING - Ignoring wrong pointing object 434 0 (offset 0)\n",
      "2026-02-12 23:14:01,822 - WARNING - Ignoring wrong pointing object 652 0 (offset 0)\n",
      "2026-02-12 23:14:01,822 - WARNING - Ignoring wrong pointing object 678 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 781 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 837 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 840 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 843 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 854 0 (offset 0)\n",
      "2026-02-12 23:14:01,823 - WARNING - Ignoring wrong pointing object 885 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 929 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 1050 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 1138 0 (offset 0)\n",
      "2026-02-12 23:14:01,824 - WARNING - Ignoring wrong pointing object 1140 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1149 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1157 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "2026-02-12 23:14:01,825 - WARNING - Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "2026-02-12 23:14:01,826 - WARNING - Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "2026-02-12 23:14:01,826 - WARNING - Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "2026-02-12 23:14:01,826 - WARNING - Ignoring wrong pointing object 1290 0 (offset 0)\n",
      "2026-02-12 23:14:01,826 - WARNING - Ignoring wrong pointing object 1425 0 (offset 0)\n",
      "2026-02-12 23:14:01,826 - WARNING - Ignoring wrong pointing object 1452 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1512 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1551 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1578 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1588 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1645 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1650 0 (offset 0)\n",
      "2026-02-12 23:14:01,827 - WARNING - Ignoring wrong pointing object 1674 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1677 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1725 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1735 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1753 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1755 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1793 0 (offset 0)\n",
      "2026-02-12 23:14:01,828 - WARNING - Ignoring wrong pointing object 1795 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 1952 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 1969 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 1978 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 1990 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 2059 0 (offset 0)\n",
      "2026-02-12 23:14:01,829 - WARNING - Ignoring wrong pointing object 2068 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2076 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2079 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2081 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2083 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2085 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2087 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2089 0 (offset 0)\n",
      "2026-02-12 23:14:01,830 - WARNING - Ignoring wrong pointing object 2091 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2093 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2095 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2097 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2099 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2101 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2103 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2105 0 (offset 0)\n",
      "2026-02-12 23:14:01,831 - WARNING - Ignoring wrong pointing object 2107 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2280 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2298 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2300 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2302 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2306 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2310 0 (offset 0)\n",
      "2026-02-12 23:14:01,832 - WARNING - Ignoring wrong pointing object 2329 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2331 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2333 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2337 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2369 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2371 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2373 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2377 0 (offset 0)\n",
      "2026-02-12 23:14:01,833 - WARNING - Ignoring wrong pointing object 2427 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2436 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2443 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2456 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2459 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2472 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2474 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2488 0 (offset 0)\n",
      "2026-02-12 23:14:01,834 - WARNING - Ignoring wrong pointing object 2490 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2493 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2545 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2559 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2569 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2594 0 (offset 0)\n",
      "2026-02-12 23:14:01,835 - WARNING - Ignoring wrong pointing object 2629 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2773 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2886 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2921 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2949 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2968 0 (offset 0)\n",
      "2026-02-12 23:14:01,836 - WARNING - Ignoring wrong pointing object 2974 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 2996 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3012 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3128 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3159 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3171 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3189 0 (offset 0)\n",
      "2026-02-12 23:14:01,837 - WARNING - Ignoring wrong pointing object 3254 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3257 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3260 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3263 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3329 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3331 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3336 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3338 0 (offset 0)\n",
      "2026-02-12 23:14:01,838 - WARNING - Ignoring wrong pointing object 3399 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3411 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3434 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3464 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3474 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3485 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3516 0 (offset 0)\n",
      "2026-02-12 23:14:01,839 - WARNING - Ignoring wrong pointing object 3528 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3577 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3593 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3603 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3624 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3639 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3644 0 (offset 0)\n",
      "2026-02-12 23:14:01,840 - WARNING - Ignoring wrong pointing object 3735 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3885 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3889 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3911 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3934 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3969 0 (offset 0)\n",
      "2026-02-12 23:14:01,841 - WARNING - Ignoring wrong pointing object 3979 0 (offset 0)\n",
      "2026-02-12 23:14:01,842 - WARNING - Ignoring wrong pointing object 4033 0 (offset 0)\n",
      "2026-02-12 23:14:01,842 - WARNING - Ignoring wrong pointing object 4036 0 (offset 0)\n",
      "2026-02-12 23:14:01,842 - WARNING - Ignoring wrong pointing object 4038 0 (offset 0)\n",
      "2026-02-12 23:14:01,842 - WARNING - Ignoring wrong pointing object 4102 0 (offset 0)\n",
      "2026-02-12 23:14:01,842 - WARNING - Ignoring wrong pointing object 4311 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4313 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4315 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4356 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4434 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4436 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4439 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4443 0 (offset 0)\n",
      "2026-02-12 23:14:01,843 - WARNING - Ignoring wrong pointing object 4446 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4449 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4452 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4455 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4612 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4631 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4674 0 (offset 0)\n",
      "2026-02-12 23:14:01,844 - WARNING - Ignoring wrong pointing object 4720 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4723 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4731 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4734 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4803 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4826 0 (offset 0)\n",
      "2026-02-12 23:14:01,845 - WARNING - Ignoring wrong pointing object 4852 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 4946 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 4969 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 5060 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 5106 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 5108 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 5144 0 (offset 0)\n",
      "2026-02-12 23:14:01,846 - WARNING - Ignoring wrong pointing object 5168 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5177 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5180 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5213 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5237 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5252 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5270 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5334 0 (offset 0)\n",
      "2026-02-12 23:14:01,847 - WARNING - Ignoring wrong pointing object 5338 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents_with_metadata_included(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d6890b80-96a3-42b6-9aec-f64227157260', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 5: Regularisation\\n29 Oct 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4b47818-c484-4621-91af-2223a4dacd4c', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ What is regularisation/penalisation?\\n▶ Best subset selection and stepwise selection\\n▶ Model selection criteria\\n▶ Ridge regression and shrinkage\\n▶ Lasso regression and the sparsity-inducing ℓ1 penalty\\nMilan Vojnović 2/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b0ba3ae-a32a-4076-be01-e1ad52dbf978', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A general picture\\n▶ Recall the linear model\\nYi = β0 + β1Xi1 + · · · + βpXip + εi\\n= β0 + β⊤Xi + εi\\nfor i = 1, . . . , n.\\n▶ Two reasons one might prefer not to use OLS estimates:\\n1. Prediction accuracy.\\n2. Model intepretability.\\nMilan Vojnović 3/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='96cb70df-6ec7-4c63-af66-3241ae45be42', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Prediction accuracy\\n▶ The LSE performs well when n ≫ p.\\n▶ But, when p is close to n, then LSE can have high variance and may result\\nin over-fitting and poor prediction on unseen observations.\\n▶ Moreover, when n < p , there are infinitely many choices of β perfectly\\nfitting the data.\\nMilan Vojnović 4/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca71f6aa-0175-4a31-962d-11dab67c115f', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Model intepretability\\n▶ When we have a large number of variables X in the model there will\\ngenerally be many that have little or no effect on Y .\\n▶ Leaving these variables in the models makes it harder to see the ‘big\\npicture’, i.e. identification of the more important variables.\\n▶ By removing irrelevant features, i.e. setting the corresponding coefficient\\nestimates to zero, we obtain a model that is more easily interpreted.\\nMilan Vojnović 5/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5726521e-da3e-4f9a-bed1-c81a25939b38', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Flexibility vs intepretability\\nMilan Vojnović 6/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='81260d23-df08-4171-ad9b-2a86103b2d23', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Two general approaches\\n▶ Regularisation. Instead of just minimising the residual sum of squares, we\\nadd in an additional criteria in training that encourages low complexity\\nmodels.\\n▶ Dimension reduction. We first find a small set of directions that best\\nsummarise the variability in our data and perform subsequent analysis on\\nsynthetic variables created using these directions.\\n– More on this in Lecture 9.\\nMilan Vojnović 7/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='27d94955-6208-4651-ac7d-adc2c2ed7cd2', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What is regularisation?\\n▶ We optimise\\nℓ(β0, β) = ∥Y − β01n − Xβ ∥2 + pen(β0, β)\\nfor some penalty function pen(β0, β).\\n▶ The penalty function is large when the coefficients are complex, thus\\nencouraging the estimated coefficients to take simpler forms.\\n▶ Typically pen(β0, β) does not depend on β0 — we do not penalise the\\nintercept term.\\n▶ Examples:\\n– pen (β) = ifelse(∥β∥0 ≤ k, 0, ∞): best subset selection.\\n– pen (β) = ifelse(∥β∥1 ≤ λ, 0, ∞): Lasso in constrained form.\\n– pen (β) = λ∥β∥1: Lasso regression (in penalised form).\\n– pen (β) = λ∥β∥2\\n2: ridge regression (in penalised form).\\nMilan Vojnović 8/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e25dfa5-b5f7-4151-9e21-ef1874ed6a84', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Best Subsect Selection\\n▶ For a fixed k, the Best Subset Selection fits all\\n\\x00p\\nk\\n\\x01\\nmodels that contain\\nexactly k predictors and choose the one with the smallest RSS.\\n▶ If k is not known, we find the best model Mk for each k = 0, 1, . . . , pand\\nthen select the single best model from M0, . . . ,Mp using cross-validated\\nprediction eror, AIC or BIC.\\n▶ Note that M0 is the null model, which contains no predictors and simply\\npredicts the sample mean for each observation.\\nMilan Vojnović 9/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='13606d96-2ecd-4f19-a772-3848e9ed2e73', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Credit data example\\nResidual Sum of Squares: SSres =P\\ni(yi − β0 − x⊤\\ni ˆβ)2\\nR2 = 1 − SSres/SStot where SStot =P\\ni(yi − ¯y)2\\nMilan Vojnović 10/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='32e5e21b-3d84-4635-b5ab-943b3db74b71', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Stepwise selection\\n▶ Computational problem: best subset selection is not a feasible approach\\nwhen p is very large.\\n▶ Statistical problem: the larger the search space, the higher the possibility\\nof selecting models that look pretty good on the training data, but have\\npoor performance on prediction for new inputs. (Overfitting and high\\nvariance from a bias-variance tradeoff perspective).\\n▶ Stepwise methods performs a greedy search on a subset of the variable\\nspace to remedy the above two issues.\\nMilan Vojnović 11/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89c9e730-25ab-4b61-a197-6bee28b01239', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Forward Stepwise Selection (FSS)\\n1. Let M0 denote the null model, which contains no predictors.\\n2. For k = 0, 1, . . . , p− 1 :\\n(a) Fit all p − k models that augment the predictors in Mk with one\\nadditional predictor.\\n(b) Pick the best among these p − k models, and call it Mk+1. Here best is\\ndefined as having the smallest RSS.\\n3. Select a single best model from among M0, . . . ,Mp using cross-validated\\nprediction error, AIC or BIC.\\nMilan Vojnović 12/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21007b8f-e433-4b58-9cc4-1f9b169bee14', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pros and cons of FSS\\n▶ Computational advantageous to the best subset selection approach\\n▶ FSS approach searches through 1 + p(p + 1)/2 models.\\n▶ It is not guaranteed to find the best possible model out of all 2p models\\ncontaining subsets of the p predictors. Why? Any example?\\nMilan Vojnović 13/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb6e22f4-e6f0-4f60-ae86-85a0cf155193', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Credit data example\\nMilan Vojnović 14/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='946302fa-aca0-455a-b542-ce1b03c5c1c2', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Backward Stepwise Selection (BSS)\\n1. Let Mp denote the full model, which contains all p predictors.\\n2. For k = p, p − 1, . . . ,1:\\n(a) Consider all k models that contain all but one of the predictors in Mk,\\nfor a total of k − 1 predictors.\\n(b) Choose the best among these k models, and call it Mk−1. Here best is\\ndefined as having the smallest RSS.\\n3. Select a single best model from among M0, . . . ,Mp using cross-validated\\nprediction error, AIC or BIC.\\nMilan Vojnović 15/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c0040649-5c33-4582-a843-2196e834cc60', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choosing the optimal model\\n▶ The model containing more predictors tend to produce smaller RSS. Why?\\n▶ We wish to choose the model with low test error.\\n▶ Therefore, RSS is not an appropriate criterion for selecting the best model\\namong a collection of models with different number of predictors.\\n– We can estimate the test error by making adjustment to the training\\nerror to account for the bias due to overfitting.\\n– Or a different cross validation type of approaches can be applied.\\nMilan Vojnović 16/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e1a4af83-0881-4599-8991-aecdd7a5fb5d', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='AIC and BIC\\n▶ The Akaike’s Information Criterion (AIC) is defined for likelihood-based\\nmodels via\\nAIC := −2 logL + 2d,\\nwhere log L is the log-likelihood of the estimated model and d is the\\nnumber of fitted parameters in the model.\\n▶ The Bayesian Information Criterion (BIC) is defined as\\nBIC := −2 logL + d log n.\\n▶ We select models with small AIC or BIC.\\nMilan Vojnović 17/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e59c44f-d5c4-4c97-a510-cc17bbaf3eb9', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='AIC explained\\n▶ The AIC is a an unbiased estimator of the risk of the maximum likelihood\\nestimator asymptotically for large number of data observations n.\\n▶ Assume Z1, . . . , Zn is i.i.d. with distribution fZ.\\n▶ Let f(·; θ) for θ ∈ Θ ⊆ Rd be a family of distributions.\\n▶ Define the risk to be the negative log-likelihood and the empirical risk as\\nfollows:\\nR(θ) = nE[− log(f(Z; θ))] and ˆR(θ) =\\nnX\\ni=1\\n− log(f(Zi; θ)).\\n▶ Let θ∗ = arg minθ R(θ) and ˆθ = arg minθ ˆR(θ).\\n– ˆθ is the MLE estimator.\\n▶ How does ˆR(ˆθ) compare to R(ˆθ)?\\nMilan Vojnović 18/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b60b8a14-00df-4360-bb00-b70a8c94f7d7', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='AIC explained cont’d\\n▶ It can be shown that (approximations are tight for large n):\\nE[ ˆR(ˆθ)] ≈ R(θ∗) − 1\\n2 B and E[R(ˆθ)] ≈ R(θ∗) + 1\\n2 B\\nwhere\\nB = nE[(ˆθ − θ∗)⊤I(θ∗)(ˆθ − θ∗)]\\nand I(θ∗) is the Fisher information matrix\\nI(θ∗) = E\\n\\x02\\n∇ log(f(Z; θ∗))∇ log(f(Z; θ∗))⊤\\x03\\n.\\n▶ It can also be shown that B → d as n goes to infinity.\\n▶ Hence, we have\\nE[ ˆR(ˆθ)] + d ≈ E[R(ˆθ)].\\n▶ This justifies the AIC criterion: AIC = −2 logL + 2d\\n– Factor 2 is non-essential; it is used for historical reasons.\\nMilan Vojnović 19/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c2731ef-4655-4bff-8b38-786065eedf69', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross validation\\n▶ All approaches generate model Mk for k = 0, 1, . . ., we aim to select an\\noptimal k.\\n▶ We can compute the cross validation error for each model Mk and choose\\nthe one with the smallest test error.\\n▶ It provides a direct estimate of the test error and makes fewer\\nassumptions about the true model.\\nMilan Vojnović 20/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='be426ee9-6d8a-489a-801a-c536aba8ea72', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ridge regression\\n▶ The least square estimate β0, β1, . . . , βp through minimising\\nRSS =\\nnX\\ni=1\\n\\x12\\nyi − β0 −\\npX\\nj=1\\nβjxij\\n\\x132\\n.\\n▶ Ridge regression estimates ˆβR\\nλ through minimising\\nnX\\ni=1\\n\\x12\\nyi − β0 −\\npX\\nj=1\\nβjxij\\n\\x132\\n+ λ\\npX\\nj=1\\nβ2\\nj ,\\nwhere λ ≥ 0 is the regularisation parameter, to be determined separately.\\n▶ Larger or smaller λ? Selecting a good λ is very important, cross validation\\nor AIC/BIC can be used for this (to be discussed).\\n▶ Note the shrinkage penalty is applied to the slope not the intercept.\\nMilan Vojnović 21/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9b33a370-ee35-4c6f-ac7d-75e052492c20', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ridge regression solution\\n▶ The ridge regression objective function written in matrix form\\nℓ(β0, β) = ∥Y − β01n − Xβ ∥2\\n2 + λ∥β∥2\\n2\\nis strictly convex when λ > 0 and hence has a unique minimiser ( ˆβ0, ˆβR\\nλ ).\\n▶ Assume 1⊤\\nn X = 0. Gradient at minimiser is 0:\\n0 = ∂ℓ\\n∂β0\\n\\x0c\\x0c\\x0c\\x0c\\nβ0= ˆβ0,β= ˆβR\\nλ\\n= −21⊤\\nn (Y − ˆβ01n − \\x08\\x08\\x08X ˆβR\\nλ )\\n=⇒ ˆβ0 = 1\\nn\\nnX\\ni=1\\nYi\\n0 = ∂ℓ\\n∂β\\n\\x0c\\x0c\\x0c\\x0c\\nβ0= ˆβ0,β= ˆβR\\nλ\\n= −2X ⊤(Y − \\x08\\x08\\x08ˆβ01n − X ˆβR\\nλ ) + 2λ ˆβR\\nλ\\n=⇒ ˆβR\\nλ = (X ⊤X + λIp)−1X ⊤Y.\\nMilan Vojnović 22/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='69a19018-287b-4912-b6a4-91db3413d269', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Credit data example (solution paths)\\n▶ Left: each curve corresponds to the ridge regression coefficient estimate for\\none of ten variables.\\n▶ Right: instead of plotting against the regularisation parameter, we display\\n∥ ˆβR\\nλ ∥2/∥ ˆβ∥2, where ˆβ denotes the LSE.\\nMilan Vojnović 23/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4052f3c0-14e8-49bb-a8be-633e263769dd', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Scaling of predictors in ridge regression\\n▶ The standard LSE are scale-invariant, multiplying Xi’s by a constantc will\\nlead to the fitted regression coefficients scaled by a factor of 1/c. The\\nprediction for the test data ˜Xi′ ˆβ remains the same.\\n▶ Ridge regression coefficient estimates can change significantly when a\\npredictor is multiplied by a constant. (Scale-invariant or not? Why?)\\n▶ Hence it is best to perform standardisation for the predictors before\\napplying ridge regression,\\n˜xij = xij − ¯xjq\\n1\\nn\\nPn\\ni=1(xij − ¯xj)2\\n.\\nMilan Vojnović 24/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f13c093-fede-46b3-93a4-859f6e80fc69', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bias-variance tradeoff for ridge regression\\nSimulated data with n = 50 observations and p = 45 predictors all having\\nnonzero coefficients. Squared bias (black), variance (green) and test MSE\\n(purple) for the ridge regression predictions on a simulated data set. The\\nhorizontal dashed lines indicate the minimum possible MSE.\\nMilan Vojnović 25/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='59b72e04-4ade-4ab4-a0aa-3d9f9db05baf', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Lasso\\n▶ Ridge regression still include all p predictors in the model. (No variable\\nselection!)\\n▶ The Lasso∗ was proposed to overcome this advantage.\\n▶ The Lasso coefficient ˆβL\\nλ minimises\\nnX\\ni=1\\n\\x12\\nyi − β0 −\\npX\\nj=1\\nβjxij\\n\\x132\\n+ λ\\npX\\nj=1\\n|βj|,\\nwhere λ ≥ 0 is the regularisation parameter.\\n▶ Selection of regularisation parameter? CV or AIC/BIC.\\n∗Robert Tibshirani, 1996, Journal of Royal Statistical Society: Series B\\nMilan Vojnović 26/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8ef14aa-e3b4-48c0-8863-e80b7e605418', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Properties of the Lasso\\n▶ The ℓ1 penalty has the effect of forcing some of the coefficients to be\\nexactly zero, thus Lasso performs variable selection and parameter\\nestimation simultaneously.\\n▶ Lasso regression is useful if we believe only a small number of measured\\ncovariates are useful in predicting the response.\\n▶ If we know the subset of nonzero variables (say s of them), we can fit an\\nLSE on these s variables to obtain prediction MSE of order O( s\\nn).\\n▶ An important property of Lasso is that without knowledge of this subset in\\nadvance, with a suitable choice of λ, the Lasso prediction MSE is O( s log p\\nn ),\\ni.e. we pay only a logarithmic price in dimension!\\nMilan Vojnović 27/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='849d6bfc-f8b1-4ac5-a53c-01c8f2174d88', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Credit data example (solution paths)\\nMilan Vojnović 28/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ebc86c0-e281-46bc-9433-bf345d4015f5', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Constrained Ridge/Lasso regressions\\n▶ One can easily show that the Lasso and Ridge regression coefficient\\nestimates solve the following problems, respectively:\\nmin\\nβ0,β\\nnX\\ni=1\\n(yi − β0 − β⊤Xi)2 subject to ∥β∥1 ≤ s\\nand\\nmin\\nβ0,β\\nnX\\ni=1\\n(yi − β0 − β⊤Xi)2 subject to ∥β∥2\\n2 ≤ s.\\nMilan Vojnović 29/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea839fda-97c9-4dd9-81e7-0f9784047428', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A graphical illustration\\nThe sold blue areas are the constraint regions, |β1| + |β2| ≤ s and β2\\n1 + β2\\n2 ≤ s,\\nwhile the red ellipses are the contours of the RSS.\\nMilan Vojnović 30/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d5f7c78d-8752-410d-b429-8518d0f4c741', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 30, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choice of λ in ridge/Lasso regressions\\n▶ Bias and variance tradeoff: As λ increases, the model becomes sparser,\\nso that the variance will decrease and the bias will increase. The test MSE\\nwill behave like a U shape.\\n▶ We then select the regularisation parameter for which the cross-validated\\nerror is the smallest.\\n▶ Finally, the model is re-fitted using all of the training data and the selected\\nvalue of the regularisation parameter.\\nMilan Vojnović 31/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37d2ea22-83f6-4637-9f2f-961fbcfabe0e', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 31, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Examples\\nCredit data example\\nA simulated data example\\nMilan Vojnović 32/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5dd709fd-6c2a-4d33-8f42-a15eda4712db', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 32, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A simple special case\\n▶ Consider a simple special case n = p and the design matrix X is an\\nidentity matrix. Also consider the regression without an intercept.\\n▶ Least squares aim to minimisePp\\nj=1(yj − βj)2 and the solution is given\\nby\\nˆβj = yj, for j = 1, . . . , p.\\n▶ Ridge regression considers minimisingPp\\nj=1(yj − βj)2 + λPp\\nj=1 β2\\nj and\\nthe solution is given by\\nˆβR\\nj = yj/(1 + λ).\\n▶ The Lasso considers minimisingPp\\nj=1(yj − βj)2 + λPp\\nj=1 |βj| and the\\nsolution is given by\\nˆβL\\nj =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\nyj − λ/2 if yj > λ/2\\nyj + λ/2 if yj < −λ/2\\n0 if |yj| ≤ λ/2.\\nMilan Vojnović 33/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc9390dd-0f06-4943-bc06-8db29a9156f6', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 33, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Graph illustration\\n▶ Ridge regression more or less shrinks every dimension of the data by the\\nsame proportion.\\n▶ The Lasso more or less shrinks all coefficients towards zero by a similar\\namount and sufficiently small coefficients are shrunken all the way to zero.\\n(soft thresholding). Any hard thresholding? See the homework.\\nMilan Vojnović 34/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bfc3fb20-a9db-44f3-9a4b-a21c534b7426', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 34, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Using ℓq penalties\\n▶ Consider the penalised criterion\\nnX\\ni=1\\n\\x12\\nyi − β0 −\\npX\\nj=1\\nβjxij\\n\\x132\\n+ λ\\npX\\nj=1\\n|βj|q\\n▶ Contours of constant values of theP\\nj |βj|q for given values of q\\n▶ The case (q = 1) (Lasso) is the smallest q such that the constraint region is\\nconvex (non-convex constraint regions make the optimisation difficult!)\\nand the largest q for which the constraint region has ‘corners’ (these\\nnonsmoothness is what gives us sparse solutions!).\\nMilan Vojnović 35/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8a737681-c3df-4faa-82c7-f8e70b3f73b1', embedding=None, metadata={'file_name': 'ST443_Lecture_5.pdf', 'page_num': 35, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The penalised likelihood approach\\n▶ Rather than considering minimising penalised least squares for linear\\nmodel with Gaussian noise, we consider more general case.\\n▶ A general penalised likelihood approach considers minimising\\nNegative log likelihood + λ · ℓ1 norm,\\nwhere λ is a non-negative parameter to tune the sparsity.\\nMilan Vojnović 36/36', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48f99736-eb2d-4d58-a402-2f7e972a4349', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 4: Cross validation and bootstrapping\\n22 Oct 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fe358ab-dac6-483e-b5c1-d5b20fc73ce6', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ Using the validation set to estimate the prediction risk\\n▶ Leave-one-out cross validation\\n▶ K-fold cross validation\\n▶ Bootstrap for uncertainty quantification\\nMilan Vojnović 2/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a5fa01a2-b042-4f25-88ab-8ef35039cdf6', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What are resampling approaches?\\n▶ Tools that involve repeatedly drawing samples from a training set and\\nrefitting a model of interest on each sample in order to obtain more\\ninformation about the fitted model.\\n– Model assessment: estimate test error rates.\\n– Model selection: select the appropriate level of model flexibility.\\n– Model aggregation: reduce variance and improve accuracy.\\n▶ They are computationally expensive! But computers can help us!\\n▶ Two resampling approaches.\\n– Cross Validation.\\n– Bootstrap.\\nMilan Vojnović 3/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8fb11790-ca8b-445b-a8d9-46074a3c17c3', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What are resampling approaches?\\nMilan Vojnović 4/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66419c2a-5a0b-4ab1-a0fe-a90fc2589d75', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ways to estimate the test errors\\n▶ Best solution: a large test set. Not available!\\n▶ Mathematical adjustments to the training error rate to estimate the test\\nerror rate, e.g. Mallow’sCp statistics, AIC and BIC (to be discussed).\\n▶ Resampling approaches: estimate the test error by holding out a subset of\\nthe training observations and applying the statistical learning approaches\\nto those held-out observations.\\nMilan Vojnović 5/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ad386312-3e0a-4143-83e2-ab041288d9c0', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The validation set approach\\n▶ Suppose that we would like to find a set of variables that give the lowest\\ntest error rate.\\n▶ Give a large dataset, we can randomly split the data into training set and\\nvalidation set or hold-out set.\\n▶ The model is fit on the training set, and the fitted model is used to predict\\nthe response for the observations in the validation set. The resulting\\nvalidation set error rate, e.g. assessed using MSE, provides an estimate of\\nthe test error rate.\\nMilan Vojnović 6/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c67b995-534a-4d8f-9e5c-77a7fe7be410', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: Auto data\\nMilan Vojnović 7/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4581ba1-b442-41ef-a4db-95ee7589debc', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: Auto data\\n▶ Suppose that we want to predict mpg from horsepower.\\n▶ Two models:\\n1. mgp ∼ horsepower\\n2. mgp ∼ horsepower + horsepower2\\n▶ Which model gives a better fit?\\n1. Randomly split Auto data into training (196 obs.) and validation data\\n(196 obs.)\\n2. Fit both models using the training dataset.\\n3. Then evaluate both models using the validation dataset.\\n4. The model with the lowest validation (testing) MSE is the winner!\\nMilan Vojnović 8/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e19766fe-fb46-4da5-8a6b-a46eb407c09e', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Results: Auto data\\n▶ Left: validation error rate for a single split.\\n▶ Right: validation method repeated 10 times, each time the split is done\\nrandomly.\\n▶ There is large variability among the MSE’s.\\nMilan Vojnović 9/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0dadf5f3-4b25-415d-bb46-17122e015907', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The validation set approach\\n▶ Advantages:\\n– Simple.\\n– Easy to implement.\\n▶ Disadvantages:\\n– The validation MSE can be highly variable.\\n– Only a subset of observations are used to fit the model (training data).\\n(Statistical methods tend to perform worse on fewer observations,\\nwhich suggests the validation set error rate may tend tooverestimate\\nthe test error rate for the model fit on the entire dataset.)\\nMilan Vojnović 10/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2f3a70da-f6be-425f-9eb4-bb76daa5a06c', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Leave-one-out cross validation (LOOCV)\\n▶ For each suggested model, do the following:\\n1. Split the dataset of size n into: training dataset of size n − 1 and\\nvalidation dataset of size 1.\\n2. Fit the model using the training data.\\n3. Validate the model on the single validation data point.\\n4. Repeat the process n times.\\n5. The Model MSE:\\nCVn( ˆf) = 1\\nn\\nPn\\ni=1 MSEi = 1\\nn\\nPn\\ni=1(Yi − ˆf (−i)(Xi))2\\nMilan Vojnović 11/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='47805625-4bb2-4a2c-b4ff-0e0ef6efc4b6', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LOOCV vs the validation set approach\\n▶ LOOCV has less bias\\n– We repeatedly fit the statistical learning method using training data\\nthat contains n − 1 observations.\\n– LOOCV tends not to overestimate the test error rate.\\n▶ LOOCV produces a less variable MSE\\n– The validation approach produces different MSE when applied\\nrepeatedly due to randomness in the spliting process, while\\nperforming LOOCV multiple times will always yield the same results.\\n(Why?)\\n▶ LOOCV is computational intensive (disadvantage).\\n– We fit each model n times.\\nMilan Vojnović 12/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bbe43f29-5c85-48c0-b714-d1c09c573934', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Shortcut in linear-fitting models\\n▶ Let Y = (Y1, . . . , Yn)⊤ and ˆY = ( ˆY1, . . . , ˆYn)⊤ be observed and predicted\\nresponses. A linear-fitting method is one for which we can write\\nˆY = SY,\\nwhere S is a n × n matrix depending on Xi’s but not onYi’s.\\n▶ For linear-fitting methods, we have\\nCVn( ˆf) = 1\\nn\\nnX\\ni=1\\n(Yi − ˆf (−i)(Xi))2 = 1\\nn\\nnX\\ni=1\\n \\nYi − ˆf(Xi)\\n1 − Sii\\n!2\\n,\\nwhere Sii is the i-th diagonal element of S.\\n▶ The Generalized Cross Validation (GCV) is an approximation to LOOCV:\\nGCV( ˆf) = 1\\nn\\nnX\\ni=1\\n\\x12 Yi − ˆf(Xi)\\n1 − trace(S)/n\\n\\x132\\n.\\n▶ trace(S) is the sum of the diagonal elements of S that sometimes can be\\ncomputed much more easily than Sii’s.\\nMilan Vojnović 13/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='19837773-f86d-490c-8fc3-de74473b51b4', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='K-fold cross validation\\n▶ LOOCV is computational intensive, so we often run K-fold CV instead.\\n▶ With K-fold CV, we divide the dataset into K parts (e.g. K = 5 or 10, rule of\\nthumb), C1, . . . , CK , where Ck denotes the indices of the observations in part k.\\nThere are nk observations in part k.\\n▶ Compute\\nCVK =\\nKX\\nk=1\\nnk\\nn MSEk,\\nwhere MSEk = (1/nk)P\\ni∈Ck\\n(Yi − ˆYi)2 and ˆYi is the fit for observation i,\\nobtained from the data with part k removed.\\n▶ Mathematically, let κ : {1, . . . , n} → { 1, . . . , K} be the assignment into the K\\nfolds. Denote by ˆf (−k) the fitted function, computed with the kth part of the data\\nremoved. Then\\nCVK ( ˆf ) = 1\\nn\\nKX\\nk=1\\nX\\ni∈Ck\\nℓ\\n\\x00\\nyi, ˆf (−k)(Xi)\\n\\x01\\n= 1\\nn\\nnX\\ni=1\\nℓ\\n\\x00\\nyi, ˆf (−κ(i))(Xi)\\n\\x01\\n.\\nMilan Vojnović 14/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a232a26e-e56e-40f9-bc69-2857feaacc5c', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='K-fold CV\\nMilan Vojnović 15/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e9eff3bd-0a4d-4205-a40d-671d190fafbe', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Auto data: LOOCV vs K-fold CV\\n▶ LOOCV is a special case of K-fold CV, where K = n.\\n▶ They are both stable, but LOOCV is more computational intensive.\\nMilan Vojnović 16/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28596b4a-49ec-4168-ac4c-d0adbc5209b4', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Auto data: validation set vs K-fold CV\\n▶ K-fold CV is more stable.\\nvalidation set approach 10-fold CV\\nMilan Vojnović 17/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='babaa02a-eb41-48b0-8e6f-242ca07f504f', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='K-fold CV on three simulated datasets\\n— True test MSE\\n— LOOCV MSE.\\n— 10-fold CV\\nMilan Vojnović 18/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='03cc61ae-5bc8-4ea9-8a49-e8db6015a3bf', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bias-variance trade-off with CV approaches\\n▶ Putting aside that LOOCV is more computational intensive than K-fold\\nCV. Which one is better LOOCV orK-fold CV?\\n– Since each training is only (K − 1)/K as big as the original training\\ndata, the estimates of prediction error will typically be biased upwards.\\n– LOOCV has higher variance than K-fold CV.\\n– There is a trade-off between what to use.\\n▶ Conclusion:\\n– To compromise, we use K-fold CV with K = 5 or 10.\\n– It has been empirically shown that they yield test error rate estimates\\nthat suffer neither from excessively high bias, nor from very high\\nvariance.\\nMilan Vojnović 19/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3ed36b36-4b6c-4e81-9244-3c9469f0f783', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CV on classification problems\\nBayes error rate: 0.133.\\nMilan Vojnović 20/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bab5716f-688b-4a0b-91ad-263d2b7aeb19', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting logistic models\\nLogistic regression is used with polynomials of x1 and x2 as covariates.\\nMilan Vojnović 21/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91c1b6bb-1a99-4bbc-b424-66a4fffd2ecf', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting logistic models\\nLogistic regression is used with polynomials of x1 and x2 as covariates.\\nMilan Vojnović 22/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='999d219f-692d-454a-840d-75a1109767e2', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='CV to choose the model complexity\\nBrown: test error\\nBlue: training error\\nBlack: 10-fold CV error\\nLeft: Logistic regression using polynomial functions of the predictors.\\nRight: KNN classifier for different values of K.\\nMilan Vojnović 23/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='87a5bcdd-01c7-4af4-bc4b-160b90f89909', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross-validating more than one parameter\\n▶ Often, a statistical learning procedure has more than one tuning parameter.\\n▶ For example, in the logistic classifier example above, we need to choose:\\n– degree parameter d\\n– learning rate of the stochastic gradient descent α\\n– and possibly a regularising parameter λ (next lecture)\\n▶ A commonly used method to choose multiple parameters is grid-search.\\n1e−05 1e−03 1e−01 1e+01 1e+03 1e+05\\n0 5 10 15 20\\nalpha\\ndeg\\nMilan Vojnović 24/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8686d456-417a-4a5d-98d6-9bfcc9b8c1fb', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross-validating more than one parameter\\n1e−05 1e−03 1e−01 1e+01 1e+03 1e+05\\n0 5 10 15 20\\nalpha\\ndeg\\n1e−05 1e−03 1e−01 1e+01 1e+03 1e+05\\n0 5 10 15 20\\nalpha\\ndeg\\nsub-grid random grid points\\nMilan Vojnović 25/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2043c31d-9842-42bd-bf90-547606a76877', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross-validating more than one parameter\\n1e−05 1e−03 1e−01 1e+01 1e+03 1e+05\\n0 5 10 15 20\\nalpha\\ndeg\\n1e−05 1e−03 1e−01 1e+01 1e+03 1e+05\\n0 5 10 15 20\\nalpha\\ndeg\\nsub-grid randomly sampled parameter points\\nMilan Vojnović 26/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89374e31-5a6e-463c-9831-88826547e264', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Another resampling approach: Bootstrap\\n▶ Bootstrap is widely applicable and extremely powerful statistical tool to\\nquantify the uncertainty associated with the given estimator or statistical\\nlearning method.\\n▶ Linear model, genearlized linear model, standard errors can be computed\\n(mathematical representation or R output).\\n▶ Bootstrap can be easily applied to a wide range of approaches including for\\nwhich a measure of variability is difficult to calculate or be obtained from\\nsoftware.\\nMilan Vojnović 27/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='344d10e0-01ac-47f2-a29d-28ea5b5d692c', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A toy example: best investment allocation\\n▶ Two financial assets that yields returns X and Y .\\n▶ We invest a fraction α and 1 − α in X and Y .\\n▶ Problem: choose α to minimize the risk.\\nvar[αX + (1 − α)Y ].\\n▶ The value of α that minimize the risk is given by\\nα = σ2\\nY − σXY\\nσ2\\nX + σ2\\nY − 2σXY\\n,\\nwhere σ2\\nX = var[X], σ2\\nY = var[Y ] and σXY = cov[X, Y ].\\n▶ We estimate α using\\nˆα = ˆσ2\\nY − ˆσXY\\nˆσ2\\nX + ˆσ2\\nY − 2ˆσXY\\n.\\nMilan Vojnović 28/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea932908-0ef0-4a02-ba73-065aa7239e0e', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='If we know the population distribution\\n▶ How to quantify the estimate for α?\\n▶ We repeat the process of simulating 100 paired observations X and Y\\n(n = 100) and estimate for 1000 times, ˆα1, ˆα2, . . . ,ˆα1000.\\n▶ Set σ2\\nX = 1, σ2\\nY = 1.25 and σXY = 0.5, so α = 0.6.\\n▶ The mean ¯α = 1\\n1000\\nP1000\\nb=1 ˆαb = 0.5996.\\n▶ The standard error\\nq\\n1\\n1000−1\\nP1000\\nb=1 (ˆαb − ¯α)2 = 0.083\\nMilan Vojnović 29/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b9535bf8-084f-4147-926d-07b656c43b1a', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A bootstrap approach\\n▶ Rather than repeatedly obtaining independent datasets from the\\npopulation (unknown), we obtain distinct datasets by repeatedly sampling\\nobservations from the original dataset.\\n▶ One simple example with 3 observations and sampling is performed with\\nreplacement:\\nMilan Vojnović 30/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d2290393-b2af-47ab-b73a-c8f5eff660da', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 30, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bootstrap procedure\\n▶ The sampling (with replacement) procedure is repeated B times to produce\\nB different bootstrap datasets Z ∗1, . . . , Z∗B, and B corresponding α\\nestimates ˆα∗1, ˆα∗2, . . . ,ˆα∗B.\\n▶ We can compute the standard error of the bootstrap estimates by the\\nsample standard deviation of the bootstrap estimates:\\nseB(ˆα) =\\nvuut 1\\nB − 1\\nBX\\nb=1\\n\\x12\\nˆα∗b − 1\\nB\\nBX\\nb′=1\\nˆα∗b′\\n\\x132\\nMilan Vojnović 31/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e6db42eb-a03d-45f2-bf58-7dd8cf740c88', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 31, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A general picture for the bootstrap\\nMilan Vojnović 32/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c41c71e-6750-4134-ae09-b00473362036', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 32, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Other uses of the bootstrap\\n▶ Mainly to obtain the standard errors of an estimate.\\n▶ Provides approximate confidence intervals for a population parameter.\\n– E.g. revisiting the histogram, the 5% and 95% quantiles of 1000 values\\nare (0.43, 0.72).\\n– This provides an approximate 90% confidence interval for α, bootstrap\\npercentage confidence interval.\\n▶ Can also be used to improve the estimate by aggregating over multiple\\nbootstrap estimates. This is called ‘bootstrap aggregation’ or bagging.\\nMore on this in Lecture 7.\\nMilan Vojnović 33/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cfd815b7-60b3-460b-89de-b1d34aa0e222', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 33, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Can the bootstrap estimate prediction error?\\n▶ One approach is to fit the model in question on a set of bootstrap samples\\nand then keep track of how well it predicts the original training set. Let\\nˆf ∗b(xi) be the predicted value at xi from the model fitted to the bth\\nbootstrap sample, our estimate is\\ncErrboot = 1\\nBn\\nBX\\nb=1\\nnX\\ni=1\\nℓ\\n\\x10\\nyi, ˆf ∗b(xi)\\n\\x11\\n▶ Each bootstrap has significant overlap with the original data. About 2/3 of\\nthe original data points in each bootstrap sample. Why?\\nP[{obs i ∈ bootstrap sample b}] = 1 −\\n\\x12\\n1 − 1\\nn\\n\\x13n\\n≈ 1 − e−1 ≈ 0.632.\\n▶ This will cause the bootstrap to underestimate the true prediction error.\\nMilan Vojnović 34/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8619c28f-4800-457e-8e00-dd0cdea6abab', embedding=None, metadata={'file_name': 'ST443_Lecture_4.pdf', 'page_num': 34, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross-validation vs bootstrapping\\n▶ Both are resampling methods.\\n– Cross-validation subsamples without replacement.\\n– Bootstrapping samples n points with replacement.\\n▶ Both quantify the error/uncertainty of the procedure.\\n– Cross-validation uses data not in the subsample to estimate the\\ntest error. It cannot be used to estimate the error of a parameter\\nestimate.\\n– Bootstrapping uses the same data to build the model and estimate\\nthe parameter. It measures the variance of the estimator and does not\\ncapture its bias.\\nMilan Vojnović 35/35', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2817fd3e-80c5-4e2e-9455-58b5e085f092', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 7: Trees and forests\\n12 Nov 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4901c846-3179-47d1-af47-a03091d35d86', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ Decision trees\\n▶ How to prune a tree\\n▶ Gradient boosting\\n▶ Bootstrap aggregation\\n▶ Random forest\\nMilan Vojnović 2/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='13911c64-b7b1-4c02-a45b-19b77b011bc9', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Tree-based methods\\n▶ We focus on the tree-based methods for regression and classification.\\n▶ These involve stratifying or segmenting the predictor space into a number\\nof simple regions.\\n▶ Since the set of splitting rules used to segment the predictor space can be\\nsummarized in a tree, these types of approaches are known as decision tree\\nmethods.\\nMilan Vojnović 3/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea017f20-5082-42a2-8d7f-744f3ba2107b', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Baseball player salary data\\n▶ Salary is colored from low (blue, green) to high (yellow, red).\\n▶ How would you stratify it?\\nMilan Vojnović 4/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99e5da2c-ab47-44f1-af8a-02ec74b76869', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Decision tree for the data\\nR1 = {Years < 4.5}, R2 = {Years ≥ 4.5 and Hits < 117.5} and\\nR3 = {Years ≥ 4.5 and Hits ≥ 117.5}.\\nMilan Vojnović 5/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f3c4163-739f-4ffc-9102-9bd4188fd219', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Details for the decision tree\\n▶ In keeping with the tree analogy, the regions R1, R2 and R3 are known as\\nterminal nodes or leaves of the tree.\\n▶ Decision tree are typically drawn upside down, in the sense the leaves are\\nat the bottom of the tree.\\n▶ The points along the tree where predictor space is split are referred to as\\ninternal nodes.\\n▶ In the hitters tree, the two internal nodes are indicated by the text\\nYears < 4.5 and Hit < 117.5.\\n▶ We refer to the segments of the trees that connect the nodes as branches.\\nMilan Vojnović 6/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='485eac66-9025-42ac-8b19-f7d08e88f31e', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Interpretation of the decision tree results\\n▶ Years is the most important factor in determining Salary and players with\\nless experience earn lower salaries than more experienced players.\\n▶ Given that a player is less experienced, the number of Hits that he made in\\nthe previous year seems to play little role in his Salary.\\n▶ But among players who have been in the major leagues for five or more\\nyears, the number of Hits made in the previous year does affect Salary,\\nand players who made more Hits last year tend to have higher salaries.\\n▶ The figure is likely an over-simplification, but compared to a regression\\nmodel, is advantageous in displaying and interpreting.\\nMilan Vojnović 7/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='824fdfcf-3f67-4d51-aafe-eddfc47c340b', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Details in tree building process\\n▶ We divide the predictor/covariate space, i.e. the set of possible values for\\nX1, . . . , Xp into J distinct and non-overlapping regions R1, R2, . . . , RJ .\\n▶ For every observation that falls into the region Rj, we make the same\\nprediction, which is simply the mean of the response values for the\\ntraning observation in Rj.\\n▶ In theory, the regions could have any shape. Decision trees divide the\\npredictor space into axis-aligned high-dimensional rectangles for simplicity\\nand interpretability.\\n▶ Our target is to find rectangles, R1, . . . , RJ that minimise the RSS\\nJX\\nj=1\\nX\\ni∈Rj\\n(yi − ˆyRj)2,\\nwhere ˆyRj = N −1\\nj\\nP\\ni:xi∈Rj yi and Nj = #{i : xi ∈ Rj}.\\nMilan Vojnović 8/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92320cb9-ac3a-4e3e-a889-bcb74385ae3a', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='More details in tree building process\\n▶ It is computationally infeasible to consider every possible partition of the\\nfeature space into J rectangles.\\n▶ We take a top-down, greedy approach that is known as recursive\\nbinary splitting.\\n▶ Top-down: it begins at the top of tree and then successively splits the\\npredictor space, each split is indicated via two new branches further down\\non the tree.\\n▶ Greedy: at each step of the tree-building process, the best split is made at\\nthe particular step, rather than looking ahead and picking a split that will\\nlead to a better tree in some future step.\\nMilan Vojnović 9/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01730fec-7f15-469f-805f-50d4b04fcb0f', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='More details in tree building process\\n▶ We first select the predictor Xj and cutpoint s such that splitting the\\npredictor space into regions {X : Xj < s} and {X : Xj ≥ s} leads the the\\ngreatest possible reduction in RSS.\\n▶ Then, we repeat the process, searching for the best predictor and cutpoint\\nto further split the data so as to minimise the RSS within each of the\\nresulting regions. Note rather than splitting the entire space, we split one\\nof the two previously identified regions. Now we have three regions.\\n▶ Again, we look to split one of these three regions further, so as to minimise\\nthe RSS. The process continues until a stopping criterion is reached, e.g. we\\nmay continue until no region contains no more than five observations.\\nMilan Vojnović 10/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='950a61e1-30da-4c98-a0d5-cfe254aea32a', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Splitting the predictor space\\nGenerally we create the partitions by iteratively splitting one of the X variables\\ninto two regions. For instance:\\n1. First split on X1 = t1.\\n2. If X1 < t1, split on X2 = t2.\\n3. If X1 > t1 split on X1 = t3.\\n4. If X1 > t3, split on X2 = t4.\\nMilan Vojnović 11/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='93bf2cc9-d055-495b-a14a-04cf168e77db', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Splitting the predictor space\\nGenerally we create the partitions by iteratively splitting one of the X variables\\ninto two regions. For instance:\\n1. First split on X1 = t1.\\n2. If X1 < t1, split on X2 = t2.\\n3. If X1 > t1 split on X1 = t3.\\n4. If X1 > t3, split on X2 = t4.\\nMilan Vojnović 11/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='51dd1db9-a7eb-4360-a286-5f6e3a886d6a', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Splitting the predictor space\\nGenerally we create the partitions by iteratively splitting one of the X variables\\ninto two regions. For instance:\\n1. First split on X1 = t1.\\n2. If X1 < t1, split on X2 = t2.\\n3. If X1 > t1 split on X1 = t3.\\n4. If X1 > t3, split on X2 = t4.\\nMilan Vojnović 11/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7fd40b4b-d9a8-4fc1-b619-73ea27661477', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Splitting the predictor space\\nGenerally we create the partitions by iteratively splitting one of the X variables\\ninto two regions. For instance:\\n1. First split on X1 = t1.\\n2. If X1 < t1, split on X2 = t2.\\n3. If X1 > t1 split on X1 = t3.\\n4. If X1 > t3, split on X2 = t4.\\nMilan Vojnović 11/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d90613ca-dbc0-4a6c-bfed-3d8eb6bbeff6', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Splitting the predictor space\\nGenerally we create the partitions by iteratively splitting one of the X variables\\ninto two regions. For instance:\\n1. First split on X1 = t1.\\n2. If X1 < t1, split on X2 = t2.\\n3. If X1 > t1 split on X1 = t3.\\n4. If X1 > t3, split on X2 = t4.\\nMilan Vojnović 11/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='41678429-ebbc-47ac-ba03-842c6687fc25', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Estimator for regression trees\\n▶ Consider the statistical learning model:\\nYi = f(Xi) + εi, i = 1, . . . , n,\\nwhere Xi = (xi1, . . . , xip)⊤.\\n▶ A regression tree T with leaf regions R1, . . . , RJ corresponds to the\\nestimator ˆf such that for x ∈ Rp,\\nˆf(x) :=\\nJX\\nj=1\\n¯YRj 1 {x ∈ Rj},\\nwhere ¯YRj := N −1\\nj\\nP\\ni:Xi∈Rj Yi and Nj = Pn\\ni=1 1 {Xi ∈ Rj}.\\n▶ The training RSS of the regression tree T is\\nRSS(T ) :=\\nJX\\nj=1\\nX\\ni:Xi∈Rj\\n(Yi − ¯YRj)2.\\nMilan Vojnović 12/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83bdd5ef-ea6b-42ea-bb7a-48958febd8fd', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pruning a tree\\n▶ The tree building process may produce good predictions on the training set,\\nbut is likely to overfit the data, leading to poor test set performance.\\n▶ We can first grow a very big tree T0 and then prune it back to obtain a\\nsubtree.\\n▶ Cost complexity pruning: we consider a sequence of trees indexed by a\\ntuning parameter α > 0. For each α, we seek a subtree Tα ⊂ T0 such that\\nTα = argmin\\nT ⊂T0\\n{RSS(T ) + α|T |},\\nwhere |T | indicates the number of leaves of T .\\nMilan Vojnović 13/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='27f8d418-af86-493b-a933-bfc6519ab18f', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choose the optimal subtree\\n▶ α is the tuning parameter that controls a trade-off between the subtree’s\\ncomplexity (variance) and its fit to the training data (bias).\\n▶ Optimal α can be obtained using cross-validation.\\n▶ We then return to the full dataset and obtain the subtree corresponding to\\nthe optimal α.\\nMilan Vojnović 14/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b3ab2ba-9f14-43cf-ae12-a175701aa715', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Algorithm for building a regression tree\\n1. Use recursive binary splitting to grow a large tree on the training data,\\nstopping only when each terminal node has fewer than some minimum\\nnumber of observations.\\n2. Apply cost complexity pruning to the large tree in order to obtain a\\nsequence of best subtrees, as a function of α.\\n3. Use K-fold cross validation to choose α. That is, divide the training\\nobservations into K folds. For each k = 1, . . . , K :\\na Repeat Steps 1 and 2 on all but the k-th fold of the training data.\\nb Evaluate the mean squared prediction error on the data in the left-out\\nk-th fold, as a function of α.\\nAverage the results for each value of α and pick α to minimise the average\\nerror.\\n4. Return the subtree from Step 2 that corresponds to the chosen value of α.\\nMilan Vojnović 15/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9fad6838-a0e6-4999-8229-1fcfb2b7c6ff', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Baseball data continued\\n▶ Randomly split the data: 132 training observations and 131 observations in\\nthe test dataset.\\n▶ We then build a large regression tree on the training data and varied α to\\ncreate subtrees with different number of terminal nodes.\\n▶ Finally, a six-fold cross validation is implemented to estimate the\\ncross-validated MSE of the trees as a function of α.\\nMilan Vojnović 16/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ad800939-6dc3-4e18-b64b-a31430297c01', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Baseball data regression tree\\nMilan Vojnović 17/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cbb2b817-1803-442b-980a-6c3cd2d10013', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Cross validation\\nMilan Vojnović 18/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a0867f72-8a47-44c2-a6b4-c162e2a7f7fe', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Classification trees\\n▶ For a classification tree, we predict that each observation belongs to the\\nmost common occurring class of training observations in the region to\\nwhich it belongs.\\n▶ We use recursive binary splitting to grow a classification tree.\\n▶ Recall that in a regression tree, we define the cost complexity criterion:\\nCα(T ) = RSS(T ) + α|T | =\\n|T |X\\nj=1\\nNjQMSE\\nj (T ) + α|T |,\\nwhere QMSE\\nj (T ) = 1\\nNj\\nP\\ni:Xi∈Rj(Yi − ¯YRj)2 is the MSE of the regression\\nin the m-th terminal node (region).\\nMilan Vojnović 19/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c025cb1c-4e22-4982-a673-ea5e40b993dc', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Classification trees\\n▶ Under classification setups, we need to substitute the loss QMSE\\nj by some\\nalternative measures.\\n▶ Misclassification Error Rate (MER): the fraction of training observations\\nin the region that do not belong to the most common class:\\nQMER\\nj (T ) = 1\\nNj\\nX\\ni:Xi∈Rj\\n1 {Yi ̸= ˆYRj } = 1 − ˆpj, ˆYRj\\n,\\nwhere ˆYRj = argmaxk ˆpj,k and ˆpj,k = 1\\nNj\\nP\\ni:Xi∈Rj 1 {Yi = k} represents\\nthe proportion of training observations in the j-th leaf region that are from\\nthe k-th class.\\nMilan Vojnović 20/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa247267-3fd2-48b8-ba23-42322d5fe337', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Two other measures of loss\\n▶ The misclassification error rate is not sufficiently sensitive for tree growing\\nand in practice two other measures are preferable.\\n▶ Gini index:\\nQGini\\nj (T ) =\\nKX\\nk=1\\nˆpj,k(1 − ˆpj,k),\\na measure of total variance across the K classes.\\n▶ Gini index is referred to as a measure of node purity — a small value\\nindicates a node contains predominately observations from a single class.\\n▶ Cross-entropy (deviance):\\nQxent\\nj (T ) = −\\nKX\\nk=1\\nˆpj,k log ˆpj,k.\\n▶ These two measures are used to evaluate the quality of a particular split,\\nsince they are more sensitive to node purity than the misclassification error\\nrate.\\nMilan Vojnović 21/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='68d9cafd-6874-4b1e-8c91-b318a83cee79', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Comparison among the three measures\\n▶ For K = 2, the three measures are min{p, 1 − p}, 2p(1 − p) and\\n−p log(p) − (1 − p) log(1 − p), respectively.\\n▶ The cross-entropy and the Gini index are differentiable and hence also\\nmore amendable to numerical optimisation.\\nMilan Vojnović 22/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='570cfdae-52e7-4045-bbbc-80c5268e4709', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: Heart data\\n▶ Binary outcome HD for 303 patients who presented with chest pain.\\n▶ An outcome value of Yes indicates the presences of heart disease based on\\nan angiographic test, while No means no heart disease.\\n▶ There are 13 predictors including Age, Sex, Chol (a cholesterol\\nmeasurement) and other heart and lung function measurements.\\n▶ Cross-entropy-based cross validation yields a pruned tree with six terminal\\nnodes, see next page.\\nMilan Vojnović 23/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='20f80127-71dd-4d60-a842-5ebadac7d286', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Milan Vojnović 24/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca3d0cff-d4d7-4c81-a702-505d4a7550e7', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Tree vs linear models\\nTop row: true linear boundary; Bottom row: true nonlinear boundary.\\nMilan Vojnović 25/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b77ba8c0-ada9-497b-a41c-b0c79067f992', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Advantages and disadvantages of trees\\n▶ Trees are easy to explain to people. They are even easier to explain than\\nlinear regression.\\n▶ Some people believe that decision trees more closely mirror human decision\\nmaking than do the regression and classification seen in previous lectures.\\n▶ Trees can be displayed graphically and easily interpreted even by a\\nnon-expert.\\n▶ Trees can easily handle qualitative predictors without the need to create\\ndummy variables.\\n▶ Trees do not have the same level of predictive accuracy as some of the\\nother regression and classification approaches seen in previous lectures.\\nMilan Vojnović 26/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c977c7b-3ac0-48eb-86ba-8e3f416349aa', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 30, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bootstrap aggregation\\n▶ The bootstrap is an extremely powerful idea but what does it have to do\\nwith statistical learning?\\n▶ Suppose that we have a procedure (such as trees, neural nets and etc.) that\\nhas high variance.\\n▶ An ideal way to reduce variance would be to take many samples from the\\npopulation, build a separate prediction model using each sample and\\naverage the resulting predictions i.e.\\nˆfavg(x) = 1\\nB\\nBX\\nb=1\\nˆf b(x).\\n▶ Of course, as discussed previously, we cannot take multiple different\\nsamples from the population.\\nMilan Vojnović 27/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a11afd2a-0474-4cf8-9b3f-dbee57d1f3c1', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 31, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bootstrap aggregation\\n▶ However, we can use the bootstrap approach which does the next best\\nthing by taking repeated samples from the training data.\\n▶ We therefore end up with B different training data sets.\\n▶ We can train our method on each data set and then average all the\\npredictions i.e.\\nˆfbag(x) = 1\\nB\\nBX\\nb=1\\nˆf ∗b(x).\\n▶ This approach is called Bagging or Bootstrap Aggregation.\\nMilan Vojnović 28/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b9fdf90e-cbaf-4ea4-8ada-e53a7a125f04', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 32, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bagging regression and classification trees\\n▶ Regression trees: We use average prediction to aggregate bootstrapped\\ntrees.\\n▶ Classification trees: for each test observation, we record the class\\npredicted by each of B trees and take a majority vote: the overall\\nprediction is the most commonly occuring class.\\n▶ Alternatively, we can take the average of B probabilities and assign to\\nthe class with the highest averaged probability.\\nMilan Vojnović 29/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ed21d5c-b880-449d-baf3-8f90c0dafeba', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 33, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Simulation to compare error rates\\n▶ Here the green line represents a simple majority vote approach.\\n▶ The purple line corresponds to averaging the probabilities.\\n▶ Both methods do better than a single tree and get close to the Bayes risk.\\nMilan Vojnović 30/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2fc5c01b-4b51-4c49-9993-d79562d553c0', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 34, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Random forest\\n▶ Random forest provides an improvement over bagged trees by way of a\\nsmall tweak that decorrelates the trees. This reduces the variance when we\\naverage the tree.\\n▶ As in bagging, we build a number of decision trees on bootstrapped\\ntraining samples.\\n▶ But in random forest, we also randomly sample predictors: each time a\\nsplit in a tree is considered, a random selection of m predictors is chosen as\\nsplit candidates from the full set of p predictors. The split is allowed to use\\nonly one of those m predictors.\\n▶ A typical choice is m = √p for classification trees and m = p/3 for\\nregression trees.\\n▶ Bagging is a special case of random forest when m = p.\\nMilan Vojnović 31/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2df6004-c813-4cce-87a2-5790af6c5b42', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 35, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Variable importance measure\\n▶ Bagged trees and random forests typically results in improved accuracy over\\nprediction using a single tree. However, it can be difficult to interpret the model.\\n▶ We record the total amount that the RSS is decreased due to splits over a given\\npredictor, averaged over all B trees. A large value indicates an important predictor.\\n▶ Variable importance plot:\\nMilan Vojnović 32/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d753a53b-49fc-4cf6-bd9f-bcc307069296', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 36, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting\\n▶ Like bagging, boosting can be applied to many statistical learning\\napproaches. We concentrate on decision trees.\\n▶ Recall that bagging involves creating copies of the original training dataset\\nusing the bootstrap, and then combining all of the trees to create a single\\npredictive model.\\n▶ Each tree is independent of trees based on a bootstrap dataset.\\n▶ Boosting works in a similar way, but the trees are grown sequentially,\\neach tree is grown using information from previously grown trees.\\nMilan Vojnović 33/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9d3b8d3-3a90-41b7-968c-e70d0d5689a6', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 37, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting Algorithm for Regression Trees\\n1. Set ˆf(x) = 0 and ri = yi for all i in the training set.\\n2. For b = 1, . . . , B, repeat:\\na Fit a tree ˆf b with d splits (d + 1 terminal nodes) to the training data\\n(X, r).\\nb Update the residuals,\\nri ← ri − λ ˆf b(xi).\\n3. Output the boosted model,\\nˆf(x) =\\nBX\\nb=1\\nλ ˆf b(x).\\nMilan Vojnović 34/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='93044eb4-2c98-4a45-9ba4-9b76518ccfa8', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 38, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Ideas behind\\n▶ Unlike fitting a single large decision tree to the data, which amounts to\\nfitting the data hard, the boosting approach instead learns slowly.\\n▶ Given the current model, we fit a decision tree to the residuals from the\\nmodel. We then add this new decision tree into then fitted function in order\\nto update the residuals.\\n▶ Each of these trees can be rather small, with just a few terminal nodes,\\ndetermined by the parameter d in the algorithm.\\n▶ By fitting small trees to the residuals, we slowly improve ˆf in areas where it\\ndoes not perform well. This shrinkage parameter λ slows the process down\\neven further, allowing more and different shaped trees to predict the\\nresiduals.\\nMilan Vojnović 35/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa8cefdf-10e3-4340-abea-59a266a0ad4f', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 39, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Regression Example\\n▶ Number of trees B (CV for selecting B); shrinkage parameter λ (typical\\nvlaues are 0.01 or 0.001) and number of splits d (interaction depth) are\\ntuning parameters for boosting.\\n▶ See further details in Chapter 10 of ESL.\\n▶ R package: gbm.\\n▶ Gradient boosted models.\\nMilan Vojnović 36/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='729ee343-6da3-4ac6-ae6e-79a7070f0a0f', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 40, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Summary\\n▶ Decision trees are simple and interpretable. But they can not lead to high\\nprediction accuracy.\\n▶ Bagging, random forests and boosting are developed to improve the\\nprediction accuracy of trees.\\n▶ Random forests and boosting are among the state-of-art methods for\\nregression or classification (supervised learning). However, their results can\\nbe difficult to interpret.\\n▶ Further details of random forests and boosting can be learnt from ESL.\\n▶ We will cover boosting in more depth in the next lecture.\\nMilan Vojnović 37/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b6b73b7-74c0-40de-ac3b-12784c59647c', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 41, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='More on cost complexity pruning\\n▶ Here we describe a commonly used cost complexity pruning algorithm,\\nknown as the weakest link pruning, which is implemented in popular\\nsoftware libraries such as scikit-learn.\\n▶ For a tree T and α ≥ 0, the cost is defined as\\nCα(T ) = Q(T ) + α|T |\\nwhere Q(T ) = P\\nt∈N(T) Q(t), N(T ) is the set of leaf nodes of T and Q(t)\\nis the cost of the region corresponding to the leaf node t (e.g. RSS for\\nregression, and misclassification error, Gini or cross-entropy for\\nclassification) and |T | is the number of leaf nodes of T .\\n▶ Let T (a) denote the subtree of T that has node a as the root.\\n▶ For any internal node a of T , let T \\\\ T (a) be the tree obtained from T by\\npruning the subtree T (a).\\nMilan Vojnović 38/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='65608916-af40-4627-898e-03c2bf0e6842', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 42, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pruning a subtree\\nMilan Vojnović 39/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5b090aea-f603-469c-ba3d-c00907d5a8d4', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 43, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The cost change by pruning a subtree\\n▶ It can be readily checked that for any internal node a of T ,\\nCα(T \\\\ T (a)) = Q(T ) − Q(T (a)) + Q(a) + α(|T | − |T (a)| + 1).\\n▶ The cost change is:\\nCα(T \\\\ T (a)) − Cα(T ) = Q(a) − Q(T (a)) + α(1 − |T (a)|).\\n▶ The pruning does not increase the cost if and only if\\nCα(T \\\\ T (a)) − Cα(T ) ≤ 0,\\nwhich is equivalent to:\\nQ(a) − Q(T (a))\\n|T (a)| − 1 ≤ α.\\nMilan Vojnović 40/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d62ff2fd-ce39-4728-8f22-cf8441f4635b', embedding=None, metadata={'file_name': 'ST443_Lecture_6.pdf', 'page_num': 44, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The weakest link pruning\\n▶ The effective alpha value of an internal node a of a tree T is defined as\\nαeff(a, T) := Q(a) − Q(T (a))\\n|T (a)| − 1 .\\n▶ The weakest link pruning algorithm sequentially prunes subtrees rooted at\\nthe internal nodes that have the lowest effective alpha value.\\n▶ A sketch of the weakest link pruning algorithm:\\n– Initialise T ← T0 where T0 is a tree that miminises Q(T )\\n– Step 1: A∗ = arg mina\\nQ(a)−Q(T(a))\\n|T(a)|−1 // find the set of weakest links\\n– Let a∗ be an arbitrary node in A∗\\n– If Q(a∗) − Q(T (a∗))/(|T (a∗)| − 1) > α: return T\\n– For each a ∈ A∗: T ← T \\\\ T (a) // prune the subtrees of weakest links\\n– Go to Step 1\\n▶ Remark: some implementations may prune one subtree in each round\\nusing some tie breaking rule.\\nMilan Vojnović 41/41', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eefbc7ed-0fef-41c7-a45c-b741ee448412', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 7: Boosting algorithms\\n19 Nov 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='974edec5-9097-4541-95f6-682b0c5c3650', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ Discrete AdaBoost\\n▶ Boosting fits an additive model\\n▶ Forward stagewise additive fitting\\n▶ Gradient boosted trees\\nMilan Vojnović 2/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b15e482-b94c-454d-ba6e-7f1f98803917', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting\\n▶ Boosting is a procedure that combines the outputs of weak classifiers to\\nproduce a powerful committee\\n▶ Applicable also to regression.\\n▶ A weak classifier is a classifier whose error rate is only slightly better\\nthan random guessing.\\n▶ The purpose of boosting is to sequentially apply a weak classification\\nalgorithm to repeatedly modified versions of the data, thereby producing a\\nsequence of weak classifiers.\\nMilan Vojnović 3/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='00229da3-c674-41bf-bcfe-900bc01f02c7', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Binary classification\\n▶ Consider binary classification with the output variable Y ∈ {−1, 1}.\\n▶ Given a vector of predictor variables x, a classifier b produces a prediction\\nthat takes one of the two values, −1 or 1.\\n▶ Given a training dataset (x1, y1), . . . ,(xn, yn), consider the empirical risk\\ndefined as the missclassification error rate:\\nε = 1\\nn\\nnX\\ni=1\\n1 (b(xi) ̸= yi).\\n▶ The risk is:\\nEX,Y [1 (b(X) ̸= Y )].\\n▶ The predictions of the weak classifiers b1, . . . , bM are combined by using\\nthe weighted majority vote:\\nb(x) = sign\\n MX\\nm=1\\nαmbm(x)\\n!\\nwhere α1, . . . , αM are weights computed by the boosting algorithm.\\nMilan Vojnović 4/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44ad6259-e08b-41d0-ac27-06efd168b953', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discrete AdaBoost algorithm\\n1. Initialise the observation weights wi = 1/n for i = 1, . . . , n\\n2. For m = 1, . . . , M:\\n(a) Fit a classifier bm to the training data using weights\\nw1, . . . , wn\\n(b) Compute the weighted error rate:\\nεm =\\nnX\\ni=1\\nwi1 (bm(xi) ̸= yi)\\n(c) Compute\\nαm = log\\n\\x121 − εm\\nεm\\n\\x13\\n(d) Set wi ← wi exp(αm1 (bm(xi) ̸= yi)) for i = 1, . . . , n\\n(e) Let Zm ←Pn\\ni=1 wi, and wi ← wi/Zm for i = 1, . . . , n\\n3. Output:\\nb(x) = sign(f(x)) where f(x) =\\nMX\\nm=1\\nαmbm(x).\\nMilan Vojnović 5/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='11fd765b-5c1c-462c-9363-a7556610c0a1', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discrete AdaBoost: comments\\n▶ The training data observations misclassified by bm have their weights\\nscaled by a factor of eαm thus increasing their relative influence for\\ninducing the next classifier bm+1.\\n▶ AdaBoost weights:\\nMilan Vojnović 6/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f99c5979-8e25-48e4-a890-3671c1a5526b', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discrete AdaBoost training error\\n▶ The classifier b of AdaBoost algorithm has the error rate on the training\\ndataset bounded as follows:\\n1\\nn\\nnX\\ni=1\\n1 (b(xi) ̸= yi) ≤\\nMY\\nm=1\\np\\n4εm(1 − εm)\\n≤ e−2 PM\\nm=1( 1\\n2 −εm)\\n2\\n.\\nProof: Homework\\n▶ Hence, if for each classifier bm, the error rate εm is such that\\nεm ≤ 1/2 − γ, for some constant γ ∈ (0, 1/2), then the error rate of b is\\nbounded by a function exponentially decreasing with M as e−2γ2M .\\nMilan Vojnović 7/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ef02b73-d9a1-4243-aa3c-8e332d902469', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discrete AdaBoost generalisation error\\n▶ For AdaBoost algorithm with M ≤ n and base classifiers from a finite\\nspace H, with probability 1 − δ, the output classifier satisfies\\nEX,Y [1 (b(X) ̸= Y )]\\n≤ 1\\nn\\nnX\\ni=1\\n1 (b(xi) ̸= yi) +\\nr\\n32 M log(en|H|/M) + log(8/δ)\\nn .\\n▶ Hence, if n is sufficiently large, the empirical risk is a good approximation\\nof the risk.\\n▶ For base classifiers from an infinite space H, a similar bound can be\\nobtained provided that H satisfies certain property (bounded\\nVC-dimension),\\n▶ The generalisation error results are established using advanced concepts\\nand results from statistical learning theory out of the scope of this course.\\nMilan Vojnović 8/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a37c9dc-1bb9-464d-b26c-75cde264246c', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Empirical evaluation\\n▶ Assume that X ∈ Rp with p = 10 where features X1, . . . , X10 are\\nindependent standard Gaussian random variables.\\n▶ The response Y is defined as:\\nY =\\n\\x1a\\n1, if P10\\ni=1 Xi > χ 2\\n10(0.5)\\n−1, otherwise\\nwhere χ2\\n10 = 0.94 is the median of a chi-squared random variable with 10\\ndegrees of freedom (sum of squares of 10 standard Gaussian random\\nvariables).\\n▶ There are n = 2,000 training data points, about 1,000 for each class, and\\n10,000 test observations.\\n▶ For the weak classifier we consider a stump (a two terminal-node\\nclassification tree).\\nMilan Vojnović 9/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='35c246ff-7651-4f39-8b67-60039621d0c7', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Empirical evaluation (cont’d)\\n▶ A single stump is a weak classifier whose performance is significantly\\nworse than that of a large tree (with 244 nodes).\\n▶ Boosted stumps significantly outperform the large tree model for\\nsufficienly large number of boosting iterations.\\nMilan Vojnović 10/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c4975015-855a-4f31-aecc-9a8e3e37b19b', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting fits an additive model\\n▶ Consider a function f of the following additive form:\\nf(x) =\\nMX\\nm=1\\nαmb(x; βm)\\nwhere α1, . . . , αM are real-valued coefficients and b(x; β) is a (basis)\\nfunction with parameter β.\\n▶ Given a training dataset (x1, y1), . . . ,(xn, yn) and a loss function L,\\nfunction f may be fit by finding parameters that solve the following loss\\nfunction minimisation problem:\\nminimise Pn\\ni=1 L\\n\\x10\\nyi,PM\\nm=1 αmb(xi; βm)\\n\\x11\\nover (αm, βm), m = 1, . . . , M.\\n▶ Solving this optimisation problem can be computationally hard. An\\nalternative is forward stagewise additive fitting, which we describe next.\\nMilan Vojnović 11/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f1d5c2cd-0c22-436e-b3dc-80dae8a4c53c', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Forward stagewise additive fitting\\n1. Initialize f0(x) = 0\\n2. For m = 1, . . . , M:\\n(a) Compute\\n(αm, βm) = arg min\\nα,β\\nnX\\ni=1\\nL (yi, fm−1(xi) + αb(xi; β))\\n(b) Set\\nfm(x) = fm−1(x) + αmb(x; βm)\\n▶ Note: new basis functions are sequentially added to the function expansion\\nwithout adjusting the parameters and coefficients of those that have\\nalready been added.\\nMilan Vojnović 12/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2265bf86-42fa-4359-aa4e-b77d7e563cd6', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: squared error loss\\n▶ Consider the squared error loss function:\\nL(y, f(x)) = (y − f(x))2.\\n▶ Then, we have\\nL(yi, fm−1(xi) + αb(xi; β)) = ( yi − fm−1(xi) − αb(xi; β))2\\n= ( rm,i − αb(xi; β))2\\nwhere rm,i is the residual rm,i = yi − fm−1(xi).\\nMilan Vojnović 13/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a7607b28-a0af-4f7d-9d6e-5dac682a0f84', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example: exponential loss\\n▶ For binary classification with Y ∈ {−1, 1}, consider the exponential loss\\nfunction:\\nL(y, f(x)) = e−cyf (x).\\nfor some constant c > 0.\\n▶ Here, we have\\nL(yi, fm−1(xi) + αb(xi; β)) = e−cyi(fm−1(xi)+αb(xi;β))\\n= wi,me−cαyib(xi;β)\\nwhere wm,i := e−cyifm−1(xi) do not dependent on the optimisation\\nvariables α and β.\\n▶ It can be shown that Discrete AdaBoost algorithm corresponds to forward\\nstagewise additive fitting with exponential loss function (with c = 1/2).\\n▶ Can you show this? (optional homework)\\nMilan Vojnović 14/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='047ecac6-1bf0-4ed4-bdc6-452edc62c44f', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting trees\\n▶ Recall that a tree model can be expressed as:\\nT (x; θ) =\\nJX\\nj=1\\nγj1 (x ∈ Rj)\\nwhere θ = {(Rj, γj), j = 1, . . . , J}, R1, . . . , RJ are disjoint regions\\nrepresented by the terminal nodes of the tree and the output is γj for every\\nx such that x ∈ Rj.\\n▶ The boosted tree model is the sum of trees:\\nfM(x) =\\nMX\\nm=1\\nT (x; θm)\\nwhich is computed in a forward stagewise manner.\\nMilan Vojnović 15/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6dc485f6-e87b-4e6a-b744-fe57a33659b4', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boosting trees cont’d\\n▶ Using the forward stagewise method, at each stage m, we need to solve\\nˆθm = arg min\\nθ\\nnX\\ni=1\\nL(yi, fm−1(xi) + T (xi; θ))\\n▶ Given the regions Rm,1, . . . , Rm,Jm, finding optimal values of\\nγm,1, . . . , γm,Jm is usually easy by solving:\\nˆγm,j = arg min\\nγ\\nX\\ni:xi∈Rj\\nL(yi, fm−1(xi) + γ)\\n▶ Finding regions Rm,1, . . . , Rm,Jm is in general difficult.\\nMilan Vojnović 16/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='812121e5-adb9-4bcd-ab2c-8ef6c69346bd', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Squared error loss case\\n▶ Regression with squared error loss:\\n▶ For the squared error loss function, the problem is not harder than for\\nfitting a single tree.\\n▶ For each stage m, T (x; ˆθm) is a regression tree that best predicts the\\nresiduals yi − fm−1(xi) according to the squared error loss, and ˆγm,j\\nis the mean of these residuals for points i such that xi ∈ Rm,j.\\n▶ Binary classification with exponential loss:\\n▶ For each stage m, the added tree T (x; θm) is a decision tree that\\nminimises the weighted error rate:\\nnX\\ni=1\\nwm,i1 (T (xi; θm) ̸= yi).\\nMilan Vojnović 17/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea2755d9-01c5-4f57-b9d6-29b514e894d6', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient boosting\\n▶ Assume that L(y, f(x)) is a differentiable function in f(x) for every y.\\n▶ Consider\\nL(f) :=\\nnX\\ni=1\\nL(yi, f(xi)).\\n▶ Let fM =PM\\nm=0 ∆fm for ∆f0, . . . ,∆fM ∈ Rn.\\n▶ Using the gradient descent update, we have\\nfm = fm−1 − ρmgm\\ni.e., ∆fm = −ρmgm, where gm is the gradient of L(f) evaluated at\\nf = fm−1 and ρm is an optimal step size, i.e.\\ngm,i = ∂L(yi, f(xi))\\n∂f (xi) |f (xi)=fm−1(xi) for i = 1, . . . , n\\nand\\nρm = arg min\\nρ\\nL(fm−1 − ρgm).\\n▶ Observe that the steepest descent is for an unconstrained optimisation\\nproblem, ignoring the fact that f(x1), . . . , f(xn) correspond to terminal\\nvalues of some trees.\\nMilan Vojnović 18/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='47536127-aeca-47c6-9789-6fec7510952f', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient boosting cont’d\\n▶ The key idea for gradient boosting for trees is to add a tree in each stage of\\nthe boosting procedure that fits pseudo-residuals defined as negative\\ngradients.\\n▶ Squared error loss function example: the goal is to find a tree with the\\nparameter:\\nˆθm = arg min\\nθ\\nnX\\ni=1\\n(−gm,i − T (xi; θ))2.\\nMilan Vojnović 19/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba612980-3712-4519-a459-6d66d0bb6088', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Loss function examples\\n▶ For regression:\\nLoss function Negative gradient\\nSquared error 1\\n2(y − f(x))2 y − f(x)\\nAbsolute loss |y − f(x)| sign(y − f(x))\\n▶ For K-class classification:\\n▶ Prediction probabilities: pk(x) = efk(x)/PK\\nl=1 efl(x)\\n▶ Multinomial deviance (negative log-likelihood):\\nL(f) := −\\nKX\\nk=1\\n1 (y = k) log(pk(x))\\n= −\\nKX\\nk=1\\n1 (y = k)fk(x) + log\\n KX\\nk=1\\nefk(x)\\n!\\n.\\n▶ Negative gradients:\\n− ∂L(f)\\n∂fk(x) = 1 (y = k) − pk(x).\\nMilan Vojnović 20/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e53f1d8-fb06-4f70-a75a-5b9c21b63487', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient boosted trees for regression\\n1. Initialise f0(x) = arg minγ\\nPn\\ni=1 L(yi, γ).\\n2. For m = 1, . . . , M:\\n(a) For i = 1, . . . , n, compute\\nrm,i = − ∂L(yi, f(xi))\\n∂f (xi) |f =fm−1\\n(b) Fit a regression tree to the targets rm,i giving terminal\\nregions Rm,1, . . . , Rm,Jm.\\n(c) For j = 1, . . . , Jm, compute\\nγm,j = arg min\\nγ\\nX\\ni:xi∈Rm,j\\nL(yi, fm−1(xi) + γ)\\n(d) Update\\nfm(x) = fm−1(x) +\\nJmX\\nj=1\\nγm,j1 (xi ∈ Rm,j)\\n3. Output: fM\\nMilan Vojnović 21/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='26802df4-19f7-44c4-952c-31288f8422c0', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient boosted trees for classification\\n▶ Consider a K-class classification problem.\\n▶ Each function fk is modelled by a gradient boosted tree fM,k.\\n▶ The loss function correspond to the multinomial deviance.\\n▶ Steps 2(a)-2(d) in the gradient boosting for regression algorithm are\\nrepeated K times at each iteration m, one time for each class k.\\n▶ The output in step 3 are K gradient boosted trees fM,1, . . . , fM,K .\\n▶ The prediction probabilities are pk(x) = efM,k (x)/PK\\nl=1 efM,l (x).\\nMilan Vojnović 22/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a95b64b2-173d-409c-99b3-27889f583499', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting a regression tree: squared error loss\\n▶ At each stage m, the aim is to find a tree T (x; θm) that approximately\\nminimises the loss function:\\nLm :=\\nnX\\ni=1\\n(rm,i − T (xi; θm))2\\n▶ For given Rm,1, . . . , Rm,Jm, optimal values of γm,1, . . . , γm,Jm,\\nγm,j =\\nP\\ni:xi∈Rm,j rm,i\\n| ˜Rm,j|\\nwhere ˜Rm,j = {i : xi ∈ Rm,j}\\n▶ For these values of γm,1, . . . , γm,Jm, we have\\nLm =\\nJmX\\nj=1\\nL( ˜Rm,j)\\nwhere\\nL(R) =\\nX\\ni∈R\\nr2\\nm,i − 1\\n|R|\\n X\\ni∈R\\nrm,i\\n!2\\n.\\nMilan Vojnović 23/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a7176d8d-3ae6-4bca-9862-cc9641083286', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting a regression tree: squared error loss ...\\n▶ Consider an arbitrary node of the tree with the corresponding region R.\\n▶ Let R1 and R2 be two partitions of R by splitting over values of a fixed\\npredictor.\\n▶ For any given Q, let ˜Q = {i : xi ∈ Q}.\\n▶ Then, the splitting aims to maximise the \"variance gain\" defined as:\\nL( ˜R) − L( ˜R1) − L( ˜R2)\\n= 1\\n| ˜R1|\\n\\uf8eb\\n\\uf8edX\\ni∈ ˜R1\\nrm,i\\n\\uf8f6\\n\\uf8f8\\n2\\n+ 1\\n| ˜R2|\\n\\uf8eb\\n\\uf8edX\\ni∈ ˜R2\\nrm,i\\n\\uf8f6\\n\\uf8f8\\n2\\n− 1\\n| ˜R|\\n\\uf8eb\\n\\uf8edX\\ni∈ ˜R\\nrm,i\\n\\uf8f6\\n\\uf8f8\\n2\\nMilan Vojnović 24/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1104706-98b7-46e1-afe1-aa11dcf29fed', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting a regression tree: general case\\n▶ For twice-differentiable loss functions, second-order approximation\\nmethods are sometimes used for fitting a regression tree.\\n▶ E.g. used by XGBoost, a popular tree boosting algorithm.\\n▶ In what follows we explain the fitting used by XGBoost algorithm.\\n▶ For each stage m, consider the loss function:\\nLm =\\nnX\\ni=1\\nL(yi, fm−1(xi) + T (xi; θm)) + Ω(θm)\\nwhere Ω is a regularisation function defined as\\nΩ(θm) = αJm + λ||γm||2.\\nMilan Vojnović 25/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='885ddbc8-6818-4662-96f6-619bb0b158a0', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Second-order approximation\\n▶ By using the limited Taylor expansion of the loss function up to the second\\norder terms, we have\\nLm ≈\\nnX\\ni=1\\n\\x12\\nL(yi, fm−1(xi)) + gm,iT (xi; θm) + 1\\n2 hm,iT (xi; θm)2\\n\\x13\\n+Ω(θm)\\nwhere\\ngm,i = ∂L(yi, f(xi))\\n∂f (xi) |f (xi)=fm−1(xi)\\nand\\nhm,i = ∂2L(yi, f(xi))\\n∂f (xi)2 |f (xi)=fm−1(xi).\\n▶ Ignoring the constant terms, we consider\\n˜Lm :=\\nnX\\ni=1\\n\\x12\\ngm,iT (xi; θm) + 1\\n2 hm,iT (xi; θm)2\\n\\x13\\n+ Ω(θm).\\nMilan Vojnović 26/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='069d5b3d-844a-4305-890d-1162ca7c5e22', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Second-order approximation cont’d\\n▶ Using the fact that T (x; θm) is a tree, we have\\n˜Lm =\\nJmX\\nj=1\\n\\uf8eb\\n\\uf8ed\\n\\uf8eb\\n\\uf8ed X\\ni:xi∈Rm,j\\ngm,i\\n\\uf8f6\\n\\uf8f8 γm,j + 1\\n2\\n\\uf8eb\\n\\uf8ed X\\ni:xi∈Rm,j\\nhm,i + λ\\n\\uf8f6\\n\\uf8f8 γ2\\nm,j\\n\\uf8f6\\n\\uf8f8+αJm.\\n▶ For any given Rm,1, . . . , Rm,Jm, the optimal values of γm,1, . . . , γm,Jm are\\nγm,j =\\nP\\ni:xi∈Rm,j(−gm,i)\\nP\\ni:xi∈Rm,j hm,i + λ .\\n▶ By plugging these values, we have\\n˜Lm = −1\\n2\\nJmX\\nj=1\\n\\x10P\\ni:xi∈Rm,j gm,i\\n\\x112\\n\\x10P\\ni:xi∈Rm,j hm,i\\n\\x11\\n+ λ\\n+ αJm.\\nMilan Vojnović 27/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='288cbb7f-ae81-4f78-ba90-29007195077b', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Second-order approximation cont’d\\n▶ Using the second-order loss function approximation yields the following\\ncriteria for finding a split at a node corresponding to the region R to\\nregions R1 and R2: find a split that maximises\\n1\\n2\\n\\x00P\\ni:xi∈R1 gm,i\\n\\x012\\n(P\\ni:xi∈R1 hm,i) + λ+1\\n2\\n\\x00P\\ni:xi∈R2 gm,i\\n\\x012\\n(P\\ni:xi∈R2 hm,i) + λ −1\\n2\\n\\x00P\\ni:xi∈R gm,i\\n\\x012\\n(P\\ni:xi∈R hm,i) + λ −α\\n▶ The squared-error loss function case without regularisation is a special case\\nwhen λ = 0.\\n▶ Homework: simple exercise.\\nMilan Vojnović 28/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f1398a3d-db67-4805-a5f4-c82950ea11d5', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Some loss function examples\\n▶ For regression:\\n▶ Squared-error loss:\\nhm,i = 1.\\n▶ For K-class classification\\n▶ Multinomial deviance loss function:\\nhm,i = pk(xi)(1 − pk(xi)).\\nMilan Vojnović 29/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eb346559-53bd-432a-9b41-89f992edf418', embedding=None, metadata={'file_name': 'ST443_Lecture_7.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='GBM implementations\\n▶ R:\\n▶ gbm\\n▶ Scikit-learn:\\n▶ AdaBoostClassifier, AdaBoostRegressor\\n▶ GradientBoostingClassifier, GradientBoostingRegressor\\n▶ Scalable algorithms:\\n▶ XGBoost\\n▶ LightGBM\\n▶ Scikit-learn: HistGradientBoostingClassifier,\\nHistGradientBoostingRegressor\\nMilan Vojnović 30/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c51c4656-54ba-4161-b300-74c892b589c6', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 3: Basic nonlinear classifiers and model fitting\\n15 Oct 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='62271df8-acaf-4673-9dca-2038dccc9a1d', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ QDA\\n▶ Naive Bayes classifier\\n▶ Nearest neighbour classifier\\n▶ Bias-variance trade-off in classification\\n▶ Training loss function minimisation\\nMilan Vojnović 2/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12823bfe-c8ce-4577-968e-7a212649ef9a', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Some nonlinear classifiers\\n1. Both LDA and logistic regression classifiers result in linear decision\\nboundaries.\\n2. If the Bayes’ decision boundary is nonlinear, even the best linear classifier\\nmay incur substantial excess risk.\\n3. Nonlinear classifiers offer more flexibility\\n– Quadratic discriminant analysis\\n– Nearest neighbour classifiers\\nMilan Vojnović 3/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91b88b87-2d89-486d-bdd9-1f69ddee9f81', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Linear vs nonlinear classifiers\\nsimple flexible\\nlow-variance low-bias\\ncannot model complex boundary more risk to overfit\\nMilan Vojnović 4/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b273241c-5f5b-4908-96ac-c5b08609b038', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Quadratic discriminant analysis (QDA)\\n▶ In QDA, we assume 𝑓𝑘 is the density of 𝑁𝑝(𝜇𝑘, Σ𝑘) — both mean and\\nvariance vary with class.\\n𝑓𝑘(𝑥) = 1\\n(2𝜋)𝑝∕2(det Σ𝑘)1∕2 𝑒− 1\\n2 (𝑥−𝜇𝑘)⊤Σ−1\\n𝑘 (𝑥−𝜇𝑘).\\n▶ Bayes discriminant function is\\nlog(𝜋𝑘𝑓𝑘(𝑥)) = log( 𝜋𝑘) − 1\\n2 (𝑥 − 𝜇𝑘)⊤Σ−1\\n𝑘 (𝑥 − 𝜇𝑘) − 1\\n2 log(det Σ𝑘)\\nwhich has a term quadratic in 𝑥 depending on 𝑘.\\n▶ Use training data to estimate for 𝑘 ∈ {1, … , 𝐾},\\n̂ 𝜋𝑘 ∶= 𝑁𝑘\\n𝑛 , 𝑁 𝑘 is # of data points with label 𝑘\\n̂ 𝜇𝑘 ∶= 1\\n𝑁𝑘\\n𝑛∑\\n𝑖=1\\n𝑋𝑖1 (𝑌𝑖 = 𝑘) (class centroids)\\n̂Σ𝑘 ∶= 1\\n𝑁𝑘 − 1\\n𝑛∑\\n𝑖=1\\n(𝑋𝑖 − ̂ 𝜇𝑘)(𝑋𝑖 − ̂ 𝜇𝑘)⊤1 (𝑌𝑖 = 𝑘).\\nMilan Vojnović 5/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85d1ee51-1a6c-4509-a8af-8b05cc1b903d', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Quadratic discriminant analysis (QDA)\\n▶ Substitute in to obtain the quadratic discriminant function\\n𝛿QDA\\n𝑘 (𝑥) ∶= log( ̂ 𝜋𝑘) − 1\\n2 (𝑥 − ̂ 𝜇𝑘)⊤ ̂Σ−1\\n𝑘 (𝑥 − ̂ 𝜇𝑘) − 1\\n2 log(det ̂Σ𝑘).\\n▶ The QDA classifier is defined as\\n𝜓 QDA(𝑥) ∶= argmax\\n𝑘\\n𝛿QDA\\n𝑘 (𝑥).\\n▶ QDA uses a lot more parameters than LDA as each ̂Σ𝑘 contains 𝑝(𝑝 + 1)∕2\\nparameters, thus in total we have 𝐾𝑝(𝑝 + 1)∕2 parameters for all the\\ncovariance matrices.\\nMilan Vojnović 6/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c73243c2-4067-4cfc-aded-f8e6cab992a7', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='QDA vs LDA\\n▶ Which approach is better LDA or QDA?\\n▶ QDA will work better when Σ𝑘’s are varying between classes and we have\\nenough observations to accurately estimate Σ𝑘’s.\\n▶ LDA will work better when Σ𝑘’s are similar among classes or we do not\\nhave enough data to accurately estimate Σ𝑘’s.\\nMilan Vojnović 7/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae2375c2-4388-40d1-a6fc-8aafa9d571e6', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Approximating a Bayes classifier\\n▶ Consider using a classifier\\n𝜓(𝑥) = arg max\\n𝑘\\n̂𝑓𝑘(𝑥) ̂ 𝜋𝑘\\nwhere ̂𝑓𝑘 and ̂ 𝜋𝑘 are some estimators of 𝑓𝑘 and 𝜋𝑘.\\n▶ Estimation of 𝜋 is easy.\\n– E.g. using the frequencies of labels in the training data set.\\n▶ Estimation of the 𝑝-dimensional densities 𝑓𝑘 can be a hard problem.\\n– To alleviate the difficulty of estimating densities 𝑓𝑘, one approach is to\\nassume that they belong to a particular family of parametric densities.\\n– E.g. multivariate Gaussians for QDA and LDA.\\nMilan Vojnović 8/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='768bffff-5f35-4c89-82a6-18409e62afe4', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Naive Bayes classifiers\\n▶ Naive Bayes classifiers admit the following assumption: for each label 𝑘,\\nconditional on 𝑌 = 𝑘, predictors 𝑋 = (𝑋1, … , 𝑋𝑝) are independent, i.e.\\n𝑓𝑘(𝑥) = 𝑓𝑘,1(𝑥1) × ⋯ × 𝑓𝑘,𝑝(𝑥𝑝).\\n▶ Assuming conditional independence of predictors, reduces the complexity\\nof estimating 𝑓𝑘’s.\\n– It suffices to only estimate the conditional univariate marginals.\\n▶ Recall that having a model with less flexibility and fewer parameters, in\\ngeneral, means having a large bias and small variance.\\n▶ Different Naive Bayes classifiers differ in how 𝑓𝑘,𝑖 are estimated.\\n– Either parametric or non-parametric estimators can be used.\\nMilan Vojnović 9/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46011363-eb37-417c-82f2-7738905cb2de', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Types of estimators\\n▶ Parametric 𝑓𝑘,𝑖 belong to a family of parametric distributions.\\n– E.g., Gaussian: 𝑓𝑘,𝑖(𝑥) = 𝑁(𝑥; 𝜇𝑘,𝑖, 𝜎𝑘,𝑖).\\nNote: special case of QDA.\\n– E.g., Multinomial: 𝑓𝑘,𝑖(𝑥) = Multinomial( 𝑥; 𝑞𝑘,𝑖).\\nNote: Bernoulli is a special case.\\n▶ Non-parametric\\n– Categorical: frequency count estimators.\\n– Non-parametric density estimators:\\nE.g., histograms.\\nE.g., Kernel density function estimation.\\nMilan Vojnović 10/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30355131-99b3-452c-865a-1dc51245b07c', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nearest neighbour classifier\\n▶ The 𝓀-nearest neighbour classifier (𝓀-NN or 𝓀NN) classifies a new data 𝑥\\nby a majority voting from the 𝑘 nearest training data points to 𝑥.\\n▶ Let (𝑋(1), 𝑌(1)), … , (𝑋(𝑛), 𝑌(𝑛)) be a reordering of the training data such that\\n‖𝑋(1) − 𝑥‖2 ≤ ⋯ ≤ ‖𝑋(𝑛) − 𝑥‖2. Then\\n𝜓 𝓀NN(𝑥) ∶= argmax\\n𝑘\\n𝓀∑\\n𝑖=1\\n1 (𝑌(𝑖) = 𝑘) (arbitrary tie-breaking).\\nMilan Vojnović 11/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48a619c6-9477-4e5d-95e7-a70832b7b710', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nearest neighbour classifier\\n▶ 𝓀-NN makes intuitive sense — relative proportion of labels in the nearest 𝓀\\npoints is an approximation of the relative magnitude of the conditional\\ndensities (cf. Bayes classifier).\\n▶ It can be shown that as 𝑛 → ∞, a 1-NN classifier has a test risk at most\\ntwice that of the Bayes’ classifier.\\n– We will explain this next.\\n– See HW3 for the more general case of a 𝓀-NN classifier.\\n▶ Caution: the 𝑛 → ∞ asymptotic hides the bias-variance trade-off in the\\nfinite training sample setting.\\nMilan Vojnović 12/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d0a5a1f-64b4-4df4-ba0e-a4b54e7d89ba', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Risk bound for 𝓀 = 1\\n▶ Consider the case 𝓀 = 1 for binary classification with 𝑌 = 0 denoting a\\nnegative example and 𝑌 = 1 denoting a positive example.\\n▶ Define 𝜂(𝑥) ∶= ℙ[𝑌 = 1 ∣ 𝑋 = 𝑥], for 𝑥 ∈ ℝ𝑝.\\n▶ For any 𝑥 ∈ ℝ𝑝, let 𝑥NN denote the nearest neighbour of 𝑥 in the training\\ndataset of 𝑛 points and 𝜓(𝑥) its corresponding label.\\n▶ The conditional risk given 𝑋 = 𝑥 is\\n𝔼[1 (𝑌 ≠ 𝜓(𝑥)) ∣ 𝑋 = 𝑥] = 𝜂(𝑥)(1 − 𝜂(𝑥NN)) + (1 − 𝜂(𝑥))𝜂(𝑥NN)\\n= 2 𝜂(𝑥)(1 − 𝜂(𝑥)) + 𝑎(𝑥)\\nwhere 𝑎(𝑥) ∶= (1 − 2 𝜂(𝑥))(𝜂(𝑥NN) − 𝜂(𝑥)).\\n▶ Intuitively, we expect that under some conditions,\\n𝜂(𝑥NN) → 𝜂(𝑥) (i.e. 𝑎(𝑥) → 0) as 𝑛 → ∞.\\nMilan Vojnović 13/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ecfce7c8-b4cd-40f1-bdd6-1cacfdc559b7', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Risk bound for 𝓀 = 1 cont’d\\n▶ Continuing on the established fact, the risk is bounded as\\n𝑅(𝜓) = 2 𝔼[𝜂(𝑋)(1 − 𝜂(𝑋))] + 𝑎𝑛\\n≤ 2𝑅(𝜓 𝐵𝑎𝑦𝑒𝑠) + 𝑎𝑛\\nwhere\\n𝑅(𝜓 𝐵𝑎𝑦𝑒𝑠) = 𝔼[min{𝜂(𝑋), 1 − 𝜂(𝑋)}]\\nis the Bayes’ risk, and\\n𝑎𝑛 = 𝔼[𝑎(𝑋)].\\n▶ Under some conditions, 𝑎𝑛 → 0 as 𝑛 → ∞ as we show next.\\nMilan Vojnović 14/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='63c6b93c-d1b1-4b25-b897-e20d9ed9111b', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Consistency of 1-NN\\n▶ Continuity assumption: assume that 𝜂 is such that for some 𝐿 ≥ 0,\\n|𝜂(𝑥) − 𝜂(𝑥′)| ≤ 𝐿||𝑥 − 𝑥′||2, for every 𝑥, 𝑥′ ∈ ℝ𝑝.\\n▶ Consistency: if 𝑥NN → 𝑥 (i.e. ||𝑥NN − 𝑥||2 → 0) as 𝑛 → ∞, then\\n𝜂(𝑥NN) → 𝜂(𝑥) as 𝑛 → ∞.\\n▶ For any 𝜖 > 0,\\nℙ[||𝑋NN − 𝑥||2 > 𝜖 ] = ℙ[∩𝑛\\n𝑖=1{||𝑋𝑖 − 𝑥||2 > 𝜖 }] = ℙ[||𝑋1 − 𝑥||2 > 𝜖 ]𝑛.\\n▶ Note that\\nℙ[||𝑋1 − 𝑥||2 ≤ 𝜖] ≈ 𝑉𝑝(𝜖)𝑓𝑋(𝑥) for small 𝜖,\\nwhere 𝑉𝑝(𝜖) is the volume of a ball of radius 𝜖 in ℝ𝑝,\\n𝑉𝑝(𝜖) = 𝑐𝑝𝜖𝑝 for some constant 𝑐𝑝 > 0.\\nMilan Vojnović 15/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6441b2ea-14a1-4140-8fd6-1f2f276880fb', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Consistency of 1-NN cont’d\\n▶ Continuing on the facts we established,\\nℙ[||𝑋NN − 𝑥||2 > 𝜖 ] = (1 − ℙ[||𝑋1 − 𝑥||2 ≤ 𝜖])𝑛\\n≈ (1 − 𝑐𝑝𝜖𝑝𝑓𝑋(𝑥))𝑛 for small 𝜖\\nwhich goes to 0 as 𝑛 → ∞ if (c1) 𝑓𝑋(𝑥) > 0 and (c2) 𝑐𝑝𝜖𝑝𝑓𝑋(𝑥) < 1.\\n▶ ▶Under (c1) and (c2), for any 𝛿 ∈ (0, 1], it holds (1 − 𝑐𝑝𝜖𝑝𝑓𝑋(𝑥))𝑛 ≤ 𝛿, if\\n𝑛 ≥ 1\\n𝑐𝑝𝜖𝑝𝑓𝑋(𝑥) log(1∕𝛿).\\nMilan Vojnović 16/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='758a6833-b5eb-4d04-8350-14ef6eac5b88', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choice of 𝓀\\n▶ Main tuning parameter in 𝓀-NN is the number of neighbours to use.\\nSmall 𝓀:\\n– Infer label from a small number of representative neighbours.\\n– Low bias.\\n– High variance.\\nLarge 𝓀:\\n– Infer label from a large number of not-so-representative neighbours.\\n– High bias.\\n– Low variance.\\n▶ In practice, we choose 𝓀 via cross-validation (see next lecture).\\nMilan Vojnović 17/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4f44c32-99df-41b9-a20d-99af2cb06381', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choice of 𝓀: an example\\n▶ We use the 𝚖𝚒𝚡𝚝𝚞𝚛𝚎.𝚎𝚡𝚊𝚖𝚙𝚕𝚎 dataset from the ESL book.\\n▶ Two-dimensional covariates, two classes (blue/orange).\\n▶ Data are generated from Gaussian mixtures so the Bayes’ decision\\nboundary can be computed exactly.\\nMilan Vojnović 18/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56d1313e-cbe3-4c15-a39b-9c4d9dc262c4', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choice of 𝓀, bias-variance trade-off\\n5 10 15 20 25\\n0.08 0.10 0.12 0.14 0.16 0.18\\nk\\ntest err\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n    \\n  \\n𝓀 = 1 𝓀 = 7 𝓀 = 25\\ntest err = 0.191 test err = 0.069 test err = 0.159\\nMilan Vojnović 19/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5da1d118-4af7-4a8e-9327-15520a674535', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Choice of distance\\n▶ In addition to 𝓀, we can also choose the distance metric in 𝓀-NN\\nclassification.\\n– Most popular distance is 𝓁2 (Euclidean) distance.\\n– Can also use 𝓁1 (Manhattan) distance.\\n– There are also various other choices, e.g. Hamming distance, various\\n\"distances\" derived from similarity measures such as cosine and\\nJaccard similarity, used in information retrieval and other applications.\\n▶ If covariates are measured in distinct units, or differ substantially in\\nmagnitude, it is advisable to first standardise the covariates.\\nMilan Vojnović 20/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dbba086f-47f8-45f0-b3d1-cc9c661c7de9', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Invariant metrics\\n▶ In some problems, the training feature is invariant under certain\\ntransformations.\\n▶ We should exploit such invariance when calculating distances.\\n▶ This can either be done algebraically, or more simply by augmenting the\\ntraining data with transformed training data (e.g. rotation, translation).\\nMilan Vojnović 21/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c0ff64c8-3987-47f3-97c6-4cef5f32a4ee', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='𝓀-NN and categorical features\\n▶ Categorical features can be addressed in different ways such as:\\n– Encoding: transforming categorical features to numerical features.\\n– Distance: using a distance metric allowing for categorical features.\\n▶ Examples of encoding methods:\\n– One-hot-encoding: assigns each category to a unique standard basis\\nvector from 𝑒1, … , 𝑒𝐽 where 𝐽 denotes the distinct number of\\ncategories.∗\\nNote: it suffices to only use the first 𝐽 − 1 elements of 𝑒1, … , 𝑒𝐽 .\\n– Label encoding: assigns each category to a unique numeric value (e.g.\\nenumerating by integers 0, 1, … , 𝐽 − 1).\\nRepresenting categories with scalar values is suitable for ordinal\\ncategories (e.g. Low, Medium, High) but in general it may imply an\\norder that may not exist (e.g. Red, Blue, Green).\\n▶ An example of a distance metric allowing for categorical features:\\n– Hamming distance: 𝑑(𝑥, 𝑥′) = ∑𝑝\\n𝑖=1 1 (𝑥𝑖 ≠ 𝑥′\\n𝑖).\\n∗𝑒𝑗 is a 𝐽-dimensional vector such that 𝑒𝑗,𝑗 = 1 and 𝑒𝑗,𝑗 ′ = 0 for every 𝑗′ ≠ 𝑗\\nMilan Vojnović 22/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f3b425e-b119-41dc-ba89-b69085f4dc47', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The curse of dimensionality\\n▶ If data is high-dimensional, and noise is present in all coordinates, then\\ndistance can have high variance.\\n▶ In high-dimensions, all pairwise distances are the same order of magnitude.\\nHow does between-class and within-class pairwise distances compare for\\n𝑁𝑝(5𝑒1, 𝐼𝑝) vs 𝑁𝑝(−5𝑒1, 𝐼𝑝)?\\n0 5 10 15\\n0.0 0.1 0.2 0.3 0.4\\ndist\\ndensity\\nwithin\\nbetween\\n58 60 62 64 66 68\\n0.0 0.1 0.2 0.3 0.4\\ndist\\ndensity\\nwithin\\nbetween\\n𝑝 = 2 𝑝 = 2000\\n▶ We may need to perform dimension reduction before using 𝓀-NN (more\\nabout this in a later lecture).\\nMilan Vojnović 23/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d74e340-60f9-4542-999b-6b7a93cb9ef2', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The suprising power of simple classifiers\\n▶ LDA, QDA, Logistic classifier and 𝓀-NN are very classical methods.\\n▶ They are often the first classifiers one should try in many problems and are\\nsurprisingly powerful in many applications.\\n▶ Why does LDA/QDA/logistic classifiers work well?\\n– Not that the we truly believe in the generative models (Gaussian\\nmixture or linear model of log odds)\\n– The low signal-to-noise ratio in the data can only support simple\\nclassifiers.\\n▶ Why does 𝓀-NN work well?\\n– Can model highly irregular decision boundaries in the presence of\\nnoise.\\n– Can use the tuning parameter 𝓀 to optimise bias-variance trade-off.\\nMilan Vojnović 24/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='38310fb0-9c2b-446e-9689-d69f77205d73', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The suprising power of simple classifiers\\nComparison of classification algorithms in 22 datasets from the STATLOG\\nproject (highlighting LDA, QDA, Logistic, 𝓀-NN).\\nMilan Vojnović 25/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8160cd7c-f8cc-4ded-bdc8-349155030bd2', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Training loss function minimisation\\n▶ A classification or a regression model with unknown parameters is fitted by\\nminimising an empirical risk function (loss function) possibly also involving\\na regularisation term (Lecture 5).\\n– This is done by using an optimisation method.\\n▶ For differentiable loss functions, the (stochastic) gradient descent iterative\\noptimisation method is commonly used.\\n– E.g. used for logistic classifiers.\\n▶ Some models are non-parametric and thus require no parameter fitting\\n– E.g. an NN classifier.\\n▶ Some models require not using an iterative optimisation method.\\n– E.g. LDA and QDA may be seen as minimising the negative\\nlog-likelihood loss function (equivalent to MLE).\\nMilan Vojnović 26/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed12ff68-8092-44d7-a7e2-04459b724a29', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Fitting the logistic classifier\\n▶ Recall that the logistic classifier is\\n𝜓 logit(𝑥) = argmax\\n𝑘\\n̂𝛽⊤\\n𝑘 𝑥.\\nHow do we find good coefficients ̂𝛽1, … , ̂𝛽𝐾−1 (convention: ̂𝛽𝐾 = 0)?\\n▶ Minimising 0-1 training error?\\n( ̂𝛽1, … , ̂𝛽𝐾−1) = argmin\\n𝛽1,…,𝛽𝐾−1\\n1\\n𝑛\\n𝑛∑\\n𝑖=1\\n1 (𝑌𝑖 ≠ argmax\\n𝑘\\n𝛽⊤\\n𝑘 𝑋𝑖).\\n▶ Difficult to implement. Much easier to work with the cross-entropy loss\\n(negative log-likelihood):\\n( ̂𝛽1, … , ̂𝛽𝐾−1) = argmin\\n𝛽1,…,𝛽𝐾−1\\n− 1\\n𝑛\\n𝑛∑\\n𝑖=1\\n𝐾∑\\n𝑘=1\\n1 (𝑌𝑖 = 𝑘) log\\n(\\n𝑒𝛽⊤\\n𝑘 𝑋𝑖\\n𝑒𝛽⊤\\n1 𝑋𝑖 + ⋯ + 𝑒𝛽⊤\\n𝐾 𝑋𝑖\\n)\\n⏟ ⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞ ⏟ ⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞ ⏟\\n𝑝𝑖,𝑘(𝛽)\\n.\\n▶ Always a good idea to use a smooth loss in training.\\nMilan Vojnović 27/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c4c7d589-7fe9-4757-979e-d5120c2614e9', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient descent\\n▶ Gradient descent is a popular method for minimising a smooth function\\n𝑓 ∶ ℝ𝑝 → ℝ (e.g. the empirical risk function).\\nAlgorithm:\\n1. Initialise 𝑤(0) ∈ ℝ𝑝.\\n2. Update 𝑤(𝑡) ← 𝑤(𝑡−1) − 𝛼𝑡∇𝑓 (𝑤(𝑡−1))until convergence.\\nMilan Vojnović 28/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='559a8bfd-fdb0-487c-a3e7-9c2f4cadbe89', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient descent\\n▶ Under mild conditions on the learning rate 𝛼𝑡, gradient descent converges\\nto a local minimum.\\n▶ If the function is convex, gradient descent converges to a global minimum.\\n▶ In practice, choose 𝛼𝑡 by cross-validation (see next lecture).\\n▶ Example: in logistic regression, we initialise ̂𝛽(0)\\n𝑘 arbitrarily, and update\\n̂𝛽(𝑡)\\n𝑘 ← ̂𝛽(𝑡−1)\\n𝑘 + 𝛼𝑡\\n𝑛∑\\n𝑖=1\\n(1 (𝑌𝑖 = 𝑘) − 𝑝𝑖,𝑘( ̂𝛽(𝑡−1)))𝑋𝑖.\\n▶ For 𝑛 large, computing the gradient is still costly. A better idea is to move\\nalong the gradient as we start calculating the summands – since each\\nsummand is an unbiased estimator of the sum.\\nMilan Vojnović 29/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0e75a16-4d00-486b-8cc3-24a472e71577', embedding=None, metadata={'file_name': 'ST443_Lecture_3.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Stochastic gradient descent\\n▶ When 𝑓 ∶ ℝ𝑝 → ℝ can be written as 𝑓 = 1\\n𝑛\\n∑𝑛\\n𝑖=1 𝑓𝑖, we can use stochastic\\ngradient descent to minimise 𝑓.\\nAlgorithm:\\n1. Initialise 𝑤(0,𝑛) ∈ ℝ𝑝.\\n2. For epochs 𝑒 = 1, 2, …,\\n(i) Optionally shuffle indices.\\n(ii) Set 𝑤(𝑒,0) ← 𝑤(𝑒−1,𝑛).\\n(iii) Update 𝑤(𝑒,𝑖) ← 𝑤(𝑒,𝑖−1) − 𝛼𝑒∇𝑓𝑖(𝑤(𝑒,𝑖−1)).\\n▶ Example: in logistic regression, we have\\n̂𝛽(𝑒,𝑖)\\n𝑘 ← ̂𝛽(𝑒,𝑖−1)\\n𝑘 + 𝛼𝑒(1 (𝑌𝑖 = 𝑘) − 𝑝𝑖,𝑘( ̂𝛽(𝑒,𝑖−1)))𝑋𝑖.\\nMilan Vojnović 30/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f7614ed-49ba-4a05-9579-66a7c6bedde0', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 8: Support vector machines\\n26 Nov 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21ed14fa-fb9c-4a5a-b04e-26412b85ac11', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ Support vector machines (SVM) (including maximal margin classifiers\\n(MMC) and support vector classifier (SVC) as special cases).\\n▶ Understand the idea of support vectors.\\n▶ Understand the soft margin (and its tuning parameter) in SVC.\\n▶ Kernels and how kernel SVM is related to MMC in a reproducing kernel\\nHilbert space.\\nMilan Vojnović 2/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7bb6939d-91d6-480a-a41a-ae8c694f0a1c', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Support vector machines\\n▶ Support vector machines are two-class classifiers developed by Vladimir\\nVapnik and Corinna Cortes at Bell Labs around 1990.\\n▶ SVM places no distributional assumptions on data, but rather directly\\nsearches for hyperplanes that “optimally” separate the two classes.\\n▶ One of the best out-of-box classifiers in many machine learning problems.\\nMilan Vojnović 3/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4c942275-95d9-4444-aa8a-65c78d351ab7', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Hyperplanes define linear classifiers\\n▶ In a 𝑝-dimensional space, a hyperplane is an affine subspace of dimension\\n𝑝 − 1.\\n▶ In a 𝑝-dimensional setting, a hyperplane 𝐻 is defined by parameters\\n(𝛽0, 𝛽) ∈ ℝ × ℝ𝑝 as\\n𝐻 = {𝑥 ∈ ℝ𝑝 ∶ 𝛽0 + 𝛽⊤𝑥 = 0}.\\n▶ The vector 𝛽 = (𝛽1, … , 𝛽𝑝)⊤ ∈ ℝ𝑝 is the normal vector, in a direction\\northogonal to the surface of the hyperplane.\\n▶ Any hyperplane defines a linear classifier that classifies 𝑥 ∈ ℝ𝑝 by\\nsgn(𝛽⊤𝑥 + 𝛽0) (we label the two classes {−1, 1} for convenience).\\n▶ Distance from 𝑥 to the hyperplane 𝐻 is\\n𝑑(𝑥, 𝐻) = min\\n𝑧∈𝐻\\n||𝑧 − 𝑥||2 = |𝛽⊤𝑥 + 𝛽0|\\n||𝛽||2\\n.\\nMilan Vojnović 4/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d6bbc9f-61f7-447e-b789-d4ce31862f57', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Separating hyperplanes\\n▶ If a hyperplane defines a classifier with a zero training misclassification\\nerror rate, it is called a separating hyperplane.\\n▶ (𝛽0, 𝛽) parametrise a separating hyperplane if and only if\\nsgn(𝛽0 + 𝛽⊤𝑥𝑖) = 𝑦𝑖 for all 𝑖 = 1, … , 𝑛.\\n▶ When there are multiple separating hyperplanes, which one is more\\nsensible?\\nMilan Vojnović 5/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='abf62dcf-d4f7-4517-9a69-1b62e3cb51d2', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Maximal margin classifier\\n▶ The Maximal Margin Classifier (MMC) is the classifier corresponding to\\nthe separating hyperplane that maximises the distance to the nearest\\ntraining data.\\n▶ It solves the optimisation problem:\\nmax\\n𝛽0,𝛽∶‖𝛽‖2=1\\n𝑀 subject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 𝑀 for all 𝑖 = 1, … , 𝑛.\\n▶ The optimisers 𝛽∗\\n0 , 𝛽∗ defines the MMC and the associated objective value\\n𝑀 ∗ is the ‘margin’ around the decision boundary.\\nMilan Vojnović 6/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f06724b-e366-4d5b-9c37-37e1ee74aa99', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Computing the MMC [NON-EXAMINABLE]\\n▶ Rescaling 𝛽0 and 𝛽 by 1∕𝑀, we can rewrite the optimisation problem as\\nmax\\n𝛽0,𝛽∶‖𝛽‖2=1∕𝑀\\n𝑀 subject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 1 for all 𝑖\\n⇔ max\\n𝛽0,𝛽\\n1∕‖𝛽‖2 subject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 1 for all 𝑖\\n⇔ min\\n𝛽0,𝛽\\n1\\n2 ‖𝛽‖2\\n2 subject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 1 for all 𝑖. (P)\\n▶ Problem (P) is a convex quadratic programming optimisation problem, so\\ncan be efficiently solved.\\n▶ To understand its solution, let’s look at its Lagrangian dual.\\nMilan Vojnović 7/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7316af04-6366-4986-98e3-dbace6745bc9', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The dual formulation [NON-EXAMINABLE]\\n▶ Lagrangian for (P) is\\n𝐿(𝛽0, 𝛽; 𝜆) = 1\\n2 ‖𝛽‖2\\n2 −\\n𝑛∑\\n𝑖=1\\n𝜆𝑖(𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) − 1),\\nwhere (𝛽0, 𝛽, 𝜆) ∈ ℝ × ℝ𝑝 × ℝ𝑛\\n≥0.\\n▶ The dual objective function is\\n𝑔(𝜆) ∶= min\\n𝛽0,𝛽\\n𝐿(𝛽0, 𝛽; 𝜆)\\n=\\n{ −∞ if ∑𝑛\\n𝑖=1 𝜆𝑖𝑦𝑖 ≠ 0\\n∑𝑛\\n𝑖=1 𝜆𝑖 − 1\\n2\\n‖‖‖\\n∑𝑛\\n𝑖=1 𝜆𝑖𝑦𝑖𝑥𝑖\\n‖‖‖\\n2\\n2\\notherwise.\\n▶ Hence the dual problem is:\\nmax\\n𝜆∈ℝ𝑛\\n≥0\\n𝑛∑\\n𝑖=1\\n𝜆𝑖 − 1\\n2\\n‖‖‖\\n𝑛∑\\n𝑖=1\\n𝜆𝑖𝑦𝑖𝑥𝑖\\n‖‖‖\\n2\\n2\\nsubject to\\n𝑛∑\\n𝑖=1\\n𝜆𝑖𝑦𝑖 = 0. (D)\\nMilan Vojnović 8/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4e7271b-a9c2-42bd-8dbc-d50779af4858', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The MMC solution [NON-EXAMINABLE]\\n▶ After solving (D) to obtain a dual solution 𝜆∗ = (𝜆∗\\n1, … , 𝜆∗\\n𝑛)⊤, we use the\\nfact that the primal solution 𝛽∗\\n0 and 𝛽∗ minimises 𝐿(𝛽0, 𝛽, 𝜆∗) to solve the\\nprimal problem (P). Specifically, we have\\n𝛽∗ =\\n𝑛∑\\n𝑖=1\\n𝜆∗\\n𝑖 𝑦𝑖𝑥𝑖\\n𝛽∗\\n0 = − 1\\n2\\n(\\nmin\\n𝑖∶𝑦𝑖=1\\n𝑥⊤\\n𝑖 𝛽∗ + max\\n𝑖∶𝑦𝑖=−1\\n𝑥⊤\\n𝑖 𝛽∗\\n)\\n.\\n▶ By complementary slackness, we have 𝜆∗\\n𝑖 = 0 if 𝑦𝑖(𝑥⊤\\n𝑖 𝛽∗ + 𝛽∗\\n0 ) ≠ 1, i.e. if 𝑥𝑖\\nis not closest to the hyperplane.\\n▶ The MMC classifier is 𝜓 SVM ∶ ℝ𝑝 → {−1, 1} defined by\\n𝜓 SVM(𝑥) = sgn( 𝛽∗⊤𝑥 + 𝛽∗\\n0 ).\\nMilan Vojnović 9/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9eef8e26-8a91-4cc2-bfed-cc5a0391f96d', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Properties of the MMC solution\\n▶ The MMC solution 𝛽∗\\n0 and 𝛽∗ only depend on the subset of training\\nobservations closest to the hyperplane (𝑥𝑖 such that 𝜆∗\\n𝑖 > 0). These\\nobservations are called support vectors.\\n▶ Given any new observation 𝑥𝑛+1, the MMC output 𝜓 SVM(𝑥𝑛+1) depends on\\n𝑥1, … , 𝑥𝑛+1 only through their pairwise inner products 𝑥⊤\\n𝑖 𝑥𝑗,\\n𝑖, 𝑗 ∈ {1, … , 𝑛 + 1}. This can be seen by observing that\\n– (D) and hence dual solution 𝜆∗ depends on 𝑥1, … , 𝑥𝑛 only through\\ntheir inner products\\n– 𝛽∗\\n0 depends only on inner products of 𝑥1, … , 𝑥𝑛\\n– 𝛽∗⊤𝑥𝑛+1 depends only on inner products 𝑥⊤\\n𝑖 𝑥𝑛+1 for 𝑖 = 1, … , 𝑛.\\nMilan Vojnović 10/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='500adc73-2084-40de-b8a1-36b61faf085e', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The non-separable case\\n▶ The two classes are not separable by a hyperplane and so the maximal\\nmarginal classifier cannot be used.\\n▶ When this non-separability is due to a few ‘outliers’, we can use a\\nhyperplane that almost separates the classes, using a so-called soft\\nmargin.\\nMilan Vojnović 11/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5565629d-f7e6-4729-a311-1f3bc5b8a208', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Support vector classifier\\n▶ We allow some observations to be on the incorrect side of the margin, or\\neven the incorrect side of the hyperplane.\\n▶ Such soft-margin MMCs are called Support Vector Classifiers (SVC).\\n▶ Observations on the wrong side of the hyperplane correspond to training\\nobservations that are misclassified by support vector classifier.\\nMilan Vojnović 12/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92a79eaf-e5e7-4a47-b50f-67bde8eec4fb', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVC optimisation\\n▶ SVC may misclassify a few observations. Consider the following\\noptimisation problem:\\nmax\\n𝛽0,𝛽∶‖𝛽‖2=1\\n𝜉∈ℝ𝑛\\n≥0\\n𝑀\\nsubject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 𝑀(1 − 𝜉𝑖) for all 𝑖, and\\n𝑛∑\\n𝑖=1\\n𝜉𝑖 ≤ 𝐶.\\n▶ Here 𝐶 ≥ 0 is a tuning parameter (budget) and the optimal value 𝑀 ∗ is the\\nwidth of the margin.\\n▶ The optimisation can be equivalently written in a penalised form:\\nmin\\n(𝛽0,𝛽,𝜉 )∈ℝ×ℝ𝑝×ℝ𝑛\\n≥0\\n‖𝛽‖2\\n2 + 𝜌\\n𝑛∑\\n𝑖=1\\n𝜉𝑖 subject to 𝑦𝑖(𝛽0 + 𝛽⊤𝑥𝑖) ≥ 1 − 𝜉𝑖 ∀ 𝑖\\n▶ Here 𝜌 ≥ 0 is a tuning hyperparameter (cost).\\nMilan Vojnović 13/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e145d815-2c30-4e35-bd4f-d310ffea48ff', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVC vs logistic regression\\n▶ The SVC optimisation problem can be written in the following form:\\nmin\\n(𝛽0,𝛽)∈ℝ×ℝ𝑝\\n𝑛∑\\n𝑖=1\\n𝓁H(𝑦𝑖, 𝑓(𝑥𝑖; 𝛽0, 𝛽)) + 𝜆‖𝛽‖2\\n2\\nwhere 𝓁H(𝑦, 𝑓) = max{1 − 𝑦𝑓 ,0} is known as the hinge loss function,\\n𝑓 (𝑥; 𝛽0, 𝛽) ∶= 𝛽0 + 𝛽⊤𝑥, and 𝜆 ≥ 0 is a tuneable hyperparameter.\\n▶ The term 𝜆||𝛽||2\\n2 is the ridge penalty term.\\n▶ For logistic regression with the ridge penalty, we have the following\\noptimisation problem:\\nmin\\n(𝛽0,𝛽)∈ℝ×ℝ𝑝\\n𝑛∑\\n𝑖=1\\n𝓁BCE(𝑦𝑖, 𝑓(𝑥𝑖; 𝛽0, 𝛽)) + 𝜆‖𝛽‖2\\n2\\nwhere 𝓁BCE(𝑦, 𝑓) = log(1 + 𝑒−𝑦𝑓 ) is the binary cross-entropy or log loss.\\nMilan Vojnović 14/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='662ef7dd-7cff-4dc9-84bb-14cd5ad5e12e', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVC vs logistic regression (cont’d)\\nMilan Vojnović 15/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='88e7a2c4-fd87-48f6-a5f2-69d557c3ab24', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Properties of SVC\\n▶ Let 𝛽∗\\n0 , 𝛽∗, 𝜉∗ be the optimiser of the above problem, then the support\\nvector classifier is\\n𝜓 SVM(𝑥) = sgn( 𝛽∗⊤𝑥 + 𝛽∗\\n0 ).\\n▶ In the constrained formulation, 𝜉∗\\n𝑖 tells us where the 𝑖th observation is\\nlocated\\n– 𝜉∗\\n𝑖 = 0: the 𝑖th observation is on the correct side of the margin.\\n– 𝜉∗\\n𝑖 > 0: the 𝑖th observation is on the wrong side of the margin.\\n– 𝜉∗\\n𝑖 > 1: the 𝑖th observation is on the wrong side of the hyperplane.\\n▶ The constrained (budget) formulation may not have a solution if 𝐶 is too\\nsmall, while the penalised (cost) formulation always has a solution.\\n▶ The cost 𝜌 reflects our adverseness to observations crossing the margin.\\n– Large 𝜌 (i.e. smaller 𝐶) encourages correct classifications even if that\\nmeans the margin will be smaller.\\n– Small 𝜌 (i.e. large 𝐶) is more tolerant of violations of the margin, we\\neffectively treat more data as ‘outliers’ when constructing a wide\\nmargin.\\nMilan Vojnović 16/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3bc90191-4857-4fb6-bed0-439e8cb762e0', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Harder vs softer margins\\nHarder margin (large 𝜌) Softer margin (small 𝜌)\\ncredit: greitemann.dev/svm-demo\\nMilan Vojnović 17/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4af7577e-c51a-4f3c-b8b7-37a2800d04a1', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='More on properties of SVC\\n▶ Observations that lie directly on the margin, or on the wrong side of the\\nmargin for their class are known as support vectors.\\n▶ SVC depends only on a small subset of the training observations, which\\nmeans it is robust to the behaviour of observations far away from the\\nhyperplane.\\n▶ Similar to MMC, SVC again depends on observations only through their\\ninner products.\\nMilan Vojnović 18/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c170876e-7e38-4b2a-951a-039de84f8bd7', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Hyperplane is determined by support vectors\\nHarder margin (large 𝜌) Softer margin (small 𝜌)\\ncredit: greitemann.dev/svm-demo\\nMilan Vojnović 19/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b2a3408-eceb-4802-a4f8-6b4409928f6f', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The non-separable case (2)\\n▶ The two classes are not separable by a hyperplane and so the maximal\\nmarginal classifier cannot be used.\\n▶ When this non-separability is due to the fact that linear decision boundary\\nis not appropriate, we should try to incorporate such nonlinearity.\\nMilan Vojnović 20/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bb7a4fa-6218-4b17-8bd1-d33ad9a04786', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Support vector machines\\n▶ If we map the original feature space ℝ𝑝 to a higher dimensional feature\\nspace \\ue234 and perform SVC in \\ue234, we obtain nonlinear boundaries in ℝ𝑝.\\n▶ The classifier induced by the SVC on \\ue234 is known as a support vector\\nmachine (SVM) on ℝ𝑝.\\n▶ SVC is a special case of SVM where \\ue234 = ℝ𝑝 (and MMC is a special case of\\nSVC and hence SVM).\\nMilan Vojnović 21/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ddcba627-3270-482e-85e6-e86f2ae114a7', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Basis expansion: an example\\n▶ Enlarge the space of features by including squared terms:\\n(𝑥1, … , 𝑥𝑝) ∈ ℝ𝑝 ↦ (𝑥1, 𝑥2\\n1, 𝑥2, 𝑥2\\n2, … , 𝑥𝑝, 𝑥2\\n𝑝) ∈ ℝ2𝑝 = \\ue234\\n▶ Then the SVC optimisation problem becomes\\nmaximise\\n𝛽0,𝛽11,𝛽12…,𝛽𝑝1,𝛽𝑝2,𝜉1,…,𝜉𝑛\\n𝑀\\nsubject to\\n𝑝∑\\n𝑗=1\\n2∑\\n𝑘=1\\n𝛽2\\n𝑗𝑘 = 1, 𝜉 𝑖 ≥ 0,\\n𝑛∑\\n𝑖=1\\n𝜉𝑖 ≤ 𝐶\\n𝑦𝑖\\n(\\n𝛽0 +\\n𝑝∑\\n𝑗=1\\n𝛽𝑗1𝑥𝑖𝑗 +\\n𝑝∑\\n𝑗=1\\n𝛽𝑗2𝑥2\\n𝑖𝑗\\n)\\n≥ 𝑀(1 − 𝜉𝑖)\\n▶ Nonlinear decision boundaries.\\n▶ We can enlarge the feature space using other basis expansions.\\nMilan Vojnović 22/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='451f4412-4669-4da4-adad-bcb75411c6aa', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Basis expansion: another example\\n▶ Here we use a basis expansion of cubic polynomials\\n(𝑥1, 𝑥2) ∈ ℝ2 ↦ (𝑥1, 𝑥2, 𝑥2\\n1, 𝑥2\\n2, 𝑥1𝑥2, 𝑥3\\n1, 𝑥2\\n1𝑥2, 𝑥1𝑥2\\n2, 𝑥3\\n2) ∈ ℝ9 = \\ue234.\\n▶ The SVC in the enlarged space solves the problem in the lower-dimensional\\nspace to give nonlinear boundaries.\\nMilan Vojnović 23/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2c8767e-28ce-4f46-8003-046e653337ec', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Nonlinearities and kernels\\n▶ Explicitly specifying a large basis can be computationally expensive\\n(especially in high dimensions).\\n▶ The main idea is to enlarge our feature space to accommodate a non-linear\\nboundary between classes. The kernel method is simply an efficient\\ncomputational approach for enacting this idea.\\n▶ Recall that SVC only depends on features through their inner products.\\nMilan Vojnović 24/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='928c1759-4c3f-4cdc-9778-feea4043e3be', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Inner products and support vectors\\n▶ The MMC and SVC can both be represented as\\n𝜓 SVM(𝑥) = sgn\\n(\\n𝛽∗\\n0 +\\n𝑛∑\\n𝑖=1\\n𝜆∗\\n𝑖 𝑦𝑖⟨𝑥, 𝑥𝑖⟩\\n)\\n,\\nwhere the parameters 𝛽∗\\n0 , 𝜆∗\\n1, … , 𝜆∗\\n𝑛 can be computed using 𝑛(𝑛 − 1)∕2\\ninner products ⟨𝑥𝑖, 𝑥𝑖′ ⟩ between all pairs of training observations.\\n▶ It turns out that 𝜆∗\\n𝑖 can be nonzero only for support vectors in the solution.\\nLet \\ue23f be the collection of indices of these support points, then\\n𝜓 SVM(𝑥) = sgn\\n(\\n𝛽∗\\n0 +\\n∑\\n𝑖∈\\ue23f\\n𝜆∗\\n𝑖 𝑦𝑖⟨𝑥, 𝑥𝑖⟩\\n)\\n.\\nMilan Vojnović 25/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a704da6a-bf9f-41b6-b69c-abc3dcf9fae1', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Kernels\\n▶ In basis expansion, we first map the input space to high dimensional\\nfeature space: Φ ∶ ℝ𝑝 → \\ue234. For instance\\nΦ((𝑥1, 𝑥2)) = ( 𝑥1, 𝑥2\\n1, 𝑥1𝑥2, 𝑥2\\n2, 𝑥3\\n1, 𝑥2\\n1𝑥2, 𝑥1𝑥2\\n2, 𝑥3\\n2) ∈ ℝ9 = \\ue234.\\n▶ The function 𝓀 ∶ ℝ𝑝 × ℝ𝑝 → ℝ defined by\\n𝓀(𝑥, 𝑥′) ∶= ⟨Φ(𝑥), Φ(𝑥′)⟩\\nis called the kernel of Φ.\\n▶ Kernels can be interpreted as a similarity measure in the high-dimensional\\nfeature space.\\n▶ Hence SVM classifier for a new observation 𝑥𝑛+1 depends on\\n𝑥1, … , 𝑥𝑛, 𝑥𝑛+1 only through 𝓀(𝑥𝑖, 𝑥𝑖′ ) for 𝑖, 𝑖′ ∈ {1, … , 𝑛 + 1}.\\n▶ In other words, once we have the kernel 𝓀, we no longer need the\\nhigh-dimensional feature space \\ue234!\\nMilan Vojnović 26/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99e844bd-e239-46f3-a022-3332cedd102a', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Reproducing kernel Hilbert space\\n▶ If we use basis expansion, we still need to explicitly construct the\\nhigh-dimensional feature space \\ue234 to compute 𝓀. Can we define 𝓀 without\\nspecifying \\ue234?\\n▶ If 𝓀 ∶ ℝ𝑝 × ℝ𝑝 → ℝ satisfies 𝓀(𝑥, 𝑥′) = 𝓀(𝑥′, 𝑥) and that for any 𝑟 ∈ ℕ and\\n𝑥1, … , 𝑥𝑟 ∈ ℝ𝑝,\\n⎛\\n⎜\\n⎜⎝\\n𝓀(𝑥1, 𝑥1) ⋯ 𝓀 (𝑥1, 𝑥𝑟)\\n⋮ ⋱ ⋮\\n𝓀(𝑥𝑟, 𝑥1) ⋯ 𝓀 (𝑥𝑟, 𝑥𝑟)\\n⎞\\n⎟\\n⎟⎠\\nis positive semidefinite, we say that 𝓀 is a positive definite kernel.\\n▶ Every positive definite kernel 𝓀 is the kernel of some map Φ ∶ ℝ𝑝 → \\ue234 to\\nfeature space \\ue234.\\n▶ The feature space \\ue234 is a reproducing kernel Hilbert space and may be\\ninfinite-dimensional.\\n▶ But we do not need to know what \\ue234 is! It suffices to work with a\\npositive definite kernel 𝓀.\\nMilan Vojnović 27/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c14f31e-bd4e-4027-a6f4-c6392f786c04', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVM defined with kernels\\n▶ Given any positive definite kernel 𝓀, we can solve the SVC optimisation\\nwith 𝓀(𝑥, 𝑥′) replacing the inner product ⟨𝑥, 𝑥′⟩. The resulting classifier is\\nof the form\\n𝜓 SVM(𝑥) = sgn\\n(\\n𝛽∗\\n0 +\\n∑\\n𝑖∈\\ue23f\\n𝛼∗\\n𝑖 𝓀(𝑥, 𝑥𝑖)\\n)\\n,\\nfor the set of support vectors \\ue23f.\\n▶ This agrees with our earlier definition of SVM: 𝜓 SVM is the SVC on the\\nRKHS \\ue234 associated with 𝓀.\\nMilan Vojnović 28/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eb649e1d-7f00-4b3d-9795-775f8571fd6f', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Some popular kernels\\n▶ Linear kernel:\\n𝓀(𝑥, 𝑥′) = 𝑥⊤𝑥′.\\nSVM with a linear kernel is equivalent to SVC.\\n▶ Polynomial kernel:\\n𝓀(𝑥, 𝑥′) = ( 𝑐 + 𝛾𝑥 ⊤𝑥′)𝑑\\n▶ Gaussian kernel (also known as the radial kernel):\\n𝓀(𝑥, 𝑥′) = 𝑒−𝛾‖𝑥−𝑥′‖2\\n2 .\\nThis is probably the most popular kernel.\\n▶ Laplace kernel:\\n𝓀(𝑥, 𝑥′) = 𝑒−𝛾‖𝑥−𝑥′‖1.\\nMilan Vojnović 29/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='03e2fadf-3c36-40e9-bf73-8a91c52ea183', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVM with radial kernel\\n▶ Using the radial kernel, only nearby training observations have an effect on\\nthe class label of test observations.\\n▶ Similarity to 𝑘-NN. But instead of specifying 𝑘, the tuning parameter 𝛾\\ncontrols the effective neighbourhood size.\\n▶ Large 𝛾: more localised prediction; small 𝛾: weights decay slower from\\nnearby observations to far away observations.\\nMilan Vojnović 30/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca80c24e-6276-4897-be18-2765865a13ee', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 30, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Advantages of kernel method\\n▶ Smaller computational cost.\\n▶ It amounts to the fact that using kernels, one only needs to compute\\n𝓀(𝑥𝑖, 𝑥𝑖′ ) for all 𝑛(𝑛 − 1)∕2 distinct pairs 𝑖 and 𝑖′.\\n▶ This is done without explicitly working on the enlarged features space,\\nespecially in the high dimensional 𝑛 < 𝑝 setting (the computation is\\nintractable!)\\nMilan Vojnović 31/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6fc4d99c-b885-41c8-80fd-51264ffb32e6', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 31, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ROC curve for SVM\\n▶ SVM is a linear classifier in the high-dimensional feature space \\ue234.\\n▶ ROC curve is obtained by shifting the classification boundary in \\ue234.\\n▶ Below is the ROC curves on the Heart Data for (left) SVC versus LDA and\\n(right) SVC versus an SVM using a radial basis kernel.\\nMilan Vojnović 32/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c8eba12-f9e9-4ed7-bdaa-2a5e0f04dc25', embedding=None, metadata={'file_name': 'ST443_Lecture_8 (1).pdf', 'page_num': 32, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='SVM on more than two classes\\n▶ One versus All. Fit 𝐾 different 2-class SVM classifiers 𝜓 SVM\\n𝑘 of each class\\n𝑘 (labelled 1) versus the rest (labelled −1). Let ̂𝑓𝑘(𝑥) be the signed distance\\nof 𝑥 to the decision boundary in the RKHS \\ue234 (computable using the\\nkernel). Classify a new observation 𝑥0 to the class for which ̂𝑓𝑘(𝑥0) is the\\nlargest.\\n▶ One versus One. Fit all (𝐾\\n2\\n)pairwise classifiers 𝜓 SVM\\n𝑘,𝑘′ (𝑥) for\\n𝑘, 𝑘′ ∈ {1, … , 𝐾}. Classify 𝑥0 to the class that is most frequently assigned\\namong all pairwise classifications.\\nMilan Vojnović 33/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4a1193f0-bf9c-4678-820a-e5c4808fba4d', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 2: Linear classifiers: LDA and logistic regression\\n8 Oct 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ab2a098-8144-4d02-bb10-0e3c132e215b', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ Decision boundaries of classifiers\\n▶ Discriminant functions\\n▶ Bayes classifiers, Bayes risk and excess risk\\n▶ LDA and its implementation\\n▶ Logistic classifier and its implementation\\n▶ Confusion matrix and ROC curve\\nMilan Vojnović 2/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e25c4294-3f75-4120-9852-ddeab2b3164f', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='General setup of classification problems\\n▶ We observe training data\\n(X1, Y1), . . . ,(Xn, Yn) ∈ Rp × {1, . . . , K}\\nas i. i. d. copies of (X, Y ) ∼ P .\\n▶ Two cases for the joint distribution P :\\n– prior distribution π = (π1, . . . , πK) for class labels and conditional\\ndensities fk for the conditional distribution of X given Y = k for\\nk ∈ {1, . . . , K}.\\n– marginal density fX of X on Rp and regression functions\\nηk(x) := P[Y = k | X = x] for k ∈ {1, . . . , K}.\\n▶ A classifier is a function\\nψ : Rp → {1, . . . , K}\\nthat assigns a data point x ∈ Rp to its label ψ(x) ∈ {1, . . . , K}.\\nMilan Vojnović 3/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aab7a5f7-4361-423d-b3dc-ad216bb16fb9', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Decision boundaries\\n▶ Given a classifier ψ, its preimages\\n\\x08\\nψ−1(k) : k ∈ {1, . . . , K}\\n\\t\\npartition the sample space Rp.\\n▶ The boundaries between these preimages are called decision boundaries.\\n▶ We say a classifier is linear if all decision boundaries are affine.\\n▶ Examples of linear classifiers:\\n– Linear discriminant analysis (LDA)\\n– Logistic classifiers\\n– Support vector machines\\nMilan Vojnović 4/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6df84f5e-929a-4a01-a68d-c21234a4be2d', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Linear vs nonlinear classifiers\\nLDA k-nearest neighbour\\nMilan Vojnović 5/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd47de39-41bb-429c-baa5-2099e175f976', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discriminant function\\n▶ A popular approach for classifier construction is through discriminant\\nfunctions δk(x) for k ∈ {1, . . . , K}.\\n▶ We can construct a classifier via\\nψ(x) = argmax\\nk\\nδk(x).\\n▶ The discriminant δk(x) can be viewed as a ‘likelihood’ that datax belongs\\nto the class k. So it should be positively correlated with the regression\\nfunction ηk(x).\\n▶ Can we just take δk = ηk?\\nMilan Vojnović 6/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d83896f0-a7ea-40df-8497-96f98aad40ba', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Bayes classifier\\n▶ Can we just take δk = ηk?\\n– Yes, this will give you the Bayes classifier!\\n– But no, ηk’s are inaccessible to a learner, so this is not a practical\\nclassifier.\\n▶ For a new data point (X, Y ) ∼ P , the risk of a classifier ψ is\\nR(ψ) = E[1 (ψ(X) ̸= Y )] = P[ψ(X) ̸= Y ]\\n=\\nZ\\nx∈Rp\\nKX\\nk=1\\nηk(x)1 (ψ(x) ̸= k) fX(x)dx.\\n▶ Hence, the Bayes classifier\\nψBayes(x) := argmax\\nk\\nηk(x) = argmax\\nk\\nπkfk(x)PK\\nk′=1 πk′fk′(x)\\nminimises the test risk .\\n▶ Quantify the performance of any classifier ψ in terms of its excess risk:\\nRexcess(ψ) = R(ψ) − R(ψBayes).\\nMilan Vojnović 7/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ef1c4ed3-7821-4097-8ee0-39e30446c1ef', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Discriminant-based linear classifiers\\n▶ The Bayes classifier cannot be computed in practice.\\n▶ We can use training data to estimate the discriminant function of the\\nBayes classifier.\\n▶ Note that by Bayes’ theorem, we can either useηk or πkfk as the\\ndiscriminant function for the Bayes classifier.\\n– LDA: estimating πk and fk assuming that fk is multivariate normal.\\n– Logistic classifier: estimating ηk assuming that the regression\\nfunctions follow a logistic model.\\n▶ Both LDA and logistic classifiers are discriminant-based linear classifiers.\\nMilan Vojnović 8/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8b8bc102-de7b-454e-885c-59bebe0d474e', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Linear discriminant analysis\\n▶ In LDA, we assume fk is the density of Np(µk, Σ)\\nfk(x) = 1\\n(2π)p/2(det Σ)1/2 e− 1\\n2 (x−µk)⊤Σ−1(x−µk).\\n▶ So its Bayes discriminant function can be chosen to be\\nlog(πkfk(x)) = log πk − 1\\n2(x − µk)⊤Σ−1(x − µk) + const.\\n▶ Use training data to estimate\\nˆπk := Nk\\nn , N k is # of data points with label k\\nˆµk := 1\\nNk\\nnX\\ni=1\\nXi1 (Yi = k) (class centroids)\\nˆΣ := 1\\nN − K\\nKX\\nk=1\\nnX\\ni=1\\n(Xi − ˆµk)(Xi − ˆµk)⊤1 (Yi = k).\\nMilan Vojnović 9/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c0fbaa6-c659-4990-95a4-f06b59eceae1', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Linear discriminant analysis\\n▶ Substituting the estimated parameters into the Bayes discriminant\\nfunction, we get the linear discriminant function\\nδLDA\\nk (x) := log ˆπk − 1\\n2(x − ˆµk)⊤ ˆΣ−1(x − ˆµk).\\n▶ The LDA is defined as the linear discriminant function-based classifier\\nψLDA(x) := argmax\\nk\\nδLDA\\nk (x).\\n▶ When comparing δLDA\\nk (x) for various k, the quadratic term cancels, hence\\nthe decision boundary is linear.\\nMilan Vojnović 10/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f00613bb-958b-4e71-82cf-e5a7f22e6cde', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Sphering\\n▶ A useful way to understand LDA is through the linear transformation\\nx 7→ ˆΣ−1/2x, known as sphering.\\n▶ The discriminant function can be re-written as\\nδLDA\\nk (x) := log ˆπk − 1\\n2 ∥ˆΣ−1/2x − ˆΣ−1/2ˆµk∥2\\n2.\\n▶ Hence we classify x to the nearest class centroid in Euclidean distance in\\nthe transformed space, modulo ˆπk.\\nMilan Vojnović 11/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8647a899-f38d-4bfe-ab9f-cbc9cc73f0a5', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LDA as a dimension reduction tool\\n▶ For nearest centroid classification (in the transformed space), we may\\nproject ˆΣ−1/2x onto the affine space spanned by the class centroids first.\\n▶ LDA is a dimension reduction procedure that projects data onto RK−1.\\n▶ This is helpful for data visualisation.\\n– For up to 3 classes, we can visualise in R2\\n– For larger than 3 classes, we can further perform PCA on the\\ntransformed class centroids (known as canonical component analysis)\\nto further reduce dimension.\\nMilan Vojnović 12/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='50c0675d-7524-491d-b2bd-802735002e8c', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example of LDA\\n▶ Consider two class classification problem, assume X ∼ N(µY , σ2), where\\nµ1 = −1.25, µ2 = 1.25 with equal prior probability π1 = π2 = 0.5. What\\nis the Bayes classifier using 0-1 loss function?\\n▶ When the priors are different, we compare πkfk(x). In the right figure, the\\ndecision boundary has shifted to the left.\\n▶ In practice, we estimate π1, π2 and the class centroids from the training\\ndata.\\n▶ How does the excess risk of LDA depend on different Bayes’ and LDA\\ndecision boundaries?\\nMilan Vojnović 13/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89948c63-cdc9-4a50-90cb-f53135e15275', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Example of LDA\\n▶ Consider three class classification problem with equal prior probability\\nπ1 = π2 = π3 = 1/3.\\n▶ The dashed lines are Bayes decision boundaries, which depend only on the\\npopulation distribution and yield lowest misclassification error rates\\namong all possible classifiers.\\n▶ The solid lines are LDA decision boundaries, which are calculated from the\\ntraining data. The further the LDA boundaries deviates from the Bayes\\nboundaries, the larger the excess risk of LDA.\\nMilan Vojnović 14/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8cb576d9-8a9c-4c5a-9b2c-c521c95ec7c0', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Implementing LDA\\n▶ LDA can be implemented using the lda function in R.\\n▶ We use the classical iris dataset compiled by Fisher.\\nsetosa virginica versicolor\\nMilan Vojnović 15/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0e1b1aac-5058-40a3-abe8-fcf96e3c306c', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Classifying iris data using LDA\\nMilan Vojnović 16/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42b93d1c-e74b-41a8-a866-b6af6dfdcdc7', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Logistic classifier\\n▶ For (binary) logistic regression (assuming Xi contains an intercept term)\\nYi ∼ Bern(pi), log\\n\\x12 pi\\n1 − pi\\n\\x13\\n= β⊤X.\\n▶ Given an estimator ˆβ of β, we can estimate pi = P[Yi = 1 | Xi = x] by\\nexpit( ˆβ⊤x) = 1/(1 + e−ˆβ⊤x).\\n▶ expit(x) is the inverse logit function (also known as the logistic\\nsigmoid function)\\n▶ Given data with covariates x, we can therefore classify it based on whether\\nexpit( ˆβ⊤x) exceeds 1/2, or equivalently whether ˆβ⊤x exceeds 0.\\nMilan Vojnović 17/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d061b4fb-308d-4b61-ab79-bb77128ef755', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Credit card default data\\n▶ We let Y = 0 if ‘No’ and 1 if ‘Yes’.\\n▶ Can we use a linear regression of Y on X and classify based on whether\\nˆY > 0.5?\\nMilan Vojnović 18/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e6d0171-fa57-4299-9ff2-b00c0c1edda2', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Linear vs logistic regression\\n▶ Orange marks denote Y = 0 or 1.\\n▶ Although E[Y | X] = P[Y = 1 | X] given binary outcomes, linear\\nregression does not estimate P[Y = 1 | X] well.\\n▶ Logistic regression uses\\np(X) = 1\\n1 + e−βX ∈ (0, 1).\\nMilan Vojnović 19/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12c01f52-0264-4f8e-a067-ae9cd255a9b8', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Logistic classifier\\n▶ We can extend the idea to multiclass classification with K ≥ 2 classes:\\nlog\\n\\x12 P[Yi = k | Xi = x]\\nP[Yi = K | Xi = x]\\n\\x13\\n= β⊤\\nk x, for k = 1, . . . , K − 1\\ni.e. we model class labels as a multinomial distribution with probability\\nvector proportional to (eβ⊤\\n1 x, . . . , eβ⊤\\nK x) (convention: βK = 0).\\n▶ The logistic classifier is\\nψlogit(x) = argmax\\nk\\nˆβ⊤\\nk x.\\n▶ Note: eˆβ⊤\\nk x can be viewed as an estimator of a constant multiple of the\\nBayes discriminant function ηk(x).\\nMilan Vojnović 20/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85ac0af4-6e78-4631-90a9-363ca3766176', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Graphical representation\\n▶ The binary logistic classifier can be viewed as a linear transformation of\\ninput data composed with a nonlinear (expit / sigmoid) transformation.\\n▶ This is actually the basic building block of an artificial neural network.\\nMilan Vojnović 21/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='802b3491-119a-4d64-8cd6-8e4ce1838609', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Graphical representation\\n▶ The binary logistic classifier can be viewed as a linear transformation of\\ninput data composed with a nonlinear (expit / sigmoid) transformation.\\n▶ This is actually the basic building block of an artificial neural network.\\nMilan Vojnović 22/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21d3cfcc-0edd-47a0-b1ad-e00ae79ee083', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Graphical representation\\n▶ The binary logistic classifier can be viewed as a linear transformation of\\ninput data composed with a nonlinear (expit / sigmoid) transformation.\\n▶ This is actually the basic building block of an artificial neural network.\\nMilan Vojnović 23/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d99852a8-3533-4b27-b141-bb1cbc1d982d', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Graphical representation\\n▶ The multinomial logistic classifier can be viewed as a linear transformation\\nof input data composed with a nonlinear (expit / sigmoid) transformation\\nmapping to a probability vector.\\n▶ This nonlinear transformation is also known as the softmax operation.\\nMilan Vojnović 24/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f8282948-9895-4ce8-bd9e-0b2b40ee706e', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Implementing the logistic classifier\\n▶ We estimate the coefficients β1, . . . , βK by maximising the likelihood\\nfunction.\\n▶ The log-likelihood function is\\nℓ(β1, . . . , βK−1) =\\nnX\\ni=1\\nlog\\n\\x12 eβ⊤\\nYi Xi\\nPK\\nk′=1 eβ⊤\\nk′ Xi\\n\\x13\\n,\\nwhere βK = 0.\\n▶ Maximum likelihood estimator:\\n( ˆβ1, . . . , ˆβK−1) := argmax\\nβ1,...,βK−1\\nℓ(β1, . . . , βK−1).\\n▶ Given any new data X, classify using the discriminant function\\nψlogit(X) := argmax\\nk\\nˆβ⊤\\nk X.\\n▶ This can be done using the nnet::multinom function in R and\\nsklearn.linear_model.LogisticRegression in python.\\nMilan Vojnović 25/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='20f9e9d4-48d1-4156-84ee-2a722492137d', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Classifying iris data using logistic classifier\\nMilan Vojnović 26/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6006d7e4-3142-40ce-a1ba-6f4e9d326113', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Measuring the quality of a classifier\\n▶ Using the misclassification error rate on a test set.\\n▶ Use the confusion matrix for a more refined understanding of the\\nmisclassification errors.\\nMilan Vojnović 27/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='505717fc-c888-4add-8076-9546f09420c5', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='FPR and FNR of a binary classifier\\n▶ In a binary classification problem, the label/outcome can often be\\ninterpreted as ‘Yes/No’ or ’Positive/Negative’.\\n▶ Entries of the confusion matrix have special meaning\\nFigure: Confusion matrix of the Credit Data using the logistic classifier\\n▶ False positive rate (FPR): under true No’s, we make23/9667 ≈ 0.2% errors.\\n▶ False negative rate (FNR): under true Yes’s we make252/333 ≈ 75.7%\\nerrors.\\n▶ Perhaps we should not use 50% as threshold for predicting default?\\nMilan Vojnović 28/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f46b050f-8188-4d36-b3b5-3ec194b745fc', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='FPR and FNR of a binary classifier\\n▶ If we move the affine decision boundary of a linear classifier along its\\nnormal vector, we can trade-off FPR with FNR.\\n▶ This amounts to adding a constant to the discriminant function.\\n– For LDA, this is the same as modifying the prior distribution.\\n– For logistic classifier, this means changing the intercept term of the\\nestimated regression coefficients.\\n▶ Plotting TPR = 1 − FNR against the FPR as we move the decision\\nboundary, we arrive at the ROC (receiver operating characteristics) curve.\\nMilan Vojnović 29/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98937730-53df-4721-bb35-c77c9134e522', embedding=None, metadata={'file_name': 'ST443_Lecture_2.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ROC curve\\nFigure: ROC curve of the Credit data using logistic classifiers\\n▶ ROC curves provide a graphical illustration of two types of errors for all\\npossible thresholds.\\n▶ The performance of a (sequence of) linear binary classifier can also be\\nquantified using the area under the curve (AUC) .\\n▶ Larger AUC means better overall performance\\n(see HW2 for an interpretation).\\nMilan Vojnović 30/30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d449ab95-6be2-4a19-89b7-e08d8e204946', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 0, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ST443: Machine Learning and Data Mining\\nMilan Vojnović\\nDepartment of Statistics\\nLondon School of Economics and Political Science\\nLecture 1: Basics of statistical machine learning\\n1 Oct 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83f10b60-c80e-4842-b017-25bf6eb5cb32', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 1, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Basic information\\n▶ Office hours:\\nTuesdays, 12–1 PM, Room 8.11, Columbia House (except reading week)\\nBook via StudentHub\\n▶ Teaching assistants:\\nPhil Chan Anica Kostić\\nMilan Vojnović 2/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0442e0ac-759f-4cc3-8adc-7fca6e41b732', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 2, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Evaluation\\n▶ The course grade will be made up from\\n– 70%: 2-hour exam (Summer Term)\\n– 30%: Group project (Autumn Term)\\n– 0%: 7 problem sets\\n▶ Group project\\n– Apply machine learning techniques on real data.\\n– Implement and evaluate advanced machine learning methods on data.\\n– Report (11th week).\\nMilan Vojnović 3/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='707ad655-bb3a-4354-9251-17e996758d4b', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 3, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Software\\n▶ There are many possible statistical software (or data mining) packages that\\none could use.\\n– They are generally very expensive.\\n▶ We will use R in class.\\n– For group project and homework you may use either R or Python.\\nMilan Vojnović 4/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b48410e2-f22e-47eb-8d24-4ae94f3ae27b', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 4, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Course Materials\\n▶ Main textbook: An Introduction to Statistical Learning with Applications in R\\n(ISLR). Free download: https://www.statlearning.com/\\n▶ There is also a version of this book with applications in Python.\\nMilan Vojnović 5/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7386e34-3c7a-4abd-9e70-3257e986409b', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 5, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Course Materials\\n▶ Highly recommended book: The Elements of Statistical Learning (ESL).\\nFree download: http://statweb.stanford.edu/ tibs/ElemStatLearn/\\nMilan Vojnović 6/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7c971e04-6da1-4f2c-9f32-866af2da86f7', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 6, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Computer workshops (Very Important!)\\n▶ This is a machine learning course focusing on theory and methods.\\n▶ We will spend 90 minutes per week on computer workshops.\\n▶ For computer workshops, we will work through, in R, the statistical\\nmethods that we have learned about in lectures.\\n▶ The computer workshops are very relevant for the group project.\\n▶ Solutions to some homework problems will also be discussed.\\nMilan Vojnović 7/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3a9108e5-3f5f-446e-90b9-996fcfab377d', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 7, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Some relevant courses at LSE\\n▶ ST444 Computational Data Science, 2024 AT.\\n▶ ST449 Artificial Intelligence, 2024 AT.\\n▶ ST451 Bayesian Machine Learning, 2025 WT.\\n▶ ST455 Reinforcement Learning, 2024 WT.\\n▶ ST456 Deep Learning, 2024 WT.\\n▶ MY474 Applied Machine Learning for Social Science, 2024 AT.\\n▶ MA429 Algorithmic Techniques for Data Mining, 2024 WT.\\nMilan Vojnović 8/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cde52219-61d3-4a9b-9a75-a5b9190fb794', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 8, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Course prerequisite\\n▶ A solid foundation on linear models.\\n▶ To review relevant concepts, you may use the course notes for ST300 or\\nChapter 3 of ISL.\\n▶ Topics include\\n– Simple and multiple linear regression\\n– Closed form expression and distribution of the least squares estimators\\n– Hypothesis testing and confidence intervals\\n– Model checking\\n– Interactions and nonlinearity\\nMilan Vojnović 9/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='27625293-8550-4a4b-b1f1-2490b89058e6', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 9, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Course outline\\n▶ Ten lectures:\\n1. Basics of statistical machine learning\\n2. Linear classifiers: LDA and logistic regression\\n3. QDA and nearest neighbour classifiers\\n4. Cross validation and bootstrap\\n5. Linear model selection and regularisation\\n6. Tree-based methods\\n7. Boosting algorithms\\n8. Support vector machines\\n9. Dimension reduction\\n10. Clustering algorithms\\nMilan Vojnović 10/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2aa149f6-796d-451e-9b96-115def63286f', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 10, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Some Interesting Data Sets\\n▶ Income classification\\n▶ Credit screening\\n▶ Predicting housing price\\n▶ Response to direct mailings\\n▶ Predicting a baseball player’s salary\\n▶ Predicting mpg for a car\\n▶ Predicting risk of heart disease\\n▶ Handwritten digit recognition\\n▶ Spam email classification\\n▶ Disease subtyping based on genetic information\\n▶ ...\\nWe may not have time to cover every data set in class. However, you may want\\nto practice working with data not covered in class.\\nMilan Vojnović 11/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='046824a7-3d3b-4edd-b100-597c58ee5e24', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 11, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Boston housing data\\n▶ Aim to predict median value of homes based on crime rate, zoning, location\\nto river, air quality, schools, etc.\\n0 20 40 60 80\\n−20 −15 −10 −5 0\\ncrim\\ns(crim)\\n0 20 40 60 80 100\\n−2 0 2 4 6\\nzn\\ns(zn)\\n0 5 10 15 20 25\\n−4 −3 −2 −1 0 1 2\\nindus\\ns(indus)\\n0.4 0.5 0.6 0.7 0.8\\n−8 −6 −4 −2 0 2 4\\nnox\\ns(nox)\\n4 5 6 7 8\\n0 5 10 15\\nrm\\ns(rm)\\n0 20 40 60 80 100\\n−2 −1 0 1 2\\nage\\ns(age)\\n2 4 6 8 10 12\\n−10 −5 0 5\\ndis\\ns(dis)\\n0 5 10 15 20 25\\n−6 −4 −2 0 2 4 6\\nrad\\ns(rad)\\n200 300 400 500 600 700\\n−4 −2 0 2 4 6\\ntax\\ns(tax)\\n14 16 18 20 22\\n−4 −2 0 2 4 6 8\\nptratio\\ns(ptratio)\\n0 100 200 300 400\\n−4 −3 −2 −1 0 1 2\\nb\\ns(b)\\n10 20 30\\n−10 −5 0 5 10\\nlstat\\ns(lstat)\\nMilan Vojnović 12/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f249c3af-ffdd-4f56-afe4-5e2180e7852b', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 12, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Spam detection\\n▶ Data consists of 4600 emails sent to an individual (named George, at HP\\nlabs at early years). Each is labeled spam or email.\\n▶ Features: relative frequencies of 57 of commonly occurring words and\\npunctuation marks.\\n▶ Aim to build a customized spam filter.\\nMilan Vojnović 13/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='69120b72-9136-428f-b902-f432534e17bd', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 13, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Handwritten digit recognition\\n▶ Multiple hidden layer neural networks (deep learning) have been used to\\nread handwritten zip codes on US Mail.\\n▶ This is a classification problem with 256 predictors (gray scale values on 16\\nby 16 grids) and 10 possible categories.\\n−10 −5 0 5 10\\n−10 −5 0 5 10\\nLD1\\nLD2\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nLDA error = 16.9%\\nMilan Vojnović 14/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c0f7758-a551-4673-a276-284169c7d460', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 14, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Genome data\\n▶ Aim to classify a tissue sample into one of several cancer classes based on a\\ngene expression profile.\\nMilan Vojnović 15/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf3016f0-9c17-41d3-969d-7391a4c76cdd', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 15, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Big Data in Statistics\\n▶ What is Big Data from a statistical perspective?\\nn : number of observations; p: number of features (dimensionality).\\n1. Big n: Large sample size — requiring machine learning techniques to\\nefficiently process data (e.g. low complexity algorithms or distributed\\ncomputing).\\n2. Big p: High dimensional statistics — require assumption of some special\\nstructures in data and innovation in statistical procedures.\\nMilan Vojnović 16/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f082621-828d-4eb9-9784-9dbb15b2e056', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 16, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Big Data in Statistics\\n▶ What is Big Data from a statistical perspective?\\nn : number of observations; p: number of features (dimensionality).\\n1. Big n: Large sample size — requiring machine learning techniques to\\nefficiently process data (e.g. low complexity algorithms or distributed\\ncomputing).\\n2. Big p: High dimensional statistics — require assumption of some special\\nstructures in data and innovation in statistical procedures.\\nMilan Vojnović 16/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='daffe1e9-c74b-4b65-b4da-08ae431f02c4', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 17, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Lecture 1: Basics of statistical machine learning', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01d66237-5308-4c7e-a035-ebfaaee5e2c5', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 18, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Key learning points\\n▶ What is statistical learning?\\n▶ Supervised learning vs unsupervised learning\\n▶ Regression vs classification\\n▶ How to measure the performance of a statistical learning algorithm?\\n▶ Bias and variance trade-offs\\nMilan Vojnović 18/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0ab4457-9bf2-4417-875e-e3104af7c503', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 19, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What is statistical learning?\\n▶ A process of extracting patterns, features and making useful predictions\\nbased on data.\\n▶ Data are assumed to be generated from a random process, hence any\\nconclusion drawn is probabilistic.\\n– Larger amount of data, or better-quality data lead to more accurate\\ninference.\\n– Or we can use better learning algorithms.\\n▶ Examples\\n– Predicting cancer survival chance based on patient demographics\\n– Forecasting economic growth in the next few years\\n– Distinguish vehicles and pedestrians from camera images on\\nself-driving cars\\n– Discover new cell subtypes based on their RNA expression level data\\nMilan Vojnović 19/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4d74de5e-864c-4af3-b6ba-d38759e70379', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 20, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='What is statistical learning?\\n▶ Y : dependent variable, response variable, output.\\n▶ X = (X1, . . . , Xp)⊤, regressors, covariates, features, independent\\nvariables, inputs.\\n▶ We can model the relationship as\\nY = f (X) + ε,\\nwhere f is an unknown function and ε captures measurement error\\n(randomness) with mean zero.\\n▶ We use the training data (X1, Y1), . . . ,(Xn, Yn) to estimate f.\\nMilan Vojnović 20/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb9ab4e2-6242-496a-876e-df28b32518cc', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 21, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The function f (x)\\n▶ Is f (x) = E[Y | X = x] optimal predictor of Y with regard to\\nmean-squared prediction error?\\n▶ Yes, f (x) = E[Y | X = x] is the function that minimises\\nE\\n\\x02\\n(Y − g(X))2 | X = x\\n\\x03\\nover all functions g at all points X = x.\\n▶ ε = Y − f (x) is the irreducible error, i.e. even if we knew f (x), we would\\nstill make errors in prediction.\\nMilan Vojnović 21/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e692a6ad-b519-4d91-9715-4f36728328a3', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 22, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Supervised learning\\n▶ Supervised learning:\\n1. Regression: Y is continuous/numerical.\\n2. Classification: Y is categorical.\\n▶ We will deal with both problems.\\n– Some methods work well on both types, e.g. Neural Networks.\\n– Other methods work best on Regression, e.g. Linear Regression, or on\\nClassification, e.g. linear discriminant analysis.\\n▶ Supervised learning is all about how to estimate f. Two reasons for\\nestimating f:\\n– Prediction (predictive accuracy).\\n– Inference (estimation accuracy + uncertainty).\\nMilan Vojnović 22/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4cad29c8-66bb-477b-b38a-d8246acfb84c', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 23, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Supervised learning\\n▶ Supervised learning:\\n1. Regression: Y is continuous/numerical.\\n2. Classification: Y is categorical.\\n▶ We will deal with both problems.\\n– Some methods work well on both types, e.g. Neural Networks.\\n– Other methods work best on Regression, e.g. Linear Regression, or on\\nClassification, e.g. linear discriminant analysis.\\n▶ Supervised learning is all about how to estimate f. Two reasons for\\nestimating f:\\n– Prediction (predictive accuracy).\\n– Inference (estimation accuracy + uncertainty).\\nMilan Vojnović 22/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dde07710-a9fc-4753-84ec-d49341e03918', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 24, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unsupervised Learning\\n▶ No outcome variable, just a set of covariates measured on a set of samples.\\n▶ The goal is to extract useful summarising features based on data.\\n– Clustering: Find groups of samples that behave similarly;\\n– Principal component analysis (PCA): Find linear combinations of\\nfeatures with the most variation.\\n▶ Example: Market segmentation: we try to divide potential customers into\\ngroups based on their characteristics.\\n▶ Difficult to know how well you are doing.\\n▶ We will consider unsupervised learning at the end of this course.\\nMilan Vojnović 23/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b13f2c7f-fc6b-43cd-adbd-ad7cd6ce7d24', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 25, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A simple clustering example\\nMilan Vojnović 24/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0aee1f80-6faf-4360-94da-eb30690be77d', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 26, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Statistical learning vs machine learning\\n▶ Machine learning is a subfield of artificial intelligence.\\n– ML focused on teaching a machine how to perform a specific\\ntask and provide accurate results by identifying patterns.\\n– AI emphasise on the general ability of computers to emulate human\\nthought and perform tasks in real-world environments.\\n▶ Statistical learning is a field of study within statistics and machine\\nlearning.\\n– SL emphasis more on building statistical models from data,\\ninterpreting the model and understanding their estimation\\naccuracy and uncertainty.\\nMilan Vojnović 25/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc01b2ad-9c11-436a-ae17-54d2e125dd9e', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 27, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Statistical learning vs machine learning\\n▶ Machine learning is a subfield of artificial intelligence.\\n– ML focused on teaching a machine how to perform a specific\\ntask and provide accurate results by identifying patterns.\\n– AI emphasise on the general ability of computers to emulate human\\nthought and perform tasks in real-world environments.\\n▶ Statistical learning is a field of study within statistics and machine\\nlearning.\\n– SL emphasis more on building statistical models from data,\\ninterpreting the model and understanding their estimation\\naccuracy and uncertainty.\\nMilan Vojnović 25/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0951b1f7-e582-4192-8911-2596d51793bd', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 28, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Loss and risk\\n▶ We use a loss function, ℓ to quantify the prediction accuracy. We consider\\nmainly two types of loss functions\\n1. ℓ2 loss function for regression:\\nℓ(Y, f (X)) =\\n\\x00\\nY − f (X)\\n\\x012\\n.\\n2. 0-1 loss function for classification:\\nℓ(Y, f (X)) = 1\\n\\x00\\nY ̸= f (X)\\n\\x01\\n▶ For a given loss function, ℓ, the risk of a learning function f is defined by\\nthe expected loss\\nR(f ) = EX,Y [ℓ(Y, f (X))].\\n▶ We aim to find f (x) that minimises R(f ) pointwise, the solution is, e.g.\\n1. ℓ2 loss: f (x) = E[Y |X = x].\\n2. ℓ1 loss: f (x) = median[Y | X = x].\\nMilan Vojnović 26/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f81ef2d-3cd7-44fb-82bb-9ef3a1c71089', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 29, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Measuring the quality of fit to data\\n▶ Suppose we observe i.i.d. training data (X1, Y1), . . . ,(Xn, Yn). The\\nempirical risk of any function f is\\nRn(f ) = 1\\nn\\nnX\\ni=1\\nℓ(Yi, f(Xi)).\\n▶ We can find the empirical risk minimiser ˆf, which minimises Rn(f ).\\nThe training error of the empirical risk minimiser is Rn( ˆf ).\\n▶ Regression: Mean squared error (MSE) is given by\\nMSE = 1\\nn\\nnX\\ni=1\\n(Yi − ˆf (Xi))2.\\n▶ Classification: Misclassification error rate (MER) is given by\\nMER = 1\\nn\\nnX\\ni=1\\n1\\n\\x00\\nYi ̸= ˆf (Xi)\\n\\x01\\n.\\n▶ In either case, our method has generally been designed to make MSE or the\\nMER small on the training data we are looking at.\\nMilan Vojnović 27/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37250927-9b1d-48d7-b875-b9ac96e2b9a7', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 30, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Test errors\\n▶ What we really care about it how well the method works on new data\\n( ˜X1, ˜Y1), . . . ,( ˜Xm, ˜Ym). We call this new data test data.\\n▶ We aim to choose the method that gives the lowest test MSE or MER for\\nregression and classification problems, respectively.\\ntest MSE = 1\\nm\\nmX\\ni=1\\n( ˜Yi − ˆf ( ˜Xi))2.\\n▶ Importantly, ˆf is independent of the test data, so test MSE is a more\\naccurate approximation of the true risk of ˆf.\\nMilan Vojnović 28/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2038075-aab2-4db1-83d0-65cafa7c8378', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 31, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Flexibility vs interpretability\\n▶ For more complex or flexible models, we need to be especially aware of the\\ndistinction between training and test errors.\\n▶ Which one is more flexible?\\n1. Parametric model vs non-parametric model.\\n2. Linear model vs non-linear model.\\n3. Linear model with 10 features vs linear model with 100 features.\\nFigure: A representation of the trade-offs using different statistical learning methods. In\\ngeneral, as the flexibility of a method increases, its interpretability decreases.\\nMilan Vojnović 29/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='997d5302-21e6-48bb-82cf-5c6dfa4a48f7', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 32, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Training vs testing errors\\n▶ In general, the more flexible a method is the lower its training error rate\\nwill be, i.e. it will “ fit\" or explain the training data very well.\\n▶ However, the test error rate may in fact be higher for a more flexible\\nmethod than for a simple approach like linear regression.\\nMilan Vojnović 30/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='08b96f1c-d182-49ae-b6b3-e43bff7c1eaf', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 33, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Training vs testing errors\\n▶ In general, the more flexible a method is the lower its training error rate\\nwill be, i.e. it will “ fit\" or explain the training data very well.\\n▶ However, the test error rate may in fact be higher for a more flexible\\nmethod than for a simple approach like linear regression.\\nMilan Vojnović 30/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8724af91-ed23-45fe-a805-7604a96360a4', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 34, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bias-variance tradeoff\\n▶ Assume the true model is Y = f (X) + ε, where f (x) = E[Y | X = x].\\n▶ Suppose we fit a model ˆf based on training data and let (x0, Y0) be a test\\nobservation from the population. Then the expected test MSE at x0 is\\nE[(Y0 − ˆf (x0))2] = Bias( ˆf (x0))2 + var[ ˆf (x0)] + var[ε],\\nwhere Bias( ˆf (x0)) = E[ ˆf (x0)] − f (x0).\\n▶ As the flexibility of ˆf increases, its variance increases and its bias\\ndecreases. This corresponds to the bias-variance tradeoff.\\n▶ To minimize E[(Y0 − ˆf (x0))2], we need to select a statistical learning\\nmethod that simultaneously achieves relatively low variance and low bias.\\nMilan Vojnović 31/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='70ca922e-6325-47a5-a6e0-89004955b852', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 35, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Test error, bias and variance\\nMilan Vojnović 32/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='807db024-308c-43a5-b2f3-824b43d1dd0b', embedding=None, metadata={'file_name': 'ST443_Lecture_1.pdf', 'page_num': 36, 'doc_type': 'lecture', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A fundamental picture\\nWe must keep this picture in mind when choosing a learning method.\\nMore flexible/complicated one is not always better!\\nMilan Vojnović 33/33', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='777f7d2e-e694-4ea9-961c-a15f66f044d5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 0, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='MATHEMATICS  FOR \\nMACHINE LEAR NING\\nMarc Peter Deisenroth\\nA. Aldo Faisal\\nCheng Soon On g\\nMATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\\nThe fundamental mathematical tools needed to understand machine \\nlearning include linear algebra, analytic geometry, matrix decompositions, \\nvector calculus, optimization, probability and statistics. These topics \\nare traditionally taught in disparate courses, making it hard for data \\nscience or computer science students, or professionals, to efﬁ  ciently learn \\nthe mathematics. This self-contained textbook bridges the gap between \\nmathematical and machine learning texts, introducing the mathematical \\nconcepts with a minimum of prerequisites. It uses these concepts to \\nderive four central machine learning methods: linear regression, principal \\ncomponent analysis, Gaussian mixture models and support vector machines. \\nFor students and others with a mathematical background, these derivations \\nprovide a starting point to machine learning texts. For those learning the \\nmathematics for the ﬁ  rst time, the methods help build intuition and practical \\nexperience with applying mathematical concepts. Every chapter includes \\nworked examples and exercises to test understanding. Programming \\ntutorials are offered on the book’s web site.\\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine \\nLearning at the Department of Computing, Împerial College London.\\nA. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College \\nLondon, where he is also Reader in Neurotechnology at the Department of \\nBioengineering and the Department of Computing.\\nCHENG SOON ONG  is Principal Research Scientist at the Machine Learning \\nResearch Group, Data61, CSIRO. He is also Adjunct Associate Professor at \\nAustralian National University.\\nCover image courtesy of Daniel Bosma / Moment / Getty Images\\nCover design by Holly Johnson\\nDeisenrith et al. 9781108455145 Cover. C M Y K', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='68b6d783-4a22-4d3c-b9ea-3c857534f27c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 1, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8531082c-a5f4-4eca-b98c-bb648fb7a5b5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 2, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents\\nForeword 1\\nPart I Mathematical Foundations 9\\n1 Introduction and Motivation 11\\n1.1 Finding Words for Intuitions 12\\n1.2 Two Ways to Read This Book 13\\n1.3 Exercises and Feedback 16\\n2 Linear Algebra 17\\n2.1 Systems of Linear Equations 19\\n2.2 Matrices 22\\n2.3 Solving Systems of Linear Equations 27\\n2.4 Vector Spaces 35\\n2.5 Linear Independence 40\\n2.6 Basis and Rank 44\\n2.7 Linear Mappings 48\\n2.8 Affine Spaces 61\\n2.9 Further Reading 63\\nExercises 64\\n3 Analytic Geometry 70\\n3.1 Norms 71\\n3.2 Inner Products 72\\n3.3 Lengths and Distances 75\\n3.4 Angles and Orthogonality 76\\n3.5 Orthonormal Basis 78\\n3.6 Orthogonal Complement 79\\n3.7 Inner Product of Functions 80\\n3.8 Orthogonal Projections 81\\n3.9 Rotations 91\\n3.10 Further Reading 94\\nExercises 96\\n4 Matrix Decompositions 98\\n4.1 Determinant and Trace 99\\ni\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d3762f9e-2b09-4557-8c46-064ff1f563cc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 3, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ii Contents\\n4.2 Eigenvalues and Eigenvectors 105\\n4.3 Cholesky Decomposition 114\\n4.4 Eigendecomposition and Diagonalization 115\\n4.5 Singular Value Decomposition 119\\n4.6 Matrix Approximation 129\\n4.7 Matrix Phylogeny 134\\n4.8 Further Reading 135\\nExercises 137\\n5 Vector Calculus 139\\n5.1 Differentiation of Univariate Functions 141\\n5.2 Partial Differentiation and Gradients 146\\n5.3 Gradients of Vector-Valued Functions 149\\n5.4 Gradients of Matrices 155\\n5.5 Useful Identities for Computing Gradients 158\\n5.6 Backpropagation and Automatic Differentiation 159\\n5.7 Higher-Order Derivatives 164\\n5.8 Linearization and Multivariate Taylor Series 165\\n5.9 Further Reading 170\\nExercises 170\\n6 Probability and Distributions 172\\n6.1 Construction of a Probability Space 172\\n6.2 Discrete and Continuous Probabilities 178\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\n6.4 Summary Statistics and Independence 186\\n6.5 Gaussian Distribution 197\\n6.6 Conjugacy and the Exponential Family 205\\n6.7 Change of Variables/Inverse Transform 214\\n6.8 Further Reading 221\\nExercises 222\\n7 Continuous Optimization 225\\n7.1 Optimization Using Gradient Descent 227\\n7.2 Constrained Optimization and Lagrange Multipliers 233\\n7.3 Convex Optimization 236\\n7.4 Further Reading 246\\nExercises 247\\nPart II Central Machine Learning Problems 249\\n8 When Models Meet Data 251\\n8.1 Data, Models, and Learning 251\\n8.2 Empirical Risk Minimization 258\\n8.3 Parameter Estimation 265\\n8.4 Probabilistic Modeling and Inference 272\\n8.5 Directed Graphical Models 278\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6d395bc4-6d3a-4883-80a0-da7fdf6ddda0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 4, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents iii\\n8.6 Model Selection 283\\n9 Linear Regression 289\\n9.1 Problem Formulation 291\\n9.2 Parameter Estimation 292\\n9.3 Bayesian Linear Regression 303\\n9.4 Maximum Likelihood as Orthogonal Projection 313\\n9.5 Further Reading 315\\n10 Dimensionality Reduction with Principal Component Analysis 317\\n10.1 Problem Setting 318\\n10.2 Maximum Variance Perspective 320\\n10.3 Projection Perspective 325\\n10.4 Eigenvector Computation and Low-Rank Approximations 333\\n10.5 PCA in High Dimensions 335\\n10.6 Key Steps of PCA in Practice 336\\n10.7 Latent Variable Perspective 339\\n10.8 Further Reading 343\\n11 Density Estimation with Gaussian Mixture Models 348\\n11.1 Gaussian Mixture Model 349\\n11.2 Parameter Learning via Maximum Likelihood 350\\n11.3 EM Algorithm 360\\n11.4 Latent-Variable Perspective 363\\n11.5 Further Reading 368\\n12 Classification with Support Vector Machines 370\\n12.1 Separating Hyperplanes 372\\n12.2 Primal Support Vector Machine 374\\n12.3 Dual Support Vector Machine 383\\n12.4 Kernels 388\\n12.5 Numerical Solution 390\\n12.6 Further Reading 392\\nReferences 395\\nIndex 407\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='010f3d01-df3c-42ee-a622-6f81da45cd37', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 5, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='072c564a-f0a8-47d6-8586-1b14e1d88ebe', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 6, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foreword\\nMachine learning is the latest in a long line of attempts to distill human\\nknowledge and reasoning into a form that is suitable for constructing ma-\\nchines and engineering automated systems. As machine learning becomes\\nmore ubiquitous and its software packages become easier to use, it is nat-\\nural and desirable that the low-level technical details are abstracted away\\nand hidden from the practitioner. However, this brings with it the danger\\nthat a practitioner becomes unaware of the design decisions and, hence,\\nthe limits of machine learning algorithms.\\nThe enthusiastic practitioner who is interested to learn more about the\\nmagic behind successful machine learning algorithms currently faces a\\ndaunting set of pre-requisite knowledge:\\nProgramming languages and data analysis tools\\nLarge-scale computation and the associated frameworks\\nMathematics and statistics and how machine learning builds on it\\nAt universities, introductory courses on machine learning tend to spend\\nearly parts of the course covering some of these pre-requisites. For histori-\\ncal reasons, courses in machine learning tend to be taught in the computer\\nscience department, where students are often trained in the first two areas\\nof knowledge, but not so much in mathematics and statistics.\\nCurrent machine learning textbooks primarily focus on machine learn-\\ning algorithms and methodologies and assume that the reader is com-\\npetent in mathematics and statistics. Therefore, these books only spend\\none or two chapters on background mathematics, either at the beginning\\nof the book or as appendices. We have found many people who want to\\ndelve into the foundations of basic machine learning methods who strug-\\ngle with the mathematical knowledge required to read a machine learning\\ntextbook. Having taught undergraduate and graduate courses at universi-\\nties, we find that the gap between high school mathematics and the math-\\nematics level required to read a standard machine learning textbook is too\\nbig for many people.\\nThis book brings the mathematical foundations of basic machine learn-\\ning concepts to the fore and collects the information in a single place so\\nthat this skills gap is narrowed or even closed.\\n1\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56e7f5d5-73b3-4ac9-be91-7e2c0c224dd1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 7, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Foreword\\nWhy Another Book on Machine Learning?\\nMachine learning builds upon the language of mathematics to express\\nconcepts that seem intuitively obvious but that are surprisingly difficult\\nto formalize. Once formalized properly , we can gain insights into the task\\nwe want to solve. One common complaint of students of mathematics\\naround the globe is that the topics covered seem to have little relevance\\nto practical problems. We believe that machine learning is an obvious and\\ndirect motivation for people to learn mathematics.\\nThis book is intended to be a guidebook to the vast mathematical lit-\\nerature that forms the foundations of modern machine learning. We mo-“Math is linked in\\nthe popular mind\\nwith phobia and\\nanxiety . You’d think\\nwe’re discussing\\nspiders.” (Strogatz,\\n2014, page 281)\\ntivate the need for mathematical concepts by directly pointing out their\\nusefulness in the context of fundamental machine learning problems. In\\nthe interest of keeping the book short, many details and more advanced\\nconcepts have been left out. Equipped with the basic concepts presented\\nhere, and how they fit into the larger context of machine learning, the\\nreader can find numerous resources for further study , which we provide at\\nthe end of the respective chapters. For readers with a mathematical back-\\nground, this book provides a brief but precisely stated glimpse of machine\\nlearning. In contrast to other books that focus on methods and models\\nof machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-\\nber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers\\nand Girolami, 2016) or programmatic aspects of machine learning (M¨uller\\nand Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),\\nwe provide only four representative examples of machine learning algo-\\nrithms. Instead, we focus on the mathematical concepts behind the models\\nthemselves. We hope that readers will be able to gain a deeper understand-\\ning of the basic questions in machine learning and connect practical ques-\\ntions arising from the use of machine learning with fundamental choices\\nin the mathematical model.\\nWe do not aim to write a classical machine learning book. Instead, our\\nintention is to provide the mathematical background, applied to four cen-\\ntral machine learning problems, to make it easier to read other machine\\nlearning textbooks.\\nWho Is the Target Audience?\\nAs applications of machine learning become widespread in society , we\\nbelieve that everybody should have some understanding of its underlying\\nprinciples. This book is written in an academic mathematical style, which\\nenables us to be precise about the concepts behind machine learning. We\\nencourage readers unfamiliar with this seemingly terse style to persevere\\nand to keep the goals of each topic in mind. We sprinkle comments and\\nremarks throughout the text, in the hope that it provides useful guidance\\nwith respect to the big picture.\\nThe book assumes the reader to have mathematical knowledge commonly\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='648e7380-3909-45d1-b08b-2c04e6690c59', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 8, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foreword 3\\ncovered in high school mathematics and physics. For example, the reader\\nshould have seen derivatives and integrals before, and geometric vectors\\nin two or three dimensions. Starting from there, we generalize these con-\\ncepts. Therefore, the target audience of the book includes undergraduate\\nuniversity students, evening learners and learners participating in online\\nmachine learning courses.\\nIn analogy to music, there are three types of interaction that people\\nhave with machine learning:\\nAstute Listener The democratization of machine learning by the pro-\\nvision of open-source software, online tutorials and cloud-based tools al-\\nlows users to not worry about the specifics of pipelines. Users can focus on\\nextracting insights from data using off-the-shelf tools. This enables non-\\ntech-savvy domain experts to benefit from machine learning. This is sim-\\nilar to listening to music; the user is able to choose and discern between\\ndifferent types of machine learning, and benefits from it. More experi-\\nenced users are like music critics, asking important questions about the\\napplication of machine learning in society such as ethics, fairness, and pri-\\nvacy of the individual. We hope that this book provides a foundation for\\nthinking about the certification and risk management of machine learning\\nsystems, and allows them to use their domain expertise to build better\\nmachine learning systems.\\nExperienced Artist Skilled practitioners of machine learning can plug\\nand play different tools and libraries into an analysis pipeline. The stereo-\\ntypical practitioner would be a data scientist or engineer who understands\\nmachine learning interfaces and their use cases, and is able to perform\\nwonderful feats of prediction from data. This is similar to a virtuoso play-\\ning music, where highly skilled practitioners can bring existing instru-\\nments to life and bring enjoyment to their audience. Using the mathe-\\nmatics presented here as a primer, practitioners would be able to under-\\nstand the benefits and limits of their favorite method, and to extend and\\ngeneralize existing machine learning algorithms. We hope that this book\\nprovides the impetus for more rigorous and principled development of\\nmachine learning methods.\\nFledgling Composer As machine learning is applied to new domains,\\ndevelopers of machine learning need to develop new methods and extend\\nexisting algorithms. They are often researchers who need to understand\\nthe mathematical basis of machine learning and uncover relationships be-\\ntween different tasks. This is similar to composers of music who, within\\nthe rules and structure of musical theory , create new and amazing pieces.\\nWe hope this book provides a high-level overview of other technical books\\nfor people who want to become composers of machine learning. There is\\na great need in society for new researchers who are able to propose and\\nexplore novel approaches for attacking the many challenges of learning\\nfrom data.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eeae9783-457b-4530-934f-f1efb367952e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 9, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4 Foreword\\nAcknowledgments\\nWe are grateful to many people who looked at early drafts of the book\\nand suffered through painful expositions of concepts. We tried to imple-\\nment their ideas that we did not vehemently disagree with. We would\\nlike to especially acknowledge Christfried Webers for his careful reading\\nof many parts of the book, and his detailed suggestions on structure and\\npresentation. Many friends and colleagues have also been kind enough\\nto provide their time and energy on different versions of each chapter.\\nWe have been lucky to benefit from the generosity of the online commu-\\nnity , who have suggested improvements viahttps://github.com, which\\ngreatly improved the book.\\nThe following people have found bugs, proposed clarifications and sug-\\ngested relevant literature, either via https://github.com or personal\\ncommunication. Their names are sorted alphabetically .\\nAbdul-Ganiy Usman\\nAdam Gaier\\nAdele Jackson\\nAditya Menon\\nAlasdair Tran\\nAleksandar Krnjaic\\nAlexander Makrigiorgos\\nAlfredo Canziani\\nAli Shafti\\nAmr Khalifa\\nAndrew Tanggara\\nAngus Gruen\\nAntal A. Buss\\nAntoine Toisoul Le Cann\\nAreg Sarvazyan\\nArtem Artemev\\nArtyom Stepanov\\nBill Kromydas\\nBob Williamson\\nBoon Ping Lim\\nChao Qu\\nCheng Li\\nChris Sherlock\\nChristopher Gray\\nDaniel McNamara\\nDaniel Wood\\nDarren Siegel\\nDavid Johnston\\nDawei Chen\\nEllen Broad\\nFengkuangtian Zhu\\nFiona Condon\\nGeorgios Theodorou\\nHe Xin\\nIrene Raissa Kameni\\nJakub Nabaglo\\nJames Hensman\\nJamie Liu\\nJean Kaddour\\nJean-Paul Ebejer\\nJerry Qiang\\nJitesh Sindhare\\nJohn Lloyd\\nJonas Ngnawe\\nJon Martin\\nJustin Hsi\\nKai Arulkumaran\\nKamil Dreczkowski\\nLily Wang\\nLionel Tondji Ngoupeyou\\nLydia Kn¨ufing\\nMahmoud Aslan\\nMark Hartenstein\\nMark van der Wilk\\nMarkus Hegland\\nMartin Hewing\\nMatthew Alger\\nMatthew Lee\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa8eb923-c237-4fe2-8ad6-306ce7d77ff3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 10, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foreword 5\\nMaximus McCann\\nMengyan Zhang\\nMichael Bennett\\nMichael Pedersen\\nMinjeong Shin\\nMohammad Malekzadeh\\nNaveen Kumar\\nNico Montali\\nOscar Armas\\nPatrick Henriksen\\nPatrick Wieschollek\\nPattarawat Chormai\\nPaul Kelly\\nPetros Christodoulou\\nPiotr Januszewski\\nPranav Subramani\\nQuyu Kong\\nRagib Zaman\\nRui Zhang\\nRyan-Rhys Griffiths\\nSalomon Kabongo\\nSamuel Ogunmola\\nSandeep Mavadia\\nSarvesh Nikumbh\\nSebastian Raschka\\nSenanayak Sesh Kumar Karri\\nSeung-Heon Baek\\nShahbaz Chaudhary\\nShakir Mohamed\\nShawn Berry\\nSheikh Abdul Raheem Ali\\nSheng Xue\\nSridhar Thiagarajan\\nSyed Nouman Hasany\\nSzymon Brych\\nThomas B¨uhler\\nTimur Sharapov\\nTom Melamed\\nVincent Adam\\nVincent Dutordoir\\nVu Minh\\nWasim Aftab\\nWen Zhi\\nWojciech Stokowiec\\nXiaonan Chong\\nXiaowei Zhang\\nYazhou Hao\\nYicheng Luo\\nYoung Lee\\nYu Lu\\nYun Cheng\\nYuxiao Huang\\nZac Cranko\\nZijian Cao\\nZoe Nolan\\nContributors through GitHub, whose real names were not listed on their\\nGitHub profile, are:\\nSamDataMad\\nbumptiousmonkey\\nidoamihai\\ndeepakiim\\ninsad\\nHorizonP\\ncs-maillist\\nkudo23\\nempet\\nvictorBigand\\n17SKYE\\njessjing1995\\nWe are also very grateful to Parameswaran Raman and the many anony-\\nmous reviewers, organized by Cambridge University Press, who read one\\nor more chapters of earlier versions of the manuscript, and provided con-\\nstructive criticism that led to considerable improvements. A special men-\\ntion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\\nadvice about LATEX-related issues. Last but not least, we are very grateful\\nto our editor Lauren Cowles, who has been patiently guiding us through\\nthe gestation process of this book.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4e04ac25-462c-48af-8369-8e96e27074ea', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 11, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6 Foreword\\nTable of Symbols\\nSymbol Typical meaning\\na, b, c, α, β, γ Scalars are lowercase\\nx, y, z Vectors are bold lowercase\\nA, B, C Matrices are bold uppercase\\nx⊤, A⊤ Transpose of a vector or matrix\\nA−1 Inverse of a matrix\\n⟨x, y⟩ Inner product of x and y\\nx⊤y Dot product of x and y\\nB = (b1, b2, b3) (Ordered) tuple\\nB = [b1, b2, b3] Matrix of column vectors stacked horizontally\\nB = {b1, b2, b3} Set of vectors (unordered)\\nZ, N Integers and natural numbers, respectively\\nR, C Real and complex numbers, respectively\\nRn n-dimensional vector space of real numbers\\n∀x Universal quantifier: for all x\\n∃x Existential quantifier: there exists x\\na := b a is defined as b\\na =: b b is defined as a\\na ∝ b a is proportional to b, i.e., a = constant · b\\ng ◦ f Function composition: “g after f”\\n⇐ ⇒ If and only if\\n=⇒ Implies\\nA, C Sets\\na ∈ A a is an element of set A\\n∅ Empty set\\nA\\\\B A without B: the set of elements in A but not in B\\nD Number of dimensions; indexed by d = 1, . . . , D\\nN Number of data points; indexed by n = 1, . . . , N\\nI m Identity matrix of size m × m\\n0m,n Matrix of zeros of size m × n\\n1m,n Matrix of ones of size m × n\\nei Standard/canonical vector (where i is the component that is 1)\\ndim Dimensionality of vector space\\nrk(A) Rank of matrix A\\nIm(Φ) Image of linear mapping Φ\\nker(Φ) Kernel (null space) of a linear mapping Φ\\nspan[b1] Span (generating set) of b1\\ntr(A) Trace of A\\ndet(A) Determinant of A\\n| · | Absolute value or determinant (depending on context)\\n∥·∥ Norm; Euclidean, unless specified\\nλ Eigenvalue or Lagrange multiplier\\nEλ Eigenspace corresponding to eigenvalue λ\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6ad5ad0-5304-49ba-9017-42eb15389cbf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 12, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foreword 7\\nSymbol Typical meaning\\nx ⊥ y Vectors x and y are orthogonal\\nV Vector space\\nV ⊥ Orthogonal complement of vector space VPN\\nn=1 xn Sum of the xn: x1 + . . . + xNQN\\nn=1 xn Product of the xn: x1 · . . . · xN\\nθ Parameter vector\\n∂f\\n∂x Partial derivative of f with respect to x\\ndf\\ndx Total derivative of f with respect to x\\n∇ Gradient\\nf∗ = minx f(x) The smallest function value of f\\nx∗ ∈ arg minx f(x) The value x∗ that minimizes f (note: arg min returns a set of values)\\nL Lagrangian\\nL Negative log-likelihood\\x00n\\nk\\n\\x01\\nBinomial coefficient, n choose k\\nVX[x] Variance of x with respect to the random variable X\\nEX[x] Expectation of x with respect to the random variable X\\nCovX,Y [x, y] Covariance between x and y.\\nX ⊥ ⊥Y | Z X is conditionally independent of Y given Z\\nX ∼ p Random variable X is distributed according to p\\nN\\n\\x00\\nµ, Σ\\n\\x01\\nGaussian distribution with mean µ and covariance Σ\\nBer(µ) Bernoulli distribution with parameter µ\\nBin(N, µ) Binomial distribution with parameters N, µ\\nBeta(α, β) Beta distribution with parameters α, β\\nTable of Abbreviations and Acronyms\\nAcronym Meaning\\ne.g. Exempli gratia (Latin: for example)\\nGMM Gaussian mixture model\\ni.e. Id est (Latin: this means)\\ni.i.d. Independent, identically distributed\\nMAP Maximum a posteriori\\nMLE Maximum likelihood estimation/estimator\\nONB Orthonormal basis\\nPCA Principal component analysis\\nPPCA Probabilistic principal component analysis\\nREF Row-echelon form\\nSPD Symmetric, positive definite\\nSVM Support vector machine\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3779de38-d7b6-4efe-a61b-8afcc80da438', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 13, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f3725a2-409f-47fc-82ab-3c5cc047a557', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 14, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Part I\\nMathematical Foundations\\n9\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4e6fc60-6fac-491d-ab96-733b86da81be', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 15, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9e236781-b0fc-47ed-af31-cbdf2a53e637', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 16, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1\\nIntroduction and Motivation\\nMachine learning is about designing algorithms that automatically extract\\nvaluable information from data. The emphasis here is on “automatic”, i.e.,\\nmachine learning is concerned about general-purpose methodologies that\\ncan be applied to many datasets, while producing something that is mean-\\ningful. There are three concepts that are at the core of machine learning:\\ndata, a model, and learning.\\nSince machine learning is inherently data driven, data is at the core data\\nof machine learning. The goal of machine learning is to design general-\\npurpose methodologies to extract valuable patterns from data, ideally\\nwithout much domain-specific expertise. For example, given a large corpus\\nof documents (e.g., books in many libraries), machine learning methods\\ncan be used to automatically find relevant topics that are shared across\\ndocuments (Hoffman et al., 2010). To achieve this goal, we design mod-\\nels that are typically related to the process that generates data, similar to model\\nthe dataset we are given. For example, in a regression setting, the model\\nwould describe a function that maps inputs to real-valued outputs. To\\nparaphrase Mitchell (1997): A model is said to learn from data if its per-\\nformance on a given task improves after the data is taken into account.\\nThe goal is to find good models that generalize well to yet unseen data,\\nwhich we may care about in the future. Learning can be understood as a learning\\nway to automatically find patterns and structure in data by optimizing the\\nparameters of the model.\\nWhile machine learning has seen many success stories, and software is\\nreadily available to design and train rich and flexible machine learning\\nsystems, we believe that the mathematical foundations of machine learn-\\ning are important in order to understand fundamental principles upon\\nwhich more complicated machine learning systems are built. Understand-\\ning these principles can facilitate creating new machine learning solutions,\\nunderstanding and debugging existing approaches, and learning about the\\ninherent assumptions and limitations of the methodologies we are work-\\ning with.\\n11\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8cb5008b-c1fc-44bb-bcd8-a3b2cb1a689a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 17, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12 Introduction and Motivation\\n1.1 Finding Words for Intuitions\\nA challenge we face regularly in machine learning is that concepts and\\nwords are slippery , and a particular component of the machine learning\\nsystem can be abstracted to different mathematical concepts. For example,\\nthe word “algorithm” is used in at least two different senses in the con-\\ntext of machine learning. In the first sense, we use the phrase “machine\\nlearning algorithm” to mean a system that makes predictions based on in-\\nput data. We refer to these algorithms as predictors. In the second sense,predictor\\nwe use the exact same phrase “machine learning algorithm” to mean a\\nsystem that adapts some internal parameters of the predictor so that it\\nperforms well on future unseen input data. Here we refer to this adapta-\\ntion as training a system.training\\nThis book will not resolve the issue of ambiguity , but we want to high-\\nlight upfront that, depending on the context, the same expressions can\\nmean different things. However, we attempt to make the context suffi-\\nciently clear to reduce the level of ambiguity .\\nThe first part of this book introduces the mathematical concepts and\\nfoundations needed to talk about the three main components of a machine\\nlearning system: data, models, and learning. We will briefly outline these\\ncomponents here, and we will revisit them again in Chapter 8 once we\\nhave discussed the necessary mathematical concepts.\\nWhile not all data is numerical, it is often useful to consider data in\\na number format. In this book, we assume that data has already been\\nappropriately converted into a numerical representation suitable for read-\\ning into a computer program. Therefore, we think of data as vectors. Asdata as vectors\\nanother illustration of how subtle words are, there are (at least) three\\ndifferent ways to think about vectors: a vector as an array of numbers (a\\ncomputer science view), a vector as an arrow with a direction and magni-\\ntude (a physics view), and a vector as an object that obeys addition and\\nscaling (a mathematical view).\\nA model is typically used to describe a process for generating data, sim-model\\nilar to the dataset at hand. Therefore, good models can also be thought\\nof as simplified versions of the real (unknown) data-generating process,\\ncapturing aspects that are relevant for modeling the data and extracting\\nhidden patterns from it. A good model can then be used to predict what\\nwould happen in the real world without performing real-world experi-\\nments.\\nWe now come to the crux of the matter, the learning component oflearning\\nmachine learning. Assume we are given a dataset and a suitable model.\\nTraining the model means to use the data available to optimize some pa-\\nrameters of the model with respect to a utility function that evaluates how\\nwell the model predicts the training data. Most training methods can be\\nthought of as an approach analogous to climbing a hill to reach its peak.\\nIn this analogy , the peak of the hill corresponds to a maximum of some\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7eab330-5db0-4964-894d-deb716f8a203', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 18, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Two Ways to Read This Book 13\\ndesired performance measure. However, in practice, we are interested in\\nthe model to perform well on unseen data. Performing well on data that\\nwe have already seen (training data) may only mean that we found a\\ngood way to memorize the data. However, this may not generalize well to\\nunseen data, and, in practical applications, we often need to expose our\\nmachine learning system to situations that it has not encountered before.\\nLet us summarize the main concepts of machine learning that we cover\\nin this book:\\nWe represent data as vectors.\\nWe choose an appropriate model, either using the probabilistic or opti-\\nmization view.\\nWe learn from available data by using numerical optimization methods\\nwith the aim that the model performs well on data not used for training.\\n1.2 Two Ways to Read This Book\\nWe can consider two strategies for understanding the mathematics for\\nmachine learning:\\nBottom-up: Building up the concepts from foundational to more ad-\\nvanced. This is often the preferred approach in more technical fields,\\nsuch as mathematics. This strategy has the advantage that the reader\\nat all times is able to rely on their previously learned concepts. Unfor-\\ntunately , for a practitioner many of the foundational concepts are not\\nparticularly interesting by themselves, and the lack of motivation means\\nthat most foundational definitions are quickly forgotten.\\nTop-down: Drilling down from practical needs to more basic require-\\nments. This goal-driven approach has the advantage that the readers\\nknow at all times why they need to work on a particular concept, and\\nthere is a clear path of required knowledge. The downside of this strat-\\negy is that the knowledge is built on potentially shaky foundations, and\\nthe readers have to remember a set of words that they do not have any\\nway of understanding.\\nWe decided to write this book in a modular way to separate foundational\\n(mathematical) concepts from applications so that this book can be read\\nin both ways. The book is split into two parts, where Part I lays the math-\\nematical foundations and Part II applies the concepts from Part I to a set\\nof fundamental machine learning problems, which form four pillars of\\nmachine learning as illustrated in Figure 1.1: regression, dimensionality\\nreduction, density estimation, and classification. Chapters in Part I mostly\\nbuild upon the previous ones, but it is possible to skip a chapter and work\\nbackward if necessary . Chapters in Part II are only loosely coupled and\\ncan be read in any order. There are many pointers forward and backward\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30ff5de6-e806-4651-a456-e113b0a9f4c9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 19, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='14 Introduction and Motivation\\nFigure 1.1 The\\nfoundations and\\nfour pillars of\\nmachine learning.\\nClassiﬁcation \\nDensity \\nEstimation \\nRegression \\nDimensionality \\nReduction \\nMachine Learning\\nVector CalculusProbability & DistributionsOptimization\\nAnalytic Geometry Matrix DecompositionLinear Algebra\\nbetween the two parts of the book to link mathematical concepts with\\nmachine learning algorithms.\\nOf course there are more than two ways to read this book. Most readers\\nlearn using a combination of top-down and bottom-up approaches, some-\\ntimes building up basic mathematical skills before attempting more com-\\nplex concepts, but also choosing topics based on applications of machine\\nlearning.\\nPart I Is about Mathematics\\nThe four pillars of machine learning we cover in this book (see Figure 1.1)\\nrequire a solid mathematical foundation, which is laid out in Part I.\\nWe represent numerical data as vectors and represent a table of such\\ndata as a matrix. The study of vectors and matrices is calledlinear algebra,\\nwhich we introduce in Chapter 2. The collection of vectors as a matrix islinear algebra\\nalso described there.\\nGiven two vectors representing two objects in the real world, we want\\nto make statements about their similarity . The idea is that vectors that\\nare similar should be predicted to have similar outputs by our machine\\nlearning algorithm (our predictor). To formalize the idea of similarity be-\\ntween vectors, we need to introduce operations that take two vectors as\\ninput and return a numerical value representing their similarity . The con-\\nstruction of similarity and distances is central to analytic geometry and isanalytic geometry\\ndiscussed in Chapter 3.\\nIn Chapter 4, we introduce some fundamental concepts about matri-\\nces and matrix decomposition. Some operations on matrices are extremelymatrix\\ndecomposition useful in machine learning, and they allow for an intuitive interpretation\\nof the data and more efficient learning.\\nWe often consider data to be noisy observations of some true underly-\\ning signal. We hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning what “noise” means. We often would also like to have predictors that\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d7350cd2-aacc-49c5-8926-746d6e68b9d6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 20, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Two Ways to Read This Book 15\\nallow us to express some sort of uncertainty , e.g., to quantify the confi-\\ndence we have about the value of the prediction at a particular test data\\npoint. Quantification of uncertainty is the realm of probability theory and probability theory\\nis covered in Chapter 6.\\nTo train machine learning models, we typically find parameters that\\nmaximize some performance measure. Many optimization techniques re-\\nquire the concept of a gradient, which tells us the direction in which to\\nsearch for a solution. Chapter 5 is about vector calculus and details the vector calculus\\nconcept of gradients, which we subsequently use in Chapter 7, where we\\ntalk about optimization to find maxima/minima of functions. optimization\\nPart II Is about Machine Learning\\nThe second part of the book introduces four pillars of machine learning\\nas shown in Figure 1.1. We illustrate how the mathematical concepts in-\\ntroduced in the first part of the book are the foundation for each pillar.\\nBroadly speaking, chapters are ordered by difficulty (in ascending order).\\nIn Chapter 8, we restate the three components of machine learning\\n(data, models, and parameter estimation) in a mathematical fashion. In\\naddition, we provide some guidelines for building experimental set-ups\\nthat guard against overly optimistic evaluations of machine learning sys-\\ntems. Recall that the goal is to build a predictor that performs well on\\nunseen data.\\nIn Chapter 9, we will have a close look at linear regression, where our linear regression\\nobjective is to find functions that map inputsx ∈ RD to corresponding ob-\\nserved function values y ∈ R, which we can interpret as the labels of their\\nrespective inputs. We will discuss classical model fitting (parameter esti-\\nmation) via maximum likelihood and maximum a posteriori estimation,\\nas well as Bayesian linear regression, where we integrate the parameters\\nout instead of optimizing them.\\nChapter 10 focuses on dimensionality reduction, the second pillar in Fig- dimensionality\\nreductionure 1.1, using principal component analysis. The key objective of dimen-\\nsionality reduction is to find a compact, lower-dimensional representation\\nof high-dimensional data x ∈ RD, which is often easier to analyze than\\nthe original data. Unlike regression, dimensionality reduction is only con-\\ncerned about modeling the data – there are no labels associated with a\\ndata point x.\\nIn Chapter 11, we will move to our third pillar: density estimation. The density estimation\\nobjective of density estimation is to find a probability distribution that de-\\nscribes a given dataset. We will focus on Gaussian mixture models for this\\npurpose, and we will discuss an iterative scheme to find the parameters of\\nthis model. As in dimensionality reduction, there are no labels associated\\nwith the data points x ∈ RD. However, we do not seek a low-dimensional\\nrepresentation of the data. Instead, we are interested in a density model\\nthat describes the data.\\nChapter 12 concludes the book with an in-depth discussion of the fourth\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c41a558-363e-4b5d-900b-18a6c8d47813', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 21, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16 Introduction and Motivation\\npillar: classification. We will discuss classification in the context of supportclassification\\nvector machines. Similar to regression (Chapter 9), we have inputs x and\\ncorresponding labels y. However, unlike regression, where the labels were\\nreal-valued, the labels in classification are integers, which requires special\\ncare.\\n1.3 Exercises and Feedback\\nWe provide some exercises in Part I, which can be done mostly by pen and\\npaper. For Part II, we provide programming tutorials (jupyter notebooks)\\nto explore some properties of the machine learning algorithms we discuss\\nin this book.\\nWe appreciate that Cambridge University Press strongly supports our\\naim to democratize education and learning by making this book freely\\navailable for download at\\nhttps://mml-book.com\\nwhere tutorials, errata, and additional materials can be found. Mistakes\\ncan be reported and feedback provided using the preceding URL.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='23fec145-f374-4956-96f7-7952dbd8c143', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 22, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2\\nLinear Algebra\\nWhen formalizing intuitive concepts, a common approach is to construct a\\nset of objects (symbols) and a set of rules to manipulate these objects. This\\nis known as an algebra. Linear algebra is the study of vectors and certain algebra\\nrules to manipulate vectors. The vectors many of us know from school are\\ncalled “geometric vectors”, which are usually denoted by a small arrow\\nabove the letter, e.g., − →x and − →y . In this book, we discuss more general\\nconcepts of vectors and use a bold letter to represent them, e.g., x and y.\\nIn general, vectors are special objects that can be added together and\\nmultiplied by scalars to produce another object of the same kind. From\\nan abstract mathematical viewpoint, any object that satisfies these two\\nproperties can be considered a vector. Here are some examples of such\\nvector objects:\\n1. Geometric vectors. This example of a vector may be familiar from high\\nschool mathematics and physics. Geometric vectors – see Figure 2.1(a)\\n– are directed segments, which can be drawn (at least in two dimen-\\nsions). Two geometric vectors\\n→\\nx,\\n→\\ny can be added, such that\\n→\\nx +\\n→\\ny =\\n→\\nz\\nis another geometric vector. Furthermore, multiplication by a scalar\\nλ\\n→\\nx, λ ∈ R, is also a geometric vector. In fact, it is the original vector\\nscaled by λ. Therefore, geometric vectors are instances of the vector\\nconcepts introduced previously . Interpreting vectors as geometric vec-\\ntors enables us to use our intuitions about direction and magnitude to\\nreason about mathematical operations.\\n2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can\\nFigure 2.1\\nDifferent types of\\nvectors. Vectors can\\nbe surprising\\nobjects, including\\n(a) geometric\\nvectors\\nand (b) polynomials.\\n→\\nx →\\ny\\n→\\nx +\\n→\\ny\\n(a) Geometric vectors.\\n−2 0 2\\nx\\n−6\\n−4\\n−2\\n0\\n2\\n4\\ny (b) Polynomials.\\n17\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9375af3e-1a14-4c2f-b26e-9d213a007ac6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 23, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='18 Linear Algebra\\nbe added together, which results in another polynomial; and they can\\nbe multiplied by a scalar λ ∈ R, and the result is a polynomial as\\nwell. Therefore, polynomials are (rather unusual) instances of vectors.\\nNote that polynomials are very different from geometric vectors. While\\ngeometric vectors are concrete “drawings”, polynomials are abstract\\nconcepts. However, they are both vectors in the sense previously de-\\nscribed.\\n3. Audio signals are vectors. Audio signals are represented as a series of\\nnumbers. We can add audio signals together, and their sum is a new\\naudio signal. If we scale an audio signal, we also obtain an audio signal.\\nTherefore, audio signals are a type of vector, too.\\n4. Elements of Rn (tuples of n real numbers) are vectors. Rn is more\\nabstract than polynomials, and it is the concept we focus on in this\\nbook. For instance,\\na =\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n3\\n\\uf8f9\\n\\uf8fb ∈ R3 (2.1)\\nis an example of a triplet of numbers. Adding two vectors a, b ∈ Rn\\ncomponent-wise results in another vector: a + b = c ∈ Rn. Moreover,\\nmultiplying a ∈ Rn by λ ∈ R results in a scaled vector λa ∈ Rn.\\nConsidering vectors as elements of Rn has an additional benefit thatBe careful to check\\nwhether array\\noperations actually\\nperform vector\\noperations when\\nimplementing on a\\ncomputer.\\nit loosely corresponds to arrays of real numbers on a computer. Many\\nprogramming languages support array operations, which allow for con-\\nvenient implementation of algorithms that involve vector operations.\\nLinear algebra focuses on the similarities between these vector concepts.\\nWe can add them together and multiply them by scalars. We will largelyPavel Grinfeld’s\\nseries on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/nahclwm\\nGilbert Strang’s\\ncourse on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/bdfbu8s5\\n3Blue1Brown series\\non linear algebra:\\nhttps://tinyurl.\\ncom/h5g4kps\\nfocus on vectors in Rn since most algorithms in linear algebra are for-\\nmulated in Rn. We will see in Chapter 8 that we often consider data to\\nbe represented as vectors in Rn. In this book, we will focus on finite-\\ndimensional vector spaces, in which case there is a 1:1 correspondence\\nbetween any kind of vector and Rn. When it is convenient, we will use\\nintuitions about geometric vectors and consider array-based algorithms.\\nOne major idea in mathematics is the idea of “closure”. This is the ques-\\ntion: What is the set of all things that can result from my proposed oper-\\nations? In the case of vectors: What is the set of vectors that can result by\\nstarting with a small set of vectors, and adding them to each other and\\nscaling them? This results in a vector space (Section 2.4). The concept of\\na vector space and its properties underlie much of machine learning. The\\nconcepts introduced in this chapter are summarized in Figure 2.2.\\nThis chapter is mostly based on the lecture notes and books by Drumm\\nand Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann\\n(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ebb310f6-f718-4ab9-af8c-bcd87d3a7faf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 24, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Systems of Linear Equations 19\\nFigure 2.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.\\nVector\\nVector space\\nMatrixChapter 5Vector calculus Group\\nSystem oflinear equations\\nMatrixinverseGaussianelimination\\nLinear/affinemapping\\nLinearindependence\\nBasis\\nChapter 10DimensionalityreductionChapter 12ClassificationChapter 3Analytic geometry\\ncomposes\\nclosure\\nAbelianwith+represents\\nrepresents\\nsolved bysolves\\nproperty of\\nmaximal set\\nresources are Gilbert Strang’s Linear Algebra course at MIT and the Linear\\nAlgebra Series by 3Blue1Brown.\\nLinear algebra plays an important role in machine learning and gen-\\neral mathematics. The concepts introduced in this chapter are further ex-\\npanded to include the idea of geometry in Chapter 3. In Chapter 5, we\\nwill discuss vector calculus, where a principled knowledge of matrix op-\\nerations is essential. In Chapter 10, we will use projections (to be intro-\\nduced in Section 3.8) for dimensionality reduction with principal compo-\\nnent analysis (PCA). In Chapter 9, we will discuss linear regression, where\\nlinear algebra plays a central role for solving least-squares problems.\\n2.1 Systems of Linear Equations\\nSystems of linear equations play a central part of linear algebra. Many\\nproblems can be formulated as systems of linear equations, and linear\\nalgebra gives us the tools for solving them.\\nExample 2.1\\nA company produces products N1, . . . , Nn for which resources\\nR1, . . . , Rm are required. To produce a unit of product Nj, aij units of\\nresource Ri are needed, where i = 1, . . . , mand j = 1, . . . , n.\\nThe objective is to find an optimal production plan, i.e., a plan of how\\nmany units xj of product Nj should be produced if a total of bi units of\\nresource Ri are available and (ideally) no resources are left over.\\nIf we produce x1, . . . , xn units of the corresponding products, we need\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa392313-5416-41ad-8030-e16d428a0bd2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 25, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='20 Linear Algebra\\na total of\\nai1x1 + · · · + ainxn (2.2)\\nmany units of resource Ri. An optimal production plan(x1, . . . , xn) ∈ Rn,\\ntherefore, has to satisfy the following system of equations:\\na11x1 + · · · + a1nxn = b1\\n...\\nam1x1 + · · · + amnxn = bm\\n, (2.3)\\nwhere aij ∈ R and bi ∈ R.\\nEquation (2.3) is the general form of a system of linear equations , andsystem of linear\\nequations x1, . . . , xn are the unknowns of this system. Every n-tuple (x1, . . . , xn) ∈\\nRn that satisfies (2.3) is a solution of the linear equation system.solution\\nExample 2.2\\nThe system of linear equations\\nx1 + x2 + x3 = 3 (1)\\nx1 − x2 + 2 x3 = 2 (2)\\n2x1 + 3 x3 = 1 (3)\\n(2.4)\\nhas no solution: Adding the first two equations yields2x1+3x3 = 5, which\\ncontradicts the third equation (3).\\nLet us have a look at the system of linear equations\\nx1 + x2 + x3 = 3 (1)\\nx1 − x2 + 2 x3 = 2 (2)\\nx2 + x3 = 2 (3)\\n. (2.5)\\nFrom the first and third equation, it follows that x1 = 1. From (1)+(2),\\nwe get 2x1 + 3x3 = 5, i.e., x3 = 1. From (3), we then get that x2 = 1.\\nTherefore, (1, 1, 1) is the only possible and unique solution (verify that\\n(1, 1, 1) is a solution by plugging in).\\nAs a third example, we consider\\nx1 + x2 + x3 = 3 (1)\\nx1 − x2 + 2 x3 = 2 (2)\\n2x1 + 3 x3 = 5 (3)\\n. (2.6)\\nSince (1)+(2)=(3), we can omit the third equation (redundancy). From\\n(1) and (2), we get2x1 = 5−3x3 and 2x2 = 1+ x3. We definex3 = a ∈ R\\nas a free variable, such that any triplet\\n\\x125\\n2 − 3\\n2 a, 1\\n2 + 1\\n2 a, a\\n\\x13\\n, a ∈ R (2.7)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='64afe8ab-b62c-42fd-86d3-b55e2b51d6dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 26, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Systems of Linear Equations 21\\nFigure 2.3 The\\nsolution space of a\\nsystem of two linear\\nequations with two\\nvariables can be\\ngeometrically\\ninterpreted as the\\nintersection of two\\nlines. Every linear\\nequation represents\\na line.\\n2x1 − 4x2 = 1\\n4x1 + 4x2 = 5\\nx1\\nx2\\nis a solution of the system of linear equations, i.e., we obtain a solution\\nset that contains infinitely many solutions.\\nIn general, for a real-valued system of linear equations we obtain either\\nno, exactly one, or infinitely many solutions. Linear regression (Chapter 9)\\nsolves a version of Example 2.1 when we cannot solve the system of linear\\nequations.\\nRemark (Geometric Interpretation of Systems of Linear Equations) . In a\\nsystem of linear equations with two variables x1, x2, each linear equation\\ndefines a line on the x1x2-plane. Since a solution to a system of linear\\nequations must satisfy all equations simultaneously , the solution set is the\\nintersection of these lines. This intersection set can be a line (if the linear\\nequations describe the same line), a point, or empty (when the lines are\\nparallel). An illustration is given in Figure 2.3 for the system\\n4x1 + 4x2 = 5\\n2x1 − 4x2 = 1 (2.8)\\nwhere the solution space is the point (x1, x2) = (1, 1\\n4). Similarly , for three\\nvariables, each linear equation determines a plane in three-dimensional\\nspace. When we intersect these planes, i.e., satisfy all linear equations at\\nthe same time, we can obtain a solution set that is a plane, a line, a point\\nor empty (when the planes have no common intersection). ♢\\nFor a systematic approach to solving systems of linear equations, we\\nwill introduce a useful compact notation. We collect the coefficients aij\\ninto vectors and collect the vectors into matrices. In other words, we write\\nthe system from (2.3) in the following form:\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11\\n...\\nam1\\n\\uf8f9\\n\\uf8fa\\uf8fb x1 +\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na12\\n...\\nam2\\n\\uf8f9\\n\\uf8fa\\uf8fb x2 + · · · +\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na1n\\n...\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fb xn =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nb1\\n...\\nbm\\n\\uf8f9\\n\\uf8fa\\uf8fb (2.9)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c443b914-1d8b-4b3a-bce6-404036c8ef11', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 27, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='22 Linear Algebra\\n⇐ ⇒\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11 · · · a1n\\n... ...\\nam1 · · · amn\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nx1\\n...\\nxn\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nb1\\n...\\nbm\\n\\uf8f9\\n\\uf8fa\\uf8fb . (2.10)\\nIn the following, we will have a close look at these matrices and de-\\nfine computation rules. We will return to solving linear equations in Sec-\\ntion 2.3.\\n2.2 Matrices\\nMatrices play a central role in linear algebra. They can be used to com-\\npactly represent systems of linear equations, but they also represent linear\\nfunctions (linear mappings) as we will see later in Section 2.7. Before we\\ndiscuss some of these interesting topics, let us first define what a matrix\\nis and what kind of operations we can do with matrices. We will see more\\nproperties of matrices in Chapter 4.\\nDefinition 2.1 (Matrix). With m, n ∈ N a real-valued (m, n) matrix A ismatrix\\nan m·n-tuple of elements aij, i = 1, . . . , m, j = 1, . . . , n, which is ordered\\naccording to a rectangular scheme consisting of m rows and n columns:\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11 a12 · · · a1n\\na21 a22 · · · a2n\\n... ... ...\\nam1 am2 · · · amn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb , a ij ∈ R . (2.11)\\nBy convention(1, n)-matrices are calledrows and (m, 1)-matrices are calledrow\\ncolumns. These special matrices are also called row/column vectors.column\\nrow vector\\ncolumn vector\\nFigure 2.4 By\\nstacking its\\ncolumns, a matrix A\\ncan be represented\\nas a long vector a.\\nre-shape\\nA∈R4×2 a∈R8\\nRm×n is the set of all real-valued (m, n)-matrices. A ∈ Rm×n can be\\nequivalently represented as a ∈ Rmn by stacking all n columns of the\\nmatrix into a long vector; see Figure 2.4.\\n2.2.1 Matrix Addition and Multiplication\\nThe sum of two matricesA ∈ Rm×n, B ∈ Rm×n is defined as the element-\\nwise sum, i.e.,\\nA + B :=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11 + b11 · · · a1n + b1n\\n... ...\\nam1 + bm1 · · · amn + bmn\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ Rm×n . (2.12)\\nFor matrices A ∈ Rm×n, B ∈ Rn×k, the elements cij of the productNote the size of the\\nmatrices. C = AB ∈ Rm×k are computed as\\nC =\\nnp.einsum(’il,\\nlj’, A, B) cij =\\nnX\\nl=1\\nailblj, i = 1, . . . , m, j = 1, . . . , k. (2.13)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='78ba8023-9c75-4f3d-8d4a-3dead80ac3ec', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 28, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Matrices 23\\nThis means, to compute element cij we multiply the elements of the ith There are n columns\\nin A and n rows in\\nB so that we can\\ncompute ailblj for\\nl = 1, . . . , n.\\nCommonly , the dot\\nproduct between\\ntwo vectors a, b is\\ndenoted by a⊤b or\\n⟨a, b⟩.\\nrow of A with the jth column of B and sum them up. Later in Section 3.2,\\nwe will call this the dot product of the corresponding row and column. In\\ncases, where we need to be explicit that we are performing multiplication,\\nwe use the notation A · B to denote multiplication (explicitly showing\\n“·”).\\nRemark. Matrices can only be multiplied if their “neighboring” dimensions\\nmatch. For instance, an n × k-matrix A can be multiplied with a k × m-\\nmatrix B, but only from the left side:\\nA|{z}\\nn×k\\nB|{z}\\nk×m\\n= C|{z}\\nn×m\\n(2.14)\\nThe product BA is not defined ifm ̸= n since the neighboring dimensions\\ndo not match. ♢\\nRemark. Matrix multiplication isnot defined as an element-wise operation\\non matrix elements, i.e., cij ̸= aijbij (even if the size of A, B was cho-\\nsen appropriately). This kind of element-wise multiplication often appears\\nin programming languages when we multiply (multi-dimensional) arrays\\nwith each other, and is called a Hadamard product. ♢ Hadamard product\\nExample 2.3\\nFor A =\\n\\x141 2 3\\n3 2 1\\n\\x15\\n∈ R2×3, B =\\n\\uf8ee\\n\\uf8f0\\n0 2\\n1 −1\\n0 1\\n\\uf8f9\\n\\uf8fb ∈ R3×2, we obtain\\nAB =\\n\\x141 2 3\\n3 2 1\\n\\x15\\uf8ee\\n\\uf8f0\\n0 2\\n1 −1\\n0 1\\n\\uf8f9\\n\\uf8fb =\\n\\x142 3\\n2 5\\n\\x15\\n∈ R2×2, (2.15)\\nBA =\\n\\uf8ee\\n\\uf8f0\\n0 2\\n1 −1\\n0 1\\n\\uf8f9\\n\\uf8fb\\n\\x141 2 3\\n3 2 1\\n\\x15\\n=\\n\\uf8ee\\n\\uf8f0\\n6 4 2\\n−2 0 2\\n3 2 1\\n\\uf8f9\\n\\uf8fb ∈ R3×3 . (2.16)\\nFigure 2.5 Even if\\nboth matrix\\nmultiplications AB\\nand BA are\\ndefined, the\\ndimensions of the\\nresults can be\\ndifferent.\\nFrom this example, we can already see that matrix multiplication is not\\ncommutative, i.e., AB ̸= BA; see also Figure 2.5 for an illustration.\\nDefinition 2.2 (Identity Matrix). In Rn×n, we define the identity matrix\\nidentity matrix\\nI n :=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 · · · 0 · · · 0\\n0 1 · · · 0 · · · 0\\n... ... ... ... ... ...\\n0 0 · · · 1 · · · 0\\n... ... ... ... ... ...\\n0 0 · · · 0 · · · 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ Rn×n (2.17)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='627f71c3-9587-42b1-846d-8edd48e05232', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 29, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='24 Linear Algebra\\nas the n × n-matrix containing 1 on the diagonal and 0 everywhere else.\\nNow that we defined matrix multiplication, matrix addition and the\\nidentity matrix, let us have a look at some properties of matrices:\\nassociativity\\nAssociativity:\\n∀A ∈ Rm×n, B ∈ Rn×p, C ∈ Rp×q : (AB)C = A(BC) (2.18)\\ndistributivity\\nDistributivity:\\n∀A, B ∈ Rm×n, C, D ∈ Rn×p : (A + B)C = AC + BC (2.19a)\\nA(C + D) = AC + AD (2.19b)\\nMultiplication with the identity matrix:\\n∀A ∈ Rm×n : I mA = AI n = A (2.20)\\nNote that I m ̸= I n for m ̸= n.\\n2.2.2 Inverse and Transpose\\nDefinition 2.3 (Inverse). Consider a square matrix A ∈ Rn×n. Let matrixA square matrix\\npossesses the same\\nnumber of columns\\nand rows.\\nB ∈ Rn×n have the property that AB = I n = BA. B is called the\\ninverse of A and denoted by A−1.\\ninverse Unfortunately , not every matrix A possesses an inverse A−1. If this\\ninverse does exist, A is called regular/invertible/nonsingular, otherwiseregular\\ninvertible\\nnonsingular\\nsingular/noninvertible. When the matrix inverse exists, it is unique. In Sec-\\nsingular\\nnoninvertible\\ntion 2.3, we will discuss a general way to compute the inverse of a matrix\\nby solving a system of linear equations.\\nRemark (Existence of the Inverse of a 2 × 2-matrix). Consider a matrix\\nA :=\\n\\x14a11 a12\\na21 a22\\n\\x15\\n∈ R2×2 . (2.21)\\nIf we multiply A with\\nA′ :=\\n\\x14 a22 −a12\\n−a21 a11\\n\\x15\\n(2.22)\\nwe obtain\\nAA′ =\\n\\x14a11a22 − a12a21 0\\n0 a11a22 − a12a21\\n\\x15\\n= (a11a22 − a12a21)I .\\n(2.23)\\nTherefore,\\nA−1 = 1\\na11a22 − a12a21\\n\\x14 a22 −a12\\n−a21 a11\\n\\x15\\n(2.24)\\nif and only if a11a22 − a12a21 ̸= 0. In Section 4.1, we will see thata11a22 −\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe94a9c3-6ec6-4fea-93db-2408a91e1482', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 30, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Matrices 25\\na12a21 is the determinant of a2×2-matrix. Furthermore, we can generally\\nuse the determinant to check whether a matrix is invertible. ♢\\nExample 2.4 (Inverse Matrix)\\nThe matrices\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 2 1\\n4 4 5\\n6 7 7\\n\\uf8f9\\n\\uf8fb , B =\\n\\uf8ee\\n\\uf8f0\\n−7 −7 6\\n2 1 −1\\n4 5 −4\\n\\uf8f9\\n\\uf8fb (2.25)\\nare inverse to each other since AB = I = BA.\\nDefinition 2.4 (Transpose). For A ∈ Rm×n the matrix B ∈ Rn×m with\\nbij = aji is called the transpose of A. We write B = A⊤. transpose\\nThe main diagonal\\n(sometimes called\\n“principal diagonal”,\\n“primary diagonal”,\\n“leading diagonal”,\\nor “major diagonal”)\\nof a matrix A is the\\ncollection of entries\\nAij where i = j.\\nIn general, A⊤ can be obtained by writing the columns ofA as the rows\\nof A⊤. The following are important properties of inverses and transposes:\\nThe scalar case of\\n(2.28) is\\n1\\n2+4 = 1\\n6 ̸= 1\\n2 + 1\\n4 .\\nAA−1 = I = A−1A (2.26)\\n(AB)−1 = B−1A−1 (2.27)\\n(A + B)−1 ̸= A−1 + B−1 (2.28)\\n(A⊤)⊤ = A (2.29)\\n(AB)⊤ = B⊤A⊤ (2.30)\\n(A + B)⊤ = A⊤ + B⊤ (2.31)\\nDefinition 2.5 (Symmetric Matrix). A matrix A ∈ Rn×n is symmetric if symmetric matrix\\nA = A⊤.\\nNote that only (n, n)-matrices can be symmetric. Generally , we call\\n(n, n)-matrices also square matrices because they possess the same num- square matrix\\nber of rows and columns. Moreover, if A is invertible, then so is A⊤, and\\n(A−1)⊤ = (A⊤)−1 =: A−⊤.\\nRemark (Sum and Product of Symmetric Matrices) . The sum of symmet-\\nric matrices A, B ∈ Rn×n is always symmetric. However, although their\\nproduct is always defined, it is generally not symmetric:\\n\\x141 0\\n0 0\\n\\x15\\x14 1 1\\n1 1\\n\\x15\\n=\\n\\x141 1\\n0 0\\n\\x15\\n. (2.32)\\n♢\\n2.2.3 Multiplication by a Scalar\\nLet us look at what happens to matrices when they are multiplied by a\\nscalar λ ∈ R. Let A ∈ Rm×n and λ ∈ R. Then λA = K, Kij = λ aij.\\nPractically ,λ scales each element of A. For λ, ψ ∈ R, the following holds:\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6150bdcd-fcab-4b71-b0eb-b9c6773cd5bb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 31, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='26 Linear Algebra\\nassociativity\\nAssociativity:\\n(λψ)C = λ(ψC), C ∈ Rm×n\\nλ(BC) = (λB)C = B(λC) = (BC)λ, B ∈ Rm×n, C ∈ Rn×k.\\nNote that this allows us to move scalar values around.\\n(λC)⊤ = C ⊤λ⊤ = C ⊤λ = λC ⊤ since λ = λ⊤ for all λ ∈ R.distributivity\\nDistributivity:\\n(λ + ψ)C = λC + ψC, C ∈ Rm×n\\nλ(B + C) = λB + λC, B, C ∈ Rm×n\\nExample 2.5 (Distributivity)\\nIf we define\\nC :=\\n\\x141 2\\n3 4\\n\\x15\\n, (2.33)\\nthen for any λ, ψ ∈ R we obtain\\n(λ + ψ)C =\\n\\x14(λ + ψ)1 ( λ + ψ)2\\n(λ + ψ)3 ( λ + ψ)4\\n\\x15\\n=\\n\\x14 λ + ψ 2λ + 2ψ\\n3λ + 3ψ 4λ + 4ψ\\n\\x15\\n(2.34a)\\n=\\n\\x14 λ 2λ\\n3λ 4λ\\n\\x15\\n+\\n\\x14 ψ 2ψ\\n3ψ 4ψ\\n\\x15\\n= λC + ψC . (2.34b)\\n2.2.4 Compact Representations of Systems of Linear Equations\\nIf we consider the system of linear equations\\n2x1 + 3x2 + 5x3 = 1\\n4x1 − 2x2 − 7x3 = 8\\n9x1 + 5x2 − 3x3 = 2\\n(2.35)\\nand use the rules for matrix multiplication, we can write this equation\\nsystem in a more compact form as\\n\\uf8ee\\n\\uf8f0\\n2 3 5\\n4 −2 −7\\n9 5 −3\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\nx1\\nx2\\nx3\\n\\uf8f9\\n\\uf8fb =\\n\\uf8ee\\n\\uf8f0\\n1\\n8\\n2\\n\\uf8f9\\n\\uf8fb . (2.36)\\nNote that x1 scales the first column, x2 the second one, and x3 the third\\none.\\nGenerally , a system of linear equations can be compactly represented in\\ntheir matrix form as Ax = b; see (2.3), and the product Ax is a (linear)\\ncombination of the columns of A. We will discuss linear combinations in\\nmore detail in Section 2.5.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97bc99d1-9ef3-47f4-8cc9-3cf0f725e366', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 32, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Solving Systems of Linear Equations 27\\n2.3 Solving Systems of Linear Equations\\nIn (2.3), we introduced the general form of an equation system, i.e.,\\na11x1 + · · · + a1nxn = b1\\n...\\nam1x1 + · · · + amnxn = bm ,\\n(2.37)\\nwhere aij ∈ R and bi ∈ R are known constants and xj are unknowns,\\ni = 1, . . . , m, j = 1, . . . , n. Thus far, we saw that matrices can be used as\\na compact way of formulating systems of linear equations so that we can\\nwrite Ax = b, see (2.10). Moreover, we defined basic matrix operations,\\nsuch as addition and multiplication of matrices. In the following, we will\\nfocus on solving systems of linear equations and provide an algorithm for\\nfinding the inverse of a matrix.\\n2.3.1 Particular and General Solution\\nBefore discussing how to generally solve systems of linear equations, let\\nus have a look at an example. Consider the system of equations\\n\\x141 0 8 −4\\n0 1 2 12\\n\\x15\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\nx3\\nx4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb =\\n\\x1442\\n8\\n\\x15\\n. (2.38)\\nThe system has two equations and four unknowns. Therefore, in general\\nwe would expect infinitely many solutions. This system of equations is\\nin a particularly easy form, where the first two columns consist of a 1\\nand a 0. Remember that we want to find scalars x1, . . . , x4, such thatP4\\ni=1 xici = b, where we define ci to be the ith column of the matrix and\\nb the right-hand-side of (2.38). A solution to the problem in (2.38) can\\nbe found immediately by taking 42 times the first column and 8 times the\\nsecond column so that\\nb =\\n\\x1442\\n8\\n\\x15\\n= 42\\n\\x141\\n0\\n\\x15\\n+ 8\\n\\x140\\n1\\n\\x15\\n. (2.39)\\nTherefore, a solution is [42, 8, 0, 0]⊤. This solution is called a particular particular solution\\nsolution or special solution. However, this is not the only solution of this special solution\\nsystem of linear equations. To capture all the other solutions, we need\\nto be creative in generating 0 in a non-trivial way using the columns of\\nthe matrix: Adding 0 to our special solution does not change the special\\nsolution. To do so, we express the third column using the first two columns\\n(which are of this very simple form)\\n\\x148\\n2\\n\\x15\\n= 8\\n\\x141\\n0\\n\\x15\\n+ 2\\n\\x140\\n1\\n\\x15\\n(2.40)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c61254bf-84cb-4919-8d8b-18f10bdc29ae', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 33, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='28 Linear Algebra\\nso that 0 = 8c1 + 2c2 − 1c3 + 0c4 and (x1, x2, x3, x4) = (8 , 2, −1, 0). In\\nfact, any scaling of this solution by λ1 ∈ R produces the 0 vector, i.e.,\\n\\x141 0 8 −4\\n0 1 2 12\\n\\x15\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n8\\n2\\n−1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8 = λ1(8c1 + 2c2 − c3) = 0 . (2.41)\\nFollowing the same line of reasoning, we express the fourth column of the\\nmatrix in (2.38) using the first two columns and generate another set of\\nnon-trivial versions of 0 as\\n\\x141 0 8 −4\\n0 1 2 12\\n\\x15\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−4\\n12\\n0\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8 = λ2(−4c1 + 12c2 − c4) = 0 (2.42)\\nfor any λ2 ∈ R. Putting everything together, we obtain all solutions of the\\nequation system in (2.38), which is called the general solution, as the setgeneral solution\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nx ∈ R4 : x =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n42\\n8\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n8\\n2\\n−1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−4\\n12\\n0\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , λ1, λ2 ∈ R\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe\\n. (2.43)\\nRemark. The general approach we followed consisted of the following\\nthree steps:\\n1. Find a particular solution to Ax = b.\\n2. Find all solutions to Ax = 0.\\n3. Combine the solutions from steps 1. and 2. to the general solution.\\nNeither the general nor the particular solution is unique. ♢\\nThe system of linear equations in the preceding example was easy to\\nsolve because the matrix in (2.38) has this particularly convenient form,\\nwhich allowed us to find the particular and the general solution by in-\\nspection. However, general equation systems are not of this simple form.\\nFortunately , there exists a constructive algorithmic way of transforming\\nany system of linear equations into this particularly simple form: Gaussian\\nelimination. Key to Gaussian elimination are elementary transformations\\nof systems of linear equations, which transform the equation system into\\na simple form. Then, we can apply the three steps to the simple form that\\nwe just discussed in the context of the example in (2.38).\\n2.3.2 Elementary Transformations\\nKey to solving a system of linear equations areelementary transformationselementary\\ntransformations that keep the solution set the same, but that transform the equation system\\ninto a simpler form:\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff36929f-95f0-4aeb-a431-8624a2a0a819', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 34, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Solving Systems of Linear Equations 29\\nExchange of two equations (rows in the matrix representing the system\\nof equations)\\nMultiplication of an equation (row) with a constant λ ∈ R\\\\{0}\\nAddition of two equations (rows)\\nExample 2.6\\nFor a ∈ R, we seek all solutions of the following system of equations:\\n−2x1 + 4 x2 − 2x3 − x4 + 4 x5 = −3\\n4x1 − 8x2 + 3 x3 − 3x4 + x5 = 2\\nx1 − 2x2 + x3 − x4 + x5 = 0\\nx1 − 2x2 − 3x4 + 4 x5 = a\\n. (2.44)\\nWe start by converting this system of equations into the compact matrix\\nnotation Ax = b. We no longer mention the variables x explicitly and\\nbuild the augmented matrix (in the form\\n\\x02\\nA | b\\n\\x03\\n) augmented matrix\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−2 4 −2 −1 4 −3\\n4 −8 3 −3 1 2\\n1 −2 1 −1 1 0\\n1 −2 0 −3 4 a\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nSwap with R3\\nSwap with R1\\nwhere we used the vertical line to separate the left-hand side from the\\nright-hand side in (2.44). We use ⇝ to indicate a transformation of the\\naugmented matrix using elementary transformations. The augmented\\nmatrix\\n\\x02\\nA | b\\n\\x03\\ncompactly\\nrepresents the\\nsystem of linear\\nequations Ax = b.\\nSwapping Rows 1 and 3 leads to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −2 1 −1 1 0\\n4 −8 3 −3 1 2\\n−2 4 −2 −1 4 −3\\n1 −2 0 −3 4 a\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n−4R1\\n+2R1\\n−R1\\nWhen we now apply the indicated transformations (e.g., subtract Row 1\\nfour times from Row 2), we obtain\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −2 1 −1 1 0\\n0 0 −1 1 −3 2\\n0 0 0 −3 6 −3\\n0 0 −1 −2 3 a\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n−R2 − R3\\n⇝\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −2 1 −1 1 0\\n0 0 −1 1 −3 2\\n0 0 0 −3 6 −3\\n0 0 0 0 0 a+1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n·(−1)\\n·(− 1\\n3)\\n⇝\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −2 1 −1 1 0\\n0 0 1 −1 3 −2\\n0 0 0 1 −2 1\\n0 0 0 0 0 a+1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0c2f42dd-5a02-434a-bd38-19ff98df15b0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 35, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='30 Linear Algebra\\nThis (augmented) matrix is in a convenient form, the row-echelon formrow-echelon form\\n(REF). Reverting this compact notation back into the explicit notation with\\nthe variables we seek, we obtain\\nx1 − 2x2 + x3 − x4 + x5 = 0\\nx3 − x4 + 3 x5 = −2\\nx4 − 2x5 = 1\\n0 = a + 1\\n. (2.45)\\nOnly for a = −1 this system can be solved. A particular solution isparticular solution\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\nx3\\nx4\\nx5\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n0\\n−1\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (2.46)\\nThe general solution, which captures the set of all possible solutions, isgeneral solution\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nx ∈ R5 : x =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n0\\n−1\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n0\\n−1\\n2\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, λ 1, λ2 ∈ R\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe\\n. (2.47)\\nIn the following, we will detail a constructive way to obtain a particular\\nand general solution of a system of linear equations.\\nRemark (Pivots and Staircase Structure). The leading coefficient of a row\\n(first nonzero number from the left) is called the pivot and is alwayspivot\\nstrictly to the right of the pivot of the row above it. Therefore, any equa-\\ntion system in row-echelon form always has a “staircase” structure. ♢\\nDefinition 2.6 (Row-Echelon Form). A matrix is in row-echelon form ifrow-echelon form\\nAll rows that contain only zeros are at the bottom of the matrix; corre-\\nspondingly , all rows that contain at least one nonzero element are on\\ntop of rows that contain only zeros.\\nLooking at nonzero rows only , the first nonzero number from the left\\n(also called the pivot or the leading coefficient) is always strictly to thepivot\\nleading coefficient right of the pivot of the row above it.\\nIn other texts, it is\\nsometimes required\\nthat the pivot is 1.\\nRemark (Basic and Free Variables) . The variables corresponding to the\\npivots in the row-echelon form are called basic variables and the other\\nbasic variable variables are free variables . For example, in (2.45), x1, x3, x4 are basic\\nfree variable variables, whereas x2, x5 are free variables. ♢\\nRemark (Obtaining a Particular Solution) . The row-echelon form makes\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ef7de6d-e142-4eac-b145-312532380ea0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 36, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Solving Systems of Linear Equations 31\\nour lives easier when we need to determine a particular solution. To do\\nthis, we express the right-hand side of the equation system using the pivot\\ncolumns, such that b =PP\\ni=1 λipi, where pi, i = 1, . . . , P, are the pivot\\ncolumns. The λi are determined easiest if we start with the rightmost pivot\\ncolumn and work our way to the left.\\nIn the previous example, we would try to find λ1, λ2, λ3 so that\\nλ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−1\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n−2\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.48)\\nFrom here, we find relatively directly that λ3 = 1, λ2 = −1, λ1 = 2. When\\nwe put everything together, we must not forget the non-pivot columns\\nfor which we set the coefficients implicitly to 0. Therefore, we get the\\nparticular solution x = [2, 0, −1, 1, 0]⊤. ♢\\nRemark (Reduced Row Echelon Form). An equation system is in reduced reduced\\nrow-echelon formrow-echelon form (also: row-reduced echelon form or row canonical form) if\\nIt is in row-echelon form.\\nEvery pivot is 1.\\nThe pivot is the only nonzero entry in its column.\\n♢\\nThe reduced row-echelon form will play an important role later in Sec-\\ntion 2.3.3 because it allows us to determine the general solution of a sys-\\ntem of linear equations in a straightforward way . Gaussian\\neliminationRemark (Gaussian Elimination). Gaussian elimination is an algorithm that\\nperforms elementary transformations to bring a system of linear equations\\ninto reduced row-echelon form. ♢\\nExample 2.7 (Reduced Row Echelon Form)\\nVerify that the following matrix is in reduced row-echelon form (the pivots\\nare in bold):\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 3 0 0 3\\n0 0 1 0 9\\n0 0 0 1 −4\\n\\uf8f9\\n\\uf8fb . (2.49)\\nThe key idea for finding the solutions of Ax = 0 is to look at the non-\\npivot columns, which we will need to express as a (linear) combination of\\nthe pivot columns. The reduced row echelon form makes this relatively\\nstraightforward, and we express the non-pivot columns in terms of sums\\nand multiples of the pivot columns that are on their left: The second col-\\numn is 3 times the first column (we can ignore the pivot columns on the\\nright of the second column). Therefore, to obtain 0, we need to subtract\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f4f66953-9ea5-4314-b2c0-26f88816e569', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 37, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='32 Linear Algebra\\nthe second column from three times the first column. Now, we look at the\\nfifth column, which is our second non-pivot column. The fifth column can\\nbe expressed as 3 times the first pivot column, 9 times the second pivot\\ncolumn, and −4 times the third pivot column. We need to keep track of\\nthe indices of the pivot columns and translate this into3 times the first col-\\numn, 0 times the second column (which is a non-pivot column), 9 times\\nthe third column (which is our second pivot column), and −4 times the\\nfourth column (which is the third pivot column). Then we need to subtract\\nthe fifth column to obtain0. In the end, we are still solving a homogeneous\\nequation system.\\nTo summarize, all solutions of Ax = 0, x ∈ R5 are given by\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nx ∈ R5 : x = λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3\\n−1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3\\n0\\n9\\n−4\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, λ 1, λ2 ∈ R\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe\\n. (2.50)\\n2.3.3 The Minus-1 Trick\\nIn the following, we introduce a practical trick for reading out the solu-\\ntions x of a homogeneous system of linear equations Ax = 0, where\\nA ∈ Rk×n, x ∈ Rn.\\nTo start, we assume thatA is in reduced row-echelon form without any\\nrows that just contain zeros, i.e.,\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 · · · 0 1 ∗ · · · ∗ 0 ∗ · · · ∗ 0 ∗ · · · ∗\\n... ... 0 0 · · · 0 1 ∗ · · · ∗ ... ... ...\\n... ... ... ... ... 0 ... ... ... ... ...\\n... ... ... ... ... ... ... ... 0 ... ...\\n0 · · · 0 0 0 · · · 0 0 0 · · · 0 1 ∗ · · · ∗\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\n(2.51)\\nwhere ∗ can be an arbitrary real number, with the constraints that the first\\nnonzero entry per row must be1 and all other entries in the corresponding\\ncolumn must be 0. The columns j1, . . . , jk with the pivots (marked in\\nbold) are the standard unit vectorse1, . . . ,ek ∈ Rk. We extend this matrix\\nto an n × n-matrix ˜A by adding n − k rows of the form\\n\\x02\\n0 · · · 0 −1 0 · · · 0\\n\\x03\\n(2.52)\\nso that the diagonal of the augmented matrix ˜A contains either 1 or −1.\\nThen, the columns of ˜A that contain the −1 as pivots are solutions of\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73a1423f-dd34-48c7-9ca8-32bd3ba4baa3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 38, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Solving Systems of Linear Equations 33\\nthe homogeneous equation system Ax = 0. To be more precise, these\\ncolumns form a basis (Section 2.6.1) of the solution space of Ax = 0,\\nwhich we will later call the kernel or null space (see Section 2.7.3). kernel\\nnull space\\nExample 2.8 (Minus-1 Trick)\\nLet us revisit the matrix in (2.49), which is already in reduced REF:\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 3 0 0 3\\n0 0 1 0 9\\n0 0 0 1 −4\\n\\uf8f9\\n\\uf8fb . (2.53)\\nWe now augment this matrix to a 5 × 5 matrix by adding rows of the\\nform (2.52) at the places where the pivots on the diagonal are missing\\nand obtain\\n˜A =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 3 0 0 3\\n0 −1 0 0 0\\n0 0 1 0 9\\n0 0 0 1 −4\\n0 0 0 0 −1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (2.54)\\nFrom this form, we can immediately read out the solutions of Ax = 0 by\\ntaking the columns of ˜A, which contain −1 on the diagonal:\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nx ∈ R5 : x = λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3\\n−1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3\\n0\\n9\\n−4\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, λ 1, λ2 ∈ R\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe\\n, (2.55)\\nwhich is identical to the solution in (2.50) that we obtained by “insight”.\\nCalculating the Inverse\\nTo compute the inverse A−1 of A ∈ Rn×n, we need to find a matrix X\\nthat satisfies AX = I n. Then, X = A−1. We can write this down as\\na set of simultaneous linear equations AX = I n, where we solve for\\nX = [x1| · · · |xn]. We use the augmented matrix notation for a compact\\nrepresentation of this set of systems of linear equations and obtain\\n\\x02\\nA|I n\\n\\x03\\n⇝ · · · ⇝\\n\\x02\\nI n|A−1\\x03\\n. (2.56)\\nThis means that if we bring the augmented equation system into reduced\\nrow-echelon form, we can read out the inverse on the right-hand side of\\nthe equation system. Hence, determining the inverse of a matrix is equiv-\\nalent to solving systems of linear equations.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f76c953-ea98-458d-8989-c95a6d5b466d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 39, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='34 Linear Algebra\\nExample 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)\\nTo determine the inverse of\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 2 0\\n1 1 0 0\\n1 2 0 1\\n1 1 1 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2.57)\\nwe write down the augmented matrix\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 2 0 1 0 0 0\\n1 1 0 0 0 1 0 0\\n1 2 0 1 0 0 1 0\\n1 1 1 1 0 0 0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nand use Gaussian elimination to bring it into reduced row-echelon form\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0 −1 2 −2 2\\n0 1 0 0 1 −1 2 −2\\n0 0 1 0 1 −1 1 −1\\n0 0 0 1 −1 0 −1 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\nsuch that the desired inverse is given as its right-hand side:\\nA−1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1 2 −2 2\\n1 −1 2 −2\\n1 −1 1 −1\\n−1 0 −1 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.58)\\nWe can verify that (2.58) is indeed the inverse by performing the multi-\\nplication AA−1 and observing that we recover I4.\\n2.3.4 Algorithms for Solving a System of Linear Equations\\nIn the following, we briefly discuss approaches to solving a system of lin-\\near equations of the form Ax = b. We make the assumption that a solu-\\ntion exists. Should there be no solution, we need to resort to approximate\\nsolutions, which we do not cover in this chapter. One way to solve the ap-\\nproximate problem is using the approach of linear regression, which we\\ndiscuss in detail in Chapter 9.\\nIn special cases, we may be able to determine the inverse A−1, such\\nthat the solution of Ax = b is given as x = A−1b. However, this is\\nonly possible if A is a square matrix and invertible, which is often not the\\ncase. Otherwise, under mild assumptions (i.e., A needs to have linearly\\nindependent columns) we can use the transformation\\nAx = b ⇐ ⇒ A⊤Ax = A⊤b ⇐ ⇒ x = (A⊤A)−1A⊤b (2.59)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='307c1303-2c7c-4dc4-9799-e75d329e4d15', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 40, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Vector Spaces 35\\nand use the Moore-Penrose pseudo-inverse (A⊤A)−1A⊤ to determine the Moore-Penrose\\npseudo-inversesolution (2.59) that solves Ax = b, which also corresponds to the mini-\\nmum norm least-squares solution. A disadvantage of this approach is that\\nit requires many computations for the matrix-matrix product and comput-\\ning the inverse of A⊤A. Moreover, for reasons of numerical precision it\\nis generally not recommended to compute the inverse or pseudo-inverse.\\nIn the following, we therefore briefly discuss alternative approaches to\\nsolving systems of linear equations.\\nGaussian elimination plays an important role when computing deter-\\nminants (Section 4.1), checking whether a set of vectors is linearly inde-\\npendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),\\ncomputing the rank of a matrix (Section 2.6.2), and determining a basis\\nof a vector space (Section 2.6.1). Gaussian elimination is an intuitive and\\nconstructive way to solve a system of linear equations with thousands of\\nvariables. However, for systems with millions of variables, it is impracti-\\ncal as the required number of arithmetic operations scales cubically in the\\nnumber of simultaneous equations.\\nIn practice, systems of many linear equations are solved indirectly , by ei-\\nther stationary iterative methods, such as the Richardson method, the Ja-\\ncobi method, the Gauß-Seidel method, and the successive over-relaxation\\nmethod, or Krylov subspace methods, such as conjugate gradients, gener-\\nalized minimal residual, or biconjugate gradients. We refer to the books\\nby Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann\\n(2015) for further details.\\nLet x∗ be a solution of Ax = b. The key idea of these iterative methods\\nis to set up an iteration of the form\\nx(k+1) = Cx(k) + d (2.60)\\nfor suitable C and d that reduces the residual error∥x(k+1) − x∗∥ in every\\niteration and converges to x∗. We will introduce norms ∥ · ∥, which allow\\nus to compute similarities between vectors, in Section 3.1.\\n2.4 Vector Spaces\\nThus far, we have looked at systems of linear equations and how to solve\\nthem (Section 2.3). We saw that systems of linear equations can be com-\\npactly represented using matrix-vector notation (2.10). In the following,\\nwe will have a closer look at vector spaces, i.e., a structured space in which\\nvectors live.\\nIn the beginning of this chapter, we informally characterized vectors as\\nobjects that can be added together and multiplied by a scalar, and they\\nremain objects of the same type. Now, we are ready to formalize this,\\nand we will start by introducing the concept of a group, which is a set\\nof elements and an operation defined on these elements that keeps some\\nstructure of the set intact.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5602ea20-e7e6-4728-8476-b91dbc288329', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 41, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='36 Linear Algebra\\n2.4.1 Groups\\nGroups play an important role in computer science. Besides providing a\\nfundamental framework for operations on sets, they are heavily used in\\ncryptography , coding theory , and graphics.\\nDefinition 2.7 (Group). Consider a set G and an operation ⊗ : G ×G → G\\ndefined on G. Then G := (G, ⊗) is called a group if the following hold:group\\nclosure\\n1. Closure of G under ⊗: ∀x, y ∈ G : x ⊗ y ∈ Gassociativity\\n2. Associativity: ∀x, y, z ∈ G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)neutral element\\n3. Neutral element: ∃e ∈ G ∀ x ∈ G : x ⊗ e = x and e ⊗ x = xinverse element\\n4. Inverse element: ∀x ∈ G ∃ y ∈ G : x ⊗ y = e and y ⊗ x = e, where e is\\nthe neutral element. We often write x−1 to denote the inverse element\\nof x.\\nRemark. The inverse element is defined with respect to the operation ⊗\\nand does not necessarily mean 1\\nx. ♢\\nIf additionally ∀x, y ∈ G : x ⊗ y = y ⊗ x, then G = (G, ⊗) is an AbelianAbelian group\\ngroup (commutative).\\nExample 2.10 (Groups)\\nLet us have a look at some examples of sets with associated operations\\nand see whether they are groups:\\n(Z, +) is an Abelian group.\\n(N0, +) is not a group: Although (N0, +) possesses a neutral elementN0 := N ∪ {0}\\n(0), the inverse elements are missing.\\n(Z, ·) is not a group: Although(Z, ·) contains a neutral element (1), the\\ninverse elements for any z ∈ Z, z ̸= ±1, are missing.\\n(R, ·) is not a group since 0 does not possess an inverse element.\\n(R\\\\{0}, ·) is Abelian.\\n(Rn, +), (Zn, +), n ∈ N are Abelian if+ is defined componentwise, i.e.,\\n(x1, · · · , xn) + (y1, · · · , yn) = (x1 + y1, · · · , xn + yn). (2.61)\\nThen, (x1, · · · , xn)−1 := ( −x1, · · · , −xn) is the inverse element and\\ne = (0, · · · , 0) is the neutral element.\\n(Rm×n, +), the set of m × n-matrices is Abelian (with componentwise\\naddition as defined in (2.61)).\\nLet us have a closer look at(Rn×n, ·), i.e., the set of n × n-matrices with\\nmatrix multiplication as defined in (2.13).\\n– Closure and associativity follow directly from the definition of matrix\\nmultiplication.\\n– Neutral element: The identity matrix I n is the neutral element with\\nrespect to matrix multiplication “·” in(Rn×n, ·).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c64de834-4ca1-446e-88f4-e3c57849b1d0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 42, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Vector Spaces 37\\n– Inverse element: If the inverse exists ( A is regular), then A−1 is the\\ninverse element of A ∈ Rn×n, and in exactly this case (Rn×n, ·) is a\\ngroup, called the general linear group.\\nDefinition 2.8 (General Linear Group) . The set of regular (invertible)\\nmatrices A ∈ Rn×n is a group with respect to matrix multiplication as\\ndefined in (2.13) and is called general linear group GL(n, R). However, general linear group\\nsince matrix multiplication is not commutative, the group is not Abelian.\\n2.4.2 Vector Spaces\\nWhen we discussed groups, we looked at sets G and inner operations on\\nG, i.e., mappings G × G → G that only operate on elements in G. In the\\nfollowing, we will consider sets that in addition to an inner operation +\\nalso contain an outer operation ·, the multiplication of a vector x ∈ G by\\na scalar λ ∈ R. We can think of the inner operation as a form of addition,\\nand the outer operation as a form of scaling. Note that the inner/outer\\noperations have nothing to do with inner/outer products.\\nDefinition 2.9 (Vector Space). A real-valued vector space V = (V, +, ·) is vector space\\na set V with two operations\\n+ : V × V → V (2.62)\\n· : R × V → V (2.63)\\nwhere\\n1. (V, +) is an Abelian group\\n2. Distributivity:\\n1. ∀λ ∈ R, x, y ∈ V : λ · (x + y) = λ · x + λ · y\\n2. ∀λ, ψ ∈ R, x ∈ V : (λ + ψ) · x = λ · x + ψ · x\\n3. Associativity (outer operation): ∀λ, ψ ∈ R, x ∈ V : λ·(ψ ·x) = (λψ)·x\\n4. Neutral element with respect to the outer operation: ∀x ∈ V : 1·x = x\\nThe elements x ∈ V are called vectors. The neutral element of (V, +) is vector\\nthe zero vector 0 = [0, . . . ,0]⊤, and the inner operation + is called vector vector addition\\naddition. The elements λ ∈ R are called scalars and the outer operation scalar\\n· is a multiplication by scalars . Note that a scalar product is something multiplication by\\nscalarsdifferent, and we will get to this in Section 3.2.\\nRemark. A “vector multiplication”ab, a, b ∈ Rn, is not defined. Theoret-\\nically , we could define an element-wise multiplication, such that c = ab\\nwith cj = ajbj. This “array multiplication” is common to many program-\\nming languages but makes mathematically limited sense using the stan-\\ndard rules for matrix multiplication: By treating vectors as n × 1 matrices\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b50fc243-969a-4122-90e4-05523480b97a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 43, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='38 Linear Algebra\\n(which we usually do), we can use the matrix multiplication as defined\\nin (2.13). However, then the dimensions of the vectors do not match. Only\\nthe following multiplications for vectors are defined: ab⊤ ∈ Rn×n (outerouter product\\nproduct), a⊤b ∈ R (inner/scalar/dot product). ♢\\nExample 2.11 (Vector Spaces)\\nLet us have a look at some important examples:\\nV = Rn, n ∈ N is a vector space with operations defined as follows:\\n– Addition: x+y = (x1, . . . , xn)+( y1, . . . , yn) = (x1+y1, . . . , xn+yn)\\nfor all x, y ∈ Rn\\n– Multiplication by scalars: λx = λ(x1, . . . , xn) = ( λx1, . . . , λxn) for\\nall λ ∈ R, x ∈ Rn\\nV = Rm×n, m, n ∈ N is a vector space with\\n– Addition: A + B =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11 + b11 · · · a1n + b1n\\n... ...\\nam1 + bm1 · · · amn + bmn\\n\\uf8f9\\n\\uf8fa\\uf8fb is defined ele-\\nmentwise for all A, B ∈ V\\n– Multiplication by scalars: λA =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nλa11 · · · λa1n\\n... ...\\nλam1 · · · λamn\\n\\uf8f9\\n\\uf8fa\\uf8fb as defined in\\nSection 2.2. Remember that Rm×n is equivalent to Rmn.\\nV = C, with the standard definition of addition of complex numbers.\\nRemark. In the following, we will denote a vector space (V, +, ·) by V\\nwhen + and · are the standard vector addition and scalar multiplication.\\nMoreover, we will use the notation x ∈ V for vectors in V to simplify\\nnotation. ♢\\nRemark. The vector spaces Rn, Rn×1, R1×n are only different in the way\\nwe write vectors. In the following, we will not make a distinction between\\nRn and Rn×1, which allows us to write n-tuples as column vectorscolumn vector\\nx =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nx1\\n...\\nxn\\n\\uf8f9\\n\\uf8fa\\uf8fb . (2.64)\\nThis simplifies the notation regarding vector space operations. However,\\nwe do distinguish between Rn×1 and R1×n (the row vectors) to avoid con-row vector\\nfusion with matrix multiplication. By default, we write x to denote a col-\\numn vector, and a row vector is denoted by x⊤, the transpose of x. ♢transpose\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a511a741-e7e6-4ca5-bed4-0062202166e7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 44, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Vector Spaces 39\\n2.4.3 Vector Subspaces\\nIn the following, we will introduce vector subspaces. Intuitively , they are\\nsets contained in the original vector space with the property that when\\nwe perform vector space operations on elements within this subspace, we\\nwill never leave it. In this sense, they are “closed”. Vector subspaces are a\\nkey idea in machine learning. For example, Chapter 10 demonstrates how\\nto use vector subspaces for dimensionality reduction.\\nDefinition 2.10 (Vector Subspace). Let V = ( V, +, ·) be a vector space\\nand U ⊆ V , U ̸ = ∅. Then U = (U , +, ·) is called vector subspace of V (or vector subspace\\nlinear subspace) if U is a vector space with the vector space operations + linear subspace\\nand · restricted to U × U and R × U. We writeU ⊆ V to denote a subspace\\nU of V .\\nIf U ⊆ V and V is a vector space, then U naturally inherits many prop-\\nerties directly from V because they hold for allx ∈ V , and in particular for\\nall x ∈ U ⊆ V . This includes the Abelian group properties, the distribu-\\ntivity , the associativity and the neutral element. To determine whether\\n(U , +, ·) is a subspace of V we still do need to show\\n1. U ̸ = ∅, in particular: 0 ∈ U\\n2. Closure of U:\\na. With respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx ∈ U .\\nb. With respect to the inner operation: ∀x, y ∈ U : x + y ∈ U .\\nExample 2.12 (Vector Subspaces)\\nLet us have a look at some examples:\\nFor every vector space V , the trivial subspaces are V itself and {0}.\\nOnly example D in Figure 2.6 is a subspace ofR2 (with the usual inner/\\nouter operations). In A and C, the closure property is violated; B does\\nnot contain 0.\\nThe solution set of a homogeneous system of linear equations Ax = 0\\nwith n unknowns x = [x1, . . . , xn]⊤ is a subspace of Rn.\\nThe solution of an inhomogeneous system of linear equations Ax =\\nb, b ̸= 0 is not a subspace of Rn.\\nThe intersection of arbitrarily many subspaces is a subspace itself.\\nFigure 2.6 Not all\\nsubsets of R2 are\\nsubspaces. In A and\\nC, the closure\\nproperty is violated;\\nB does not contain\\n0. Only D is a\\nsubspace.\\n0 0 0 0\\nA B\\nC\\nD\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22d1f091-76e3-4093-961d-922ff38588dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 45, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='40 Linear Algebra\\nRemark. Every subspace U ⊆ (Rn, +, ·) is the solution space of a homo-\\ngeneous system of linear equations Ax = 0 for x ∈ Rn. ♢\\n2.5 Linear Independence\\nIn the following, we will have a close look at what we can do with vectors\\n(elements of the vector space). In particular, we can add vectors together\\nand multiply them with scalars. The closure property guarantees that we\\nend up with another vector in the same vector space. It is possible to find\\na set of vectors with which we can represent every vector in the vector\\nspace by adding them together and scaling them. This set of vectors is\\na basis, and we will discuss them in Section 2.6.1. Before we get there,\\nwe will need to introduce the concepts of linear combinations and linear\\nindependence.\\nDefinition 2.11 (Linear Combination). Consider a vector space V and a\\nfinite number of vectors x1, . . . ,xk ∈ V . Then, every v ∈ V of the form\\nv = λ1x1 + · · · + λkxk =\\nkX\\ni=1\\nλixi ∈ V (2.65)\\nwith λ1, . . . , λk ∈ R is a linear combination of the vectors x1, . . . ,xk.linear combination\\nThe 0-vector can always be written as the linear combination of k vec-\\ntors x1, . . . ,xk because 0 = Pk\\ni=1 0xi is always true. In the following,\\nwe are interested in non-trivial linear combinations of a set of vectors to\\nrepresent 0, i.e., linear combinations of vectors x1, . . . ,xk, where not all\\ncoefficients λi in (2.65) are 0.\\nDefinition 2.12 (Linear (In)dependence). Let us consider a vector space\\nV with k ∈ N and x1, . . . ,xk ∈ V . If there is a non-trivial linear com-\\nbination, such that 0 = Pk\\ni=1 λixi with at least one λi ̸= 0, the vectors\\nx1, . . . ,xk are linearly dependent. If only the trivial solution exists, i.e.,linearly dependent\\nλ1 = . . . = λk = 0 the vectors x1, . . . ,xk are linearly independent.linearly\\nindependent\\nLinear independence is one of the most important concepts in linear\\nalgebra. Intuitively , a set of linearly independent vectors consists of vectors\\nthat have no redundancy , i.e., if we remove any of those vectors from\\nthe set, we will lose something. Throughout the next sections, we will\\nformalize this intuition more.\\nExample 2.13 (Linearly Dependent Vectors)\\nA geographic example may help to clarify the concept of linear indepen-\\ndence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is\\nmight say ,“You can get to Kigali by first going506 km Northwest to Kam-\\npala (Uganda) and then374 kmSouthwest.”. This is sufficient information\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e25ddf36-843d-4a83-b3e7-ac4606cebb5c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 46, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Linear Independence 41\\nto describe the location of Kigali because the geographic coordinate sys-\\ntem may be considered a two-dimensional vector space (ignoring altitude\\nand the Earth’s curved surface). The person may add, “It is about751 km\\nWest of here.” Although this last statement is true, it is not necessary to\\nfind Kigali given the previous information (see Figure 2.7 for an illus-\\ntration). In this example, the “ 506 km Northwest” vector (blue) and the\\n“374 km Southwest” vector (purple) are linearly independent. This means\\nthe Southwest vector cannot be described in terms of the Northwest vec-\\ntor, and vice versa. However, the third “751 km West” vector (black) is a\\nlinear combination of the other two vectors, and it makes the set of vec-\\ntors linearly dependent. Equivalently , given “751 km West” and “374 km\\nSouthwest” can be linearly combined to obtain “506 km Northwest”.\\nFigure 2.7\\nGeographic example\\n(with crude\\napproximations to\\ncardinal directions)\\nof linearly\\ndependent vectors\\nin a\\ntwo-dimensional\\nspace (plane).\\n506 km Northwest\\n751 km West\\n374 km Southwest\\n374 km Southwest\\nKampala\\nNairobi\\nKigali\\nRemark. The following properties are useful to find out whether vectors\\nare linearly independent:\\nk vectors are either linearly dependent or linearly independent. There\\nis no third option.\\nIf at least one of the vectors x1, . . . ,xk is 0 then they are linearly de-\\npendent. The same holds if two vectors are identical.\\nThe vectors {x1, . . . ,xk : xi ̸= 0, i = 1 , . . . , k}, k ⩾ 2, are linearly\\ndependent if and only if (at least) one of them is a linear combination\\nof the others. In particular, if one vector is a multiple of another vector,\\ni.e., xi = λxj, λ ∈ R then the set {x1, . . . ,xk : xi ̸= 0, i = 1, . . . , k}\\nis linearly dependent.\\nA practical way of checking whether vectorsx1, . . . ,xk ∈ V are linearly\\nindependent is to use Gaussian elimination: Write all vectors as columns\\nof a matrix A and perform Gaussian elimination until the matrix is in\\nrow echelon form (the reduced row-echelon form is unnecessary here):\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a6765739-787d-4079-af22-0efae6a62369', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 47, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='42 Linear Algebra\\n– The pivot columns indicate the vectors, which are linearly indepen-\\ndent of the vectors on the left. Note that there is an ordering of vec-\\ntors when the matrix is built.\\n– The non-pivot columns can be expressed as linear combinations of\\nthe pivot columns on their left. For instance, the row-echelon form\\n\\x141 3 0\\n0 0 2\\n\\x15\\n(2.66)\\ntells us that the first and third columns are pivot columns. The sec-\\nond column is a non-pivot column because it is three times the first\\ncolumn.\\nAll column vectors are linearly independent if and only if all columns\\nare pivot columns. If there is at least one non-pivot column, the columns\\n(and, therefore, the corresponding vectors) are linearly dependent.\\n♢\\nExample 2.14\\nConsider R4 with\\nx1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n2\\n−3\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , x2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , x3 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−2\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.67)\\nTo check whether they are linearly dependent, we follow the general ap-\\nproach and solve\\nλ1x1 + λ2x2 + λ3x3 = λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n2\\n−3\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb + λ3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−2\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb = 0 (2.68)\\nfor λ1, . . . , λ3. We write the vectors xi, i = 1 , 2, 3, as the columns of a\\nmatrix and apply elementary row operations until we identify the pivot\\ncolumns:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 −1\\n2 1 −2\\n−3 0 1\\n4 2 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ⇝ · · · ⇝\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 −1\\n0 1 0\\n0 0 1\\n0 0 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.69)\\nHere, every column of the matrix is a pivot column. Therefore, there is no\\nnon-trivial solution, and we require λ1 = 0 , λ2 = 0 , λ3 = 0 to solve the\\nequation system. Hence, the vectors x1, x2, x3 are linearly independent.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7db136c-a4ab-494b-83e0-906205c2d27d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 48, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Linear Independence 43\\nRemark. Consider a vector space V with k linearly independent vectors\\nb1, . . . ,bk and m linear combinations\\nx1 =\\nkX\\ni=1\\nλi1bi ,\\n...\\nxm =\\nkX\\ni=1\\nλimbi .\\n(2.70)\\nDefining B = [ b1, . . . ,bk] as the matrix whose columns are the linearly\\nindependent vectors b1, . . . ,bk, we can write\\nxj = Bλj , λj =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nλ1j\\n...\\nλkj\\n\\uf8f9\\n\\uf8fa\\uf8fb , j = 1, . . . , m , (2.71)\\nin a more compact form.\\nWe want to test whether x1, . . . ,xm are linearly independent. For this\\npurpose, we follow the general approach of testing whenPm\\nj=1 ψjxj = 0.\\nWith (2.71), we obtain\\nmX\\nj=1\\nψjxj =\\nmX\\nj=1\\nψjBλj = B\\nmX\\nj=1\\nψjλj . (2.72)\\nThis means that {x1, . . . ,xm} are linearly independent if and only if the\\ncolumn vectors {λ1, . . . ,λm} are linearly independent.\\n♢\\nRemark. In a vector spaceV , m linear combinations ofk vectors x1, . . . ,xk\\nare linearly dependent if m > k . ♢\\nExample 2.15\\nConsider a set of linearly independent vectors b1, b2, b3, b4 ∈ Rn and\\nx1 = b1 − 2b2 + b3 − b4\\nx2 = −4b1 − 2b2 + 4 b4\\nx3 = 2 b1 + 3 b2 − b3 − 3b4\\nx4 = 17 b1 − 10b2 + 11 b3 + b4\\n. (2.73)\\nAre the vectors x1, . . . ,x4 ∈ Rn linearly independent? To answer this\\nquestion, we investigate whether the column vectors\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n−2\\n1\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−4\\n−2\\n0\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n2\\n3\\n−1\\n−3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n17\\n−10\\n11\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe\\n(2.74)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b39b6e0d-e40f-4623-ad4a-f59e7c753bf8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 49, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='44 Linear Algebra\\nare linearly independent. The reduced row-echelon form of the corre-\\nsponding linear equation system with coefficient matrix\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −4 2 17\\n−2 −2 3 −10\\n1 0 −1 11\\n−1 4 −3 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2.75)\\nis given as\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 −7\\n0 1 0 −15\\n0 0 1 −18\\n0 0 0 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.76)\\nWe see that the corresponding linear equation system is non-trivially solv-\\nable: The last column is not a pivot column, andx4 = −7x1−15x2−18x3.\\nTherefore, x1, . . . ,x4 are linearly dependent as x4 can be expressed as a\\nlinear combination of x1, . . . ,x3.\\n2.6 Basis and Rank\\nIn a vector space V , we are particularly interested in sets of vectorsA that\\npossess the property that any vector v ∈ V can be obtained by a linear\\ncombination of vectors in A. These vectors are special vectors, and in the\\nfollowing, we will characterize them.\\n2.6.1 Generating Set and Basis\\nDefinition 2.13 (Generating Set and Span). Consider a vector space V =\\n(V, +, ·) and set of vectors A = {x1, . . . ,xk} ⊆ V . If every vector v ∈\\nV can be expressed as a linear combination of x1, . . . ,xk, A is called a\\ngenerating set of V . The set of all linear combinations of vectors in A isgenerating set\\ncalled the span of A. If A spans the vector space V , we write V = span[A]span\\nor V = span[x1, . . . ,xk].\\nGenerating sets are sets of vectors that span vector (sub)spaces, i.e.,\\nevery vector can be represented as a linear combination of the vectors\\nin the generating set. Now, we will be more specific and characterize the\\nsmallest generating set that spans a vector (sub)space.\\nDefinition 2.14 (Basis). Consider a vector space V = (V, +, ·) and A ⊆\\nV. A generating set A of V is called minimal if there exists no smaller setminimal\\n˜A ⊊ A ⊆ V that spans V . Every linearly independent generating set of V\\nis minimal and is called a basis of V .basis\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8be6ee6-1334-488e-8873-bb3660f575e2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 50, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.6 Basis and Rank 45\\nLet V = ( V, +, ·) be a vector space and B ⊆ V , B ̸ = ∅. Then, the\\nfollowing statements are equivalent: A basis is a minimal\\ngenerating set and a\\nmaximal linearly\\nindependent set of\\nvectors.\\nB is a basis of V .\\nB is a minimal generating set.\\nB is a maximal linearly independent set of vectors inV , i.e., adding any\\nother vector to this set will make it linearly dependent.\\nEvery vector x ∈ V is a linear combination of vectors fromB, and every\\nlinear combination is unique, i.e., with\\nx =\\nkX\\ni=1\\nλibi =\\nkX\\ni=1\\nψibi (2.77)\\nand λi, ψi ∈ R, bi ∈ B it follows that λi = ψi, i = 1, . . . , k.\\nExample 2.16\\nIn R3, the canonical/standard basis is canonical basis\\nB =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe . (2.78)\\nDifferent bases in R3 are\\nB1 =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe , B2 =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n0.5\\n0.8\\n0.4\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1.8\\n0.3\\n0.3\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n−2.2\\n−1.3\\n3.5\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe . (2.79)\\nThe set\\nA =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n2\\n3\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n2\\n−1\\n0\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n−4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe\\n(2.80)\\nis linearly independent, but not a generating set (and no basis) of R4:\\nFor instance, the vector [1, 0, 0, 0]⊤ cannot be obtained by a linear com-\\nbination of elements in A.\\nRemark. Every vector space V possesses a basis B. The preceding exam-\\nples show that there can be many bases of a vector space V , i.e., there is\\nno unique basis. However, all bases possess the same number of elements,\\nthe basis vectors. ♢ basis vector\\nWe only consider finite-dimensional vector spaces V . In this case, the\\ndimension of V is the number of basis vectors of V , and we write dim(V ). dimension\\nIf U ⊆ V is a subspace of V , then dim(U) ⩽ dim(V ) and dim(U) =\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a86141f-2be8-4774-8a34-488de8f8dcda', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 51, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='46 Linear Algebra\\ndim(V ) if and only if U = V . Intuitively , the dimension of a vector space\\ncan be thought of as the number of independent directions in this vector\\nspace.The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors.\\nRemark. The dimension of a vector space is not necessarily the number\\nof elements in a vector. For instance, the vector space V = span[\\n\\x140\\n1\\n\\x15\\n] is\\none-dimensional, although the basis vector possesses two elements. ♢\\nRemark. A basis of a subspace U = span[x1, . . . ,xm] ⊆ Rn can be found\\nby executing the following steps:\\n1. Write the spanning vectors as columns of a matrix A\\n2. Determine the row-echelon form of A.\\n3. The spanning vectors associated with the pivot columns are a basis of\\nU.\\n♢\\nExample 2.17 (Determining a Basis)\\nFor a vector subspace U ⊆ R5, spanned by the vectors\\nx1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n2\\n−1\\n−1\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, x2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n−1\\n1\\n2\\n−2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, x3 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3\\n−4\\n3\\n5\\n−3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, x4 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−1\\n8\\n−5\\n−6\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ R5, (2.81)\\nwe are interested in finding out which vectorsx1, . . . ,x4 are a basis forU.\\nFor this, we need to check whether x1, . . . ,x4 are linearly independent.\\nTherefore, we need to solve\\n4X\\ni=1\\nλixi = 0 , (2.82)\\nwhich leads to a homogeneous system of equations with matrix\\n\\x02\\nx1, x2, x3, x4\\n\\x03\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 2 3 −1\\n2 −1 −4 8\\n−1 1 3 −5\\n−1 2 5 −6\\n−1 −2 −3 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (2.83)\\nWith the basic transformation rules for systems of linear equations, we\\nobtain the row-echelon form\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 2 3 −1\\n2 −1 −4 8\\n−1 1 3 −5\\n−1 2 5 −6\\n−1 −2 −3 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⇝ · · · ⇝\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 2 3 −1\\n0 1 2 −2\\n0 0 0 1\\n0 0 0 0\\n0 0 0 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1bd49023-76be-4f90-95e2-f03abf6be76a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 52, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.6 Basis and Rank 47\\nSince the pivot columns indicate which set of vectors is linearly indepen-\\ndent, we see from the row-echelon form that x1, x2, x4 are linearly inde-\\npendent (because the system of linear equations λ1x1 + λ2x2 + λ4x4 = 0\\ncan only be solved with λ1 = λ2 = λ4 = 0). Therefore, {x1, x2, x4} is a\\nbasis of U.\\n2.6.2 Rank\\nThe number of linearly independent columns of a matrix A ∈ Rm×n\\nequals the number of linearly independent rows and is called the rank rank\\nof A and is denoted by rk(A).\\nRemark. The rank of a matrix has some important properties:\\nrk(A) = rk(A⊤), i.e., the column rank equals the row rank.\\nThe columns of A ∈ Rm×n span a subspace U ⊆ Rm with dim(U) =\\nrk(A). Later we will call this subspace the image or range. A basis of\\nU can be found by applying Gaussian elimination to A to identify the\\npivot columns.\\nThe rows of A ∈ Rm×n span a subspace W ⊆ Rn with dim(W ) =\\nrk(A). A basis of W can be found by applying Gaussian elimination to\\nA⊤.\\nFor all A ∈ Rn×n it holds that A is regular (invertible) if and only if\\nrk(A) = n.\\nFor all A ∈ Rm×n and all b ∈ Rm it holds that the linear equation\\nsystem Ax = b can be solved if and only if rk(A) = rk( A|b), where\\nA|b denotes the augmented system.\\nFor A ∈ Rm×n the subspace of solutions for Ax = 0 possesses dimen-\\nsion n − rk(A). Later, we will call this subspace the kernel or the null kernel\\nnull spacespace.\\nA matrix A ∈ Rm×n has full rank if its rank equals the largest possible full rank\\nrank for a matrix of the same dimensions. This means that the rank of\\na full-rank matrix is the lesser of the number of rows and columns, i.e.,\\nrk(A) = min( m, n). A matrix is said to be rank deficient if it does not rank deficient\\nhave full rank.\\n♢\\nExample 2.18 (Rank)\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 0 1\\n0 1 1\\n0 0 0\\n\\uf8f9\\n\\uf8fb.\\nA has two linearly independent rows/columns so that rk(A) = 2.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ce6724f4-caf2-474a-9ad0-9868daa56389', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 53, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='48 Linear Algebra\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 2 1\\n−2 −3 1\\n3 5 0\\n\\uf8f9\\n\\uf8fb .\\nWe use Gaussian elimination to determine the rank:\\n\\uf8ee\\n\\uf8f0\\n1 2 1\\n−2 −3 1\\n3 5 0\\n\\uf8f9\\n\\uf8fb ⇝ · · · ⇝\\n\\uf8ee\\n\\uf8f0\\n1 2 1\\n0 1 3\\n0 0 0\\n\\uf8f9\\n\\uf8fb . (2.84)\\nHere, we see that the number of linearly independent rows and columns\\nis 2, such that rk(A) = 2.\\n2.7 Linear Mappings\\nIn the following, we will study mappings on vector spaces that preserve\\ntheir structure, which will allow us to define the concept of a coordinate.\\nIn the beginning of the chapter, we said that vectors are objects that can be\\nadded together and multiplied by a scalar, and the resulting object is still\\na vector. We wish to preserve this property when applying the mapping:\\nConsider two real vector spaces V, W . A mapping Φ : V → W preserves\\nthe structure of the vector space if\\nΦ(x + y) = Φ(x) + Φ(y) (2.85)\\nΦ(λx) = λΦ(x) (2.86)\\nfor all x, y ∈ V and λ ∈ R. We can summarize this in the following\\ndefinition:\\nDefinition 2.15 (Linear Mapping). For vector spaces V, W , a mapping\\nΦ : V → W is called a linear mapping (or vector space homomorphism /linear mapping\\nvector space\\nhomomorphism\\nlinear transformation) if\\nlinear\\ntransformation\\n∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y) . (2.87)\\nIt turns out that we can represent linear mappings as matrices (Sec-\\ntion 2.7.1). Recall that we can also collect a set of vectors as columns of a\\nmatrix. When working with matrices, we have to keep in mind what the\\nmatrix represents: a linear mapping or a collection of vectors. We will see\\nmore about linear mappings in Chapter 4. Before we continue, we will\\nbriefly introduce special mappings.\\nDefinition 2.16 (Injective, Surjective, Bijective). Consider a mapping Φ :\\nV → W , where V, W can be arbitrary sets. Then Φ is called\\ninjective\\nInjective if ∀x, y ∈ V : Φ(x) = Φ(y) = ⇒ x = y.surjective\\nSurjective if Φ(V) = W.bijective\\nBijective if it is injective and surjective.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d5372fd6-a6bb-4d2c-82e0-fe2fe0c2aa12', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 54, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 49\\nIf Φ is surjective, then every element in W can be “reached” from V\\nusing Φ. A bijective Φ can be “undone”, i.e., there exists a mapping Ψ :\\nW → V so that Ψ ◦ Φ(x) = x. This mapping Ψ is then called the inverse\\nof Φ and normally denoted by Φ−1.\\nWith these definitions, we introduce the following special cases of linear\\nmappings between vector spaces V and W :\\nisomorphism\\nIsomorphism: Φ : V → W linear and bijective endomorphism\\nEndomorphism: Φ : V → V linear automorphism\\nAutomorphism: Φ : V → V linear and bijective\\nWe define idV : V → V , x 7→ x as the identity mapping or identity identity mapping\\nidentity\\nautomorphism\\nautomorphism in V .\\nExample 2.19 (Homomorphism)\\nThe mapping Φ : R2 → C, Φ(x) = x1 + ix2, is a homomorphism:\\nΦ\\n\\x12\\x14x1\\nx2\\n\\x15\\n+\\n\\x14y1\\ny2\\n\\x15\\x13\\n= (x1 + y1) + i(x2 + y2) = x1 + ix2 + y1 + iy2\\n= Φ\\n\\x12\\x14x1\\nx2\\n\\x15\\x13\\n+ Φ\\n\\x12\\x14y1\\ny2\\n\\x15\\x13\\nΦ\\n\\x12\\nλ\\n\\x14x1\\nx2\\n\\x15\\x13\\n= λx1 + λix2 = λ(x1 + ix2) = λΦ\\n\\x12\\x14x1\\nx2\\n\\x15\\x13\\n.\\n(2.88)\\nThis also justifies why complex numbers can be represented as tuples in\\nR2: There is a bijective linear mapping that converts the elementwise addi-\\ntion of tuples in R2 into the set of complex numbers with the correspond-\\ning addition. Note that we only showed linearity , but not the bijection.\\nTheorem 2.17 (Theorem 3.59 in Axler (2015)). Finite-dimensional vector\\nspaces V and W are isomorphic if and only if dim(V ) = dim(W ).\\nTheorem 2.17 states that there exists a linear, bijective mapping be-\\ntween two vector spaces of the same dimension. Intuitively , this means\\nthat vector spaces of the same dimension are kind of the same thing, as\\nthey can be transformed into each other without incurring any loss.\\nTheorem 2.17 also gives us the justification to treat Rm×n (the vector\\nspace of m × n-matrices) and Rmn (the vector space of vectors of length\\nmn) the same, as their dimensions are mn, and there exists a linear, bi-\\njective mapping that transforms one into the other.\\nRemark. Consider vector spaces V, W, X. Then:\\nFor linear mappings Φ : V → W and Ψ : W → X, the mapping\\nΨ ◦ Φ : V → X is also linear.\\nIf Φ : V → W is an isomorphism, then Φ−1 : W → V is an isomor-\\nphism, too.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5664e47c-8bb2-421d-802d-11544192781a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 55, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='50 Linear Algebra\\nFigure 2.8 Two\\ndifferent coordinate\\nsystems defined by\\ntwo sets of basis\\nvectors. A vector x\\nhas different\\ncoordinate\\nrepresentations\\ndepending on which\\ncoordinate system is\\nchosen.\\nx x\\ne1\\ne2\\nb1\\nb2\\nIf Φ : V → W, Ψ : V → W are linear, then Φ + Ψ and λΦ, λ ∈ R, are\\nlinear, too.\\n♢\\n2.7.1 Matrix Representation of Linear Mappings\\nAny n-dimensional vector space is isomorphic to Rn (Theorem 2.17). We\\nconsider a basis {b1, . . . ,bn} of an n-dimensional vector space V . In the\\nfollowing, the order of the basis vectors will be important. Therefore, we\\nwrite\\nB = (b1, . . . ,bn) (2.89)\\nand call this n-tuple an ordered basis of V .ordered basis\\nRemark (Notation). We are at the point where notation gets a bit tricky .\\nTherefore, we summarize some parts here.B = (b1, . . . ,bn) is an ordered\\nbasis, B = {b1, . . . ,bn} is an (unordered) basis, and B = [b1, . . . ,bn] is a\\nmatrix whose columns are the vectors b1, . . . ,bn. ♢\\nDefinition 2.18 (Coordinates). Consider a vector space V and an ordered\\nbasis B = (b1, . . . ,bn) of V . For any x ∈ V we obtain a unique represen-\\ntation (linear combination)\\nx = α1b1 + . . . + αnbn (2.90)\\nof x with respect to B. Then α1, . . . , αn are the coordinates of x withcoordinate\\nrespect to B, and the vector\\nα =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nα1\\n...\\nαn\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ Rn (2.91)\\nis the coordinate vector/coordinate representation of x with respect to thecoordinate vector\\ncoordinate\\nrepresentation\\nordered basis B.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9fecc822-0dea-4f36-af93-60727154c8cf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 56, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 51\\nA basis effectively defines a coordinate system. We are familiar with the\\nCartesian coordinate system in two dimensions, which is spanned by the\\ncanonical basis vectors e1, e2. In this coordinate system, a vector x ∈ R2\\nhas a representation that tells us how to linearly combine e1 and e2 to\\nobtain x. However, any basis of R2 defines a valid coordinate system,\\nand the same vector x from before may have a different coordinate rep-\\nresentation in the (b1, b2) basis. In Figure 2.8, the coordinates of x with\\nrespect to the standard basis (e1, e2) is [2, 2]⊤. However, with respect to\\nthe basis (b1, b2) the same vector x is represented as [1.09, 0.72]⊤, i.e.,\\nx = 1 .09b1 + 0.72b2. In the following sections, we will discover how to\\nobtain this representation.\\nExample 2.20\\nLet us have a look at a geometric vector x ∈ R2 with coordinates [2, 3]⊤ Figure 2.9\\nDifferent coordinate\\nrepresentations of a\\nvector x, depending\\non the choice of\\nbasis.\\ne1\\ne2 b2\\nb1\\nx=−12b1+52b2\\nx= 2e1+ 3e2\\nwith respect to the standard basis(e1, e2) of R2. This means, we can write\\nx = 2e1 + 3e2. However, we do not have to choose the standard basis to\\nrepresent this vector. If we use the basis vectorsb1 = [1, −1]⊤, b2 = [1, 1]⊤\\nwe will obtain the coordinates 1\\n2[−1, 5]⊤ to represent the same vector with\\nrespect to (b1, b2) (see Figure 2.9).\\nRemark. For an n-dimensional vector space V and an ordered basis B\\nof V , the mapping Φ : Rn → V , Φ(ei) = bi, i = 1 , . . . , n, is linear\\n(and because of Theorem 2.17 an isomorphism), where (e1, . . . ,en) is\\nthe standard basis of Rn.\\n♢\\nNow we are ready to make an explicit connection between matrices and\\nlinear mappings between finite-dimensional vector spaces.\\nDefinition 2.19 (Transformation Matrix). Consider vector spaces V, W\\nwith corresponding (ordered) basesB = (b1, . . . ,bn) and C = (c1, . . . ,cm).\\nMoreover, we consider a linear mapping Φ : V → W . For j ∈ {1, . . . , n},\\nΦ(bj) = α1jc1 + · · · + αmjcm =\\nmX\\ni=1\\nαijci (2.92)\\nis the unique representation of Φ(bj) with respect to C. Then, we call the\\nm × n-matrix AΦ, whose elements are given by\\nAΦ(i, j) = αij , (2.93)\\nthe transformation matrix of Φ (with respect to the ordered bases B of V transformation\\nmatrixand C of W ).\\nThe coordinates of Φ(bj) with respect to the ordered basis C of W\\nare the j-th column of AΦ. Consider (finite-dimensional) vector spaces\\nV, W with ordered bases B, C and a linear mapping Φ : V → W with\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4a87e24-7c7c-482b-96dd-92732aee7db4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 57, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='52 Linear Algebra\\ntransformation matrix AΦ. If ˆx is the coordinate vector of x ∈ V with\\nrespect to B and ˆy the coordinate vector of y = Φ(x) ∈ W with respect\\nto C, then\\nˆy = AΦˆx . (2.94)\\nThis means that the transformation matrix can be used to map coordinates\\nwith respect to an ordered basis in V to coordinates with respect to an\\nordered basis in W .\\nExample 2.21 (Transformation Matrix)\\nConsider a homomorphism Φ : V → W and ordered bases B =\\n(b1, . . . ,b3) of V and C = (c1, . . . ,c4) of W . With\\nΦ(b1) = c1 − c2 + 3c3 − c4\\nΦ(b2) = 2c1 + c2 + 7c3 + 2c4\\nΦ(b3) = 3c2 + c3 + 4c4\\n(2.95)\\nthe transformation matrix AΦ with respect to B and C satisfies Φ(bk) =P4\\ni=1 αikci for k = 1, . . . ,3 and is given as\\nAΦ = [α1, α2, α3] =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 2 0\\n−1 1 3\\n3 7 1\\n−1 2 4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , (2.96)\\nwhere the αj, j = 1, 2, 3, are the coordinate vectors ofΦ(bj) with respect\\nto C.\\nExample 2.22 (Linear Transformations of Vectors)\\nFigure 2.10 Three\\nexamples of linear\\ntransformations of\\nthe vectors shown\\nas dots in (a);\\n(b) Rotation by 45◦;\\n(c) Stretching of the\\nhorizontal\\ncoordinates by 2;\\n(d) Combination of\\nreflection, rotation\\nand stretching.\\n(a) Original data.\\n (b) Rotation by 45◦.\\n (c) Stretch along the\\nhorizontal axis.\\n(d) General linear\\nmapping.\\nWe consider three linear transformations of a set of vectors in R2 with\\nthe transformation matrices\\nA1 =\\n\\x14cos( π\\n4 ) − sin( π\\n4 )\\nsin( π\\n4 ) cos( π\\n4 )\\n\\x15\\n, A2 =\\n\\x142 0\\n0 1\\n\\x15\\n, A3 = 1\\n2\\n\\x143 −1\\n1 −1\\n\\x15\\n. (2.97)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de209b4c-0143-44c4-9f65-c3cd33c25aad', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 58, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 53\\nFigure 2.10 gives three examples of linear transformations of a set of vec-\\ntors. Figure 2.10(a) shows 400 vectors in R2, each of which is represented\\nby a dot at the corresponding (x1, x2)-coordinates. The vectors are ar-\\nranged in a square. When we use matrixA1 in (2.97) to linearly transform\\neach of these vectors, we obtain the rotated square in Figure 2.10(b). If we\\napply the linear mapping represented by A2, we obtain the rectangle in\\nFigure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d)\\nshows the original square from Figure 2.10(a) when linearly transformed\\nusing A3, which is a combination of a reflection, a rotation, and a stretch.\\n2.7.2 Basis Change\\nIn the following, we will have a closer look at how transformation matrices\\nof a linear mapping Φ : V → W change if we change the bases in V and\\nW . Consider two ordered bases\\nB = (b1, . . . ,bn), ˜B = (˜b1, . . . ,˜bn) (2.98)\\nof V and two ordered bases\\nC = (c1, . . . ,cm), ˜C = (˜c1, . . . ,˜cm) (2.99)\\nof W . Moreover, AΦ ∈ Rm×n is the transformation matrix of the linear\\nmapping Φ : V → W with respect to the bases B and C, and ˜AΦ ∈ Rm×n\\nis the corresponding transformation mapping with respect to ˜B and ˜C.\\nIn the following, we will investigate how A and ˜A are related, i.e., how/\\nwhether we can transform AΦ into ˜AΦ if we choose to perform a basis\\nchange from B, C to ˜B, ˜C.\\nRemark. We effectively get different coordinate representations of the\\nidentity mapping idV . In the context of Figure 2.9, this would mean to\\nmap coordinates with respect to (e1, e2) onto coordinates with respect to\\n(b1, b2) without changing the vector x. By changing the basis and corre-\\nspondingly the representation of vectors, the transformation matrix with\\nrespect to this new basis can have a particularly simple form that allows\\nfor straightforward computation. ♢\\nExample 2.23 (Basis Change)\\nConsider a transformation matrix\\nA =\\n\\x142 1\\n1 2\\n\\x15\\n(2.100)\\nwith respect to the canonical basis in R2. If we define a new basis\\nB = (\\n\\x141\\n1\\n\\x15\\n,\\n\\x14 1\\n−1\\n\\x15\\n) (2.101)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1745169c-304d-41d3-bda6-4bc3a3c4cb63', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 59, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='54 Linear Algebra\\nwe obtain a diagonal transformation matrix\\n˜A =\\n\\x143 0\\n0 1\\n\\x15\\n(2.102)\\nwith respect to B, which is easier to work with than A.\\nIn the following, we will look at mappings that transform coordinate\\nvectors with respect to one basis into coordinate vectors with respect to\\na different basis. We will state our main result first and then provide an\\nexplanation.\\nTheorem 2.20 (Basis Change). For a linear mappingΦ : V → W , ordered\\nbases\\nB = (b1, . . . ,bn), ˜B = (˜b1, . . . ,˜bn) (2.103)\\nof V and\\nC = (c1, . . . ,cm), ˜C = (˜c1, . . . ,˜cm) (2.104)\\nof W , and a transformation matrix AΦ of Φ with respect to B and C, the\\ncorresponding transformation matrix ˜AΦ with respect to the bases ˜B and ˜C\\nis given as\\n˜AΦ = T −1AΦS . (2.105)\\nHere, S ∈ Rn×n is the transformation matrix of idV that maps coordinates\\nwith respect to ˜B onto coordinates with respect to B, and T ∈ Rm×m is the\\ntransformation matrix of idW that maps coordinates with respect to ˜C onto\\ncoordinates with respect to C.\\nProof Following Drumm and Weil (2001), we can write the vectors of\\nthe new basis ˜B of V as a linear combination of the basis vectors of B,\\nsuch that\\n˜bj = s1jb1 + · · · + snjbn =\\nnX\\ni=1\\nsijbi , j = 1, . . . , n . (2.106)\\nSimilarly , we write the new basis vectors ˜C of W as a linear combination\\nof the basis vectors of C, which yields\\n˜ck = t1kc1 + · · · + tmkcm =\\nmX\\nl=1\\ntlkcl , k = 1, . . . , m . (2.107)\\nWe define S = (( sij)) ∈ Rn×n as the transformation matrix that maps\\ncoordinates with respect to ˜B onto coordinates with respect to B and\\nT = ((tlk)) ∈ Rm×m as the transformation matrix that maps coordinates\\nwith respect to ˜C onto coordinates with respect toC. In particular, thejth\\ncolumn of S is the coordinate representation of ˜bj with respect to B and\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7499f5de-59a4-419e-9cc4-06a3306cbb73', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 60, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 55\\nthe kth column of T is the coordinate representation of ˜ck with respect to\\nC. Note that both S and T are regular.\\nWe are going to look atΦ(˜bj) from two perspectives. First, applying the\\nmapping Φ, we get that for all j = 1, . . . , n\\nΦ(˜bj) =\\nmX\\nk=1\\n˜akj˜ck\\n|{z}\\n∈W\\n(2.107)\\n=\\nmX\\nk=1\\n˜akj\\nmX\\nl=1\\ntlkcl =\\nmX\\nl=1\\n mX\\nk=1\\ntlk˜akj\\n!\\ncl , (2.108)\\nwhere we first expressed the new basis vectors ˜ck ∈ W as linear com-\\nbinations of the basis vectors cl ∈ W and then swapped the order of\\nsummation.\\nAlternatively , when we express the ˜bj ∈ V as linear combinations of\\nbj ∈ V , we arrive at\\nΦ(˜bj)\\n(2.106)\\n= Φ\\n nX\\ni=1\\nsijbi\\n!\\n=\\nnX\\ni=1\\nsijΦ(bi) =\\nnX\\ni=1\\nsij\\nmX\\nl=1\\nalicl (2.109a)\\n=\\nmX\\nl=1\\n nX\\ni=1\\nalisij\\n!\\ncl , j = 1, . . . , n , (2.109b)\\nwhere we exploited the linearity of Φ. Comparing (2.108) and (2.109b),\\nit follows for all j = 1, . . . , nand l = 1, . . . , mthat\\nmX\\nk=1\\ntlk˜akj =\\nnX\\ni=1\\nalisij (2.110)\\nand, therefore,\\nT ˜AΦ = AΦS ∈ Rm×n , (2.111)\\nsuch that\\n˜AΦ = T −1AΦS , (2.112)\\nwhich proves Theorem 2.20.\\nTheorem 2.20 tells us that with a basis change inV (B is replaced with\\n˜B) and W (C is replaced with ˜C), the transformation matrix AΦ of a\\nlinear mapping Φ : V → W is replaced by an equivalent matrix ˜AΦ with\\n˜AΦ = T −1AΦS. (2.113)\\nFigure 2.11 illustrates this relation: Consider a homomorphism Φ : V →\\nW and ordered bases B, ˜B of V and C, ˜C of W . The mapping ΦCB is an\\ninstantiation of Φ and maps basis vectors of B onto linear combinations\\nof basis vectors of C. Assume that we know the transformation matrixAΦ\\nof ΦCB with respect to the ordered bases B, C. When we perform a basis\\nchange from B to ˜B in V and from C to ˜C in W , we can determine the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d63a9a15-3849-4c6d-9183-512c45a41342', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 61, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='56 Linear Algebra\\nFigure 2.11 For a\\nhomomorphism\\nΦ : V → W and\\nordered bases B, ˜B\\nof V and C, ˜C of W\\n(marked in blue),\\nwe can express the\\nmapping Φ ˜C ˜B with\\nrespect to the bases\\n˜B, ˜C equivalently as\\na composition of the\\nhomomorphisms\\nΦ ˜C ˜B =\\nΞ ˜CC ◦ ΦCB ◦ ΨB ˜B\\nwith respect to the\\nbases in the\\nsubscripts. The\\ncorresponding\\ntransformation\\nmatrices are in red.\\nV W\\nB\\n˜B ˜C\\nC\\nΦ\\nΦCB\\nΦ˜C˜B\\nΨB˜B ΞC˜CS T\\n˜AΦ\\nAΦ\\nV W\\nB\\n˜B ˜C\\nC\\nΦ\\nΦCB\\nΦ˜C˜B\\nΨB˜B Ξ˜CC= Ξ−1\\nC˜CS T−1\\n˜AΦ\\nAΦ\\nVector spaces\\nOrdered bases\\ncorresponding transformation matrix ˜AΦ as follows: First, we find the ma-\\ntrix representation of the linear mappingΨB ˜B : V → V that maps coordi-\\nnates with respect to the new basis ˜B onto the (unique) coordinates with\\nrespect to the “old” basisB (in V ). Then, we use the transformation ma-\\ntrix AΦ of ΦCB : V → W to map these coordinates onto the coordinates\\nwith respect to C in W . Finally , we use a linear mappingΞ ˜CC : W → W\\nto map the coordinates with respect to C onto coordinates with respect to\\n˜C. Therefore, we can express the linear mappingΦ ˜C ˜B as a composition of\\nlinear mappings that involve the “old” basis:\\nΦ ˜C ˜B = Ξ ˜CC ◦ ΦCB ◦ ΨB ˜B = Ξ−1\\nC ˜C ◦ ΦCB ◦ ΨB ˜B . (2.114)\\nConcretely , we useΨB ˜B = idV and ΞC ˜C = idW , i.e., the identity mappings\\nthat map vectors onto themselves, but with respect to a different basis.\\nDefinition 2.21(Equivalence). Two matricesA, ˜A ∈ Rm×n are equivalentequivalent\\nif there exist regular matrices S ∈ Rn×n and T ∈ Rm×m, such that\\n˜A = T −1AS.\\nDefinition 2.22 (Similarity). Two matrices A, ˜A ∈ Rn×n are similar ifsimilar\\nthere exists a regular matrix S ∈ Rn×n with ˜A = S−1AS\\nRemark. Similar matrices are always equivalent. However, equivalent ma-\\ntrices are not necessarily similar. ♢\\nRemark. Consider vector spaces V, W, X. From the remark that follows\\nTheorem 2.17, we already know that for linear mappings Φ : V → W\\nand Ψ : W → X the mapping Ψ ◦ Φ : V → X is also linear. With\\ntransformation matrices AΦ and AΨ of the corresponding mappings, the\\noverall transformation matrix is AΨ◦Φ = AΨAΦ. ♢\\nIn light of this remark, we can look at basis changes from the perspec-\\ntive of composing linear mappings:\\nAΦ is the transformation matrix of a linear mapping ΦCB : V → W\\nwith respect to the bases B, C.\\n˜AΦ is the transformation matrix of the linear mapping Φ ˜C ˜B : V → W\\nwith respect to the bases ˜B, ˜C.\\nS is the transformation matrix of a linear mapping ΨB ˜B : V → V\\n(automorphism) that represents ˜B in terms of B. Normally ,Ψ = idV is\\nthe identity mapping in V .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f78e0a6e-c725-479e-82eb-68e8291f5dde', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 62, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 57\\nT is the transformation matrix of a linear mapping ΞC ˜C : W → W\\n(automorphism) that represents ˜C in terms of C. Normally ,Ξ = idW is\\nthe identity mapping in W .\\nIf we (informally) write down the transformations just in terms of bases,\\nthen AΦ : B → C, ˜AΦ : ˜B → ˜C, S : ˜B → B, T : ˜C → C and\\nT −1 : C → ˜C, and\\n˜B → ˜C = ˜B → B→ C → ˜C (2.115)\\n˜AΦ = T −1AΦS . (2.116)\\nNote that the execution order in (2.116) is from right to left because vec-\\ntors are multiplied at the right-hand side so that x 7→ Sx 7→ AΦ(Sx) 7→\\nT −1\\x00\\nAΦ(Sx)\\n\\x01\\n= ˜AΦx.\\nExample 2.24 (Basis Change)\\nConsider a linear mapping Φ : R3 → R4 whose transformation matrix is\\nAΦ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 2 0\\n−1 1 3\\n3 7 1\\n−1 2 4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2.117)\\nwith respect to the standard bases\\nB = (\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb) , C = (\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb). (2.118)\\nWe seek the transformation matrix ˜AΦ of Φ with respect to the new bases\\n˜B = (\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n1\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fb) ∈ R3, ˜C = (\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n1\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb) . (2.119)\\nThen,\\nS =\\n\\uf8ee\\n\\uf8f0\\n1 0 1\\n1 1 0\\n0 1 1\\n\\uf8f9\\n\\uf8fb , T =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 0 1\\n1 0 1 0\\n0 1 1 0\\n0 0 0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , (2.120)\\nwhere the ith column of S is the coordinate representation of ˜bi in\\nterms of the basis vectors of B. Since B is the standard basis, the co-\\nordinate representation is straightforward to find. For a general basis B,\\nwe would need to solve a linear equation system to find the λi such that\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aba926d2-a72e-43ff-86ab-c0d890f06805', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 63, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='58 Linear Algebra\\nP3\\ni=1 λibi = ˜bj, j = 1, . . . ,3. Similarly , thejth column of T is the coordi-\\nnate representation of ˜cj in terms of the basis vectors of C.\\nTherefore, we obtain\\n˜AΦ = T −1AΦS = 1\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 −1 −1\\n1 −1 1 −1\\n−1 1 1 1\\n0 0 0 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n3 2 1\\n0 4 2\\n10 8 4\\n1 6 3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2.121a)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−4 −4 −2\\n6 0 0\\n4 8 4\\n1 6 3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (2.121b)\\nIn Chapter 4, we will be able to exploit the concept of a basis change\\nto find a basis with respect to which the transformation matrix of an en-\\ndomorphism has a particularly simple (diagonal) form. In Chapter 10, we\\nwill look at a data compression problem and find a convenient basis onto\\nwhich we can project the data while minimizing the compression loss.\\n2.7.3 Image and Kernel\\nThe image and kernel of a linear mapping are vector subspaces with cer-\\ntain important properties. In the following, we will characterize them\\nmore carefully .\\nDefinition 2.23 (Image and Kernel).\\nFor Φ : V → W , we define the kernel/null spacekernel\\nnull space\\nker(Φ) := Φ−1(0W ) = {v ∈ V : Φ(v) = 0W } (2.122)\\nand the image/rangeimage\\nrange\\nIm(Φ) := Φ(V ) = {w ∈ W |∃v ∈ V : Φ(v) = w} . (2.123)\\nWe also call V and W also the domain and codomain of Φ, respectively .domain\\ncodomain\\nIntuitively , the kernel is the set of vectorsv ∈ V that Φ maps onto the\\nneutral element 0W ∈ W . The image is the set of vectors w ∈ W that\\ncan be “reached” by Φ from any vector in V . An illustration is given in\\nFigure 2.12.\\nRemark. Consider a linear mapping Φ : V → W , where V, W are vector\\nspaces.\\nIt always holds that Φ(0V ) = 0W and, therefore, 0V ∈ ker(Φ). In\\nparticular, the null space is never empty .\\nIm(Φ) ⊆ W is a subspace of W , and ker(Φ) ⊆ V is a subspace of V .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9854f38d-3093-44bc-9b11-2673440d68fc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 64, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7 Linear Mappings 59\\nFigure 2.12 Kernel\\nand image of a\\nlinear mapping\\nΦ : V → W .\\nIm(Φ)\\n0W\\nker(Φ)\\n0V\\nΦ : V → WV W\\nΦ is injective (one-to-one) if and only if ker(Φ) = {0}.\\n♢\\nRemark (Null Space and Column Space). Let us consider A ∈ Rm×n and\\na linear mapping Φ : Rn → Rm, x 7→ Ax.\\nFor A = [a1, . . . ,an], where ai are the columns of A, we obtain\\nIm(Φ) = {Ax : x ∈ Rn} =\\n( nX\\ni=1\\nxiai : x1, . . . , xn ∈ R\\n)\\n(2.124a)\\n= span[a1, . . . ,an] ⊆ Rm , (2.124b)\\ni.e., the image is the span of the columns of A, also called the column column space\\nspace. Therefore, the column space (image) is a subspace of Rm, where\\nm is the “height” of the matrix.\\nrk(A) = dim(Im(Φ)).\\nThe kernel/null space ker(Φ) is the general solution to the homoge-\\nneous system of linear equations Ax = 0 and captures all possible\\nlinear combinations of the elements in Rn that produce 0 ∈ Rm.\\nThe kernel is a subspace of Rn, where n is the “width” of the matrix.\\nThe kernel focuses on the relationship among the columns, and we can\\nuse it to determine whether/how we can express a column as a linear\\ncombination of other columns.\\n♢\\nExample 2.25 (Image and Kernel of a Linear Mapping)\\nThe mapping\\nΦ : R4 → R2,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\nx3\\nx4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb 7→\\n\\x141 2 −1 0\\n1 0 0 1\\n\\x15\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\nx3\\nx4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb =\\n\\x14x1 + 2x2 − x3\\nx1 + x4\\n\\x15\\n(2.125a)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31c01d4b-f9f8-4308-985d-9e9178e987b4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 65, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='60 Linear Algebra\\n= x1\\n\\x141\\n1\\n\\x15\\n+ x2\\n\\x142\\n0\\n\\x15\\n+ x3\\n\\x14−1\\n0\\n\\x15\\n+ x4\\n\\x140\\n1\\n\\x15\\n(2.125b)\\nis linear. To determineIm(Φ), we can take the span of the columns of the\\ntransformation matrix and obtain\\nIm(Φ) = span[\\n\\x141\\n1\\n\\x15\\n,\\n\\x142\\n0\\n\\x15\\n,\\n\\x14−1\\n0\\n\\x15\\n,\\n\\x140\\n1\\n\\x15\\n] . (2.126)\\nTo compute the kernel (null space) of Φ, we need to solve Ax = 0, i.e.,\\nwe need to solve a homogeneous equation system. To do this, we use\\nGaussian elimination to transform A into reduced row-echelon form:\\n\\x141 2 −1 0\\n1 0 0 1\\n\\x15\\n⇝ · · · ⇝\\n\\x141 0 0 1\\n0 1 − 1\\n2 − 1\\n2\\n\\x15\\n. (2.127)\\nThis matrix is in reduced row-echelon form, and we can use the Minus-\\n1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively ,\\nwe can express the non-pivot columns (columns 3 and 4) as linear com-\\nbinations of the pivot columns (columns 1 and 2). The third column a3 is\\nequivalent to − 1\\n2 times the second column a2. Therefore, 0 = a3+ 1\\n2 a2. In\\nthe same way , we see thata4 = a1− 1\\n2 a2 and, therefore,0 = a1− 1\\n2 a2−a4.\\nOverall, this gives us the kernel (null space) as\\nker(Φ) = span[\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n1\\n2\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n1\\n2\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb] . (2.128)\\nrank-nullity\\ntheorem Theorem 2.24 (Rank-Nullity Theorem). For vector spaces V, W and a lin-\\near mapping Φ : V → W it holds that\\ndim(ker(Φ)) + dim(Im(Φ)) = dim(V ) . (2.129)\\nThe rank-nullity theorem is also referred to as the fundamental theoremfundamental\\ntheorem of linear\\nmappings\\nof linear mappings (Axler, 2015, theorem 3.22). The following are direct\\nconsequences of Theorem 2.24:\\nIf dim(Im(Φ)) < dim(V ), then ker(Φ) is non-trivial, i.e., the kernel\\ncontains more than 0V and dim(ker(Φ)) ⩾ 1.\\nIf AΦ is the transformation matrix ofΦ with respect to an ordered basis\\nand dim(Im(Φ)) < dim(V ), then the system of linear equationsAΦx =\\n0 has infinitely many solutions.\\nIf dim(V ) = dim(W ), then the three-way equivalence\\nΦ is injective ⇐ ⇒ Φ is surjective ⇐ ⇒ Φ is bijective\\nholds since Im(Φ) ⊆ W .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7dbb773a-604c-4d6a-956d-7471a45a9267', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 66, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.8 Affine Spaces 61\\n2.8 Affine Spaces\\nIn the following, we will take a closer look at spaces that are offset from\\nthe origin, i.e., spaces that are no longer vector subspaces. Moreover, we\\nwill briefly discuss properties of mappings between these affine spaces,\\nwhich resemble linear mappings.\\nRemark. In the machine learning literature, the distinction between linear\\nand affine is sometimes not clear so that we can find references to affine\\nspaces/mappings as linear spaces/mappings. ♢\\n2.8.1 Affine Subspaces\\nDefinition 2.25 (Affine Subspace). Let V be a vector space, x0 ∈ V and\\nU ⊆ V a subspace. Then the subset\\nL = x0 + U := {x0 + u : u ∈ U } (2.130a)\\n= {v ∈ V |∃u ∈ U : v = x0 + u} ⊆ V (2.130b)\\nis called affine subspace or linear manifold of V . U is called direction or affine subspace\\nlinear manifold\\ndirection\\ndirection space, and x0 is called support point. In Chapter 12, we refer to\\ndirection space\\nsupport point\\nsuch a subspace as a hyperplane.\\nhyperplane\\nNote that the definition of an affine subspace excludes 0 if x0 /∈ U.\\nTherefore, an affine subspace is not a (linear) subspace (vector subspace)\\nof V for x0 /∈ U.\\nExamples of affine subspaces are points, lines, and planes in R3, which\\ndo not (necessarily) go through the origin.\\nRemark. Consider two affine subspaces L = x0 + U and ˜L = ˜x0 + ˜U of a\\nvector space V . Then, L ⊆ ˜L if and only if U ⊆ ˜U and x0 − ˜x0 ∈ ˜U.\\nAffine subspaces are often described byparameters: Consider ak-dimen-\\nsional affine space L = x0 + U of V . If (b1, . . . ,bk) is an ordered basis of\\nU, then every element x ∈ L can be uniquely described as\\nx = x0 + λ1b1 + . . . + λkbk , (2.131)\\nwhere λ1, . . . , λk ∈ R. This representation is called parametric equation parametric equation\\nof L with directional vectors b1, . . . ,bk and parameters λ1, . . . , λk. ♢ parameters\\nExample 2.26 (Affine Subspaces)\\nOne-dimensional affine subspaces are called lines and can be written line\\nas y = x0 + λb1, where λ ∈ R and U = span[ b1] ⊆ Rn is a one-\\ndimensional subspace of Rn. This means that a line is defined by a sup-\\nport point x0 and a vector b1 that defines the direction. See Figure 2.13\\nfor an illustration.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='26c38069-1ef3-430e-83fe-6c682b97c2e2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 67, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='62 Linear Algebra\\nTwo-dimensional affine subspaces of Rn are called planes. The para-plane\\nmetric equation for planes is y = x0 + λ1b1 + λ2b2, where λ1, λ2 ∈ R\\nand U = span[ b1, b2] ⊆ Rn. This means that a plane is defined by a\\nsupport point x0 and two linearly independent vectors b1, b2 that span\\nthe direction space.\\nIn Rn, the (n − 1)-dimensional affine subspaces are called hyperplanes,hyperplane\\nand the corresponding parametric equation is y = x0 +Pn−1\\ni=1 λibi,\\nwhere b1, . . . ,bn−1 form a basis of an (n − 1)-dimensional subspace\\nU of Rn. This means that a hyperplane is defined by a support point\\nx0 and (n − 1) linearly independent vectors b1, . . . ,bn−1 that span the\\ndirection space. In R2, a line is also a hyperplane. In R3, a plane is also\\na hyperplane.\\nFigure 2.13 Lines\\nare affine subspaces.\\nVectors y on a line\\nx0 + λb1 lie in an\\naffine subspace L\\nwith support point\\nx0 and direction b1.\\n0\\nx0\\nb1\\ny\\nL = x0 + λb1\\nRemark (Inhomogeneous systems of linear equations and affine subspaces).\\nFor A ∈ Rm×n and x ∈ Rm, the solution of the system of linear equa-\\ntions Aλ = x is either the empty set or an affine subspace of Rn of\\ndimension n − rk(A). In particular, the solution of the linear equation\\nλ1b1 + . . . + λnbn = x, where (λ1, . . . , λn) ̸= (0, . . . ,0), is a hyperplane\\nin Rn.\\nIn Rn, every k-dimensional affine subspace is the solution of an inho-\\nmogeneous system of linear equations Ax = b, where A ∈ Rm×n, b ∈\\nRm and rk(A) = n − k. Recall that for homogeneous equation systems\\nAx = 0 the solution was a vector subspace, which we can also think of\\nas a special affine space with support point x0 = 0. ♢\\n2.8.2 Affine Mappings\\nSimilar to linear mappings between vector spaces, which we discussed\\nin Section 2.7, we can define affine mappings between two affine spaces.\\nLinear and affine mappings are closely related. Therefore, many properties\\nthat we already know from linear mappings, e.g., that the composition of\\nlinear mappings is a linear mapping, also hold for affine mappings.\\nDefinition 2.26 (Affine Mapping). For two vector spaces V, W , a linear\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='38b001aa-93b1-4099-973d-62ae6f19ef5c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 68, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.9 Further Reading 63\\nmapping Φ : V → W , and a ∈ W , the mapping\\nϕ : V → W (2.132)\\nx 7→ a + Φ(x) (2.133)\\nis an affine mapping from V to W . The vector a is called the translation affine mapping\\ntranslation vectorvector of ϕ.\\nEvery affine mapping ϕ : V → W is also the composition of a linear\\nmapping Φ : V → W and a translation τ : W → W in W , such that\\nϕ = τ ◦ Φ. The mappings Φ and τ are uniquely determined.\\nThe composition ϕ′ ◦ ϕ of affine mappings ϕ : V → W , ϕ′ : W → X is\\naffine.\\nIf ϕ is bijective, affine mappings keep the geometric structure invariant.\\nThey then also preserve the dimension and parallelism.\\n2.9 Further Reading\\nThere are many resources for learning linear algebra, including the text-\\nbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and\\nMehrmann (2015). There are also several online resources that we men-\\ntioned in the introduction to this chapter. We only covered Gaussian elim-\\nination here, but there are many other approaches for solving systems of\\nlinear equations, and we refer to numerical linear algebra textbooks by\\nStoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and\\nJohnson (2013) for an in-depth discussion.\\nIn this book, we distinguish between the topics of linear algebra (e.g.,\\nvectors, matrices, linear independence, basis) and topics related to the\\ngeometry of a vector space. In Chapter 3, we will introduce the inner\\nproduct, which induces a norm. These concepts allow us to define angles,\\nlengths and distances, which we will use for orthogonal projections. Pro-\\njections turn out to be key in many machine learning algorithms, such as\\nlinear regression and principal component analysis, both of which we will\\ncover in Chapters 9 and 10, respectively .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ea0596c-451a-48ab-86fb-a0b37fd6f593', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 69, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='64 Linear Algebra\\nExercises\\n2.1 We consider (R\\\\{−1}, ⋆), where\\na ⋆ b := ab + a + b, a, b ∈ R\\\\{−1} (2.134)\\na. Show that (R\\\\{−1}, ⋆) is an Abelian group.\\nb. Solve\\n3 ⋆ x ⋆ x = 15\\nin the Abelian group (R\\\\{−1}, ⋆), where ⋆ is defined in (2.134).\\n2.2 Let n be in N\\\\{0}. Let k, x be in Z. We define the congruence class ¯k of the\\ninteger k as the set\\nk = {x ∈ Z | x − k = 0 (mod n)}\\n= {x ∈ Z | ∃a ∈ Z: (x − k = n · a)} .\\nWe now define Z/nZ (sometimes written Zn) as the set of all congruence\\nclasses modulo n. Euclidean division implies that this set is a finite set con-\\ntaining n elements:\\nZn = {0, 1, . . . ,n − 1}\\nFor all a, b ∈ Zn, we define\\na ⊕ b := a + b\\na. Show that (Zn, ⊕) is a group. Is it Abelian?\\nb. We now define another operation ⊗ for all a and b in Zn as\\na ⊗ b = a × b , (2.135)\\nwhere a × b represents the usual multiplication in Z.\\nLet n = 5. Draw the times table of the elements of Z5\\\\{0} under ⊗, i.e.,\\ncalculate the products a ⊗ b for all a and b in Z5\\\\{0}.\\nHence, show that Z5\\\\{0} is closed under ⊗ and possesses a neutral\\nelement for ⊗. Display the inverse of all elements in Z5\\\\{0} under ⊗.\\nConclude that (Z5\\\\{0}, ⊗) is an Abelian group.\\nc. Show that (Z8\\\\{0}, ⊗) is not a group.\\nd. We recall that the B ´ezout theorem states that two integers a and b are\\nrelatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers\\nu and v such that au + bv = 1. Show that (Zn\\\\{0}, ⊗) is a group if and\\nonly if n ∈ N\\\\{0} is prime.\\n2.3 Consider the set G of 3 × 3 matrices defined as follows:\\nG =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n1 x z\\n0 1 y\\n0 0 1\\n\\uf8f9\\n\\uf8fb ∈ R3×3\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\nx, y, z ∈ R\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\nWe define · as the standard matrix multiplication.\\nIs (G, ·) a group? If yes, is it Abelian? Justify your answer.\\n2.4 Compute the following matrix products, if possible:\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f5097523-a0f2-45f8-82e3-8c88ab4688f0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 70, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 65\\na.\\n\\uf8ee\\n\\uf8f0\\n1 2\\n4 5\\n7 8\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n1 1 0\\n0 1 1\\n1 0 1\\n\\uf8f9\\n\\uf8fb\\nb.\\n\\uf8ee\\n\\uf8f0\\n1 2 3\\n4 5 6\\n7 8 9\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n1 1 0\\n0 1 1\\n1 0 1\\n\\uf8f9\\n\\uf8fb\\nc.\\n\\uf8ee\\n\\uf8f0\\n1 1 0\\n0 1 1\\n1 0 1\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n1 2 3\\n4 5 6\\n7 8 9\\n\\uf8f9\\n\\uf8fb\\nd.\\n\\x14\\n1 2 1 2\\n4 1 −1 −4\\n\\x15\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0 3\\n1 −1\\n2 1\\n5 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\ne.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0 3\\n1 −1\\n2 1\\n5 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\x14\\n1 2 1 2\\n4 1 −1 −4\\n\\x15\\n2.5 Find the set S of all solutions in x of the following inhomogeneous linear\\nsystems Ax = b, where A and b are defined as follows:\\na.\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 −1 −1\\n2 5 −7 −5\\n2 −1 1 3\\n5 2 −4 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , b =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n−2\\n4\\n6\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nb.\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 −1 0 0 1\\n1 1 0 −3 0\\n2 −1 0 1 −1\\n−1 2 0 −2 −1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , b =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n3\\n6\\n5\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n2.6 Using Gaussian elimination, find all solutions of the inhomogeneous equa-\\ntion system Ax = b with\\nA =\\n\\uf8ee\\n\\uf8f0\\n0 1 0 0 1 0\\n0 0 0 1 1 0\\n0 1 0 0 0 1\\n\\uf8f9\\n\\uf8fb , b =\\n\\uf8ee\\n\\uf8f0\\n2\\n−1\\n1\\n\\uf8f9\\n\\uf8fb .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c9b7915-cba5-45e6-8db8-e3f3367af729', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 71, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='66 Linear Algebra\\n2.7 Find all solutions in x =\\n\\uf8ee\\n\\uf8f0\\nx1\\nx2\\nx3\\n\\uf8f9\\n\\uf8fb ∈ R3 of the equation system Ax = 12 x,\\nwhere\\nA =\\n\\uf8ee\\n\\uf8f0\\n6 4 3\\n6 0 9\\n0 8 0\\n\\uf8f9\\n\\uf8fb\\nandP3\\ni=1 xi = 1.\\n2.8 Determine the inverses of the following matrices if possible:\\na.\\nA =\\n\\uf8ee\\n\\uf8f0\\n2 3 4\\n3 4 5\\n4 5 6\\n\\uf8f9\\n\\uf8fb\\nb.\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 1 0\\n0 1 1 0\\n1 1 0 1\\n1 1 1 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n2.9 Which of the following sets are subspaces of R3?\\na. A = {(λ, λ + µ3, λ − µ3) | λ, µ ∈ R}\\nb. B = {(λ2, −λ2, 0) | λ ∈ R}\\nc. Let γ be in R.\\nC = {(ξ1, ξ2, ξ3) ∈ R3 | ξ1 − 2ξ2 + 3ξ3 = γ}\\nd. D = {(ξ1, ξ2, ξ3) ∈ R3 | ξ2 ∈ Z}\\n2.10 Are the following sets of vectors linearly independent?\\na.\\nx1 =\\n\\uf8ee\\n\\uf8f0\\n2\\n−1\\n3\\n\\uf8f9\\n\\uf8fb , x2 =\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n−2\\n\\uf8f9\\n\\uf8fb , x3 =\\n\\uf8ee\\n\\uf8f0\\n3\\n−3\\n8\\n\\uf8f9\\n\\uf8fb\\nb.\\nx1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n2\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, x2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n0\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, x3 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n2.11 Write\\ny =\\n\\uf8ee\\n\\uf8f0\\n1\\n−2\\n5\\n\\uf8f9\\n\\uf8fb\\nas linear combination of\\nx1 =\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb , x2 =\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n3\\n\\uf8f9\\n\\uf8fb , x3 =\\n\\uf8ee\\n\\uf8f0\\n2\\n−1\\n1\\n\\uf8f9\\n\\uf8fb\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='255b08bf-6850-40c0-b36b-f60998d42aec', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 72, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 67\\n2.12 Consider two subspaces of R4:\\nU1 = span[\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n−3\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n2\\n−1\\n0\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n1\\n−1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb] , U 2 = span[\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−2\\n2\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n2\\n−2\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−3\\n6\\n−2\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb] .\\nDetermine a basis of U1 ∩ U2.\\n2.13 Consider two subspaces U1 and U2, where U1 is the solution space of the\\nhomogeneous equation system A1x = 0 and U2 is the solution space of the\\nhomogeneous equation system A2x = 0 with\\nA1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 1\\n1 −2 −1\\n2 1 3\\n1 0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , A2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n3 −3 0\\n1 2 3\\n7 −5 2\\n3 −1 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb .\\na. Determine the dimension of U1, U2.\\nb. Determine bases of U1 and U2.\\nc. Determine a basis of U1 ∩ U2.\\n2.14 Consider two subspaces U1 and U2, where U1 is spanned by the columns of\\nA1 and U2 is spanned by the columns of A2 with\\nA1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0 1\\n1 −2 −1\\n2 1 3\\n1 0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , A2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n3 −3 0\\n1 2 3\\n7 −5 2\\n3 −1 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb .\\na. Determine the dimension of U1, U2\\nb. Determine bases of U1 and U2\\nc. Determine a basis of U1 ∩ U2\\n2.15 Let F = {(x, y, z) ∈ R3 | x+y−z = 0} and G = {(a−b, a+b, a−3b) | a, b ∈ R}.\\na. Show that F and G are subspaces of R3.\\nb. Calculate F ∩ G without resorting to any basis vector.\\nc. Find one basis for F and one forG, calculate F ∩G using the basis vectors\\npreviously found and check your result with the previous question.\\n2.16 Are the following mappings linear?\\na. Let a, b ∈ R.\\nΦ : L1([a, b]) → R\\nf 7→ Φ(f) =\\nZ b\\na\\nf(x)dx ,\\nwhere L1([a, b]) denotes the set of integrable functions on [a, b].\\nb.\\nΦ : C1 → C0\\nf 7→ Φ(f) = f ′ ,\\nwhere for k ⩾ 1, Ck denotes the set of k times continuously differen-\\ntiable functions, and C0 denotes the set of continuous functions.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3f9a4dc-2e4e-4fe1-b6e2-a57ae59e3a0b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 73, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='68 Linear Algebra\\nc.\\nΦ : R → R\\nx 7→ Φ(x) = cos(x)\\nd.\\nΦ : R3 → R2\\nx 7→\\n\\x14\\n1 2 3\\n1 4 3\\n\\x15\\nx\\ne. Let θ be in [0, 2π[ and\\nΦ : R2 → R2\\nx 7→\\n\\x14\\ncos(θ) sin( θ)\\n− sin(θ) cos( θ)\\n\\x15\\nx\\n2.17 Consider the linear mapping\\nΦ : R3 → R4\\nΦ\\n\\uf8eb\\n\\uf8ed\\n\\uf8ee\\n\\uf8f0\\nx1\\nx2\\nx3\\n\\uf8f9\\n\\uf8fb\\n\\uf8f6\\n\\uf8f8 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n3x1 + 2x2 + x3\\nx1 + x2 + x3\\nx1 − 3x2\\n2x1 + 3x2 + x3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nFind the transformation matrix AΦ.\\nDetermine rk(AΦ).\\nCompute the kernel and image ofΦ. What aredim(ker(Φ)) and dim(Im(Φ))?\\n2.18 Let E be a vector space. Let f and g be two automorphisms on E such that\\nf ◦ g = id E (i.e., f ◦ g is the identity mapping idE). Show that ker(f) =\\nker(g ◦ f), Im(g) = Im(g ◦ f) and that ker(f) ∩ Im(g) = {0E}.\\n2.19 Consider an endomorphism Φ : R3 → R3 whose transformation matrix\\n(with respect to the standard basis in R3) is\\nAΦ =\\n\\uf8ee\\n\\uf8f0\\n1 1 0\\n1 −1 0\\n1 1 1\\n\\uf8f9\\n\\uf8fb .\\na. Determine ker(Φ) and Im(Φ).\\nb. Determine the transformation matrix ˜AΦ with respect to the basis\\nB = (\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n1\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb) ,\\ni.e., perform a basis change toward the new basis B.\\n2.20 Let us consider b1, b2, b′\\n1, b′\\n2, 4 vectors of R2 expressed in the standard basis\\nof R2 as\\nb1 =\\n\\x14\\n2\\n1\\n\\x15\\n, b2 =\\n\\x14\\n−1\\n−1\\n\\x15\\n, b′\\n1 =\\n\\x14\\n2\\n−2\\n\\x15\\n, b′\\n2 =\\n\\x14\\n1\\n1\\n\\x15\\nand let us define two ordered bases B = (b1, b2) and B′ = (b′\\n1, b′\\n2) of R2.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ead38553-b073-4cf2-bd45-132e5bb8d4d5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 74, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 69\\na. Show that B and B′ are two bases of R2 and draw those basis vectors.\\nb. Compute the matrix P 1 that performs a basis change from B′ to B.\\nc. We consider c1, c2, c3, three vectors of R3 defined in the standard basis\\nof R3 as\\nc1 =\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n−1\\n\\uf8f9\\n\\uf8fb , c2 =\\n\\uf8ee\\n\\uf8f0\\n0\\n−1\\n2\\n\\uf8f9\\n\\uf8fb , c3 =\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n−1\\n\\uf8f9\\n\\uf8fb\\nand we define C = (c1, c2, c3).\\n(i) Show that C is a basis of R3, e.g., by using determinants (see\\nSection 4.1).\\n(ii) Let us call C′ = ( c′\\n1, c′\\n2, c′\\n3) the standard basis of R3. Determine\\nthe matrix P 2 that performs the basis change from C to C′.\\nd. We consider a homomorphism Φ : R2 − →R3, such that\\nΦ(b1 + b2) = c2 + c3\\nΦ(b1 − b2) = 2 c1 − c2 + 3c3\\nwhere B = (b1, b2) and C = (c1, c2, c3) are ordered bases of R2 and R3,\\nrespectively .\\nDetermine the transformation matrix AΦ of Φ with respect to the or-\\ndered bases B and C.\\ne. Determine A′, the transformation matrix of Φ with respect to the bases\\nB′ and C′.\\nf. Let us consider the vector x ∈ R2 whose coordinates in B′ are [2, 3]⊤.\\nIn other words, x = 2b′\\n1 + 3b′\\n2.\\n(i) Calculate the coordinates of x in B.\\n(ii) Based on that, compute the coordinates of Φ(x) expressed in C.\\n(iii) Then, write Φ(x) in terms of c′\\n1, c′\\n2, c′\\n3.\\n(iv) Use the representation of x in B′ and the matrix A′ to find this\\nresult directly .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='10afe021-ce92-4835-a290-b25a95dd6fe0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 75, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3\\nAnalytic Geometry\\nIn Chapter 2, we studied vectors, vector spaces, and linear mappings at\\na general but abstract level. In this chapter, we will add some geomet-\\nric interpretation and intuition to all of these concepts. In particular, we\\nwill look at geometric vectors and compute their lengths and distances\\nor angles between two vectors. To be able to do this, we equip the vec-\\ntor space with an inner product that induces the geometry of the vector\\nspace. Inner products and their corresponding norms and metrics capture\\nthe intuitive notions of similarity and distances, which we use to develop\\nthe support vector machine in Chapter 12. We will then use the concepts\\nof lengths and angles between vectors to discuss orthogonal projections,\\nwhich will play a central role when we discuss principal component anal-\\nysis in Chapter 10 and regression via maximum likelihood estimation in\\nChapter 9. Figure 3.1 gives an overview of how concepts in this chapter\\nare related and how they are connected to other chapters of the book.\\nFigure 3.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.\\nInner product\\nNorm\\nLengths Orthogonalprojection Angles Rotations\\nChapter 4Matrixdecomposition\\nChapter 10Dimensionalityreduction\\nChapter 9Regression\\nChapter 12Classification\\ninduces\\n70\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e4feaab-bda3-40aa-a0a7-5055afcbd1df', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 76, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Norms 71\\nFigure 3.3 For\\ndifferent norms, the\\nred lines indicate\\nthe set of vectors\\nwith norm 1. Left:\\nManhattan norm;\\nRight: Euclidean\\ndistance.\\n1\\n1\\n1\\n1\\n∥x ∥1 = 1\\n∥x ∥2 = 1\\n3.1 Norms\\nWhen we think of geometric vectors, i.e., directed line segments that start\\nat the origin, then intuitively the length of a vector is the distance of the\\n“end” of this directed line segment from the origin. In the following, we\\nwill discuss the notion of the length of vectors using the concept of a norm.\\nDefinition 3.1 (Norm). A norm on a vector space V is a function norm\\n∥ · ∥ : V → R , (3.1)\\nx 7→ ∥x∥ , (3.2)\\nwhich assigns each vector x its length ∥x∥ ∈ R, such that for all λ ∈ R length\\nand x, y ∈ V the following hold:\\nabsolutely\\nhomogeneousAbsolutely homogeneous: ∥λx∥ = |λ|∥x∥\\ntriangle inequalityTriangle inequality: ∥x + y∥ ⩽ ∥x∥ + ∥y∥\\npositive definitePositive definite: ∥x∥ ⩾ 0 and ∥x∥ = 0 ⇐ ⇒ x = 0\\nFigure 3.2 Triangle\\ninequality .\\na\\nb\\nc ≤ a + bIn geometric terms, the triangle inequality states that for any triangle,\\nthe sum of the lengths of any two sides must be greater than or equal\\nto the length of the remaining side; see Figure 3.2 for an illustration.\\nDefinition 3.1 is in terms of a general vector space V (Section 2.4), but\\nin this book we will only consider a finite-dimensional vector space Rn.\\nRecall that for a vectorx ∈ Rn we denote the elements of the vector using\\na subscript, that is, xi is the ith element of the vector x.\\nExample 3.1 (Manhattan Norm)\\nThe Manhattan norm on Rn is defined for x ∈ Rn as Manhattan norm\\n∥x∥1 :=\\nnX\\ni=1\\n|xi| , (3.3)\\nwhere | · | is the absolute value. The left panel of Figure 3.3 shows all\\nvectors x ∈ R2 with ∥x∥1 = 1 . The Manhattan norm is also called ℓ1 ℓ1 norm\\nnorm.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b8994ed-a984-48fb-8dbb-30959b79b7a5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 77, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='72 Analytic Geometry\\nExample 3.2 (Euclidean Norm)\\nThe Euclidean norm of x ∈ Rn is defined asEuclidean norm\\n∥x∥2 :=\\nvuut\\nnX\\ni=1\\nx2\\ni =\\n√\\nx⊤x (3.4)\\nand computes the Euclidean distance of x from the origin. The right panelEuclidean distance\\nof Figure 3.3 shows all vectors x ∈ R2 with ∥x∥2 = 1 . The Euclidean\\nnorm is also called ℓ2 norm.ℓ2 norm\\nRemark. Throughout this book, we will use the Euclidean norm (3.4) by\\ndefault if not stated otherwise. ♢\\n3.2 Inner Products\\nInner products allow for the introduction of intuitive geometrical con-\\ncepts, such as the length of a vector and the angle or distance between\\ntwo vectors. A major purpose of inner products is to determine whether\\nvectors are orthogonal to each other.\\n3.2.1 Dot Product\\nWe may already be familiar with a particular type of inner product, the\\nscalar product/dot product in Rn, which is given byscalar product\\ndot product\\nx⊤y =\\nnX\\ni=1\\nxiyi . (3.5)\\nWe will refer to this particular inner product as the dot product in this\\nbook. However, inner products are more general concepts with specific\\nproperties, which we will now introduce.\\n3.2.2 General Inner Products\\nRecall the linear mapping from Section 2.7, where we can rearrange the\\nmapping with respect to addition and multiplication with a scalar. A bi-bilinear mapping\\nlinear mapping Ω is a mapping with two arguments, and it is linear in\\neach argument, i.e., when we look at a vector space V then it holds that\\nfor all x, y, z ∈ V, λ, ψ ∈ R that\\nΩ(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z) (3.6)\\nΩ(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z) . (3.7)\\nHere, (3.6) asserts that Ω is linear in the first argument, and (3.7) asserts\\nthat Ω is linear in the second argument (see also (2.87)).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73e03683-6a9e-4107-ae73-8a72cd16443e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 78, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Inner Products 73\\nDefinition 3.2. Let V be a vector space and Ω : V × V → R be a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\nΩ is called symmetric if Ω(x, y) = Ω( y, x) for all x, y ∈ V , i.e., the symmetric\\norder of the arguments does not matter.\\nΩ is called positive definite if positive definite\\n∀x ∈ V \\\\{0} : Ω(x, x) > 0 , Ω(0, 0) = 0 . (3.8)\\nDefinition 3.3. Let V be a vector space and Ω : V × V → R be a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\nA positive definite, symmetric bilinear mappingΩ : V ×V → R is called\\nan inner product on V . We typically write ⟨x, y⟩ instead of Ω(x, y). inner product\\nThe pair (V, ⟨·, ·⟩) is called an inner product space or (real) vector space inner product space\\nvector space with\\ninner product\\nwith inner product. If we use the dot product defined in (3.5), we call\\n(V, ⟨·, ·⟩) a Euclidean vector space.\\nEuclidean vector\\nspaceWe will refer to these spaces as inner product spaces in this book.\\nExample 3.3 (Inner Product That Is Not the Dot Product)\\nConsider V = R2. If we define\\n⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2x2y2 (3.9)\\nthen ⟨·, ·⟩ is an inner product but different from the dot product. The proof\\nwill be an exercise.\\n3.2.3 Symmetric, Positive Definite Matrices\\nSymmetric, positive definite matrices play an important role in machine\\nlearning, and they are defined via the inner product. In Section 4.3, we\\nwill return to symmetric, positive definite matrices in the context of matrix\\ndecompositions. The idea of symmetric positive semidefinite matrices is\\nkey in the definition of kernels (Section 12.4).\\nConsider an n-dimensional vector space V with an inner product ⟨·, ·⟩ :\\nV × V → R (see Definition 3.3) and an ordered basis B = (b1, . . . ,bn) of\\nV . Recall from Section 2.6.1 that any vectors x, y ∈ V can be written as\\nlinear combinations of the basis vectors so that x =Pn\\ni=1 ψibi ∈ V and\\ny =Pn\\nj=1 λjbj ∈ V for suitable ψi, λj ∈ R. Due to the bilinearity of the\\ninner product, it holds for all x, y ∈ V that\\n⟨x, y⟩ =\\n* nX\\ni=1\\nψibi,\\nnX\\nj=1\\nλjbj\\n+\\n=\\nnX\\ni=1\\nnX\\nj=1\\nψi ⟨bi, bj⟩ λj = ˆx⊤Aˆy , (3.10)\\nwhere Aij := ⟨bi, bj⟩ and ˆx, ˆy are the coordinates of x and y with respect\\nto the basis B. This implies that the inner product ⟨·, ·⟩ is uniquely deter-\\nmined through A. The symmetry of the inner product also means that A\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f9b155e-520c-4eef-9526-e69bdfeee972', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 79, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='74 Analytic Geometry\\nis symmetric. Furthermore, the positive definiteness of the inner product\\nimplies that\\n∀x ∈ V \\\\{0} : x⊤Ax > 0 . (3.11)\\nDefinition 3.4 (Symmetric, Positive Definite Matrix). A symmetric matrix\\nA ∈ Rn×n that satisfies (3.11) is called symmetric, positive definite , orsymmetric, positive\\ndefinite just positive definite. If only ⩾ holds in (3.11), then A is called symmetric,\\npositive definite\\nsymmetric, positive\\nsemidefinite\\npositive semidefinite.\\nExample 3.4 (Symmetric, Positive Definite Matrices)\\nConsider the matrices\\nA1 =\\n\\x149 6\\n6 5\\n\\x15\\n, A2 =\\n\\x149 6\\n6 3\\n\\x15\\n. (3.12)\\nA1 is positive definite because it is symmetric and\\nx⊤A1x =\\n\\x02\\nx1 x2\\n\\x03\\x149 6\\n6 5\\n\\x15\\x14 x1\\nx2\\n\\x15\\n(3.13a)\\n= 9x2\\n1 + 12x1x2 + 5x2\\n2 = (3x1 + 2x2)2 + x2\\n2 > 0 (3.13b)\\nfor all x ∈ V \\\\{0}. In contrast, A2 is symmetric but not positive definite\\nbecause x⊤A2x = 9x2\\n1 + 12x1x2 + 3x2\\n2 = (3x1 + 2x2)2 − x2\\n2 can be less\\nthan 0, e.g., for x = [2, −3]⊤.\\nIf A ∈ Rn×n is symmetric, positive definite, then\\n⟨x, y⟩ = ˆx⊤Aˆy (3.14)\\ndefines an inner product with respect to an ordered basis B, where ˆx and\\nˆy are the coordinate representations of x, y ∈ V with respect to B.\\nTheorem 3.5. For a real-valued, finite-dimensional vector space V and an\\nordered basis B of V , it holds that ⟨·, ·⟩ : V × V → R is an inner product if\\nand only if there exists a symmetric, positive definite matrix A ∈ Rn×n with\\n⟨x, y⟩ = ˆx⊤Aˆy . (3.15)\\nThe following properties hold if A ∈ Rn×n is symmetric and positive\\ndefinite:\\nThe null space (kernel) of A consists only of 0 because x⊤Ax > 0 for\\nall x ̸= 0. This implies that Ax ̸= 0 if x ̸= 0.\\nThe diagonal elements aii of A are positive because aii = e⊤\\ni Aei > 0,\\nwhere ei is the ith vector of the standard basis in Rn.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba5bf693-02ca-46df-978e-2f573ea45842', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 80, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Lengths and Distances 75\\n3.3 Lengths and Distances\\nIn Section 3.1, we already discussed norms that we can use to compute\\nthe length of a vector. Inner products and norms are closely related in the\\nsense that any inner product induces a norm Inner products\\ninduce norms.\\n∥x∥ :=\\nq\\n⟨x, x⟩ (3.16)\\nin a natural way , such that we can compute lengths of vectors using the in-\\nner product. However, not every norm is induced by an inner product. The\\nManhattan norm (3.3) is an example of a norm without a corresponding\\ninner product. In the following, we will focus on norms that are induced\\nby inner products and introduce geometric concepts, such as lengths, dis-\\ntances, and angles.\\nRemark (Cauchy-Schwarz Inequality). For an inner product vector space\\n(V, ⟨·, ·⟩) the induced norm ∥ · ∥ satisfies the Cauchy-Schwarz inequality Cauchy-Schwarz\\ninequality\\n| ⟨x, y⟩ | ⩽ ∥x∥∥y∥ . (3.17)\\n♢\\nExample 3.5 (Lengths of Vectors Using Inner Products)\\nIn geometry , we are often interested in lengths of vectors. We can now use\\nan inner product to compute them using (3.16). Let us take x = [1, 1]⊤ ∈\\nR2. If we use the dot product as the inner product, with (3.16) we obtain\\n∥x∥ =\\n√\\nx⊤x =\\n√\\n12 + 12 =\\n√\\n2 (3.18)\\nas the length of x. Let us now choose a different inner product:\\n⟨x, y⟩ := x⊤\\n\\x14 1 − 1\\n2\\n− 1\\n2 1\\n\\x15\\ny = x1y1 − 1\\n2(x1y2 + x2y1) + x2y2 . (3.19)\\nIf we compute the norm of a vector, then this inner product returns smaller\\nvalues than the dot product if x1 and x2 have the same sign (and x1x2 >\\n0); otherwise, it returns greater values than the dot product. With this\\ninner product, we obtain\\n⟨x, x⟩ = x2\\n1 − x1x2 + x2\\n2 = 1 − 1 + 1 = 1 =⇒ ∥ x∥ =\\n√\\n1 = 1 , (3.20)\\nsuch that x is “shorter” with this inner product than with the dot product.\\nDefinition 3.6 (Distance and Metric) . Consider an inner product space\\n(V, ⟨·, ·⟩). Then\\nd(x, y) := ∥x − y∥ =\\nq\\n⟨x − y, x − y⟩ (3.21)\\nis called the distance between x and y for x, y ∈ V . If we use the dot distance\\nproduct as the inner product, then the distance is calledEuclidean distance. Euclidean distance\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4dc48e7c-fce9-4b0d-b500-4e7ca086bcb8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 81, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='76 Analytic Geometry\\nThe mapping\\nd : V × V → R (3.22)\\n(x, y) 7→ d(x, y) (3.23)\\nis called a metric.metric\\nRemark. Similar to the length of a vector, the distance between vectors\\ndoes not require an inner product: a norm is sufficient. If we have a norm\\ninduced by an inner product, the distance may vary depending on the\\nchoice of the inner product. ♢\\nA metric d satisfies the following:\\n1. d is positive definite, i.e., d(x, y) ⩾ 0 for all x, y ∈ V and d(x, y) =positive definite\\n0 ⇐ ⇒ x = y .\\n2. d is symmetric, i.e., d(x, y) = d(y, x) for all x, y ∈ V .symmetric\\ntriangle inequality 3. Triangle inequality: d(x, z) ⩽ d(x, y) + d(y, z) for all x, y, z ∈ V .\\nRemark. At first glance, the lists of properties of inner products and met-\\nrics look very similar. However, by comparing Definition 3.3 with Defini-\\ntion 3.6 we observe that ⟨x, y⟩ and d(x, y) behave in opposite directions.\\nVery similar x and y will result in a large value for the inner product and\\na small value for the metric. ♢\\n3.4 Angles and Orthogonality\\nFigure 3.4 When\\nrestricted to [0, π]\\nthen f(ω) = cos(ω)\\nreturns a unique\\nnumber in the\\ninterval [−1, 1].\\n0 π/2 π\\nω\\n−1\\n0\\n1\\ncos(ω)\\nIn addition to enabling the definition of lengths of vectors, as well as the\\ndistance between two vectors, inner products also capture the geometry\\nof a vector space by defining the angle ω between two vectors. We use\\nthe Cauchy-Schwarz inequality (3.17) to define angles ω in inner prod-\\nuct spaces between two vectors x, y, and this notion coincides with our\\nintuition in R2 and R3. Assume that x ̸= 0, y ̸= 0. Then\\n−1 ⩽ ⟨x, y⟩\\n∥x∥ ∥y∥ ⩽ 1 . (3.24)\\nTherefore, there exists a unique ω ∈ [0, π], illustrated in Figure 3.4, with\\ncos ω = ⟨x, y⟩\\n∥x∥ ∥y∥ . (3.25)\\nThe number ω is the angle between the vectors x and y. Intuitively , theangle\\nangle between two vectors tells us how similar their orientations are. For\\nexample, using the dot product, the angle between x and y = 4x, i.e., y\\nis a scaled version of x, is 0: Their orientation is the same.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2be09132-04d6-42ee-ba11-3c79780c50c4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 82, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Angles and Orthogonality 77\\nExample 3.6 (Angle between Vectors)\\nLet us compute the angle betweenx = [1, 1]⊤ ∈ R2 and y = [1, 2]⊤ ∈ R2; Figure 3.5 The\\nangle ω between\\ntwo vectors x, y is\\ncomputed using the\\ninner product.\\ny\\nx\\n10\\n1\\nω\\nsee Figure 3.5, where we use the dot product as the inner product. Then\\nwe get\\ncos ω = ⟨x, y⟩p\\n⟨x, x⟩ ⟨y, y⟩ = x⊤yp\\nx⊤xy⊤y\\n= 3√\\n10 , (3.26)\\nand the angle between the two vectors is arccos( 3√\\n10) ≈ 0.32 rad, which\\ncorresponds to about 18◦.\\nA key feature of the inner product is that it also allows us to characterize\\nvectors that are orthogonal.\\nDefinition 3.7 (Orthogonality). Two vectorsx and y are orthogonal if and orthogonal\\nonly if ⟨x, y⟩ = 0, and we write x ⊥ y. If additionally ∥x∥ = 1 = ∥y∥,\\ni.e., the vectors are unit vectors, then x and y are orthonormal. orthonormal\\nAn implication of this definition is that the 0-vector is orthogonal to\\nevery vector in the vector space.\\nRemark. Orthogonality is the generalization of the concept of perpendic-\\nularity to bilinear forms that do not have to be the dot product. In our\\ncontext, geometrically , we can think of orthogonal vectors as having a\\nright angle with respect to a specific inner product. ♢\\nExample 3.7 (Orthogonal Vectors)\\nFigure 3.6 The\\nangle ω between\\ntwo vectors x, y can\\nchange depending\\non the inner\\nproduct.\\ny x\\n−1 10\\n1\\nω\\nConsider two vectors x = [1 , 1]⊤, y = [ −1, 1]⊤ ∈ R2; see Figure 3.6.\\nWe are interested in determining the angle ω between them using two\\ndifferent inner products. Using the dot product as the inner product yields\\nan angle ω between x and y of 90◦, such that x ⊥ y. However, if we\\nchoose the inner product\\n⟨x, y⟩ = x⊤\\n\\x142 0\\n0 1\\n\\x15\\ny , (3.27)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca1af0dc-db36-48f5-834f-6ea2a8c59025', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 83, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='78 Analytic Geometry\\nwe get that the angle ω between x and y is given by\\ncos ω = ⟨x, y⟩\\n∥x∥∥y∥ = −1\\n3 =⇒ ω ≈ 1.91 rad ≈ 109.5◦ , (3.28)\\nand x and y are not orthogonal. Therefore, vectors that are orthogonal\\nwith respect to one inner product do not have to be orthogonal with re-\\nspect to a different inner product.\\nDefinition 3.8 (Orthogonal Matrix). A square matrix A ∈ Rn×n is an\\northogonal matrix if and only if its columns are orthonormal so thatorthogonal matrix\\nAA⊤ = I = A⊤A , (3.29)\\nwhich implies that\\nA−1 = A⊤ , (3.30)\\ni.e., the inverse is obtained by simply transposing the matrix.It is convention to\\ncall these matrices\\n“orthogonal” but a\\nmore precise\\ndescription would\\nbe “orthonormal”.\\nTransformations by orthogonal matrices are special because the length\\nof a vector x is not changed when transforming it using an orthogonal\\nmatrix A. For the dot product, we obtain\\nTransformations\\nwith orthogonal\\nmatrices preserve\\ndistances and\\nangles.\\n∥Ax∥\\n2\\n= (Ax)⊤(Ax) = x⊤A⊤Ax = x⊤Ix = x⊤x = ∥x∥\\n2\\n. (3.31)\\nMoreover, the angle between any two vectors x, y, as measured by their\\ninner product, is also unchanged when transforming both of them using\\nan orthogonal matrix A. Assuming the dot product as the inner product,\\nthe angle of the images Ax and Ay is given as\\ncos ω = (Ax)⊤(Ay)\\n∥Ax∥ ∥Ay∥ = x⊤A⊤Ayq\\nx⊤A⊤Axy⊤A⊤Ay\\n= x⊤y\\n∥x∥ ∥y∥ , (3.32)\\nwhich gives exactly the angle between x and y. This means that orthog-\\nonal matrices A with A⊤ = A−1 preserve both angles and distances. It\\nturns out that orthogonal matrices define transformations that are rota-\\ntions (with the possibility of flips). In Section 3.9, we will discuss more\\ndetails about rotations.\\n3.5 Orthonormal Basis\\nIn Section 2.6.1, we characterized properties of basis vectors and found\\nthat in an n-dimensional vector space, we need n basis vectors, i.e., n\\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\\ninner products to compute the length of vectors and the angle between\\nvectors. In the following, we will discuss the special case where the basis\\nvectors are orthogonal to each other and where the length of each basis\\nvector is 1. We will call this basis then an orthonormal basis.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bb846e4-3d4f-45c7-9eb1-b94d0ebc1470', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 84, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6 Orthogonal Complement 79\\nLet us introduce this more formally .\\nDefinition 3.9 (Orthonormal Basis). Consider an n-dimensional vector\\nspace V and a basis {b1, . . . ,bn} of V . If\\n⟨bi, bj⟩ = 0 for i ̸= j (3.33)\\n⟨bi, bi⟩ = 1 (3.34)\\nfor all i, j = 1, . . . , nthen the basis is called an orthonormal basis (ONB). orthonormal basis\\nONBIf only (3.33) is satisfied, then the basis is called anorthogonal basis. Note\\northogonal basisthat (3.34) implies that every basis vector has length/norm 1.\\nRecall from Section 2.6.1 that we can use Gaussian elimination to find a\\nbasis for a vector space spanned by a set of vectors. Assume we are given\\na set {˜b1, . . . ,˜bn} of non-orthogonal and unnormalized basis vectors. We\\nconcatenate them into a matrix ˜B = [˜b1, . . . ,˜bn] and apply Gaussian elim-\\nination to the augmented matrix (Section 2.3.2) [ ˜B ˜B\\n⊤\\n| ˜B] to obtain an\\northonormal basis. This constructive way to iteratively build an orthonor-\\nmal basis {b1, . . . ,bn} is called the Gram-Schmidt process (Strang, 2003).\\nExample 3.8 (Orthonormal Basis)\\nThe canonical/standard basis for a Euclidean vector space Rn is an or-\\nthonormal basis, where the inner product is the dot product of vectors.\\nIn R2, the vectors\\nb1 = 1√\\n2\\n\\x141\\n1\\n\\x15\\n, b2 = 1√\\n2\\n\\x14 1\\n−1\\n\\x15\\n(3.35)\\nform an orthonormal basis since b⊤\\n1 b2 = 0 and ∥b1∥ = 1 = ∥b2∥.\\nWe will exploit the concept of an orthonormal basis in Chapter 12 and\\nChapter 10 when we discuss support vector machines and principal com-\\nponent analysis.\\n3.6 Orthogonal Complement\\nHaving defined orthogonality , we will now look at vector spaces that are\\northogonal to each other. This will play an important role in Chapter 10,\\nwhen we discuss linear dimensionality reduction from a geometric per-\\nspective.\\nConsider a D-dimensional vector space V and an M-dimensional sub-\\nspace U ⊆ V . Then itsorthogonal complement U ⊥ is a(D−M)-dimensional orthogonal\\ncomplementsubspace of V and contains all vectors in V that are orthogonal to every\\nvector in U. Furthermore, U ∩ U ⊥ = {0} so that any vector x ∈ V can be\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='55f7a72c-b185-4649-845a-a87d82501ac7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 85, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='80 Analytic Geometry\\nFigure 3.7 A plane\\nU in a\\nthree-dimensional\\nvector space can be\\ndescribed by its\\nnormal vector,\\nwhich spans its\\northogonal\\ncomplement U ⊥.\\ne3\\ne1\\ne2\\nw\\nU\\nuniquely decomposed into\\nx =\\nMX\\nm=1\\nλmbm +\\nD−MX\\nj=1\\nψjb⊥\\nj , λ m, ψ j ∈ R , (3.36)\\nwhere (b1, . . . ,bM) is a basis of U and (b⊥\\n1 , . . . ,b⊥\\nD−M) is a basis of U ⊥.\\nTherefore, the orthogonal complement can also be used to describe a\\nplane U (two-dimensional subspace) in a three-dimensional vector space.\\nMore specifically , the vectorw with ∥w∥ = 1, which is orthogonal to the\\nplane U, is the basis vector of U ⊥. Figure 3.7 illustrates this setting. All\\nvectors that are orthogonal to w must (by construction) lie in the plane\\nU. The vector w is called the normal vector of U.normal vector\\nGenerally , orthogonal complements can be used to describe hyperplanes\\nin n-dimensional vector and affine spaces.\\n3.7 Inner Product of Functions\\nThus far, we looked at properties of inner products to compute lengths,\\nangles and distances. We focused on inner products of finite-dimensional\\nvectors. In the following, we will look at an example of inner products of\\na different type of vectors: inner products of functions.\\nThe inner products we discussed so far were defined for vectors with a\\nfinite number of entries. We can think of a vector x ∈ Rn as a function\\nwith n function values. The concept of an inner product can be generalized\\nto vectors with an infinite number of entries (countably infinite) and also\\ncontinuous-valued functions (uncountably infinite). Then the sum over\\nindividual components of vectors (see Equation (3.5) for example) turns\\ninto an integral.\\nAn inner product of two functions u : R → R and v : R → R can be\\ndefined as the definite integral\\n⟨u, v⟩ :=\\nZ b\\na\\nu(x)v(x)dx (3.37)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1758a90f-e4e5-4e03-a68d-83a47916874a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 86, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 Orthogonal Projections 81\\nfor lower and upper limits a, b < ∞, respectively . As with our usual inner\\nproduct, we can define norms and orthogonality by looking at the inner\\nproduct. If (3.37) evaluates to 0, the functions u and v are orthogonal. To\\nmake the preceding inner product mathematically precise, we need to take\\ncare of measures and the definition of integrals, leading to the definition of\\na Hilbert space. Furthermore, unlike inner products on finite-dimensional\\nvectors, inner products on functions may diverge (have infinite value). All\\nthis requires diving into some more intricate details of real and functional\\nanalysis, which we do not cover in this book.\\nExample 3.9 (Inner Product of Functions)\\nIf we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure 3.8 f(x) =\\nsin(x) cos(x).\\n−2.5 0.0 2.5\\nx\\n−0.5\\n0.0\\n0.5\\nsin(x) cos(x)of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e.,\\nf(−x) = −f(x). Therefore, the integral with limits a = −π, b = π of this\\nproduct evaluates to 0. Therefore, sin and cos are orthogonal functions.\\nRemark. It also holds that the collection of functions\\n{1, cos(x), cos(2x), cos(3x), . . .} (3.38)\\nis orthogonal if we integrate from −π to π, i.e., any pair of functions are\\northogonal to each other. The collection of functions in (3.38) spans a\\nlarge subspace of the functions that are even and periodic on[−π, π), and\\nprojecting functions onto this subspace is the fundamental idea behind\\nFourier series. ♢\\nIn Section 6.4.6, we will have a look at a second type of unconventional\\ninner products: the inner product of random variables.\\n3.8 Orthogonal Projections\\nProjections are an important class of linear transformations (besides rota-\\ntions and reflections) and play an important role in graphics, coding the-\\nory , statistics and machine learning. In machine learning, we often deal\\nwith data that is high-dimensional. High-dimensional data is often hard\\nto analyze or visualize. However, high-dimensional data quite often pos-\\nsesses the property that only a few dimensions contain most information,\\nand most other dimensions are not essential to describe key properties\\nof the data. When we compress or visualize high-dimensional data, we\\nwill lose information. To minimize this compression loss, we ideally find\\nthe most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a\\ncommon expression\\nfor data\\nrepresentation.\\ndata can be represented as vectors, and in this chapter, we will discuss\\nsome of the fundamental tools for data compression. More specifically , we\\ncan project the original high-dimensional data onto a lower-dimensional\\nfeature space and work in this lower-dimensional space to learn more\\nabout the dataset and extract relevant patterns. For example, machine\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e7d027c8-5b36-4e7f-81a8-b54726767e07', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 87, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='82 Analytic Geometry\\nFigure 3.9\\nOrthogonal\\nprojection (orange\\ndots) of a\\ntwo-dimensional\\ndataset (blue dots)\\nonto a\\none-dimensional\\nsubspace (straight\\nline).\\n−4 −2 0 2 4\\nx1\\n−2\\n−1\\n0\\n1\\n2\\nx2\\nlearning algorithms, such as principal component analysis (PCA) by Pear-\\nson (1901) and Hotelling (1933) and deep neural networks (e.g., deep\\nauto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension-\\nality reduction. In the following, we will focus on orthogonal projections,\\nwhich we will use in Chapter 10 for linear dimensionality reduction and\\nin Chapter 12 for classification. Even linear regression, which we discuss\\nin Chapter 9, can be interpreted using orthogonal projections. For a given\\nlower-dimensional subspace, orthogonal projections of high-dimensional\\ndata retain as much information as possible and minimize the difference/\\nerror between the original data and the corresponding projection. An il-\\nlustration of such an orthogonal projection is given in Figure 3.9. Before\\nwe detail how to obtain these projections, let us define what a projection\\nactually is.\\nDefinition 3.10 (Projection). Let V be a vector space and U ⊆ V a\\nsubspace of V . A linear mapping π : V → U is called a projection ifprojection\\nπ2 = π ◦ π = π.\\nSince linear mappings can be expressed by transformation matrices (see\\nSection 2.7), the preceding definition applies equally to a special kind\\nof transformation matrices, the projection matrices P π, which exhibit theprojection matrix\\nproperty that P 2\\nπ = P π.\\nIn the following, we will derive orthogonal projections of vectors in the\\ninner product space (Rn, ⟨·, ·⟩) onto subspaces. We will start with one-\\ndimensional subspaces, which are also called lines. If not mentioned oth-line\\nerwise, we assume the dot product ⟨x, y⟩ = x⊤y as the inner product.\\n3.8.1 Projection onto One-Dimensional Subspaces (Lines)\\nAssume we are given a line (one-dimensional subspace) through the ori-\\ngin with basis vector b ∈ Rn. The line is a one-dimensional subspace\\nU ⊆ Rn spanned by b. When we project x ∈ Rn onto U, we seek the\\nvector πU(x) ∈ U that is closest to x. Using geometric arguments, let\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90d0c4f2-9299-427f-a47c-ec87fb2435d3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 88, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 Orthogonal Projections 83\\nFigure 3.10\\nExamples of\\nprojections onto\\none-dimensional\\nsubspaces.\\nb\\nx\\nπU(x)\\nω\\n(a) Projection of x ∈ R2 onto a subspace U\\nwith basis vector b.\\ncosωω\\nsinω\\nb\\nx\\n(b) Projection of a two-dimensional vector\\nx with ∥x∥ = 1 onto a one-dimensional\\nsubspace spanned by b.\\nus characterize some properties of the projection πU(x) (Figure 3.10(a)\\nserves as an illustration):\\nThe projection πU(x) is closest to x, where “closest” implies that the\\ndistance ∥x − πU(x)∥ is minimal. It follows that the segmentπU(x) − x\\nfrom πU(x) to x is orthogonal to U, and therefore the basis vector b of\\nU. The orthogonality condition yields ⟨πU(x) − x, b⟩ = 0 since angles\\nbetween vectors are defined via the inner product. λ is then the\\ncoordinate of πU(x)\\nwith respect to b.\\nThe projection πU(x) of x onto U must be an element of U and, there-\\nfore, a multiple of the basis vector b that spans U. Hence, πU(x) = λb,\\nfor some λ ∈ R.\\nIn the following three steps, we determine the coordinateλ, the projection\\nπU(x) ∈ U, and the projection matrix P π that maps any x ∈ Rn onto U:\\n1. Finding the coordinate λ. The orthogonality condition yields\\n⟨x − πU(x), b⟩ = 0\\nπU(x)=λb\\n⇐ ⇒ ⟨ x − λb, b⟩ = 0 . (3.39)\\nWe can now exploit the bilinearity of the inner product and arrive at With a general inner\\nproduct, we get\\nλ = ⟨x, b⟩ if\\n∥b∥ = 1.⟨x, b⟩ − λ ⟨b, b⟩ = 0 ⇐ ⇒ λ = ⟨x, b⟩\\n⟨b, b⟩ = ⟨b, x⟩\\n∥b∥2 . (3.40)\\nIn the last step, we exploited the fact that inner products are symmet-\\nric. If we choose ⟨·, ·⟩ to be the dot product, we obtain\\nλ = b⊤x\\nb⊤b = b⊤x\\n∥b∥2 . (3.41)\\nIf ∥b∥ = 1, then the coordinate λ of the projection is given by b⊤x.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='feca3c11-27a8-4502-a8e4-775317eece31', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 89, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='84 Analytic Geometry\\n2. Finding the projection point πU(x) ∈ U. Since πU(x) = λb, we imme-\\ndiately obtain with (3.40) that\\nπU(x) = λb = ⟨x, b⟩\\n∥b∥2 b = b⊤x\\n∥b∥2 b , (3.42)\\nwhere the last equality holds for the dot product only . We can also\\ncompute the length of πU(x) by means of Definition 3.1 as\\n∥πU(x)∥ = ∥λb∥ = |λ| ∥b∥ . (3.43)\\nHence, our projection is of length |λ| times the length of b. This also\\nadds the intuition that λ is the coordinate of πU(x) with respect to the\\nbasis vector b that spans our one-dimensional subspace U.\\nIf we use the dot product as an inner product, we get\\n∥πU(x)∥\\n(3.42)\\n= |b⊤x|\\n∥b∥2 ∥b∥\\n(3.25)\\n= | cos ω| ∥x∥ ∥b∥ ∥b∥\\n∥b∥2 = | cos ω| ∥x∥ .\\n(3.44)\\nHere, ω is the angle between x and b. This equation should be familiar\\nfrom trigonometry: If ∥x∥ = 1, then x lies on the unit circle. It follows\\nthat the projection onto the horizontal axis spanned by b is exactlyThe horizontal axis\\nis a one-dimensional\\nsubspace.\\ncos ω, and the length of the corresponding vector πU(x) = |cos ω|. An\\nillustration is given in Figure 3.10(b).\\n3. Finding the projection matrix P π. We know that a projection is a lin-\\near mapping (see Definition 3.10). Therefore, there exists a projection\\nmatrix P π, such that πU(x) = P πx. With the dot product as inner\\nproduct and\\nπU(x) = λb = bλ = b b⊤x\\n∥b∥2 = bb⊤\\n∥b∥2 x , (3.45)\\nwe immediately see that\\nP π = bb⊤\\n∥b∥2 . (3.46)\\nNote that bb⊤ (and, consequently ,P π) is a symmetric matrix (of rankProjection matrices\\nare always\\nsymmetric.\\n1), and ∥b∥2 = ⟨b, b⟩ is a scalar.\\nThe projection matrixP π projects any vectorx ∈ Rn onto the line through\\nthe origin with direction b (equivalently , the subspaceU spanned by b).\\nRemark. The projection πU(x) ∈ Rn is still an n-dimensional vector and\\nnot a scalar. However, we no longer requiren coordinates to represent the\\nprojection, but only a single one if we want to express it with respect to\\nthe basis vector b that spans the subspace U: λ. ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b701b54a-7e5e-4b09-964b-b5038c0d1e7f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 90, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 Orthogonal Projections 85\\nFigure 3.11\\nProjection onto a\\ntwo-dimensional\\nsubspace U with\\nbasis b1, b2. The\\nprojection πU(x) of\\nx ∈ R3 onto U can\\nbe expressed as a\\nlinear combination\\nof b1, b2 and the\\ndisplacement vector\\nx − πU(x) is\\northogonal to both\\nb1 and b2.\\n0\\nx\\nb1\\nb2\\nU\\nπU(x)\\nx − πU(x)\\nExample 3.10 (Projection onto a Line)\\nFind the projection matrix P π onto the line through the origin spanned\\nby b =\\n\\x02\\n1 2 2\\n\\x03⊤\\n. b is a direction and a basis of the one-dimensional\\nsubspace (line through origin).\\nWith (3.46), we obtain\\nP π = bb⊤\\nb⊤b = 1\\n9\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n2\\n\\uf8f9\\n\\uf8fb\\x02\\n1 2 2\\n\\x03\\n= 1\\n9\\n\\uf8ee\\n\\uf8f0\\n1 2 2\\n2 4 4\\n2 4 4\\n\\uf8f9\\n\\uf8fb . (3.47)\\nLet us now choose a particular x and see whether it lies in the subspace\\nspanned by b. For x =\\n\\x02\\n1 1 1\\n\\x03⊤\\n, the projection is\\nπU(x) = P πx = 1\\n9\\n\\uf8ee\\n\\uf8f0\\n1 2 2\\n2 4 4\\n2 4 4\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb = 1\\n9\\n\\uf8ee\\n\\uf8f0\\n5\\n10\\n10\\n\\uf8f9\\n\\uf8fb ∈ span[\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n2\\n\\uf8f9\\n\\uf8fb] . (3.48)\\nNote that the application of P π to πU(x) does not change anything, i.e.,\\nP ππU(x) = πU(x). This is expected because according to Definition 3.10,\\nwe know that a projection matrix P π satisfies P 2\\nπx = P πx for all x.\\nRemark. With the results from Chapter 4, we can show that πU(x) is an\\neigenvector of P π, and the corresponding eigenvalue is 1. ♢\\n3.8.2 Projection onto General Subspaces\\nIf U is given by a set\\nof spanning vectors,\\nwhich are not a\\nbasis, make sure\\nyou determine a\\nbasis b1, . . . ,bm\\nbefore proceeding.\\nIn the following, we look at orthogonal projections of vectors x ∈ Rn\\nonto lower-dimensional subspaces U ⊆ Rn with dim(U) = m ⩾ 1. An\\nillustration is given in Figure 3.11.\\nAssume that (b1, . . . ,bm) is an ordered basis ofU. Any projection πU(x)\\nonto U is necessarily an element of U. Therefore, they can be represented\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='512e0dce-63a3-403e-ba52-e13b0e5ab6a5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 91, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='86 Analytic Geometry\\nas linear combinations of the basis vectors b1, . . . ,bm of U, such that\\nπU(x) =Pm\\ni=1 λibi.The basis vectors\\nform the columns of\\nB ∈ Rn×m, where\\nB = [b1, . . . ,bm].\\nAs in the 1D case, we follow a three-step procedure to find the projec-\\ntion πU(x) and the projection matrix P π:\\n1. Find the coordinates λ1, . . . , λm of the projection (with respect to the\\nbasis of U), such that the linear combination\\nπU(x) =\\nmX\\ni=1\\nλibi = Bλ , (3.49)\\nB = [b1, . . . ,bm] ∈ Rn×m, λ = [λ1, . . . , λm]⊤ ∈ Rm , (3.50)\\nis closest to x ∈ Rn. As in the 1D case, “closest” means “minimum\\ndistance”, which implies that the vector connecting πU(x) ∈ U and\\nx ∈ Rn must be orthogonal to all basis vectors of U. Therefore, we\\nobtain m simultaneous conditions (assuming the dot product as the\\ninner product)\\n⟨b1, x − πU(x)⟩ = b⊤\\n1 (x − πU(x)) = 0 (3.51)\\n...\\n⟨bm, x − πU(x)⟩ = b⊤\\nm(x − πU(x)) = 0 (3.52)\\nwhich, with πU(x) = Bλ, can be written as\\nb⊤\\n1 (x − Bλ) = 0 (3.53)\\n...\\nb⊤\\nm(x − Bλ) = 0 (3.54)\\nsuch that we obtain a homogeneous linear equation system\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nb⊤\\n1\\n...\\nb⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8f0x − Bλ\\n\\uf8f9\\n\\uf8fb = 0 ⇐ ⇒ B⊤(x − Bλ) = 0 (3.55)\\n⇐ ⇒ B⊤Bλ = B⊤x . (3.56)\\nThe last expression is called normal equation. Since b1, . . . ,bm are anormal equation\\nbasis of U and, therefore, linearly independent, B⊤B ∈ Rm×m is reg-\\nular and can be inverted. This allows us to solve for the coefficients/\\ncoordinates\\nλ = (B⊤B)−1B⊤x . (3.57)\\nThe matrix (B⊤B)−1B⊤ is also called the pseudo-inverse of B, whichpseudo-inverse\\ncan be computed for non-square matricesB. It only requires thatB⊤B\\nis positive definite, which is the case if B is full rank. In practical ap-\\nplications (e.g., linear regression), we often add a “jitter term” ϵI to\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a85218d6-a409-40ef-89dd-11132bc6f43b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 92, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 Orthogonal Projections 87\\nB⊤B to guarantee increased numerical stability and positive definite-\\nness. This “ridge” can be rigorously derived using Bayesian inference.\\nSee Chapter 9 for details.\\n2. Find the projection πU(x) ∈ U. We already established that πU(x) =\\nBλ. Therefore, with (3.57)\\nπU(x) = B(B⊤B)−1B⊤x . (3.58)\\n3. Find the projection matrix P π. From (3.58), we can immediately see\\nthat the projection matrix that solves P πx = πU(x) must be\\nP π = B(B⊤B)−1B⊤ . (3.59)\\nRemark. The solution for projecting onto general subspaces includes the\\n1D case as a special case: If dim(U) = 1, then B⊤B ∈ R is a scalar and\\nwe can rewrite the projection matrix in (3.59) P π = B(B⊤B)−1B⊤ as\\nP π = BB ⊤\\nB⊤B , which is exactly the projection matrix in (3.46). ♢\\nExample 3.11 (Projection onto a Two-dimensional Subspace)\\nFor a subspace U = span[\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n2\\n\\uf8f9\\n\\uf8fb] ⊆ R3 and x =\\n\\uf8ee\\n\\uf8f0\\n6\\n0\\n0\\n\\uf8f9\\n\\uf8fb ∈ R3 find the\\ncoordinates λ of x in terms of the subspace U, the projection point πU(x)\\nand the projection matrix P π.\\nFirst, we see that the generating set of U is a basis (linear indepen-\\ndence) and write the basis vectors of U into a matrix B =\\n\\uf8ee\\n\\uf8f0\\n1 0\\n1 1\\n1 2\\n\\uf8f9\\n\\uf8fb.\\nSecond, we compute the matrix B⊤B and the vector B⊤x as\\nB⊤B =\\n\\x141 1 1\\n0 1 2\\n\\x15\\uf8ee\\n\\uf8f0\\n1 0\\n1 1\\n1 2\\n\\uf8f9\\n\\uf8fb =\\n\\x143 3\\n3 5\\n\\x15\\n, B⊤x =\\n\\x141 1 1\\n0 1 2\\n\\x15\\uf8ee\\n\\uf8f0\\n6\\n0\\n0\\n\\uf8f9\\n\\uf8fb =\\n\\x146\\n0\\n\\x15\\n.\\n(3.60)\\nThird, we solve the normal equation B⊤Bλ = B⊤x to find λ:\\n\\x143 3\\n3 5\\n\\x15\\x14 λ1\\nλ2\\n\\x15\\n=\\n\\x146\\n0\\n\\x15\\n⇐ ⇒ λ =\\n\\x14 5\\n−3\\n\\x15\\n. (3.61)\\nFourth, the projection πU(x) of x onto U, i.e., into the column space of\\nB, can be directly computed via\\nπU(x) = Bλ =\\n\\uf8ee\\n\\uf8f0\\n5\\n2\\n−1\\n\\uf8f9\\n\\uf8fb . (3.62)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd10981b-3dd9-4025-8a9b-0d31cd9ec8d7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 93, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='88 Analytic Geometry\\nThe corresponding projection error is the norm of the difference vectorprojection error\\nbetween the original vector and its projection onto U, i.e.,The projection error\\nis also called the\\nreconstruction error. ∥x − πU(x)∥ =\\n\\r\\r\\r\\n\\x02\\n1 −2 1\\n\\x03⊤\\r\\r\\r =\\n√\\n6 . (3.63)\\nFifth, the projection matrix (for any x ∈ R3) is given by\\nP π = B(B⊤B)−1B⊤ = 1\\n6\\n\\uf8ee\\n\\uf8f0\\n5 2 −1\\n2 2 2\\n−1 2 5\\n\\uf8f9\\n\\uf8fb . (3.64)\\nTo verify the results, we can (a) check whether the displacement vector\\nπU(x) − x is orthogonal to all basis vectors of U, and (b) verify that\\nP π = P 2\\nπ (see Definition 3.10).\\nRemark. The projections πU(x) are still vectors in Rn although they lie in\\nan m-dimensional subspace U ⊆ Rn. However, to represent a projected\\nvector we only need the m coordinates λ1, . . . , λm with respect to the\\nbasis vectors b1, . . . ,bm of U. ♢\\nRemark. In vector spaces with general inner products, we have to pay\\nattention when computing angles and distances, which are defined by\\nmeans of the inner product. ♢We can find\\napproximate\\nsolutions to\\nunsolvable linear\\nequation systems\\nusing projections.\\nProjections allow us to look at situations where we have a linear system\\nAx = b without a solution. Recall that this means that b does not lie in\\nthe span of A, i.e., the vector b does not lie in the subspace spanned by\\nthe columns of A. Given that the linear equation cannot be solved exactly ,\\nwe can find an approximate solution. The idea is to find the vector in the\\nsubspace spanned by the columns ofA that is closest tob, i.e., we compute\\nthe orthogonal projection of b onto the subspace spanned by the columns\\nof A. This problem arises often in practice, and the solution is called the\\nleast-squares solution (assuming the dot product as the inner product) ofleast-squares\\nsolution an overdetermined system. This is discussed further in Section 9.4. Using\\nreconstruction errors (3.63) is one possible approach to derive principal\\ncomponent analysis (Section 10.3).\\nRemark. We just looked at projections of vectorsx onto a subspace U with\\nbasis vectors {b1, . . . ,bk}. If this basis is an ONB, i.e., (3.33) and (3.34)\\nare satisfied, the projection equation (3.58) simplifies greatly to\\nπU(x) = BB ⊤x (3.65)\\nsince B⊤B = I with coordinates\\nλ = B⊤x . (3.66)\\nThis means that we no longer have to compute the inverse from (3.58),\\nwhich saves computation time. ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='74702847-901c-4174-8336-29476e64c8a5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 94, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 Orthogonal Projections 89\\n3.8.3 Gram-Schmidt Orthogonalization\\nProjections are at the core of the Gram-Schmidt method that allows us to\\nconstructively transform any basis(b1, . . . ,bn) of an n-dimensional vector\\nspace V into an orthogonal/orthonormal basis (u1, . . . ,un) of V . This\\nbasis always exists (Liesen and Mehrmann, 2015) and span[b1, . . . ,bn] =\\nspan[u1, . . . ,un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt\\northogonalizationconstructs an orthogonal basis (u1, . . . ,un) from any basis (b1, . . . ,bn) of\\nV as follows:\\nu1 := b1 (3.67)\\nuk := bk − πspan[u1,...,uk−1](bk) , k = 2, . . . , n . (3.68)\\nIn (3.68), the kth basis vector bk is projected onto the subspace spanned\\nby the first k − 1 constructed orthogonal vectors u1, . . . ,uk−1; see Sec-\\ntion 3.8.2. This projection is then subtracted from bk and yields a vector\\nuk that is orthogonal to the (k − 1)-dimensional subspace spanned by\\nu1, . . . ,uk−1. Repeating this procedure for all n basis vectors b1, . . . ,bn\\nyields an orthogonal basis (u1, . . . ,un) of V . If we normalize the uk, we\\nobtain an ONB where ∥uk∥ = 1 for k = 1, . . . , n.\\nExample 3.12 (Gram-Schmidt Orthogonalization)\\nFigure 3.12\\nGram-Schmidt\\northogonalization.\\n(a) non-orthogonal\\nbasis (b1, b2) of R2;\\n(b) first constructed\\nbasis vector u1 and\\northogonal\\nprojection of b2\\nonto span[u1];\\n(c) orthogonal basis\\n(u1, u2) of R2.\\nb1\\nb2\\n0\\n(a) Original non-orthogonal\\nbasis vectors b1, b2.\\nu1\\nb2\\n0 πspan[u1](b2)\\n(b) First new basis vector\\nu1 = b1 and projection of b2\\nonto the subspace spanned by\\nu1.\\nu1\\nb2\\n0 πspan[u1](b2)\\nu2\\n(c) Orthogonal basis vectors u1\\nand u2 = b2 − πspan[u1](b2).\\nConsider a basis (b1, b2) of R2, where\\nb1 =\\n\\x142\\n0\\n\\x15\\n, b2 =\\n\\x141\\n1\\n\\x15\\n; (3.69)\\nsee also Figure 3.12(a). Using the Gram-Schmidt method, we construct an\\northogonal basis (u1, u2) of R2 as follows (assuming the dot product as\\nthe inner product):\\nu1 := b1 =\\n\\x142\\n0\\n\\x15\\n, (3.70)\\nu2 := b2 − πspan[u1](b2)\\n(3.45)\\n= b2 − u1u⊤\\n1\\n∥u1∥\\n2 b2 =\\n\\x141\\n1\\n\\x15\\n−\\n\\x141 0\\n0 0\\n\\x15\\x14 1\\n1\\n\\x15\\n=\\n\\x140\\n1\\n\\x15\\n.\\n(3.71)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='164eb41a-d5c6-45e1-9b08-5f0d07249a10', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 95, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='90 Analytic Geometry\\nFigure 3.13\\nProjection onto an\\naffine space.\\n(a) original setting;\\n(b) setting shifted\\nby −x0 so that\\nx − x0 can be\\nprojected onto the\\ndirection space U;\\n(c) projection is\\ntranslated back to\\nx0 + πU(x − x0),\\nwhich gives the final\\northogonal\\nprojection πL(x).\\nL\\nx0\\nx\\nb2\\nb10\\n(a) Setting.\\nb10\\nx−x0\\nU=L−x0\\nπU(x−x0)b2\\n(b) Reduce problem to pro-\\njection πU onto vector sub-\\nspace.\\nL\\nx0\\nx\\nb2\\nb10\\nπL(x)\\n(c) Add support point back in\\nto get affine projection πL.\\nThese steps are illustrated in Figures 3.12(b) and (c). We immediately see\\nthat u1 and u2 are orthogonal, i.e., u⊤\\n1 u2 = 0.\\n3.8.4 Projection onto Affine Subspaces\\nThus far, we discussed how to project a vector onto a lower-dimensional\\nsubspace U. In the following, we provide a solution to projecting a vector\\nonto an affine subspace.\\nConsider the setting in Figure 3.13(a). We are given an affine spaceL =\\nx0 + U, where b1, b2 are basis vectors of U. To determine the orthogonal\\nprojection πL(x) of x onto L, we transform the problem into a problem\\nthat we know how to solve: the projection onto a vector subspace. In\\norder to get there, we subtract the support point x0 from x and from L,\\nso that L − x0 = U is exactly the vector subspace U. We can now use the\\northogonal projections onto a subspace we discussed in Section 3.8.2 and\\nobtain the projection πU(x − x0), which is illustrated in Figure 3.13(b).\\nThis projection can now be translated back into L by adding x0, such that\\nwe obtain the orthogonal projection onto an affine space L as\\nπL(x) = x0 + πU(x − x0) , (3.72)\\nwhere πU(·) is the orthogonal projection onto the subspace U, i.e., the\\ndirection space of L; see Figure 3.13(c).\\nFrom Figure 3.13, it is also evident that the distance ofx from the affine\\nspace L is identical to the distance of x − x0 from U, i.e.,\\nd(x, L) = ∥x − πL(x)∥ = ∥x − (x0 + πU(x − x0))∥ (3.73a)\\n= d(x − x0, πU(x − x0)) = d(x − x0, U) . (3.73b)\\nWe will use projections onto an affine subspace to derive the concept of\\na separating hyperplane in Section 12.1.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='79a9bc1b-7177-4eb1-bbf5-afd5b84e0702', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 96, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.9 Rotations 91\\nFigure 3.14 A\\nrotation rotates\\nobjects in a plane\\nabout the origin. If\\nthe rotation angle is\\npositive, we rotate\\ncounterclockwise.\\nOriginal\\nRotated by 112.5◦\\nFigure 3.15 The\\nrobotic arm needs to\\nrotate its joints in\\norder to pick up\\nobjects or to place\\nthem correctly .\\nFigure taken\\nfrom (Deisenroth\\net al., 2015).\\n3.9 Rotations\\nLength and angle preservation, as discussed in Section 3.4, are the two\\ncharacteristics of linear mappings with orthogonal transformation matri-\\nces. In the following, we will have a closer look at specific orthogonal\\ntransformation matrices, which describe rotations.\\nA rotation is a linear mapping (more specifically , an automorphism of rotation\\na Euclidean vector space) that rotates a plane by an angle θ about the\\norigin, i.e., the origin is a fixed point. For a positive angle θ > 0, by com-\\nmon convention, we rotate in a counterclockwise direction. An example is\\nshown in Figure 3.14, where the transformation matrix is\\nR =\\n\\x14−0.38 −0.92\\n0.92 −0.38\\n\\x15\\n. (3.74)\\nImportant application areas of rotations include computer graphics and\\nrobotics. For example, in robotics, it is often important to know how to\\nrotate the joints of a robotic arm in order to pick up or place an object,\\nsee Figure 3.15.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6134c13f-c27a-4db5-b373-4c97b1d0b50d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 97, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='92 Analytic Geometry\\nFigure 3.16\\nRotation of the\\nstandard basis in R2\\nby an angle θ.\\ne1\\ne2\\nθ\\nθ\\nΦ(e2) = [−sinθ,cosθ]⊤\\nΦ(e1) = [cosθ,sinθ]⊤\\ncosθ\\nsinθ\\n−sinθ\\ncosθ\\n3.9.1 Rotations in R2\\nConsider the standard basis\\n\\x1a\\ne1 =\\n\\x141\\n0\\n\\x15\\n, e2 =\\n\\x140\\n1\\n\\x15\\x1b\\nof R2, which defines\\nthe standard coordinate system in R2. We aim to rotate this coordinate\\nsystem by an angle θ as illustrated in Figure 3.16. Note that the rotated\\nvectors are still linearly independent and, therefore, are a basis ofR2. This\\nmeans that the rotation performs a basis change.\\nRotations Φ are linear mappings so that we can express them by a\\nrotation matrix R(θ). Trigonometry (see Figure 3.16) allows us to de-rotation matrix\\ntermine the coordinates of the rotated axes (the image of Φ) with respect\\nto the standard basis in R2. We obtain\\nΦ(e1) =\\n\\x14cos θ\\nsin θ\\n\\x15\\n, Φ(e2) =\\n\\x14− sin θ\\ncos θ\\n\\x15\\n. (3.75)\\nTherefore, the rotation matrix that performs the basis change into the\\nrotated coordinates R(θ) is given as\\nR(θ) =\\n\\x02\\nΦ(e1) Φ( e2)\\n\\x03\\n=\\n\\x14cos θ − sin θ\\nsin θ cos θ\\n\\x15\\n. (3.76)\\n3.9.2 Rotations in R3\\nIn contrast to the R2 case, in R3 we can rotate any two-dimensional plane\\nabout a one-dimensional axis. The easiest way to specify the general rota-\\ntion matrix is to specify how the images of the standard basise1, e2, e3 are\\nsupposed to be rotated, and making sure these imagesRe1, Re2, Re3 are\\northonormal to each other. We can then obtain a general rotation matrix\\nR by combining the images of the standard basis.\\nTo have a meaningful rotation angle, we have to define what “coun-\\nterclockwise” means when we operate in more than two dimensions. We\\nuse the convention that a “counterclockwise” (planar) rotation about an\\naxis refers to a rotation about an axis when we look at the axis “head on,\\nfrom the end toward the origin”. InR3, there are therefore three (planar)\\nrotations about the three standard basis vectors (see Figure 3.17):\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85aa75bb-b210-4a0c-a136-27583b54651a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 98, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.9 Rotations 93\\nFigure 3.17\\nRotation of a vector\\n(gray) in R3 by an\\nangle θ about the\\ne3-axis. The rotated\\nvector is shown in\\nblue.\\ne1\\ne2\\ne3\\nθ\\nRotation about the e1-axis\\nR1(θ) =\\n\\x02\\nΦ(e1) Φ( e2) Φ( e3)\\n\\x03\\n=\\n\\uf8ee\\n\\uf8f0\\n1 0 0\\n0 cos θ − sin θ\\n0 sin θ cos θ\\n\\uf8f9\\n\\uf8fb . (3.77)\\nHere, the e1 coordinate is fixed, and the counterclockwise rotation is\\nperformed in the e2e3 plane.\\nRotation about the e2-axis\\nR2(θ) =\\n\\uf8ee\\n\\uf8f0\\ncos θ 0 sin θ\\n0 1 0\\n− sin θ 0 cos θ\\n\\uf8f9\\n\\uf8fb . (3.78)\\nIf we rotate the e1e3 plane about the e2 axis, we need to look at the e2\\naxis from its “tip” toward the origin.\\nRotation about the e3-axis\\nR3(θ) =\\n\\uf8ee\\n\\uf8f0\\ncos θ − sin θ 0\\nsin θ cos θ 0\\n0 0 1\\n\\uf8f9\\n\\uf8fb . (3.79)\\nFigure 3.17 illustrates this.\\n3.9.3 Rotations in n Dimensions\\nThe generalization of rotations from 2D and 3D to n-dimensional Eu-\\nclidean vector spaces can be intuitively described as fixing n − 2 dimen-\\nsions and restrict the rotation to a two-dimensional plane in the n-dimen-\\nsional space. As in the three-dimensional case, we can rotate any plane\\n(two-dimensional subspace of Rn).\\nDefinition 3.11 (Givens Rotation). Let V be an n-dimensional Euclidean\\nvector space and Φ : V → V an automorphism with transformation ma-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='741d0927-d026-4021-b9f8-b1d1c503fe06', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 99, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='94 Analytic Geometry\\ntrix\\nRij(θ) :=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nI i−1 0 · · · · · · 0\\n0 cos θ 0 − sin θ 0\\n0 0 I j−i−1 0 0\\n0 sin θ 0 cos θ 0\\n0 · · · · · · 0 I n−j\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ Rn×n , (3.80)\\nfor 1 ⩽ i < j ⩽ n and θ ∈ R. Then Rij(θ) is called a Givens rotation.Givens rotation\\nEssentially ,Rij(θ) is the identity matrix I n with\\nrii = cos θ , r ij = − sin θ , r ji = sin θ , r jj = cos θ . (3.81)\\nIn two dimensions (i.e., n = 2), we obtain (3.76) as a special case.\\n3.9.4 Properties of Rotations\\nRotations exhibit a number of useful properties, which can be derived by\\nconsidering them as orthogonal matrices (Definition 3.8):\\nRotations preserve distances, i.e.,∥x−y∥ = ∥Rθ(x)−Rθ(y)∥. In other\\nwords, rotations leave the distance between any two points unchanged\\nafter the transformation.\\nRotations preserve angles, i.e., the angle between Rθx and Rθy equals\\nthe angle between x and y.\\nRotations in three (or more) dimensions are generally not commuta-\\ntive. Therefore, the order in which rotations are applied is important,\\neven if they rotate about the same point. Only in two dimensions vector\\nrotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for all\\nϕ, θ ∈ [0, 2π). They form an Abelian group (with multiplication) only if\\nthey rotate about the same point (e.g., the origin).\\n3.10 Further Reading\\nIn this chapter, we gave a brief overview of some of the important concepts\\nof analytic geometry , which we will use in later chapters of the book.\\nFor a broader and more in-depth overview of some of the concepts we\\npresented, we refer to the following excellent books: Axler (2015) and\\nBoyd and Vandenberghe (2018).\\nInner products allow us to determine specific bases of vector (sub)spaces,\\nwhere each vector is orthogonal to all others (orthogonal bases) using the\\nGram-Schmidt method. These bases are important in optimization and\\nnumerical algorithms for solving linear equation systems. For instance,\\nKrylov subspace methods, such as conjugate gradients or the generalized\\nminimal residual method (GMRES), minimize residual errors that are or-\\nthogonal to each other (Stoer and Burlirsch, 2002).\\nIn machine learning, inner products are important in the context of\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2b63e55d-c966-4a56-a02b-94c0b5b6a7ca', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 100, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.10 Further Reading 95\\nkernel methods (Sch¨olkopf and Smola, 2002). Kernel methods exploit the\\nfact that many linear algorithms can be expressed purely by inner prod-\\nuct computations. Then, the “kernel trick” allows us to compute these\\ninner products implicitly in a (potentially infinite-dimensional) feature\\nspace, without even knowing this feature space explicitly . This allowed the\\n“non-linearization” of many algorithms used in machine learning, such as\\nkernel-PCA (Sch¨olkopf et al., 1997) for dimensionality reduction. Gaus-\\nsian processes (Rasmussen and Williams, 2006) also fall into the category\\nof kernel methods and are the current state of the art in probabilistic re-\\ngression (fitting curves to data points). The idea of kernels is explored\\nfurther in Chapter 12.\\nProjections are often used in computer graphics, e.g., to generate shad-\\nows. In optimization, orthogonal projections are often used to (iteratively)\\nminimize residual errors. This also has applications in machine learning,\\ne.g., in linear regression where we want to find a (linear) function that\\nminimizes the residual errors, i.e., the lengths of the orthogonal projec-\\ntions of the data onto the linear function (Bishop, 2006). We will investi-\\ngate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\\nuses projections to reduce the dimensionality of high-dimensional data.\\nWe will discuss this in more detail in Chapter 10.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7c0f1a35-4d94-48a3-9767-61457791c06e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 101, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='96 Analytic Geometry\\nExercises\\n3.1 Show that ⟨·, ·⟩ defined for all x = [x1, x2]⊤ ∈ R2 and y = [y1, y2]⊤ ∈ R2 by\\n⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2(x2y2)\\nis an inner product.\\n3.2 Consider R2 with ⟨·, ·⟩ defined for all x and y in R2 as\\n⟨x, y⟩ := x⊤\\n\\x14\\n2 0\\n1 2\\n\\x15\\n| {z }\\n=:A\\ny .\\nIs ⟨·, ·⟩ an inner product?\\n3.3 Compute the distance between\\nx =\\n\\uf8ee\\n\\uf8f0\\n1\\n2\\n3\\n\\uf8f9\\n\\uf8fb , y =\\n\\uf8ee\\n\\uf8f0\\n−1\\n−1\\n0\\n\\uf8f9\\n\\uf8fb\\nusing\\na. ⟨x, y⟩ := x⊤y\\nb. ⟨x, y⟩ := x⊤Ay , A :=\\n\\uf8ee\\n\\uf8f0\\n2 1 0\\n1 3 −1\\n0 −1 2\\n\\uf8f9\\n\\uf8fb\\n3.4 Compute the angle between\\nx =\\n\\x14\\n1\\n2\\n\\x15\\n, y =\\n\\x14\\n−1\\n−1\\n\\x15\\nusing\\na. ⟨x, y⟩ := x⊤y\\nb. ⟨x, y⟩ := x⊤By , B :=\\n\\x14\\n2 1\\n1 3\\n\\x15\\n3.5 Consider the Euclidean vector space R5 with the dot product. A subspace\\nU ⊆ R5 and x ∈ R5 are given by\\nU = span[\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n−1\\n2\\n0\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n−3\\n1\\n−1\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−3\\n4\\n1\\n2\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−3\\n5\\n0\\n7\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n] , x =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−1\\n−9\\n−1\\n4\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\na. Determine the orthogonal projection πU(x) of x onto U\\nb. Determine the distance d(x, U)\\n3.6 Consider R3 with the inner product\\n⟨x, y⟩ := x⊤\\n\\uf8ee\\n\\uf8f0\\n2 1 0\\n1 2 −1\\n0 −1 2\\n\\uf8f9\\n\\uf8fb y .\\nFurthermore, we define e1, e2, e3 as the standard/canonical basis in R3.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8984c222-7211-48d3-aa52-f61a221a83f3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 102, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 97\\na. Determine the orthogonal projection πU(e2) of e2 onto\\nU = span[e1, e3] .\\nHint: Orthogonality is defined through the inner product.\\nb. Compute the distance d(e2, U).\\nc. Draw the scenario: standard basis vectors and πU(e2)\\n3.7 Let V be a vector space and π an endomorphism of V .\\na. Prove that π is a projection if and only if idV − π is a projection, where\\nidV is the identity endomorphism on V .\\nb. Assume now that π is a projection. CalculateIm(idV −π) and ker(idV −π)\\nas a function of Im(π) and ker(π).\\n3.8 Using the Gram-Schmidt method, turn the basis B = ( b1, b2) of a two-\\ndimensional subspace U ⊆ R3 into an ONB C = (c1, c2) of U, where\\nb1 :=\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb , b2 :=\\n\\uf8ee\\n\\uf8f0\\n−1\\n2\\n0\\n\\uf8f9\\n\\uf8fb .\\n3.9 Let n ∈ N and let x1, . . . , xn > 0 be n positive real numbers so that x1 +\\n. . . + xn = 1. Use the Cauchy-Schwarz inequality and show that\\na. Pn\\ni=1 x2\\ni ⩾ 1\\nn\\nb. Pn\\ni=1\\n1\\nxi\\n⩾ n2\\nHint: Think about the dot product on Rn. Then, choose specific vectors\\nx, y ∈ Rn and apply the Cauchy-Schwarz inequality .\\n3.10 Rotate the vectors\\nx1 :=\\n\\x14\\n2\\n3\\n\\x15\\n, x2 :=\\n\\x14\\n0\\n−1\\n\\x15\\nby 30◦.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d3304d16-25b8-4b8b-b833-10a41cdf563f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 103, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4\\nMatrix Decompositions\\nIn Chapters 2 and 3, we studied ways to manipulate and measure vectors,\\nprojections of vectors, and linear mappings. Mappings and transforma-\\ntions of vectors can be conveniently described as operations performed by\\nmatrices. Moreover, data is often represented in matrix form as well, e.g.,\\nwhere the rows of the matrix represent different people and the columns\\ndescribe different features of the people, such as weight, height, and socio-\\neconomic status. In this chapter, we present three aspects of matrices: how\\nto summarize matrices, how matrices can be decomposed, and how these\\ndecompositions can be used for matrix approximations.\\nWe first consider methods that allow us to describe matrices with just\\na few numbers that characterize the overall properties of matrices. We\\nwill do this in the sections on determinants (Section 4.1) and eigenval-\\nues (Section 4.2) for the important special case of square matrices. These\\ncharacteristic numbers have important mathematical consequences and\\nallow us to quickly grasp what useful properties a matrix has. From here\\nwe will proceed to matrix decomposition methods: An analogy for ma-\\ntrix decomposition is the factoring of numbers, such as the factoring of\\n21 into prime numbers 7 · 3. For this reason matrix decomposition is also\\noften referred to as matrix factorization. Matrix decompositions are usedmatrix factorization\\nto describe a matrix by means of a different representation using factors\\nof interpretable matrices.\\nWe will first cover a square-root-like operation for symmetric, positive\\ndefinite matrices, the Cholesky decomposition (Section 4.3). From here\\nwe will look at two related methods for factorizing matrices into canoni-\\ncal forms. The first one is known as matrix diagonalization (Section 4.4),\\nwhich allows us to represent the linear mapping using a diagonal trans-\\nformation matrix if we choose an appropriate basis. The second method,\\nsingular value decomposition (Section 4.5), extends this factorization to\\nnon-square matrices, and it is considered one of the fundamental concepts\\nin linear algebra. These decompositions are helpful, as matrices represent-\\ning numerical data are often very large and hard to analyze. We conclude\\nthe chapter with a systematic overview of the types of matrices and the\\ncharacteristic properties that distinguish them in the form of a matrix tax-\\nonomy (Section 4.7).\\nThe methods that we cover in this chapter will become important in\\n98\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bed0f342-4a02-4bb1-a403-6229805b81fd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 104, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.1 Determinant and Trace 99\\nFigure 4.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.\\nDeterminant Invertibility Cholesky\\nEigenvalues\\nEigenvectors Orthogonal matrix Diagonalization\\nSVD\\nChapter 6Probability& distributions\\nChapter 10Dimensionalityreduction\\ntests used in\\nused in\\nused indetermines\\nused in\\nused in\\nused in\\nconstructs used in\\nused in\\nused in\\nboth subsequent mathematical chapters, such as Chapter 6, but also in\\napplied chapters, such as dimensionality reduction in Chapters 10 or den-\\nsity estimation in Chapter 11. This chapter’s overall structure is depicted\\nin the mind map of Figure 4.1.\\n4.1 Determinant and Trace The determinant\\nnotation |A| must\\nnot be confused\\nwith the absolute\\nvalue.\\nDeterminants are important concepts in linear algebra. A determinant is\\na mathematical object in the analysis and solution of systems of linear\\nequations. Determinants are only defined for square matrices A ∈ Rn×n,\\ni.e., matrices with the same number of rows and columns. In this book,\\nwe write the determinant as det(A) or sometimes as |A| so that\\ndet(A) =\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\na11 a12 . . . a 1n\\na21 a22 . . . a 2n\\n... ... ...\\nan1 an2 . . . a nn\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n. (4.1)\\nThe determinant of a square matrix A ∈ Rn×n is a function that maps A determinant\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='58c4ffac-a4dd-451d-9d10-fa5ef6a26028', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 105, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='100 Matrix Decompositions\\nonto a real number. Before providing a definition of the determinant for\\ngeneral n × n matrices, let us have a look at some motivating examples,\\nand define determinants for some special matrices.\\nExample 4.1 (Testing for Matrix Invertibility)\\nLet us begin with exploring if a square matrix A is invertible (see Sec-\\ntion 2.2.2). For the smallest cases, we already know when a matrix\\nis invertible. If A is a 1 × 1 matrix, i.e., it is a scalar number, then\\nA = a =⇒ A−1 = 1\\na. Thus a 1\\na = 1 holds, if and only if a ̸= 0.\\nFor 2 × 2 matrices, by the definition of the inverse (Definition 2.3), we\\nknow that AA−1 = I. Then, with (2.24), the inverse of A is\\nA−1 = 1\\na11a22 − a12a21\\n\\x14 a22 −a12\\n−a21 a11\\n\\x15\\n. (4.2)\\nHence, A is invertible if and only if\\na11a22 − a12a21 ̸= 0 . (4.3)\\nThis quantity is the determinant of A ∈ R2×2, i.e.,\\ndet(A) =\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\na11 a12\\na21 a22\\n\\x0c\\x0c\\x0c\\x0c\\x0c = a11a22 − a12a21 . (4.4)\\nExample 4.1 points already at the relationship between determinants\\nand the existence of inverse matrices. The next theorem states the same\\nresult for n × n matrices.\\nTheorem 4.1. For any square matrixA ∈ Rn×n it holds that A is invertible\\nif and only if det(A) ̸= 0.\\nWe have explicit (closed-form) expressions for determinants of small\\nmatrices in terms of the elements of the matrix. For n = 1,\\ndet(A) = det(a11) = a11 . (4.5)\\nFor n = 2,\\ndet(A) =\\n\\x0c\\x0c\\x0c\\x0c\\na11 a12\\na21 a22\\n\\x0c\\x0c\\x0c\\x0c = a11a22 − a12a21 , (4.6)\\nwhich we have observed in the preceding example.\\nFor n = 3 (known as Sarrus’ rule),\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\na11 a12 a13\\na21 a22 a23\\na31 a32 a33\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n= a11a22a33 + a21a32a13 + a31a12a23 (4.7)\\n− a31a22a13 − a11a32a23 − a21a12a33 .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aef800e2-34e6-4673-8a2c-c4997d6028e5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 106, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.1 Determinant and Trace 101\\nFor a memory aid of the product terms in Sarrus’ rule, try tracing the\\nelements of the triple products in the matrix.\\nWe call a square matrix T an upper-triangular matrix if Tij = 0 for upper-triangular\\nmatrixi > j , i.e., the matrix is zero below its diagonal. Analogously , we define a\\nlower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\\nmatrixangular matrix T ∈ Rn×n, the determinant is the product of the diagonal\\nelements, i.e.,\\ndet(T ) =\\nnY\\ni=1\\nTii . (4.8)\\nThe determinant is\\nthe signed volume\\nof the parallelepiped\\nformed by the\\ncolumns of the\\nmatrix.\\nFigure 4.2 The area\\nof the parallelogram\\n(shaded region)\\nspanned by the\\nvectors b and g is\\n|det([b, g])|.\\nb\\ng\\nFigure 4.3 The\\nvolume of the\\nparallelepiped\\n(shaded volume)\\nspanned by vectors\\nr, b, g is\\n|det([r, b, g])|.\\nb\\ng r\\nExample 4.2 (Determinants as Measures of Volume)\\nThe notion of a determinant is natural when we consider it as a mapping\\nfrom a set of n vectors spanning an object in Rn. It turns out that the de-\\nterminant det(A) is the signed volume of ann-dimensional parallelepiped\\nformed by columns of the matrix A.\\nFor n = 2 , the columns of the matrix form a parallelogram; see Fig-\\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\\nogram shrinks, too. Consider two vectors b, g that form the columns of a\\nmatrix A = [b, g]. Then, the absolute value of the determinant ofA is the\\narea of the parallelogram with vertices 0, b, g, b + g. In particular, if b, g\\nare linearly dependent so that b = λg for some λ ∈ R, they no longer\\nform a two-dimensional parallelogram. Therefore, the corresponding area\\nis 0. On the contrary , ifb, g are linearly independent and are multiples of\\nthe canonical basis vectors e1, e2 then they can be written as b =\\n\\x14b\\n0\\n\\x15\\nand\\ng =\\n\\x140\\ng\\n\\x15\\n, and the determinant is\\n\\x0c\\x0c\\x0c\\x0c\\nb 0\\n0 g\\n\\x0c\\x0c\\x0c\\x0c = bg − 0 = bg.\\nThe sign of the determinant indicates the orientation of the spanning\\nvectors b, g with respect to the standard basis (e1, e2). In our figure, flip-\\nping the order tog, b swaps the columns ofA and reverses the orientation\\nof the shaded area. This becomes the familiar formula: area = height ×\\nlength. This intuition extends to higher dimensions. In R3, we consider\\nthree vectors r, b, g ∈ R3 spanning the edges of a parallelepiped, i.e., a\\nsolid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\\ndeterminant\\nindicates the\\norientation of the\\nspanning vectors.\\nsolute value of the determinant of the3 × 3 matrix [r, b, g] is the volume\\nof the solid. Thus, the determinant acts as a function that measures the\\nsigned volume formed by column vectors composed in a matrix.\\nConsider the three linearly independent vectors r, g, b ∈ R3 given as\\nr =\\n\\uf8ee\\n\\uf8f0\\n2\\n0\\n−8\\n\\uf8f9\\n\\uf8fb , g =\\n\\uf8ee\\n\\uf8f0\\n6\\n1\\n0\\n\\uf8f9\\n\\uf8fb , b =\\n\\uf8ee\\n\\uf8f0\\n1\\n4\\n−1\\n\\uf8f9\\n\\uf8fb . (4.9)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fabcf27-d05c-449f-99b3-5decb64b707f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 107, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='102 Matrix Decompositions\\nWriting these vectors as the columns of a matrix\\nA = [r, g, b] =\\n\\uf8ee\\n\\uf8f0\\n2 6 1\\n0 1 4\\n−8 0 −1\\n\\uf8f9\\n\\uf8fb (4.10)\\nallows us to compute the desired volume as\\nV = |det(A)| = 186 . (4.11)\\nComputing the determinant of an n × n matrix requires a general algo-\\nrithm to solve the cases forn > 3, which we are going to explore in the fol-\\nlowing. Theorem 4.2 below reduces the problem of computing the deter-\\nminant of an n×n matrix to computing the determinant of(n−1)×(n−1)\\nmatrices. By recursively applying the Laplace expansion (Theorem 4.2),\\nwe can therefore compute determinants of n × n matrices by ultimately\\ncomputing determinants of 2 × 2 matrices.\\nLaplace expansion\\nTheorem 4.2 (Laplace Expansion). Consider a matrix A ∈ Rn×n. Then,\\nfor all j = 1, . . . , n:\\n1. Expansion along column jdet(Ak,j) is called\\na minor and\\n(−1)k+j det(Ak,j)\\na cofactor. det(A) =\\nnX\\nk=1\\n(−1)k+jakj det(Ak,j) . (4.12)\\n2. Expansion along row j\\ndet(A) =\\nnX\\nk=1\\n(−1)k+jajk det(Aj,k) . (4.13)\\nHere Ak,j ∈ R(n−1)×(n−1) is the submatrix of A that we obtain when delet-\\ning row k and column j.\\nExample 4.3 (Laplace Expansion)\\nLet us compute the determinant of\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 2 3\\n3 1 2\\n0 0 1\\n\\uf8f9\\n\\uf8fb (4.14)\\nusing the Laplace expansion along the first row. Applying (4.13) yields\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n1 2 3\\n3 1 2\\n0 0 1\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n= (−1)1+1 · 1\\n\\x0c\\x0c\\x0c\\x0c\\n1 2\\n0 1\\n\\x0c\\x0c\\x0c\\x0c\\n+ (−1)1+2 · 2\\n\\x0c\\x0c\\x0c\\x0c\\n3 2\\n0 1\\n\\x0c\\x0c\\x0c\\x0c + (−1)1+3 · 3\\n\\x0c\\x0c\\x0c\\x0c\\n3 1\\n0 0\\n\\x0c\\x0c\\x0c\\x0c .\\n(4.15)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c160fed5-1c35-40e6-9959-8b404633eb4b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 108, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.1 Determinant and Trace 103\\nWe use (4.6) to compute the determinants of all2 ×2 matrices and obtain\\ndet(A) = 1(1 − 0) − 2(3 − 0) + 3(0 − 0) = −5 . (4.16)\\nFor completeness we can compare this result to computing the determi-\\nnant using Sarrus’ rule (4.7):\\ndet(A) = 1·1·1+3·0·3+0·2·2−0·1·3−1·0·2−3·2·1 = 1−6 = −5 . (4.17)\\nFor A ∈ Rn×n the determinant exhibits the following properties:\\nThe determinant of a matrix product is the product of the corresponding\\ndeterminants, det(AB) = det(A)det(B).\\nDeterminants are invariant to transposition, i.e., det(A) = det(A⊤).\\nIf A is regular (invertible), then det(A−1) = 1\\ndet(A).\\nSimilar matrices (Definition 2.22) possess the same determinant. There-\\nfore, for a linear mapping Φ : V → V all transformation matrices AΦ\\nof Φ have the same determinant. Thus, the determinant is invariant to\\nthe choice of basis of a linear mapping.\\nAdding a multiple of a column/row to another one does not change\\ndet(A).\\nMultiplication of a column/row with λ ∈ R scales det(A) by λ. In\\nparticular, det(λA) = λn det(A).\\nSwapping two rows/columns changes the sign of det(A).\\nBecause of the last three properties, we can use Gaussian elimination (see\\nSection 2.1) to compute det(A) by bringing A into row-echelon form.\\nWe can stop Gaussian elimination when we have A in a triangular form\\nwhere the elements below the diagonal are all0. Recall from (4.8) that the\\ndeterminant of a triangular matrix is the product of the diagonal elements.\\nTheorem 4.3. A square matrix A ∈ Rn×n has det(A) ̸= 0 if and only if\\nrk(A) = n. In other words, A is invertible if and only if it is full rank.\\nWhen mathematics was mainly performed by hand, the determinant\\ncalculation was considered an essential way to analyze matrix invertibil-\\nity . However, contemporary approaches in machine learning use direct\\nnumerical methods that superseded the explicit calculation of the deter-\\nminant. For example, in Chapter 2, we learned that inverse matrices can\\nbe computed by Gaussian elimination. Gaussian elimination can thus be\\nused to compute the determinant of a matrix.\\nDeterminants will play an important theoretical role for the following\\nsections, especially when we learn about eigenvalues and eigenvectors\\n(Section 4.2) through the characteristic polynomial.\\nDefinition 4.4. The trace of a square matrix A ∈ Rn×n is defined as trace\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c307fe57-c583-4277-b4c9-b3cbaecb3b50', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 109, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='104 Matrix Decompositions\\ntr(A) :=\\nnX\\ni=1\\naii , (4.18)\\ni.e. , the trace is the sum of the diagonal elements of A.\\nThe trace satisfies the following properties:\\ntr(A + B) = tr(A) + tr(B) for A, B ∈ Rn×n\\ntr(αA) = αtr(A) , α ∈ R for A ∈ Rn×n\\ntr(I n) = n\\ntr(AB) = tr(BA) for A ∈ Rn×k, B ∈ Rk×n\\nIt can be shown that only one function satisfies these four properties to-\\ngether – the trace (Gohberg et al., 2012).\\nThe properties of the trace of matrix products are more general. Specif-\\nically , the trace is invariant under cyclic permutations, i.e.,The trace is\\ninvariant under\\ncyclic permutations. tr(AKL) = tr(KLA) (4.19)\\nfor matrices A ∈ Ra×k, K ∈ Rk×l, L ∈ Rl×a. This property generalizes to\\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\\nfollows that for two vectors x, y ∈ Rn\\ntr(xy⊤) = tr(y⊤x) = y⊤x ∈ R . (4.20)\\nGiven a linear mapping Φ : V → V , where V is a vector space, we\\ndefine the trace of this map by using the trace of matrix representation\\nof Φ. For a given basis of V , we can describe Φ by means of the transfor-\\nmation matrix A. Then the trace of Φ is the trace of A. For a different\\nbasis of V , it holds that the corresponding transformation matrix B of Φ\\ncan be obtained by a basis change of the form S−1AS for suitable S (see\\nSection 2.7.2). For the corresponding trace of Φ, this means\\ntr(B) = tr(S−1AS)\\n(4.19)\\n= tr(ASS −1) = tr(A) . (4.21)\\nHence, while matrix representations of linear mappings are basis depen-\\ndent the trace of a linear mapping Φ is independent of the basis.\\nIn this section, we covered determinants and traces as functions char-\\nacterizing a square matrix. Taking together our understanding of determi-\\nnants and traces we can now define an important equation describing a\\nmatrix A in terms of a polynomial, which we will use extensively in the\\nfollowing sections.\\nDefinition 4.5 (Characteristic Polynomial). For λ ∈ R and a square ma-\\ntrix A ∈ Rn×n\\npA(λ) := det(A − λI) (4.22a)\\n= c0 + c1λ + c2λ2 + · · · + cn−1λn−1 + (−1)nλn , (4.22b)\\nc0, . . . , cn−1 ∈ R, is the characteristic polynomial of A. In particular,characteristic\\npolynomial\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2f5d05cc-a463-4ca3-be03-fb2d787c46b8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 110, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Eigenvalues and Eigenvectors 105\\nc0 = det(A) , (4.23)\\ncn−1 = (−1)n−1tr(A) . (4.24)\\nThe characteristic polynomial (4.22a) will allow us to compute eigen-\\nvalues and eigenvectors, covered in the next section.\\n4.2 Eigenvalues and Eigenvectors\\nWe will now get to know a new way to characterize a matrix and its associ-\\nated linear mapping. Recall from Section 2.7.1 that every linear mapping\\nhas a unique transformation matrix given an ordered basis. We can in-\\nterpret linear mappings and their associated transformation matrices by\\nperforming an “eigen” analysis. As we will see, the eigenvalues of a lin- Eigen is a German\\nword meaning\\n“characteristic”,\\n“self”, or “own”.\\near mapping will tell us how a special set of vectors, the eigenvectors, is\\ntransformed by the linear mapping.\\nDefinition 4.6. Let A ∈ Rn×n be a square matrix. Then λ ∈ R is an\\neigenvalue of A and x ∈ Rn\\\\{0} is the corresponding eigenvector of A if eigenvalue\\neigenvector\\nAx = λx . (4.25)\\nWe call (4.25) the eigenvalue equation. eigenvalue equation\\nRemark. In the linear algebra literature and software, it is often a conven-\\ntion that eigenvalues are sorted in descending order, so that the largest\\neigenvalue and associated eigenvector are called the first eigenvalue and\\nits associated eigenvector, and the second largest called the second eigen-\\nvalue and its associated eigenvector, and so on. However, textbooks and\\npublications may have different or no notion of orderings. We do not want\\nto presume an ordering in this book if not stated explicitly . ♢\\nThe following statements are equivalent:\\nλ is an eigenvalue of A ∈ Rn×n.\\nThere exists an x ∈ Rn\\\\{0} with Ax = λx, or equivalently , (A −\\nλI n)x = 0 can be solved non-trivially , i.e.,x ̸= 0.\\nrk(A − λI n) < n.\\ndet(A − λI n) = 0.\\nDefinition 4.7 (Collinearity and Codirection). Two vectors that point in\\nthe same direction are called codirected. Two vectors are collinear if they codirected\\ncollinearpoint in the same or the opposite direction.\\nRemark (Non-uniqueness of eigenvectors) . If x is an eigenvector of A\\nassociated with eigenvalue λ, then for any c ∈ R\\\\{0} it holds that cx is\\nan eigenvector of A with the same eigenvalue since\\nA(cx) = cAx = cλx = λ(cx) . (4.26)\\nThus, all vectors that are collinear to x are also eigenvectors of A.\\n♢\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8fb3cdf0-1776-4644-8947-f0e8e9d05cbb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 111, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='106 Matrix Decompositions\\nTheorem 4.8. λ ∈ R is an eigenvalue of A ∈ Rn×n if and only if λ is a\\nroot of the characteristic polynomial pA(λ) of A.\\nDefinition 4.9. Let a square matrixA have an eigenvalueλi. The algebraicalgebraic\\nmultiplicity multiplicity of λi is the number of times the root appears in the character-\\nistic polynomial.\\nDefinition 4.10 (Eigenspace and Eigenspectrum). For A ∈ Rn×n, the set\\nof all eigenvectors of A associated with an eigenvalue λ spans a subspace\\nof Rn, which is called theeigenspace of A with respect to λ and is denotedeigenspace\\nby Eλ. The set of all eigenvalues of A is called the eigenspectrum, or justeigenspectrum\\nspectrum, of A.spectrum\\nIf λ is an eigenvalue of A ∈ Rn×n, then the corresponding eigenspace\\nEλ is the solution space of the homogeneous system of linear equations\\n(A−λI)x = 0. Geometrically , the eigenvector corresponding to a nonzero\\neigenvalue points in a direction that is stretched by the linear mapping.\\nThe eigenvalue is the factor by which it is stretched. If the eigenvalue is\\nnegative, the direction of the stretching is flipped.\\nExample 4.4 (The Case of the Identity Matrix)\\nThe identity matrix I ∈ Rn×n has characteristic polynomial pI(λ) =\\ndet(I − λI) = (1 − λ)n = 0, which has only one eigenvalueλ = 1 that oc-\\ncurs n times. Moreover, Ix = λx = 1x holds for all vectors x ∈ Rn\\\\{0}.\\nBecause of this, the sole eigenspace E1 of the identity matrix spans n di-\\nmensions, and all n standard basis vectors of Rn are eigenvectors of I.\\nUseful properties regarding eigenvalues and eigenvectors include the\\nfollowing:\\nA matrix A and its transpose A⊤ possess the same eigenvalues, but not\\nnecessarily the same eigenvectors.\\nThe eigenspace Eλ is the null space of A − λI since\\nAx = λx ⇐ ⇒ Ax − λx = 0 (4.27a)\\n⇐ ⇒ (A − λI)x = 0 ⇐ ⇒ x ∈ ker(A − λI). (4.27b)\\nSimilar matrices (see Definition 2.22) possess the same eigenvalues.\\nTherefore, a linear mapping Φ has eigenvalues that are independent of\\nthe choice of basis of its transformation matrix. This makes eigenvalues,\\ntogether with the determinant and the trace, key characteristic param-\\neters of a linear mapping as they are all invariant under basis change.\\nSymmetric, positive definite matrices always have positive, real eigen-\\nvalues.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a6b66cc-f2e5-4aab-b113-98f1c136835f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 112, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Eigenvalues and Eigenvectors 107\\nExample 4.5 (Computing Eigenvalues, Eigenvectors, and\\nEigenspaces)\\nLet us find the eigenvalues and eigenvectors of the 2 × 2 matrix\\nA =\\n\\x144 2\\n1 3\\n\\x15\\n. (4.28)\\nStep 1: Characteristic Polynomial. From our definition of the eigen-\\nvector x ̸= 0 and eigenvalue λ of A, there will be a vector such that\\nAx = λx, i.e., (A − λI)x = 0. Since x ̸= 0, this requires that the kernel\\n(null space) of A − λI contains more elements than just 0. This means\\nthat A − λI is not invertible and therefore det(A − λI) = 0. Hence, we\\nneed to compute the roots of the characteristic polynomial (4.22a) to find\\nthe eigenvalues.\\nStep 2: Eigenvalues. The characteristic polynomial is\\npA(λ) = det(A − λI) (4.29a)\\n= det\\n\\x12\\x144 2\\n1 3\\n\\x15\\n−\\n\\x14λ 0\\n0 λ\\n\\x15\\x13\\n=\\n\\x0c\\x0c\\x0c\\x0c\\n4 − λ 2\\n1 3 − λ\\n\\x0c\\x0c\\x0c\\x0c (4.29b)\\n= (4 − λ)(3 − λ) − 2 · 1 . (4.29c)\\nWe factorize the characteristic polynomial and obtain\\np(λ) = (4 − λ)(3 − λ) − 2 · 1 = 10 − 7λ + λ2 = (2 − λ)(5 − λ) (4.30)\\ngiving the roots λ1 = 2 and λ2 = 5.\\nStep 3: Eigenvectors and Eigenspaces. We find the eigenvectors that\\ncorrespond to these eigenvalues by looking at vectors x such that\\n\\x144 − λ 2\\n1 3 − λ\\n\\x15\\nx = 0 . (4.31)\\nFor λ = 5 we obtain\\n\\x144 − 5 2\\n1 3 − 5\\n\\x15\\x14 x1\\nx2\\n\\x15\\n=\\n\\x14−1 2\\n1 −2\\n\\x15\\x14 x1\\nx2\\n\\x15\\n= 0 . (4.32)\\nWe solve this homogeneous system and obtain a solution space\\nE5 = span[\\n\\x142\\n1\\n\\x15\\n] . (4.33)\\nThis eigenspace is one-dimensional as it possesses a single basis vector.\\nAnalogously , we find the eigenvector forλ = 2 by solving the homoge-\\nneous system of equations\\n\\x144 − 2 2\\n1 3 − 2\\n\\x15\\nx =\\n\\x142 2\\n1 1\\n\\x15\\nx = 0 . (4.34)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cae3528c-e2f5-44fa-9b7b-301b266bd98f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 113, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='108 Matrix Decompositions\\nThis means any vector x =\\n\\x14x1\\nx2\\n\\x15\\n, where x2 = −x1, such as\\n\\x14 1\\n−1\\n\\x15\\n, is an\\neigenvector with eigenvalue 2. The corresponding eigenspace is given as\\nE2 = span[\\n\\x14 1\\n−1\\n\\x15\\n] . (4.35)\\nThe two eigenspaces E5 and E2 in Example 4.5 are one-dimensional\\nas they are each spanned by a single vector. However, in other cases\\nwe may have multiple identical eigenvalues (see Definition 4.9) and the\\neigenspace may have more than one dimension.\\nDefinition 4.11. Let λi be an eigenvalue of a square matrix A. Then the\\ngeometric multiplicity of λi is the number of linearly independent eigen-geometric\\nmultiplicity vectors associated with λi. In other words, it is the dimensionality of the\\neigenspace spanned by the eigenvectors associated with λi.\\nRemark. A specific eigenvalue’s geometric multiplicity must be at least\\none because every eigenvalue has at least one associated eigenvector. An\\neigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic-\\nity , but it may be lower. ♢\\nExample 4.6\\nThe matrix A =\\n\\x142 1\\n0 2\\n\\x15\\nhas two repeated eigenvaluesλ1 = λ2 = 2 and an\\nalgebraic multiplicity of 2. The eigenvalue has, however, only one distinct\\nunit eigenvector x1 =\\n\\x141\\n0\\n\\x15\\nand, thus, geometric multiplicity 1.\\nGraphical Intuition in Two Dimensions\\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\\nues using different linear mappings. Figure 4.4 depicts five transformation\\nmatrices A1, . . . ,A5 and their impact on a square grid of points, centered\\nat the origin:In geometry , the\\narea-preserving\\nproperties of this\\ntype of shearing\\nparallel to an axis is\\nalso known as\\nCavalieri’s principle\\nof equal areas for\\nparallelograms\\n(Katz, 2004).\\nA1 =\\n\\x14 1\\n2 0\\n0 2\\n\\x15\\n. The direction of the two eigenvectors correspond to the\\ncanonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis\\nis extended by a factor of2 (eigenvalue λ1 = 2), and the horizontal axis\\nis compressed by factor 1\\n2 (eigenvalue λ2 = 1\\n2). The mapping is area\\npreserving (det(A1) = 1 = 2 · 1\\n2).\\nA2 =\\n\\x141 1\\n2\\n0 1\\n\\x15\\ncorresponds to a shearing mapping , i.e., it shears the\\npoints along the horizontal axis to the right if they are on the positive\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0956cdc-9e6f-47d3-8ae0-27ec58beffde', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 114, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Eigenvalues and Eigenvectors 109\\nFigure 4.4\\nDeterminants and\\neigenspaces.\\nOverview of five\\nlinear mappings and\\ntheir associated\\ntransformation\\nmatrices\\nAi ∈ R2×2\\nprojecting 400\\ncolor-coded points\\nx ∈ R2 (left\\ncolumn) onto target\\npoints Aix (right\\ncolumn). The\\ncentral column\\ndepicts the first\\neigenvector,\\nstretched by its\\nassociated\\neigenvalue λ1, and\\nthe second\\neigenvector\\nstretched by its\\neigenvalue λ2. Each\\nrow depicts the\\neffect of one of five\\ntransformation\\nmatrices Ai with\\nrespect to the\\nstandard basis.\\ndet(A) = 1.0\\nλ1 = 2.0\\nλ2 = 0.5\\ndet(A) = 1.0\\nλ1 = 1.0\\nλ2 = 1.0\\ndet(A) = 1.0\\nλ1 = (0.87-0.5j)\\nλ2 = (0.87+0.5j)\\ndet(A) = 0.0\\nλ1 = 0.0\\nλ2 = 2.0\\ndet(A) = 0.75\\nλ1 = 0.5\\nλ2 = 1.5\\nhalf of the vertical axis, and to the left vice versa. This mapping is area\\npreserving (det(A2) = 1 ). The eigenvalue λ1 = 1 = λ2 is repeated\\nand the eigenvectors are collinear (drawn here for emphasis in two\\nopposite directions). This indicates that the mapping acts only along\\none direction (the horizontal axis).\\nA3 =\\n\\x14cos( π\\n6 ) − sin( π\\n6 )\\nsin( π\\n6 ) cos( π\\n6 )\\n\\x15\\n= 1\\n2\\n\\x14√\\n3 −1\\n1\\n√\\n3\\n\\x15\\nThe matrix A3 rotates the\\npoints by π\\n6 rad = 30 ◦ counter-clockwise and has only complex eigen-\\nvalues, reflecting that the mapping is a rotation (hence, no eigenvectors\\nare drawn). A rotation has to be volume preserving, and so the deter-\\nminant is 1. For more details on rotations, we refer to Section 3.9.\\nA4 =\\n\\x14 1 −1\\n−1 1\\n\\x15\\nrepresents a mapping in the standard basis that col-\\nlapses a two-dimensional domain onto one dimension. Since one eigen-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3020708-08fc-4fb3-bbba-c57096d83cec', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 115, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='110 Matrix Decompositions\\nvalue is 0, the space in direction of the (blue) eigenvector corresponding\\nto λ1 = 0 collapses, while the orthogonal (red) eigenvector stretches\\nspace by a factor λ2 = 2. Therefore, the area of the image is 0.\\nA5 =\\n\\x141 1\\n21\\n2 1\\n\\x15\\nis a shear-and-stretch mapping that scales space by 75%\\nsince | det(A5)| = 3\\n4. It stretches space along the (red) eigenvector\\nof λ2 by a factor 1.5 and compresses it along the orthogonal (blue)\\neigenvector by a factor 0.5.\\nExample 4.7 (Eigenspectrum of a Biological Neural Network)\\nFigure 4.5\\nCaenorhabditis\\nelegans neural\\nnetwork (Kaiser and\\nHilgetag,\\n2006).(a) Sym-\\nmetrized\\nconnectivity matrix;\\n(b) Eigenspectrum.\\n0 50 100 150 200 250\\nneuron index\\n0\\n50\\n100\\n150\\n200\\n250 neuron index\\n(a) Connectivity matrix.\\n0 100 200\\nindex of sorted eigenvalue\\n−10\\n−5\\n0\\n5\\n10\\n15\\n20\\n25\\neigenvalue\\n (b) Eigenspectrum.\\nMethods to analyze and learn from network data are an essential com-\\nponent of machine learning methods. The key to understanding networks\\nis the connectivity between network nodes, especially if two nodes are\\nconnected to each other or not. In data science applications, it is often\\nuseful to study the matrix that captures this connectivity data.\\nWe build a connectivity/adjacency matrixA ∈ R277×277 of the complete\\nneural network of the worm C.Elegans. Each row/column represents one\\nof the 277 neurons of this worm’s brain. The connectivity matrix A has\\na value of aij = 1 if neuron i talks to neuron j through a synapse, and\\naij = 0 otherwise. The connectivity matrix is not symmetric, which im-\\nplies that eigenvalues may not be real valued. Therefore, we compute a\\nsymmetrized version of the connectivity matrix as Asym := A + A⊤. This\\nnew matrix Asym is shown in Figure 4.5(a) and has a nonzero value aij if\\nand only if two neurons are connected (white pixels), irrespective of the\\ndirection of the connection. In Figure 4.5(b), we show the correspond-\\ning eigenspectrum of Asym. The horizontal axis shows the index of the\\neigenvalues, sorted in descending order. The vertical axis shows the corre-\\nsponding eigenvalue. The S-like shape of this eigenspectrum is typical for\\nmany biological neural networks. The underlying mechanism responsible\\nfor this is an area of active neuroscience research.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56b1cdd8-16c5-49f0-97fc-24937a1783ce', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 116, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Eigenvalues and Eigenvectors 111\\nTheorem 4.12. The eigenvectors x1, . . . ,xn of a matrix A ∈ Rn×n with n\\ndistinct eigenvalues λ1, . . . , λn are linearly independent.\\nThis theorem states that eigenvectors of a matrix with n distinct eigen-\\nvalues form a basis of Rn.\\nDefinition 4.13. A square matrix A ∈ Rn×n is defective if it possesses defective\\nfewer than n linearly independent eigenvectors.\\nA non-defective matrix A ∈ Rn×n does not necessarily require n dis-\\ntinct eigenvalues, but it does require that the eigenvectors form a basis of\\nRn. Looking at the eigenspaces of a defective matrix, it follows that the\\nsum of the dimensions of the eigenspaces is less than n. Specifically , a de-\\nfective matrix has at least one eigenvalue λi with an algebraic multiplicity\\nm > 1 and a geometric multiplicity of less than m.\\nRemark. A defective matrix cannot have n distinct eigenvalues, as distinct\\neigenvalues have linearly independent eigenvectors (Theorem 4.12). ♢\\nTheorem 4.14. Given a matrix A ∈ Rm×n, we can always obtain a sym-\\nmetric, positive semidefinite matrix S ∈ Rn×n by defining\\nS := A⊤A . (4.36)\\nRemark. If rk(A) = n, then S := A⊤A is symmetric, positive definite.\\n♢\\nUnderstanding why Theorem 4.14 holds is insightful for how we can\\nuse symmetrized matrices: Symmetry requires S = S⊤, and by insert-\\ning (4.36) we obtain S = A⊤A = A⊤(A⊤)⊤ = (A⊤A)⊤ = S⊤. More-\\nover, positive semidefiniteness (Section 3.2.3) requires that x⊤Sx ⩾ 0\\nand inserting (4.36) we obtain x⊤Sx = x⊤A⊤Ax = ( x⊤A⊤)(Ax) =\\n(Ax)⊤(Ax) ⩾ 0, because the dot product computes a sum of squares\\n(which are themselves non-negative).\\nspectral theorem\\nTheorem 4.15 (Spectral Theorem). If A ∈ Rn×n is symmetric, there ex-\\nists an orthonormal basis of the corresponding vector space V consisting of\\neigenvectors of A, and each eigenvalue is real.\\nA direct implication of the spectral theorem is that the eigendecompo-\\nsition of a symmetric matrix A exists (with real eigenvalues), and that\\nwe can find an ONB of eigenvectors so that A = P DP ⊤, where D is\\ndiagonal and the columns of P contain the eigenvectors.\\nExample 4.8\\nConsider the matrix\\nA =\\n\\uf8ee\\n\\uf8f0\\n3 2 2\\n2 3 2\\n2 2 3\\n\\uf8f9\\n\\uf8fb . (4.37)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e073b590-6053-4ddd-9434-bf18ff9e7135', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 117, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='112 Matrix Decompositions\\nThe characteristic polynomial of A is\\npA(λ) = −(λ − 1)2(λ − 7) , (4.38)\\nso that we obtain the eigenvalues λ1 = 1 and λ2 = 7 , where λ1 is a\\nrepeated eigenvalue. Following our standard procedure for computing\\neigenvectors, we obtain the eigenspaces\\nE1 = span[\\n\\uf8ee\\n\\uf8f0\\n−1\\n1\\n0\\n\\uf8f9\\n\\uf8fb\\n| {z }\\n=:x1\\n,\\n\\uf8ee\\n\\uf8f0\\n−1\\n0\\n1\\n\\uf8f9\\n\\uf8fb\\n| {z }\\n=:x2\\n], E 7 = span[\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fb\\n|{z}\\n=:x3\\n] . (4.39)\\nWe see that x3 is orthogonal to both x1 and x2. However, since x⊤\\n1 x2 =\\n1 ̸= 0 , they are not orthogonal. The spectral theorem (Theorem 4.15)\\nstates that there exists an orthogonal basis, but the one we have is not\\northogonal. However, we can construct one.\\nTo construct such a basis, we exploit the fact that x1, x2 are eigenvec-\\ntors associated with the same eigenvalue λ. Therefore, for any α, β ∈ R it\\nholds that\\nA(αx1 + βx2) = Ax1α + Ax2β = λ(αx1 + βx2) , (4.40)\\ni.e., any linear combination of x1 and x2 is also an eigenvector of A as-\\nsociated with λ. The Gram-Schmidt algorithm (Section 3.8.3) is a method\\nfor iteratively constructing an orthogonal/orthonormal basis from a set of\\nbasis vectors using such linear combinations. Therefore, even ifx1 and x2\\nare not orthogonal, we can apply the Gram-Schmidt algorithm and find\\neigenvectors associated with λ1 = 1 that are orthogonal to each other\\n(and to x3). In our example, we will obtain\\nx′\\n1 =\\n\\uf8ee\\n\\uf8f0\\n−1\\n1\\n0\\n\\uf8f9\\n\\uf8fb , x′\\n2 = 1\\n2\\n\\uf8ee\\n\\uf8f0\\n−1\\n−1\\n2\\n\\uf8f9\\n\\uf8fb , (4.41)\\nwhich are orthogonal to each other, orthogonal tox3, and eigenvectors of\\nA associated with λ1 = 1.\\nBefore we conclude our considerations of eigenvalues and eigenvectors\\nit is useful to tie these matrix characteristics together with the concepts of\\nthe determinant and the trace.\\nTheorem 4.16. The determinant of a matrix A ∈ Rn×n is the product of\\nits eigenvalues, i.e.,\\ndet(A) =\\nnY\\ni=1\\nλi , (4.42)\\nwhere λi ∈ C are (possibly repeated) eigenvalues of A.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17a45238-e2dc-4957-9d16-4dc783e5b764', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 118, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Eigenvalues and Eigenvectors 113\\nFigure 4.6\\nGeometric\\ninterpretation of\\neigenvalues. The\\neigenvectors of A\\nget stretched by the\\ncorresponding\\neigenvalues. The\\narea of the unit\\nsquare changes by\\n|λ1λ2|, the\\nperimeter changes\\nby a factor of\\n1\\n2 (|λ1| + |λ2|).\\nx1\\nx2\\nv1\\nv2\\nA\\nTheorem 4.17. The trace of a matrix A ∈ Rn×n is the sum of its eigenval-\\nues, i.e.,\\ntr(A) =\\nnX\\ni=1\\nλi , (4.43)\\nwhere λi ∈ C are (possibly repeated) eigenvalues of A.\\nLet us provide a geometric intuition of these two theorems. Consider\\na matrix A ∈ R2×2 that possesses two linearly independent eigenvectors\\nx1, x2. For this example, we assume(x1, x2) are an ONB ofR2 so that they\\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\\nFrom Section 4.1, we know that the determinant computes the change of\\narea of unit square under the transformation A. In this example, we can\\ncompute the change of area explicitly: Mapping the eigenvectors using\\nA gives us vectors v1 = Ax1 = λ1x1 and v2 = Ax2 = λ2x2, i.e., the\\nnew vectors vi are scaled versions of the eigenvectors xi, and the scaling\\nfactors are the corresponding eigenvalues λi. v1, v2 are still orthogonal,\\nand the area of the rectangle they span is |λ1λ2|.\\nGiven that x1, x2 (in our example) are orthonormal, we can directly\\ncompute the perimeter of the unit square as 2(1 + 1). Mapping the eigen-\\nvectors using A creates a rectangle whose perimeter is 2(|λ1| + |λ2|).\\nTherefore, the sum of the absolute values of the eigenvalues tells us how\\nthe perimeter of the unit square changes under the transformation matrix\\nA.\\nExample 4.9 (Google’s PageRank – Webpages as Eigenvectors)\\nGoogle uses the eigenvector corresponding to the maximal eigenvalue of\\na matrix A to determine the rank of a page for search. The idea for the\\nPageRank algorithm, developed at Stanford University by Larry Page and\\nSergey Brin in 1996, was that the importance of any web page can be ap-\\nproximated by the importance of pages that link to it. For this, they write\\ndown all web sites as a huge directed graph that shows which page links\\nto which. PageRank computes the weight (importance) xi ⩾ 0 of a web\\nsite ai by counting the number of pages pointing to ai. Moreover, PageR-\\nank takes into account the importance of the web sites that link toai. The\\nnavigation behavior of a user is then modeled by a transition matrix A of\\nthis graph that tells us with what (click) probability somebody will end up\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6d9ee26b-d409-4e8f-920e-d3f2556e3043', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 119, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='114 Matrix Decompositions\\non a different web site. The matrix A has the property that for any ini-\\ntial rank/importance vector x of a web site the sequence x, Ax, A2x, . . .\\nconverges to a vector x∗. This vector is called the PageRank and satisfiesPageRank\\nAx∗ = x∗, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\\nA. After normalizing x∗, such that ∥x∗∥ = 1, we can interpret the entries\\nas probabilities. More details and different perspectives on PageRank can\\nbe found in the original technical report (Page et al., 1999).\\n4.3 Cholesky Decomposition\\nThere are many ways to factorize special types of matrices that we en-\\ncounter often in machine learning. In the positive real numbers, we have\\nthe square-root operation that gives us a decomposition of the number\\ninto identical components, e.g., 9 = 3 · 3. For matrices, we need to be\\ncareful that we compute a square-root-like operation on positive quanti-\\nties. For symmetric, positive definite matrices (see Section 3.2.3), we can\\nchoose from a number of square-root equivalent operations. The CholeskyCholesky\\ndecomposition decomposition/Cholesky factorization provides a square-root equivalent op-\\nCholesky\\nfactorization\\neration on symmetric, positive definite matrices that is useful in practice.\\nTheorem 4.18 (Cholesky Decomposition). A symmetric, positive definite\\nmatrix A can be factorized into a product A = LL⊤, where L is a lower-\\ntriangular matrix with positive diagonal elements:\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11 · · · a1n\\n... ... ...\\nan1 · · · ann\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nl11 · · · 0\\n... ... ...\\nln1 · · · lnn\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nl11 · · · ln1\\n... ... ...\\n0 · · · lnn\\n\\uf8f9\\n\\uf8fa\\uf8fb . (4.44)\\nL is called the Cholesky factor of A, and L is unique.Cholesky factor\\nExample 4.10 (Cholesky Factorization)\\nConsider a symmetric, positive definite matrix A ∈ R3×3. We are inter-\\nested in finding its Cholesky factorization A = LL⊤, i.e.,\\nA =\\n\\uf8ee\\n\\uf8f0\\na11 a21 a31\\na21 a22 a32\\na31 a32 a33\\n\\uf8f9\\n\\uf8fb = LL⊤ =\\n\\uf8ee\\n\\uf8f0\\nl11 0 0\\nl21 l22 0\\nl31 l32 l33\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\nl11 l21 l31\\n0 l22 l32\\n0 0 l33\\n\\uf8f9\\n\\uf8fb . (4.45)\\nMultiplying out the right-hand side yields\\nA =\\n\\uf8ee\\n\\uf8f0\\nl2\\n11 l21l11 l31l11\\nl21l11 l2\\n21 + l2\\n22 l31l21 + l32l22\\nl31l11 l31l21 + l32l22 l2\\n31 + l2\\n32 + l2\\n33\\n\\uf8f9\\n\\uf8fb . (4.46)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd23bc2b-cc70-4b26-af52-c2b7d8245586', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 120, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Eigendecomposition and Diagonalization 115\\nComparing the left-hand side of (4.45) and the right-hand side of (4.46)\\nshows that there is a simple pattern in the diagonal elements lii:\\nl11 = √a11 , l 22 =\\nq\\na22 − l2\\n21 , l 33 =\\nq\\na33 − (l2\\n31 + l2\\n32) . (4.47)\\nSimilarly for the elements below the diagonal ( lij, where i > j ), there is\\nalso a repeating pattern:\\nl21 = 1\\nl11\\na21 , l 31 = 1\\nl11\\na31 , l 32 = 1\\nl22\\n(a32 − l31l21) . (4.48)\\nThus, we constructed the Cholesky decomposition for any symmetric, pos-\\nitive definite 3 × 3 matrix. The key realization is that we can backward\\ncalculate what the components lij for the L should be, given the values\\naij for A and previously computed values of lij.\\nThe Cholesky decomposition is an important tool for the numerical\\ncomputations underlying machine learning. Here, symmetric positive def-\\ninite matrices require frequent manipulation, e.g., the covariance matrix\\nof a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\\ndefinite. The Cholesky factorization of this covariance matrix allows us to\\ngenerate samples from a Gaussian distribution. It also allows us to perform\\na linear transformation of random variables, which is heavily exploited\\nwhen computing gradients in deep stochastic models, such as the varia-\\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\\n2014). The Cholesky decomposition also allows us to compute determi-\\nnants very efficiently . Given the Cholesky decomposition A = LL⊤, we\\nknow that det(A) = det( L) det(L⊤) = det( L)2. Since L is a triangular\\nmatrix, the determinant is simply the product of its diagonal entries so\\nthat det(A) = Q\\ni l2\\nii. Thus, many numerical software packages use the\\nCholesky decomposition to make computations more efficient.\\n4.4 Eigendecomposition and Diagonalization\\nA diagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\\nments, i.e., they are of the form\\nD =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nc1 · · · 0\\n... ... ...\\n0 · · · cn\\n\\uf8f9\\n\\uf8fa\\uf8fb . (4.49)\\nThey allow fast computation of determinants, powers, and inverses. The\\ndeterminant is the product of its diagonal entries, a matrix power Dk is\\ngiven by each diagonal element raised to the power k, and the inverse\\nD−1 is the reciprocal of its diagonal elements if all of them are nonzero.\\nIn this section, we will discuss how to transform matrices into diagonal\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb235d40-09fd-44b3-9103-4b616a4e25e1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 121, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='116 Matrix Decompositions\\nform. This is an important application of the basis change we discussed in\\nSection 2.7.2 and eigenvalues from Section 4.2.\\nRecall that two matrices A, D are similar (Definition 2.22) if there ex-\\nists an invertible matrix P , such that D = P −1AP . More specifically , we\\nwill look at matrices A that are similar to diagonal matrices D that con-\\ntain the eigenvalues of A on the diagonal.\\nDefinition 4.19 (Diagonalizable). A matrix A ∈ Rn×n is diagonalizablediagonalizable\\nif it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\\nP ∈ Rn×n such that D = P −1AP .\\nIn the following, we will see that diagonalizing a matrix A ∈ Rn×n is\\na way of expressing the same linear mapping but in another basis (see\\nSection 2.6.1), which will turn out to be a basis that consists of the eigen-\\nvectors of A.\\nLet A ∈ Rn×n, let λ1, . . . , λn be a set of scalars, and let p1, . . . ,pn be a\\nset of vectors in Rn. We define P := [p1, . . . ,pn] and let D ∈ Rn×n be a\\ndiagonal matrix with diagonal entries λ1, . . . , λn. Then we can show that\\nAP = P D (4.50)\\nif and only if λ1, . . . , λn are the eigenvalues of A and p1, . . . ,pn are cor-\\nresponding eigenvectors of A.\\nWe can see that this statement holds because\\nAP = A[p1, . . . ,pn] = [Ap1, . . . ,Apn] , (4.51)\\nP D = [p1, . . . ,pn]\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nλ1 0\\n...\\n0 λn\\n\\uf8f9\\n\\uf8fa\\uf8fb = [λ1p1, . . . , λnpn] . (4.52)\\nThus, (4.50) implies that\\nAp1 = λ1p1 (4.53)\\n...\\nApn = λnpn . (4.54)\\nTherefore, the columns of P must be eigenvectors of A.\\nOur definition of diagonalization requires that P ∈ Rn×n is invertible,\\ni.e., P has full rank (Theorem 4.3). This requires us to have n linearly\\nindependent eigenvectors p1, . . . ,pn, i.e., the pi form a basis of Rn.\\nTheorem 4.20 (Eigendecomposition). A square matrix A ∈ Rn×n can be\\nfactored into\\nA = P DP −1 , (4.55)\\nwhere P ∈ Rn×n and D is a diagonal matrix whose diagonal entries are\\nthe eigenvalues of A, if and only if the eigenvectors of A form a basis of Rn.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9e181fe8-9fef-469c-b7a7-f9c852b91344', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 122, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Eigendecomposition and Diagonalization 117\\nFigure 4.7 Intuition\\nbehind the\\neigendecomposition\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left: P −1\\nperforms a basis\\nchange (here drawn\\nin R2 and depicted\\nas a rotation-like\\noperation) from the\\nstandard basis into\\nthe eigenbasis.\\nBottom-left to\\nbottom-right: D\\nperforms a scaling\\nalong the remapped\\northogonal\\neigenvectors,\\ndepicted here by a\\ncircle being\\nstretched to an\\nellipse. Bottom-right\\nto top-right: P\\nundoes the basis\\nchange (depicted as\\na reverse rotation)\\nand restores the\\noriginal coordinate\\nframe.\\ne1\\ne2\\np1\\np2\\np1\\np2\\ne1e2\\np1\\np2\\nλ1p1\\nλ2p2\\ne1\\ne2\\nAe1\\nAe2\\nP−1\\nD\\nP\\nA\\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\\nized and that the columns ofP are the n eigenvectors of A. For symmetric\\nmatrices we can obtain even stronger outcomes for the eigenvalue decom-\\nposition.\\nTheorem 4.21. A symmetric matrix S ∈ Rn×n can always be diagonalized.\\nTheorem 4.21 follows directly from the spectral theorem 4.15. More-\\nover, the spectral theorem states that we can find an ONB of eigenvectors\\nof Rn. This makes P an orthogonal matrix so that D = P ⊤AP .\\nRemark. The Jordan normal form of a matrix offers a decomposition that\\nworks for defective matrices (Lang, 1987) but is beyond the scope of this\\nbook. ♢\\nGeometric Intuition for the Eigendecomposition\\nWe can interpret the eigendecomposition of a matrix as follows (see also\\nFigure 4.7): Let A be the transformation matrix of a linear mapping with\\nrespect to the standard basis ei (blue arrows). P −1 performs a basis\\nchange from the standard basis into the eigenbasis. Then, the diagonal\\nD scales the vectors along these axes by the eigenvalues λi. Finally ,P\\ntransforms these scaled vectors back into the standard/canonical coordi-\\nnates yielding λipi.\\nExample 4.11 (Eigendecomposition)\\nLet us compute the eigendecomposition of A = 1\\n2\\n\\x14 5 −2\\n−2 5\\n\\x15\\n.\\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e8334663-46b5-4859-a919-428bb522ca95', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 123, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='118 Matrix Decompositions\\npolynomial of A is\\ndet(A − λI) = det\\n\\x12\\x14 5\\n2 − λ −1\\n−1 5\\n2 − λ\\n\\x15\\x13\\n(4.56a)\\n= ( 5\\n2 − λ)2 − 1 = λ2 − 5λ + 21\\n4 = (λ − 7\\n2)(λ − 3\\n2) . (4.56b)\\nTherefore, the eigenvalues of A are λ1 = 7\\n2 and λ2 = 3\\n2 (the roots of the\\ncharacteristic polynomial), and the associated (normalized) eigenvectors\\nare obtained via\\nAp1 = 7\\n2 p1 , Ap2 = 3\\n2 p2 . (4.57)\\nThis yields\\np1 = 1√\\n2\\n\\x14 1\\n−1\\n\\x15\\n, p2 = 1√\\n2\\n\\x141\\n1\\n\\x15\\n. (4.58)\\nStep 2: Check for existence. The eigenvectors p1, p2 form a basis of R2.\\nTherefore, A can be diagonalized.\\nStep 3: Construct the matrix P to diagonalize A. We collect the eigen-\\nvectors of A in P so that\\nP = [p1, p2] = 1√\\n2\\n\\x14 1 1\\n−1 1\\n\\x15\\n. (4.59)\\nWe then obtain\\nP −1AP =\\n\\x14 7\\n2 0\\n0 3\\n2\\n\\x15\\n= D . (4.60)\\nEquivalently , we get (exploiting that P −1 = P ⊤ since the eigenvectorsFigure 4.7 visualizes\\nthe\\neigendecomposition\\nof A =\\n\\x14 5 −2\\n−2 5\\n\\x15\\nas a sequence of\\nlinear\\ntransformations.\\np1 and p2 in this example form an ONB)\\n1\\n2\\n\\x14 5 −2\\n−2 5\\n\\x15\\n| {z }\\nA\\n= 1√\\n2\\n\\x14 1 1\\n−1 1\\n\\x15\\n| {z }\\nP\\n\\x14 7\\n2 0\\n0 3\\n2\\n\\x15\\n| {z }\\nD\\n1√\\n2\\n\\x141 −1\\n1 1\\n\\x15\\n| {z }\\nP −1\\n. (4.61)\\nDiagonal matrices D can efficiently be raised to a power. Therefore,\\nwe can find a matrix power for a matrix A ∈ Rn×n via the eigenvalue\\ndecomposition (if it exists) so that\\nAk = (P DP −1)k = P DkP −1 . (4.62)\\nComputing Dk is efficient because we apply this operation individually\\nto any diagonal element.\\nAssume that the eigendecomposition A = P DP −1 exists. Then,\\ndet(A) = det(P DP −1) = det(P ) det(D) det(P −1) (4.63a)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46d53bef-ac61-4d31-97cc-1ba78738c128', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 124, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Singular Value Decomposition 119\\n= det(D) =\\nY\\ni\\ndii (4.63b)\\nallows for an efficient computation of the determinant of A.\\nThe eigenvalue decomposition requires square matrices. It would be\\nuseful to perform a decomposition on general matrices. In the next sec-\\ntion, we introduce a more general matrix decomposition technique, the\\nsingular value decomposition.\\n4.5 Singular Value Decomposition\\nThe singular value decomposition (SVD) of a matrix is a central matrix\\ndecomposition method in linear algebra. It has been referred to as the\\n“fundamental theorem of linear algebra” (Strang, 1993) because it can be\\napplied to all matrices, not only to square matrices, and it always exists.\\nMoreover, as we will explore in the following, the SVD of a matrix A,\\nwhich represents a linear mapping Φ : V → W , quantifies the change\\nbetween the underlying geometry of these two vector spaces. We recom-\\nmend the work by Kalman (1996) and Roy and Banerjee (2014) for a\\ndeeper overview of the mathematics of the SVD.\\nSVD theorem\\nTheorem 4.22 (SVD Theorem). Let A ∈ Rm×n be a rectangular matrix of\\nrank r ∈ [0, min(m, n)]. The SVD of A is a decomposition of the form SVD\\nsingular value\\ndecomposition\\n= UA V⊤Σ\\nm\\nn\\nm\\nm\\nm\\nn\\nn\\nn\\n(4.64)\\nwith an orthogonal matrixU ∈ Rm×m with column vectorsui, i = 1, . . . , m,\\nand an orthogonal matrix V ∈ Rn×n with column vectors vj, j = 1, . . . , n.\\nMoreover,Σ is an m × n matrix with Σii = σi ⩾ 0 and Σij = 0, i ̸= j.\\nThe diagonal entries σi, i = 1, . . . , r, of Σ are called the singular values, singular values\\nui are called the left-singular vectors, and vj are called the right-singular left-singular vectors\\nright-singular\\nvectors\\nvectors. By convention, the singular values are ordered, i.e., σ1 ⩾ σ2 ⩾\\nσr ⩾ 0.\\nThe singular value matrix Σ is unique, but it requires some attention. singular value\\nmatrixObserve that the Σ ∈ Rm×n is rectangular. In particular,Σ is of the same\\nsize as A. This means that Σ has a diagonal submatrix that contains the\\nsingular values and needs additional zero padding. Specifically , ifm > n ,\\nthen the matrix Σ has diagonal structure up to row n and then consists of\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='80d21469-3aaf-4376-9063-350cee4096ef', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 125, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='120 Matrix Decompositions\\nFigure 4.8 Intuition\\nbehind the SVD of a\\nmatrix A ∈ R3×2\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left: V ⊤\\nperforms a basis\\nchange in R2.\\nBottom-left to\\nbottom-right: Σ\\nscales and maps\\nfrom R2 to R3. The\\nellipse in the\\nbottom-right lives in\\nR3. The third\\ndimension is\\northogonal to the\\nsurface of the\\nelliptical disk.\\nBottom-right to\\ntop-right: U\\nperforms a basis\\nchange within R3.\\nv2\\nv1\\n σ2u2\\nσ1u1\\ne2\\ne1\\nσ2e2\\nσ1e1\\nA\\nV⊤\\nΣ\\nU\\n0⊤ row vectors from n + 1 to m below so that\\nΣ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nσ1 0 0\\n0 ... 0\\n0 0 σn\\n0 . . . 0\\n... ...\\n0 . . . 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (4.65)\\nIf m < n , the matrix Σ has a diagonal structure up to column m and\\ncolumns that consist of 0 from m + 1 to n:\\nΣ =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nσ1 0 0 0 . . . 0\\n0 ... 0 ... ...\\n0 0 σm 0 . . . 0\\n\\uf8f9\\n\\uf8fa\\uf8fb . (4.66)\\nRemark. The SVD exists for any matrix A ∈ Rm×n. ♢\\n4.5.1 Geometric Intuitions for the SVD\\nThe SVD offers geometric intuitions to describe a transformation matrix\\nA. In the following, we will discuss the SVD as sequential linear trans-\\nformations performed on the bases. In Example 4.12, we will then apply\\ntransformation matrices of the SVD to a set of vectors inR2, which allows\\nus to visualize the effect of each transformation more clearly .\\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\\nsponding linear mapping (recall Section 2.7.1) Φ : Rn → Rm into three\\noperations; see Figure 4.8. The SVD intuition follows superficially a simi-\\nlar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\\nspeaking, the SVD performs a basis change via V ⊤ followed by a scal-\\ning and augmentation (or reduction) in dimensionality via the singular\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6abd2811-f57d-46e0-b31c-a440217d21ee', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 126, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Singular Value Decomposition 121\\nvalue matrix Σ. Finally , it performs a second basis change viaU. The SVD\\nentails a number of important details and caveats, which is why we will\\nreview our intuition in more detail. It is useful to review\\nbasis changes\\n(Section 2.7.2),\\northogonal matrices\\n(Definition 3.8) and\\northonormal bases\\n(Section 3.5).\\nAssume we are given a transformation matrix of a linear mapping Φ :\\nRn → Rm with respect to the standard bases B and C of Rn and Rm,\\nrespectively . Moreover, assume a second basis˜B of Rn and ˜C of Rm. Then\\n1. The matrix V performs a basis change in the domain Rn from ˜B (rep-\\nresented by the red and orange vectors v1 and v2 in the top-left of Fig-\\nure 4.8) to the standard basis B. V ⊤ = V −1 performs a basis change\\nfrom B to ˜B. The red and orange vectors are now aligned with the\\ncanonical basis in the bottom-left of Figure 4.8.\\n2. Having changed the coordinate system to ˜B, Σ scales the new coordi-\\nnates by the singular values σi (and adds or deletes dimensions), i.e.,\\nΣ is the transformation matrix of Φ with respect to ˜B and ˜C, rep-\\nresented by the red and orange vectors being stretched and lying in\\nthe e1-e2 plane, which is now embedded in a third dimension in the\\nbottom-right of Figure 4.8.\\n3. U performs a basis change in the codomainRm from ˜C into the canoni-\\ncal basis of Rm, represented by a rotation of the red and orange vectors\\nout of the e1-e2 plane. This is shown in the top-right of Figure 4.8.\\nThe SVD expresses a change of basis in both the domain and codomain.\\nThis is in contrast with the eigendecomposition that operates within the\\nsame vector space, where the same basis change is applied and then un-\\ndone. What makes the SVD special is that these two different bases are\\nsimultaneously linked by the singular value matrix Σ.\\nExample 4.12 (Vectors and the SVD)\\nConsider a mapping of a square grid of vectors X ∈ R2 that fit in a box of\\nsize 2 × 2 centered at the origin. Using the standard basis, we map these\\nvectors using\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 −0.8\\n0 1\\n1 0\\n\\uf8f9\\n\\uf8fb = UΣV ⊤ (4.67a)\\n=\\n\\uf8ee\\n\\uf8f0\\n−0.79 0 −0.62\\n0.38 −0.78 −0.49\\n−0.48 −0.62 0 .62\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n1.62 0\\n0 1 .0\\n0 0\\n\\uf8f9\\n\\uf8fb\\n\\x14−0.78 0 .62\\n−0.62 −0.78\\n\\x15\\n. (4.67b)\\nWe start with a set of vectors X (colored dots; see top-left panel of Fig-\\nure 4.9) arranged in a grid. We then apply V ⊤ ∈ R2×2, which rotates X .\\nThe rotated vectors are shown in the bottom-left panel of Figure 4.9. We\\nnow map these vectors using the singular value matrixΣ to the codomain\\nR3 (see the bottom-right panel in Figure 4.9). Note that all vectors lie in\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b71b87b8-11e6-4087-9ae8-63f515f2b017', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 127, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='122 Matrix Decompositions\\nthe x1-x2 plane. The third coordinate is always 0. The vectors in the x1-x2\\nplane have been stretched by the singular values.\\nThe direct mapping of the vectors X by A to the codomain R3 equals\\nthe transformation of X by UΣV ⊤, where U performs a rotation within\\nthe codomain R3 so that the mapped vectors are no longer restricted to\\nthe x1-x2 plane; they still are on a plane as shown in the top-right panel\\nof Figure 4.9.\\nFigure 4.9 SVD and\\nmapping of vectors\\n(represented by\\ndiscs). The panels\\nfollow the same\\nanti-clockwise\\nstructure of\\nFigure 4.8.\\n−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\\nx1\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nx2\\nx1\\n-1.5 -0.5\\n0.5\\n1.5\\nx2\\n-1.5\\n-0.5\\n0.5\\n1.5\\nx3\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\\nx1\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nx2\\nx1\\n-1.5 -0.5 0.5 1.5\\nx2\\n-1.5\\n-0.5\\n0.5\\n1.5\\nx3\\n0\\n4.5.2 Construction of the SVD\\nWe will next discuss why the SVD exists and show how to compute it\\nin detail. The SVD of a general matrix shares some similarities with the\\neigendecomposition of a square matrix.\\nRemark. Compare the eigendecomposition of an SPD matrix\\nS = S⊤ = P DP ⊤ (4.68)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='27d8a1dd-7afb-4574-8834-76fa497a9d12', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 128, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Singular Value Decomposition 123\\nwith the corresponding SVD\\nS = UΣV ⊤ . (4.69)\\nIf we set\\nU = P = V , D = Σ , (4.70)\\nwe see that the SVD of SPD matrices is their eigendecomposition. ♢\\nIn the following, we will explore why Theorem 4.22 holds and how\\nthe SVD is constructed. Computing the SVD of A ∈ Rm×n is equivalent\\nto finding two sets of orthonormal bases U = ( u1, . . . ,um) and V =\\n(v1, . . . ,vn) of the codomain Rm and the domain Rn, respectively . From\\nthese ordered bases, we will construct the matrices U and V .\\nOur plan is to start with constructing the orthonormal set of right-\\nsingular vectors v1, . . . ,vn ∈ Rn. We then construct the orthonormal set\\nof left-singular vectors u1, . . . ,um ∈ Rm. Thereafter, we will link the two\\nand require that the orthogonality of the vi is preserved under the trans-\\nformation of A. This is important because we know that the images Avi\\nform a set of orthogonal vectors. We will then normalize these images by\\nscalar factors, which will turn out to be the singular values.\\nLet us begin with constructing the right-singular vectors. The spectral\\ntheorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\\nmatrix form an ONB, which also means it can be diagonalized. More-\\nover, from Theorem 4.14 we can always construct a symmetric, positive\\nsemidefinite matrix A⊤A ∈ Rn×n from any rectangular matrix A ∈\\nRm×n. Thus, we can always diagonalize A⊤A and obtain\\nA⊤A = P DP ⊤ = P\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nλ1 · · · 0\\n... ... ...\\n0 · · · λn\\n\\uf8f9\\n\\uf8fa\\uf8fb P ⊤ , (4.71)\\nwhere P is an orthogonal matrix, which is composed of the orthonormal\\neigenbasis. The λi ⩾ 0 are the eigenvalues of A⊤A. Let us assume the\\nSVD of A exists and inject (4.64) into (4.71). This yields\\nA⊤A = (UΣV ⊤)⊤(UΣV ⊤) = V Σ⊤U ⊤UΣV ⊤ , (4.72)\\nwhere U , V are orthogonal matrices. Therefore, with U ⊤U = I we ob-\\ntain\\nA⊤A = V Σ⊤ΣV ⊤ = V\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nσ2\\n1 0 0\\n0 ... 0\\n0 0 σ2\\nn\\n\\uf8f9\\n\\uf8fa\\uf8fb V ⊤ . (4.73)\\nComparing now (4.71) and (4.73), we identify\\nV ⊤ = P ⊤ , (4.74)\\nσ2\\ni = λi . (4.75)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1060e895-c535-485e-8b37-da521e79c6b6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 129, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='124 Matrix Decompositions\\nTherefore, the eigenvectors ofA⊤A that compose P are the right-singular\\nvectors V of A (see (4.74)). The eigenvalues of A⊤A are the squared\\nsingular values of Σ (see (4.75)).\\nTo obtain the left-singular vectors U, we follow a similar procedure.\\nWe start by computing the SVD of the symmetric matrix AA⊤ ∈ Rm×m\\n(instead of the previous A⊤A ∈ Rn×n). The SVD of A yields\\nAA⊤ = (UΣV ⊤)(UΣV ⊤)⊤ = UΣV ⊤V Σ⊤U ⊤ (4.76a)\\n= U\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nσ2\\n1 0 0\\n0 ... 0\\n0 0 σ2\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fb U ⊤ . (4.76b)\\nThe spectral theorem tells us that AA⊤ = SDS ⊤ can be diagonalized\\nand we can find an ONB of eigenvectors of AA⊤, which are collected in\\nS. The orthonormal eigenvectors of AA⊤ are the left-singular vectors U\\nand form an orthonormal basis in the codomain of the SVD.\\nThis leaves the question of the structure of the matrix Σ. Since AA⊤\\nand A⊤A have the same nonzero eigenvalues (see page 106), the nonzero\\nentries of the Σ matrices in the SVD for both cases have to be the same.\\nThe last step is to link up all the parts we touched upon so far. We have\\nan orthonormal set of right-singular vectors in V . To finish the construc-\\ntion of the SVD, we connect them with the orthonormal vectors U. To\\nreach this goal, we use the fact the images of the vi under A have to be\\northogonal, too. We can show this by using the results from Section 3.4.\\nWe require that the inner product between Avi and Avj must be 0 for\\ni ̸= j. For any two orthogonal eigenvectors vi, vj, i ̸= j, it holds that\\n(Avi)⊤(Avj) = v⊤\\ni (A⊤A)vj = v⊤\\ni (λjvj) = λjv⊤\\ni vj = 0 . (4.77)\\nFor the case m ⩾ r, it holds that {Av1, . . . ,Avr} is a basis of an r-\\ndimensional subspace of Rm.\\nTo complete the SVD construction, we need left-singular vectors that\\nare orthonormal: We normalize the images of the right-singular vectors\\nAvi and obtain\\nui := Avi\\n∥Avi∥ = 1√λi\\nAvi = 1\\nσi\\nAvi , (4.78)\\nwhere the last equality was obtained from (4.75) and (4.76b), showing\\nus that the eigenvalues of AA⊤ are such that σ2\\ni = λi.\\nTherefore, the eigenvectors of A⊤A, which we know are the right-\\nsingular vectors vi, and their normalized images underA, the left-singular\\nvectors ui, form two self-consistent ONBs that are connected through the\\nsingular value matrix Σ.\\nLet us rearrange (4.78) to obtain the singular value equationsingular value\\nequation\\nAvi = σiui , i = 1, . . . , r . (4.79)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='68b0d54b-2d68-4a44-a3a8-0f725ca4d6dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 130, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Singular Value Decomposition 125\\nThis equation closely resembles the eigenvalue equation (4.25), but the\\nvectors on the left- and the right-hand sides are not the same.\\nFor n < m , (4.79) holds only for i ⩽ n, but (4.79) says nothing about\\nthe ui for i > n . However, we know by construction that they are or-\\nthonormal. Conversely , form < n , (4.79) holds only fori ⩽ m. For i > m ,\\nwe have Avi = 0 and we still know that the vi form an orthonormal set.\\nThis means that the SVD also supplies an orthonormal basis of the kernel\\n(null space) of A, the set of vectors x with Ax = 0 (see Section 2.7.3).\\nConcatenating the vi as the columns of V and the ui as the columns of\\nU yields\\nAV = UΣ , (4.80)\\nwhere Σ has the same dimensions as A and a diagonal structure for rows\\n1, . . . , r. Hence, right-multiplying with V ⊤ yields A = UΣV ⊤, which is\\nthe SVD of A.\\nExample 4.13 (Computing the SVD)\\nLet us find the singular value decomposition of\\nA =\\n\\x14 1 0 1\\n−2 1 0\\n\\x15\\n. (4.81)\\nThe SVD requires us to compute the right-singular vectorsvj, the singular\\nvalues σk, and the left-singular vectors ui.\\nStep 1: Right-singular vectors as the eigenbasis of A⊤A.\\nWe start by computing\\nA⊤A =\\n\\uf8ee\\n\\uf8f0\\n1 −2\\n0 1\\n1 0\\n\\uf8f9\\n\\uf8fb\\n\\x14 1 0 1\\n−2 1 0\\n\\x15\\n=\\n\\uf8ee\\n\\uf8f0\\n5 −2 1\\n−2 1 0\\n1 0 1\\n\\uf8f9\\n\\uf8fb . (4.82)\\nWe compute the singular values and right-singular vectors vj through\\nthe eigenvalue decomposition of A⊤A, which is given as\\nA⊤A =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n5√\\n30 0 −1√\\n6\\n−2√\\n30\\n1√\\n5\\n−2√\\n6\\n1√\\n30\\n2√\\n5\\n1√\\n6\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8f0\\n6 0 0\\n0 1 0\\n0 0 0\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n5√\\n30\\n−2√\\n30\\n1√\\n30\\n0 1√\\n5\\n2√\\n5\\n−1√\\n6\\n−2√\\n6\\n1√\\n6\\n\\uf8f9\\n\\uf8fa\\uf8fb = P DP ⊤ ,\\n(4.83)\\nand we obtain the right-singular vectors as the columns of P so that\\nV = P =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n5√\\n30 0 −1√\\n6\\n−2√\\n30\\n1√\\n5\\n−2√\\n6\\n1√\\n30\\n2√\\n5\\n1√\\n6\\n\\uf8f9\\n\\uf8fa\\uf8fb . (4.84)\\nStep 2: Singular-value matrix.\\nAs the singular values σi are the square roots of the eigenvalues of\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6cdd4647-5b1d-42a9-a1db-57c37d92eaf7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 131, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='126 Matrix Decompositions\\nA⊤A we obtain them straight from D. Since rk(A) = 2 , there are only\\ntwo nonzero singular values: σ1 =\\n√\\n6 and σ2 = 1 . The singular value\\nmatrix must be the same size as A, and we obtain\\nΣ =\\n\\x14√\\n6 0 0\\n0 1 0\\n\\x15\\n. (4.85)\\nStep 3: Left-singular vectors as the normalized image of the right-\\nsingular vectors.\\nWe find the left-singular vectors by computing the image of the right-\\nsingular vectors under A and normalizing them by dividing them by their\\ncorresponding singular value. We obtain\\nu1 = 1\\nσ1\\nAv1 = 1√\\n6\\n\\x14 1 0 1\\n−2 1 0\\n\\x15\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n5√\\n30\\n−2√\\n30\\n1√\\n30\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\"\\n1√\\n5\\n− 2√\\n5\\n#\\n, (4.86)\\nu2 = 1\\nσ2\\nAv2 = 1\\n1\\n\\x14 1 0 1\\n−2 1 0\\n\\x15\\uf8ee\\n\\uf8f0\\n0\\n1√\\n5\\n2√\\n5\\n\\uf8f9\\n\\uf8fb =\\n\"\\n2√\\n5\\n1√\\n5\\n#\\n, (4.87)\\nU = [u1, u2] = 1√\\n5\\n\\x14 1 2\\n−2 1\\n\\x15\\n. (4.88)\\nNote that on a computer the approach illustrated here has poor numerical\\nbehavior, and the SVD ofA is normally computed without resorting to the\\neigenvalue decomposition of A⊤A.\\n4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition\\nLet us consider the eigendecomposition A = P DP −1 and the SVD A =\\nUΣV ⊤ and review the core elements of the past sections.\\nThe SVD always exists for any matrixRm×n. The eigendecomposition is\\nonly defined for square matrices Rn×n and only exists if we can find a\\nbasis of eigenvectors of Rn.\\nThe vectors in the eigendecomposition matrix P are not necessarily\\northogonal, i.e., the change of basis is not a simple rotation and scaling.\\nOn the other hand, the vectors in the matrices U and V in the SVD are\\northonormal, so they do represent rotations.\\nBoth the eigendecomposition and the SVD are compositions of three\\nlinear mappings:\\n1. Change of basis in the domain\\n2. Independent scaling of each new basis vector and mapping from do-\\nmain to codomain\\n3. Change of basis in the codomain\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d430213-8ceb-4f16-8cc4-84fcba8f43da', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 132, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Singular Value Decomposition 127\\nFigure 4.10 Movie\\nratings of three\\npeople for four\\nmovies and its SVD\\ndecomposition.5 4 15 5 00 0 51 0 4\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nAli\\nBeatrix\\nChandra\\nStar WarsBlade RunnerAmelieDelicatessen\\n=\\n−0.6710 0.0236 0.4647 −0.5774−0.7197 0.2054 −0.4759 0.4619−0.0939 −0.7705 −0.5268 −0.3464−0.1515 −0.6030 0.5293 −0.5774\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n9.6438 0 00 6.3639 00 0 0.70560 0 0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n−0.7367 −0.6515 −0.18110.0852 0.1762 −0.98070.6708 −0.7379 −0.0743\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nA key difference between the eigendecomposition and the SVD is that\\nin the SVD, domain and codomain can be vector spaces of different\\ndimensions.\\nIn the SVD, the left- and right-singular vector matrices U and V are\\ngenerally not inverse of each other (they perform basis changes in dif-\\nferent vector spaces). In the eigendecomposition, the basis change ma-\\ntrices P and P −1 are inverses of each other.\\nIn the SVD, the entries in the diagonal matrix Σ are all real and non-\\nnegative, which is not generally true for the diagonal matrix in the\\neigendecomposition.\\nThe SVD and the eigendecomposition are closely related through their\\nprojections\\n– The left-singular vectors of A are eigenvectors of AA⊤\\n– The right-singular vectors of A are eigenvectors of A⊤A.\\n– The nonzero singular values of A are the square roots of the nonzero\\neigenvalues of both AA⊤ and A⊤A.\\nFor symmetric matrices A ∈ Rn×n, the eigenvalue decomposition and\\nthe SVD are one and the same, which follows from the spectral theo-\\nrem 4.15.\\nExample 4.14 (Finding Structure in Movie Ratings and Consumers)\\nLet us add a practical interpretation of the SVD by analyzing data on\\npeople and their preferred movies. Consider three viewers (Ali, Beatrix,\\nChandra) rating four different movies ( Star Wars, Blade Runner, Amelie,\\nDelicatessen). Their ratings are values between 0 (worst) and 5 (best) and\\nencoded in a data matrix A ∈ R4×3 as shown in Figure 4.10. Each row\\nrepresents a movie and each column a user. Thus, the column vectors of\\nmovie ratings, one for each viewer, are xAli, xBeatrix, xChandra.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d63847a-4568-435b-bd2e-55795e24969c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 133, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='128 Matrix Decompositions\\nFactoring A using the SVD offers us a way to capture the relationships\\nof how people rate movies, and especially if there is a structure linking\\nwhich people like which movies. Applying the SVD to our data matrix A\\nmakes a number of assumptions:\\n1. All viewers rate movies consistently using the same linear mapping.\\n2. There are no errors or noise in the ratings.\\n3. We interpret the left-singular vectors ui as stereotypical movies and\\nthe right-singular vectors vj as stereotypical viewers.\\nWe then make the assumption that any viewer’s specific movie preferences\\ncan be expressed as a linear combination of the vj. Similarly , any movie’s\\nlike-ability can be expressed as a linear combination of the ui. Therefore,\\na vector in the domain of the SVD can be interpreted as a viewer in the\\n“space” of stereotypical viewers, and a vector in the codomain of the SVD\\ncorrespondingly as a movie in the “space” of stereotypical movies. Let usThese two “spaces”\\nare only\\nmeaningfully\\nspanned by the\\nrespective viewer\\nand movie data if\\nthe data itself covers\\na sufficient diversity\\nof viewers and\\nmovies.\\ninspect the SVD of our movie-user matrix. The first left-singular vector u1\\nhas large absolute values for the two science fiction movies and a large\\nfirst singular value (red shading in Figure 4.10). Thus, this groups a type\\nof users with a specific set of movies (science fiction theme). Similarly , the\\nfirst right-singular v1 shows large absolute values for Ali and Beatrix, who\\ngive high ratings to science fiction movies (green shading in Figure 4.10).\\nThis suggests that v1 reflects the notion of a science fiction lover.\\nSimilarly ,u2, seems to capture a French art house film theme, andv2 in-\\ndicates that Chandra is close to an idealized lover of such movies. An ide-\\nalized science fiction lover is a purist and only loves science fiction movies,\\nso a science fiction loverv1 gives a rating of zero to everything but science\\nfiction themed—this logic is implied by the diagonal substructure for the\\nsingular value matrix Σ. A specific movie is therefore represented by how\\nit decomposes (linearly) into its stereotypical movies. Likewise, a person\\nwould be represented by how they decompose (via linear combination)\\ninto movie themes.\\nIt is worth to briefly discuss SVD terminology and conventions, as there\\nare different versions used in the literature. While these differences can\\nbe confusing, the mathematics remains invariant to them.\\nFor convenience in notation and abstraction, we use an SVD notation\\nwhere the SVD is described as having two square left- and right-singular\\nvector matrices, but a non-square singular value matrix. Our defini-\\ntion (4.64) for the SVD is sometimes called the full SVD.full SVD\\nSome authors define the SVD a bit differently and focus on square sin-\\ngular matrices. Then, for A ∈ Rm×n and m ⩾ n,\\nA\\nm×n\\n= U\\nm×n\\nΣ\\nn×n\\nV ⊤\\nn×n\\n. (4.89)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba530452-1a86-415f-9b9c-a26319f5026c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 134, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.6 Matrix Approximation 129\\nSometimes this formulation is called thereduced SVD (e.g., Datta (2010)) reduced SVD\\nor the SVD (e.g., Press et al. (2007)). This alternative format changes\\nmerely how the matrices are constructed but leaves the mathematical\\nstructure of the SVD unchanged. The convenience of this alternative\\nformulation is that Σ is diagonal, as in the eigenvalue decomposition.\\nIn Section 4.6, we will learn about matrix approximation techniques\\nusing the SVD, which is also called the truncated SVD. truncated SVD\\nIt is possible to define the SVD of a rank- r matrix A so that U is an\\nm × r matrix, Σ a diagonal matrix r × r, and V an r × n matrix.\\nThis construction is very similar to our definition, and ensures that the\\ndiagonal matrix Σ has only nonzero entries along the diagonal. The\\nmain convenience of this alternative notation is that Σ is diagonal, as\\nin the eigenvalue decomposition.\\nA restriction that the SVD for A only applies to m × n matrices with\\nm > n is practically unnecessary . Whenm < n , the SVD decomposition\\nwill yield Σ with more zero columns than rows and, consequently , the\\nsingular values σm+1, . . . , σn are 0.\\nThe SVD is used in a variety of applications in machine learning from\\nleast-squares problems in curve fitting to solving systems of linear equa-\\ntions. These applications harness various important properties of the SVD,\\nits relation to the rank of a matrix, and its ability to approximate matrices\\nof a given rank with lower-rank matrices. Substituting a matrix with its\\nSVD has often the advantage of making calculation more robust to nu-\\nmerical rounding errors. As we will explore in the next section, the SVD’s\\nability to approximate matrices with “simpler” matrices in a principled\\nmanner opens up machine learning applications ranging from dimension-\\nality reduction and topic modeling to data compression and clustering.\\n4.6 Matrix Approximation\\nWe considered the SVD as a way to factorize A = UΣV ⊤ ∈ Rm×n into\\nthe product of three matrices, where U ∈ Rm×m and V ∈ Rn×n are or-\\nthogonal and Σ contains the singular values on its main diagonal. Instead\\nof doing the full SVD factorization, we will now investigate how the SVD\\nallows us to represent a matrix A as a sum of simpler (low-rank) matrices\\nAi, which lends itself to a matrix approximation scheme that is cheaper\\nto compute than the full SVD.\\nWe construct a rank-1 matrix Ai ∈ Rm×n as\\nAi := uiv⊤\\ni , (4.90)\\nwhich is formed by the outer product of the ith orthogonal column vector\\nof U and V . Figure 4.11 shows an image of Stonehenge, which can be\\nrepresented by a matrix A ∈ R1432×1910, and some outer products Ai, as\\ndefined in (4.90).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22dfcd62-1d4b-40f1-b4dd-573226b5f6fd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 135, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='130 Matrix Decompositions\\nFigure 4.11 Image\\nprocessing with the\\nSVD. (a) The\\noriginal grayscale\\nimage is a\\n1, 432 × 1, 910\\nmatrix of values\\nbetween 0 (black)\\nand 1 (white).\\n(b)–(f) Rank-1\\nmatrices\\nA1, . . . ,A5 and\\ntheir corresponding\\nsingular values\\nσ1, . . . , σ5. The\\ngrid-like structure of\\neach rank-1 matrix\\nis imposed by the\\nouter-product of the\\nleft and\\nright-singular\\nvectors.\\n(a) Original image A.\\n (b) A1, σ 1 ≈ 228, 052.\\n (c) A2, σ 2 ≈ 40, 647.\\n(d) A3, σ 3 ≈ 26, 125.\\n (e) A4, σ 4 ≈ 20, 232.\\n (f) A5, σ 5 ≈ 15, 436.\\nA matrix A ∈ Rm×n of rank r can be written as a sum of rank-1 matrices\\nAi so that\\nA =\\nrX\\ni=1\\nσiuiv⊤\\ni =\\nrX\\ni=1\\nσiAi , (4.91)\\nwhere the outer-product matrices Ai are weighted by the ith singular\\nvalue σi. We can see why (4.91) holds: The diagonal structure of the\\nsingular value matrix Σ multiplies only matching left- and right-singular\\nvectors uiv⊤\\ni and scales them by the corresponding singular value σi. All\\nterms Σijuiv⊤\\nj vanish for i ̸= j because Σ is a diagonal matrix. Any terms\\ni > r vanish because the corresponding singular values are 0.\\nIn (4.90), we introduced rank- 1 matrices Ai. We summed up the r in-\\ndividual rank-1 matrices to obtain a rank- r matrix A; see (4.91). If the\\nsum does not run over all matrices Ai, i = 1 , . . . , r, but only up to an\\nintermediate value k < r , we obtain a rank-k approximationrank-k\\napproximation\\nbA(k) :=\\nkX\\ni=1\\nσiuiv⊤\\ni =\\nkX\\ni=1\\nσiAi (4.92)\\nof A with rk(bA(k)) = k. Figure 4.12 shows low-rank approximations\\nbA(k) of an original image A of Stonehenge. The shape of the rocks be-\\ncomes increasingly visible and clearly recognizable in the rank- 5 approx-\\nimation. While the original image requires 1, 432 · 1, 910 = 2 , 735, 120\\nnumbers, the rank-5 approximation requires us only to store the five sin-\\ngular values and the five left- and right-singular vectors (1, 432 and 1, 910-\\ndimensional each) for a total of5 ·(1, 432+1 , 910+1) = 16 , 715 numbers\\n– just above 0.6% of the original.\\nTo measure the difference (error) betweenA and its rank-k approxima-\\ntion bA(k), we need the notion of a norm. In Section 3.1, we already used\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b0e99bf7-6d11-4a37-8986-300874eaa909', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 136, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.6 Matrix Approximation 131\\nFigure 4.12 Image\\nreconstruction with\\nthe SVD. (a)\\nOriginal image.\\n(b)–(f) Image\\nreconstruction using\\nthe low-rank\\napproximation of\\nthe SVD, where the\\nrank-k\\napproximation is\\ngiven by bA(k) =Pk\\ni=1 σiAi.\\n(a) Original image A.\\n (b) Rank-1 approximationbA(1).\\n(c) Rank-2 approximationbA(2).\\n(d) Rank-3 approximationbA(3).\\n(e) Rank-4 approximationbA(4).\\n(f) Rank-5 approximation bA(5).\\nnorms on vectors that measure the length of a vector. By analogy we can\\nalso define norms on matrices.\\nDefinition 4.23(Spectral Norm of a Matrix). For x ∈ Rn\\\\{0}, the spectral spectral norm\\nnorm of a matrix A ∈ Rm×n is defined as\\n∥A∥2 := max\\nx\\n∥Ax∥2\\n∥x∥2\\n. (4.93)\\nWe introduce the notation of a subscript in the matrix norm (left-hand\\nside), similar to the Euclidean norm for vectors (right-hand side), which\\nhas subscript 2. The spectral norm (4.93) determines how long any vector\\nx can at most become when multiplied by A.\\nTheorem 4.24. The spectral norm of A is its largest singular value σ1.\\nWe leave the proof of this theorem as an exercise.\\nEckart-Young\\ntheoremTheorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)). Con-\\nsider a matrix A ∈ Rm×n of rank r and let B ∈ Rm×n be a matrix of rank\\nk. For any k ⩽ r with bA(k) =Pk\\ni=1 σiuiv⊤\\ni it holds that\\nbA(k) = argminrk(B)=k ∥A − B∥2 , (4.94)\\r\\r\\rA − bA(k)\\n\\r\\r\\r\\n2\\n= σk+1 . (4.95)\\nThe Eckart-Young theorem states explicitly how much error we intro-\\nduce by approximating A using a rank- k approximation. We can inter-\\npret the rank- k approximation obtained with the SVD as a projection of\\nthe full-rank matrix A onto a lower-dimensional space of rank-at-most- k\\nmatrices. Of all possible projections, the SVD minimizes the error (with\\nrespect to the spectral norm) between A and any rank-k approximation.\\nWe can retrace some of the steps to understand why (4.95) should hold.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='51db5e93-5d84-4b36-a155-e1f21cd2b5c7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 137, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='132 Matrix Decompositions\\nWe observe that the difference between A − bA(k) is a matrix containing\\nthe sum of the remaining rank-1 matrices\\nA − bA(k) =\\nrX\\ni=k+1\\nσiuiv⊤\\ni . (4.96)\\nBy Theorem 4.24, we immediately obtain σk+1 as the spectral norm of the\\ndifference matrix. Let us have a closer look at (4.94). If we assume that\\nthere is another matrix B with rk(B) ⩽ k, such that\\n∥A − B∥2 <\\n\\r\\r\\rA − bA(k)\\n\\r\\r\\r\\n2\\n, (4.97)\\nthen there exists an at least (n − k)-dimensional null space Z ⊆ Rn, such\\nthat x ∈ Z implies that Bx = 0. Then it follows that\\n∥Ax∥2 = ∥(A − B)x∥2 , (4.98)\\nand by using a version of the Cauchy-Schwartz inequality (3.17) that en-\\ncompasses norms of matrices, we obtain\\n∥Ax∥2 ⩽ ∥A − B∥2 ∥x∥2 < σ k+1 ∥x∥2 . (4.99)\\nHowever, there exists a (k + 1)-dimensional subspace where ∥Ax∥2 ⩾\\nσk+1 ∥x∥2, which is spanned by the right-singular vectors vj, j ⩽ k + 1 of\\nA. Adding up dimensions of these two spaces yields a number greater than\\nn, as there must be a nonzero vector in both spaces. This is a contradiction\\nof the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\\nThe Eckart-Young theorem implies that we can use SVD to reduce a\\nrank-r matrix A to a rank- k matrix bA in a principled, optimal (in the\\nspectral norm sense) manner. We can interpret the approximation ofA by\\na rank-k matrix as a form of lossy compression. Therefore, the low-rank\\napproximation of a matrix appears in many machine learning applications,\\ne.g., image processing, noise filtering, and regularization of ill-posed prob-\\nlems. Furthermore, it plays a key role in dimensionality reduction and\\nprincipal component analysis, as we will see in Chapter 10.\\nExample 4.15 (Finding Structure in Movie Ratings and Consumers\\n(continued))\\nComing back to our movie-rating example, we can now apply the con-\\ncept of low-rank approximations to approximate the original data matrix.\\nRecall that our first singular value captures the notion of science fiction\\ntheme in movies and science fiction lovers. Thus, by using only the first\\nsingular value term in a rank-1 decomposition of the movie-rating matrix,\\nwe obtain the predicted ratings\\nA1 = u1v⊤\\n1 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n−0.6710\\n−0.7197\\n−0.0939\\n−0.1515\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\x02\\n−0.7367 −0.6515 −0.1811\\n\\x03\\n(4.100a)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a20c3989-f1f8-4727-9daf-bc35f3ae240b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 138, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.7 Matrix Phylogeny 133\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0.4943 0 .4372 0 .1215\\n0.5302 0 .4689 0 .1303\\n0.0692 0 .0612 0 .0170\\n0.1116 0 .0987 0 .0274\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (4.100b)\\nThis first rank- 1 approximation A1 is insightful: it tells us that Ali and\\nBeatrix like science fiction movies, such as Star Wars and Bladerunner\\n(entries have values > 0.4), but fails to capture the ratings of the other\\nmovies by Chandra. This is not surprising, as Chandra’s type of movies is\\nnot captured by the first singular value. The second singular value gives\\nus a better rank-1 approximation for those movie-theme lovers:\\nA2 = u2v⊤\\n2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0.0236\\n0.2054\\n−0.7705\\n−0.6030\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\x02\\n0.0852 0 .1762 −0.9807\\n\\x03\\n(4.101a)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0.0020 0 .0042 −0.0231\\n0.0175 0 .0362 −0.2014\\n−0.0656 −0.1358 0 .7556\\n−0.0514 −0.1063 0 .5914\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (4.101b)\\nIn this second rank- 1 approximation A2, we capture Chandra’s ratings\\nand movie types well, but not the science fiction movies. This leads us to\\nconsider the rank-2 approximation bA(2), where we combine the first two\\nrank-1 approximations\\nbA(2) = σ1A1 + σ2A2 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n4.7801 4 .2419 1 .0244\\n5.2252 4 .7522 −0.0250\\n0.2493 −0.2743 4 .9724\\n0.7495 0 .2756 4 .0278\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb . (4.102)\\nbA(2) is similar to the original movie ratings table\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n5 4 1\\n5 5 0\\n0 0 5\\n1 0 4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb , (4.103)\\nand this suggests that we can ignore the contribution of A3. We can in-\\nterpret this so that in the data table there is no evidence of a third movie-\\ntheme/movie-lovers category . This also means that the entire space of\\nmovie-themes/movie-lovers in our example is a two-dimensional space\\nspanned by science fiction and French art house movies and lovers.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d7cfc17-69be-4936-9c12-4a5cb6b703f9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 139, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='134 Matrix Decompositions\\nFigure 4.13 A\\nfunctional\\nphylogeny of\\nmatrices\\nencountered in\\nmachine learning.\\nReal matrices∃Pseudo-inverse∃SVD\\nSquare∃Determinant∃Trace\\nNonsquare\\nDefective Singular\\nNon-defective(diagonalizable)\\nSingular\\nNormal Non-normal\\nSymmetriceigenvalues∈R\\nPositive definite\\nCholeskyeigenvalues>0\\nDiagonal\\nIdentitymatrix\\n∃Inverse Matrix\\nRegular(invertible)\\nOrthogonalRotation\\nRn×n Rn×m\\nNo basis of\\neigenvectors\\nBasis of\\neigenvectors\\nA⊤A=AA⊤ A⊤A̸=AA⊤\\nColumns are\\northogonal\\neigenvectors\\nA⊤A=AA\\n⊤\\n=I\\ndet\\n̸= 0\\ndet\\n̸= 0\\ndet = 0\\n4.7 Matrix Phylogeny\\nThe word\\n“phylogenetic”\\ndescribes how we\\ncapture the\\nrelationships among\\nindividuals or\\ngroups and derived\\nfrom the Greek\\nwords for “tribe”\\nand “source”.\\nIn Chapters 2 and 3, we covered the basics of linear algebra and analytic\\ngeometry . In this chapter, we looked at fundamental characteristics of ma-\\ntrices and linear mappings. Figure 4.13 depicts the phylogenetic tree of\\nrelationships between different types of matrices (black arrows indicating\\n“is a subset of”) and the covered operations we can perform on them (in\\nblue). We consider all real matrices A ∈ Rn×m. For non-square matrices\\n(where n ̸= m), the SVD always exists, as we saw in this chapter. Focus-\\ning on square matrices A ∈ Rn×n, the determinant informs us whether a\\nsquare matrix possesses an inverse matrix, i.e., whether it belongs to the\\nclass of regular, invertible matrices. If the squaren × n matrix possesses n\\nlinearly independent eigenvectors, then the matrix is non-defective and an\\neigendecomposition exists (Theorem 4.12). We know that repeated eigen-\\nvalues may result in defective matrices, which cannot be diagonalized.\\nNon-singular and non-defective matrices are not the same. For exam-\\nple, a rotation matrix will be invertible (determinant is nonzero) but not\\ndiagonalizable in the real numbers (eigenvalues are not guaranteed to be\\nreal numbers).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='943e8279-1e88-432e-9f21-b7e6bf68e24a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 140, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.8 Further Reading 135\\nWe dive further into the branch of non-defective squaren × n matrices.\\nA is normal if the condition A⊤A = AA⊤ holds. Moreover, if the more\\nrestrictive condition holds that A⊤A = AA⊤ = I, then A is called or-\\nthogonal (see Definition 3.8). The set of orthogonal matrices is a subset of\\nthe regular (invertible) matrices and satisfies A⊤ = A−1.\\nNormal matrices have a frequently encountered subset, the symmetric\\nmatrices S ∈ Rn×n, which satisfy S = S⊤. Symmetric matrices have only\\nreal eigenvalues. A subset of the symmetric matrices consists of the pos-\\nitive definite matrices P that satisfy the condition of x⊤P x > 0 for all\\nx ∈ Rn\\\\{0}. In this case, a unique Cholesky decomposition exists (Theo-\\nrem 4.18). Positive definite matrices have only positive eigenvalues and\\nare always invertible (i.e., have a nonzero determinant).\\nAnother subset of symmetric matrices consists of the diagonal matrices\\nD. Diagonal matrices are closed under multiplication and addition, but do\\nnot necessarily form a group (this is only the case if all diagonal entries\\nare nonzero so that the matrix is invertible). A special diagonal matrix is\\nthe identity matrix I.\\n4.8 Further Reading\\nMost of the content in this chapter establishes underlying mathematics\\nand connects them to methods for studying mappings, many of which are\\nat the heart of machine learning at the level of underpinning software so-\\nlutions and building blocks for almost all machine learning theory . Matrix\\ncharacterization using determinants, eigenspectra, and eigenspaces pro-\\nvides fundamental features and conditions for categorizing and analyzing\\nmatrices. This extends to all forms of representations of data and map-\\npings involving data, as well as judging the numerical stability of compu-\\ntational operations on such matrices (Press et al., 2007).\\nDeterminants are fundamental tools in order to invert matrices and\\ncompute eigenvalues “by hand”. However, for almost all but the smallest\\ninstances, numerical computation by Gaussian elimination outperforms\\ndeterminants (Press et al., 2007). Determinants remain nevertheless a\\npowerful theoretical concept, e.g., to gain intuition about the orientation\\nof a basis based on the sign of the determinant. Eigenvectors can be used\\nto perform basis changes to transform data into the coordinates of mean-\\ningful orthogonal, feature vectors. Similarly , matrix decomposition meth-\\nods, such as the Cholesky decomposition, reappear often when we com-\\npute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\\nthe Cholesky decomposition enables us to compute the reparametrization\\ntrick where we want to perform continuous differentiation over random\\nvariables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;\\nKingma and Welling, 2014).\\nEigendecomposition is fundamental in enabling us to extract mean-\\ningful and interpretable information that characterizes linear mappings.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3e2461f2-395d-4dae-95de-d095315a81ff', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 141, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='136 Matrix Decompositions\\nTherefore, the eigendecomposition underlies a general class of machine\\nlearning algorithms called spectral methods that perform eigendecomposi-\\ntion of a positive-definite kernel. These spectral decomposition methods\\nencompass classical approaches to statistical data analysis, such as the\\nfollowing:\\nprincipal component\\nanalysis Principal component analysis(PCA (Pearson, 1901), see also Chapter 10),\\nin which a low-dimensional subspace, which explains most of the vari-\\nability in the data, is sought.Fisher discriminant\\nanalysis Fisher discriminant analysis, which aims to determine a separating hy-\\nperplane for data classification (Mika et al., 1999).multidimensional\\nscaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\\nThe computational efficiency of these methods typically comes from find-\\ning the best rank- k approximation to a symmetric, positive semidefinite\\nmatrix. More contemporary examples of spectral methods have different\\norigins, but each of them requires the computation of the eigenvectors\\nand eigenvalues of a positive-definite kernel, such as Isomap (TenenbaumIsomap\\net al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), HessianLaplacian\\neigenmaps\\nHessian eigenmaps\\neigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\\nspectral clustering\\nMalik, 2000). The core computations of these are generally underpinned\\nby low-rank matrix approximation techniques (Belabbas and Wolfe, 2009)\\nas we encountered here via the SVD.\\nThe SVD allows us to discover some of the same kind of information as\\nthe eigendecomposition. However, the SVD is more generally applicable\\nto non-square matrices and data tables. These matrix factorization meth-\\nods become relevant whenever we want to identify heterogeneity in data\\nwhen we want to perform data compression by approximation, e.g., in-\\nstead of storingn×m values just storing(n+m)k values, or when we want\\nto perform data pre-processing, e.g., to decorrelate predictor variables of\\na design matrix (Ormoneit et al., 2001). The SVD operates on matrices,\\nwhich we can interpret as rectangular arrays with two indices (rows and\\ncolumns). The extension of matrix-like structure to higher-dimensional\\narrays are called tensors. It turns out that the SVD is the special case of\\na more general family of decompositions that operate on such tensors\\n(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-\\ntions on tensors are, for example, theTucker decomposition(Tucker, 1966)Tucker\\ndecomposition or the CP decomposition (Carroll and Chang, 1970).\\nCP decomposition The SVD low-rank approximation is frequently used in machine learn-\\ning for computational efficiency reasons. This is because it reduces the\\namount of memory and operations with nonzero multiplications we need\\nto perform on potentially very large matrices of data (Trefethen and Bau III,\\n1997). Moreover, low-rank approximations are used to operate on ma-\\ntrices that may contain missing values as well as for purposes of lossy\\ncompression and dimensionality reduction (Moonen and De Moor, 1995;\\nMarkovsky, 2011).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3cd01362-0355-44e4-ab2b-256308973829', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 142, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 137\\nExercises\\n4.1 Compute the determinant using the Laplace expansion (using the first row)\\nand the Sarrus rule for\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 3 5\\n2 4 6\\n0 2 4\\n\\uf8f9\\n\\uf8fb .\\n4.2 Compute the following determinant efficiently:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2 0 1 2 0\\n2 −1 0 1 1\\n0 1 2 1 2\\n−2 0 2 −1 2\\n2 0 0 1 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n4.3 Compute the eigenspaces of\\na.\\nA :=\\n\\x14\\n1 0\\n1 1\\n\\x15\\nb.\\nB :=\\n\\x14\\n−2 2\\n2 1\\n\\x15\\n4.4 Compute all eigenspaces of\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0 −1 1 1\\n−1 1 −2 3\\n2 −1 0 0\\n1 −1 1 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb .\\n4.5 Diagonalizability of a matrix is unrelated to its invertibility . Determine for\\nthe following four matrices whether they are diagonalizable and/or invert-\\nible\\n\\x14\\n1 0\\n0 1\\n\\x15\\n,\\n\\x14\\n1 0\\n0 0\\n\\x15\\n,\\n\\x14\\n1 1\\n0 1\\n\\x15\\n,\\n\\x14\\n0 1\\n0 0\\n\\x15\\n.\\n4.6 Compute the eigenspaces of the following transformation matrices. Are they\\ndiagonalizable?\\na. For\\nA =\\n\\uf8ee\\n\\uf8f0\\n2 3 0\\n1 4 3\\n0 0 1\\n\\uf8f9\\n\\uf8fb\\nb. For\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 1 0 0\\n0 0 0 0\\n0 0 0 0\\n0 0 0 0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa031ebf-14ea-4ca6-829c-90b60ecb9d2f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 143, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='138 Matrix Decompositions\\n4.7 Are the following matrices diagonalizable? If yes, determine their diagonal\\nform and a basis with respect to which the transformation matrices are di-\\nagonal. If no, give reasons why they are not diagonalizable.\\na.\\nA =\\n\\x14\\n0 1\\n−8 4\\n\\x15\\nb.\\nA =\\n\\uf8ee\\n\\uf8f0\\n1 1 1\\n1 1 1\\n1 1 1\\n\\uf8f9\\n\\uf8fb\\nc.\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n5 4 2 1\\n0 1 −1 −1\\n−1 −1 3 0\\n1 1 −1 2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nd.\\nA =\\n\\uf8ee\\n\\uf8f0\\n5 −6 −6\\n−1 4 2\\n3 −6 −4\\n\\uf8f9\\n\\uf8fb\\n4.8 Find the SVD of the matrix\\nA =\\n\\x14\\n3 2 2\\n2 3 −2\\n\\x15\\n.\\n4.9 Find the singular value decomposition of\\nA =\\n\\x14\\n2 2\\n−1 1\\n\\x15\\n.\\n4.10 Find the rank-1 approximation of\\nA =\\n\\x14\\n3 2 2\\n2 3 −2\\n\\x15\\n4.11 Show that for any A ∈ Rm×n the matrices A⊤A and AA⊤ possess the\\nsame nonzero eigenvalues.\\n4.12 Show that for x ̸= 0 Theorem 4.24 holds, i.e., show that\\nmax\\nx\\n∥Ax∥2\\n∥x∥2\\n= σ1 ,\\nwhere σ1 is the largest singular value of A ∈ Rm×n.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a754cbd-1df0-4ffb-8838-e7cf48890e01', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 144, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5\\nVector Calculus\\nMany algorithms in machine learning optimize an objective function with\\nrespect to a set of desired model parameters that control how well a model\\nexplains the data: Finding good parameters can be phrased as an opti-\\nmization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-\\near regression (see Chapter 9), where we look at curve-fitting problems\\nand optimize linear weight parameters to maximize the likelihood; (ii)\\nneural-network auto-encoders for dimensionality reduction and data com-\\npression, where the parameters are the weights and biases of each layer,\\nand where we minimize a reconstruction error by repeated application of\\nthe chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\\nmodeling data distributions, where we optimize the location and shape\\nparameters of each mixture component to maximize the likelihood of the\\nmodel. Figure 5.1 illustrates some of these problems, which we typically\\nsolve by using optimization algorithms that exploit gradient information\\n(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-\\nter are related and how they are connected to other chapters of the book.\\nCentral to this chapter is the concept of a function. A function f is\\na quantity that relates two quantities to each other. In this book, these\\nquantities are typically inputs x ∈ RD and targets (function values) f(x),\\nwhich we assume are real-valued if not stated otherwise. Here RD is the\\ndomain of f, and the function values f(x) are the image/codomain of f. domain\\nimage/codomain\\nFigure 5.1 Vector\\ncalculus plays a\\ncentral role in (a)\\nregression (curve\\nfitting) and (b)\\ndensity estimation,\\ni.e., modeling data\\ndistributions.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\n(a) Regression problem: Find parameters,\\nsuch that the curve explains the observations\\n(crosses) well.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n(b) Density estimation with a Gaussian mixture\\nmodel: Find means and covariances, such that\\nthe data (dots) can be explained well.\\n139\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='80fac17e-a860-45c7-b667-590387f315f6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 145, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='140 Vector Calculus\\nFigure 5.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.\\nDifference quotient\\nPartial derivatives\\nJacobianHessian\\nTaylor series\\nChapter 7Optimization\\nChapter 6Probability\\nChapter 9Regression\\nChapter 10Dimensionalityreduction\\nChapter 11Density estimation\\nChapter 12Classification\\ndefinescollected inused in\\nused in\\nused in\\nused in\\nused in\\nused in\\nused in\\nSection 2.7.3 provides much more detailed discussion in the context of\\nlinear functions. We often write\\nf : RD → R (5.1a)\\nx 7→ f(x) (5.1b)\\nto specify a function, where (5.1a) specifies that f is a mapping from\\nRD to R and (5.1b) specifies the explicit assignment of an input x to\\na function value f(x). A function f assigns every input x exactly one\\nfunction value f(x).\\nExample 5.1\\nRecall the dot product as a special case of an inner product (Section 3.2).\\nIn the previous notation, the function f(x) = x⊤x, x ∈ R2, would be\\nspecified as\\nf : R2 → R (5.2a)\\nx 7→ x2\\n1 + x2\\n2 . (5.2b)\\nIn this chapter, we will discuss how to compute gradients of functions,\\nwhich is often essential to facilitate learning in machine learning models\\nsince the gradient points in the direction of steepest ascent. Therefore,\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe9c2032-aa63-4b1a-800f-e12149befc8f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 146, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1 Differentiation of Univariate Functions 141\\nFigure 5.3 The\\naverage incline of a\\nfunction f between\\nx0 and x0 + δx is\\nthe incline of the\\nsecant (blue)\\nthrough f(x0) and\\nf(x0 + δx) and\\ngiven by δy/δx.\\nδy\\nδx\\nf (x)\\nx\\ny\\nf (x0)\\nf (x0 +δx)\\nvector calculus is one of the fundamental mathematical tools we need in\\nmachine learning. Throughout this book, we assume that functions are\\ndifferentiable. With some additional technical definitions, which we do\\nnot cover here, many of the approaches presented can be extended to\\nsub-differentials (functions that are continuous but not differentiable at\\ncertain points). We will look at an extension to the case of functions with\\nconstraints in Chapter 7.\\n5.1 Differentiation of Univariate Functions\\nIn the following, we briefly revisit differentiation of a univariate function,\\nwhich may be familiar from high school mathematics. We start with the\\ndifference quotient of a univariate function y = f(x), x, y ∈ R, which we\\nwill subsequently use to define derivatives.\\nDefinition 5.1 (Difference Quotient). The difference quotient difference quotient\\nδy\\nδx := f(x + δx) − f(x)\\nδx (5.3)\\ncomputes the slope of the secant line through two points on the graph of\\nf. In Figure 5.3, these are the points with x-coordinates x0 and x0 + δx.\\nThe difference quotient can also be considered the average slope of f\\nbetween x and x + δx if we assume f to be a linear function. In the limit\\nfor δx → 0, we obtain the tangent of f at x, if f is differentiable. The\\ntangent is then the derivative of f at x.\\nDefinition 5.2 (Derivative). More formally , forh > 0 the derivative of f derivative\\nat x is defined as the limit\\ndf\\ndx := lim\\nh→0\\nf(x + h) − f(x)\\nh , (5.4)\\nand the secant in Figure 5.3 becomes a tangent.\\nThe derivative of f points in the direction of steepest ascent of f.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01b61a4b-0a4d-4bfd-90fa-b80198a852ab', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 147, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='142 Vector Calculus\\nExample 5.2 (Derivative of a Polynomial)\\nWe want to compute the derivative of f(x) = xn, n ∈ N. We may already\\nknow that the answer will be nxn−1, but we want to derive this result\\nusing the definition of the derivative as the limit of the difference quotient.\\nUsing the definition of the derivative in (5.4), we obtain\\ndf\\ndx = lim\\nh→0\\nf(x + h) − f(x)\\nh (5.5a)\\n= lim\\nh→0\\n(x + h)n − xn\\nh (5.5b)\\n= lim\\nh→0\\nPn\\ni=0\\n\\x00n\\ni\\n\\x01\\nxn−ihi − xn\\nh . (5.5c)\\nWe see thatxn =\\n\\x00n\\n0\\n\\x01\\nxn−0h0. By starting the sum at1, the xn-term cancels,\\nand we obtain\\ndf\\ndx = lim\\nh→0\\nPn\\ni=1\\n\\x00n\\ni\\n\\x01\\nxn−ihi\\nh (5.6a)\\n= lim\\nh→0\\nnX\\ni=1\\n \\nn\\ni\\n!\\nxn−ihi−1 (5.6b)\\n= lim\\nh→0\\n \\nn\\n1\\n!\\nxn−1 +\\nnX\\ni=2\\n \\nn\\ni\\n!\\nxn−ihi−1\\n| {z }\\n→0 as h→0\\n(5.6c)\\n= n!\\n1!(n − 1)! xn−1 = nxn−1 . (5.6d)\\n5.1.1 Taylor Series\\nThe Taylor series is a representation of a function f as an infinite sum of\\nterms. These terms are determined using derivatives of f evaluated at x0.\\nDefinition 5.3 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial\\nf : R → R at x0 is defined asWe define t0 := 1\\nfor all t ∈ R.\\nTn(x) :=\\nnX\\nk=0\\nf (k)(x0)\\nk! (x − x0)k , (5.7)\\nwhere f (k)(x0) is the kth derivative of f at x0 (which we assume exists)\\nand f (k)(x0)\\nk! are the coefficients of the polynomial.\\nDefinition 5.4 (Taylor Series). For a smooth functionf ∈ C ∞, f : R → R,\\nthe Taylor series of f at x0 is defined asTaylor series\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='10e8428b-e46a-40d2-8f73-b7e9baa07398', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 148, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1 Differentiation of Univariate Functions 143\\nT∞(x) =\\n∞X\\nk=0\\nf (k)(x0)\\nk! (x − x0)k . (5.8)\\nFor x0 = 0 , we obtain the Maclaurin series as a special instance of the f ∈ C ∞ means that\\nf is continuously\\ndifferentiable\\ninfinitely many\\ntimes.\\nMaclaurin series\\nTaylor series. If f(x) = T∞(x), then f is called analytic.\\nanalytic\\nRemark. In general, a Taylor polynomial of degree n is an approximation\\nof a function, which does not need to be a polynomial. The Taylor poly-\\nnomial is similar to f in a neighborhood around x0. However, a Taylor\\npolynomial of degree n is an exact representation of a polynomial f of\\ndegree k ⩽ n since all derivatives f (i), i > k vanish. ♢\\nExample 5.3 (Taylor Polynomial)\\nWe consider the polynomial\\nf(x) = x4 (5.9)\\nand seek the Taylor polynomial T6, evaluated at x0 = 1. We start by com-\\nputing the coefficients f (k)(1) for k = 0, . . . ,6:\\nf(1) = 1 (5.10)\\nf ′(1) = 4 (5.11)\\nf ′′(1) = 12 (5.12)\\nf (3)(1) = 24 (5.13)\\nf (4)(1) = 24 (5.14)\\nf (5)(1) = 0 (5.15)\\nf (6)(1) = 0 (5.16)\\nTherefore, the desired Taylor polynomial is\\nT6(x) =\\n6X\\nk=0\\nf (k)(x0)\\nk! (x − x0)k (5.17a)\\n= 1 + 4(x − 1) + 6(x − 1)2 + 4(x − 1)3 + (x − 1)4 + 0 . (5.17b)\\nMultiplying out and re-arranging yields\\nT6(x) = (1 − 4 + 6 − 4 + 1) +x(4 − 12 + 12 − 4)\\n+ x2(6 − 12 + 6) +x3(4 − 4) + x4 (5.18a)\\n= x4 = f(x) , (5.18b)\\ni.e., we obtain an exact representation of the original function.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ad5d9ba-3971-4638-940d-e07297bd41f0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 149, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='144 Vector Calculus\\nFigure 5.4 Taylor\\npolynomials. The\\noriginal function\\nf(x) =\\nsin(x) + cos(x)\\n(black, solid) is\\napproximated by\\nTaylor polynomials\\n(dashed) around\\nx0 = 0.\\nHigher-order Taylor\\npolynomials\\napproximate the\\nfunction f better\\nand more globally .\\nT10 is already\\nsimilar to f in\\n[−4, 4].\\n−4 −2 0 2 4\\nx\\n−2\\n0\\n2\\n4\\ny\\nf\\nT0\\nT1\\nT5\\nT10\\nExample 5.4 (Taylor Series)\\nConsider the function in Figure 5.4 given by\\nf(x) = sin(x) + cos(x) ∈ C ∞ . (5.19)\\nWe seek a Taylor series expansion of f at x0 = 0, which is the Maclaurin\\nseries expansion of f. We obtain the following derivatives:\\nf(0) = sin(0) + cos(0) = 1 (5.20)\\nf ′(0) = cos(0) − sin(0) = 1 (5.21)\\nf ′′(0) = − sin(0) − cos(0) = −1 (5.22)\\nf (3)(0) = − cos(0) + sin(0) = −1 (5.23)\\nf (4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\\n...\\nWe can see a pattern here: The coefficients in our Taylor series are only\\n±1 (since sin(0) = 0), each of which occurs twice before switching to the\\nother one. Furthermore, f (k+4)(0) = f (k)(0).\\nTherefore, the full Taylor series expansion of f at x0 = 0 is given by\\nT∞(x) =\\n∞X\\nk=0\\nf (k)(x0)\\nk! (x − x0)k (5.25a)\\n= 1 + x − 1\\n2! x2 − 1\\n3! x3 + 1\\n4! x4 + 1\\n5! x5 − · · · (5.25b)\\n= 1 − 1\\n2! x2 + 1\\n4! x4 ∓ · · · + x − 1\\n3! x3 + 1\\n5! x5 ∓ · · · (5.25c)\\n=\\n∞X\\nk=0\\n(−1)k 1\\n(2k)! x2k +\\n∞X\\nk=0\\n(−1)k 1\\n(2k + 1)!x2k+1 (5.25d)\\n= cos(x) + sin(x) , (5.25e)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed3216d5-b627-4308-b289-2f72df46649f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 150, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1 Differentiation of Univariate Functions 145\\nwhere we used the power series representations power series\\nrepresentation\\ncos(x) =\\n∞X\\nk=0\\n(−1)k 1\\n(2k)! x2k , (5.26)\\nsin(x) =\\n∞X\\nk=0\\n(−1)k 1\\n(2k + 1)!x2k+1 . (5.27)\\nFigure 5.4 shows the corresponding first Taylor polynomials Tn for n =\\n0, 1, 5, 10.\\nRemark. A Taylor series is a special case of a power series\\nf(x) =\\n∞X\\nk=0\\nak(x − c)k (5.28)\\nwhere ak are coefficients and c is a constant, which has the special form\\nin Definition 5.4. ♢\\n5.1.2 Differentiation Rules\\nIn the following, we briefly state basic differentiation rules, where we\\ndenote the derivative of f by f ′.\\nProduct rule: (f(x)g(x))′ = f ′(x)g(x) + f(x)g′(x) (5.29)\\nQuotient rule:\\n\\x12 f(x)\\ng(x)\\n\\x13′\\n= f ′(x)g(x) − f(x)g′(x)\\n(g(x))2 (5.30)\\nSum rule: (f(x) + g(x))′ = f ′(x) + g′(x) (5.31)\\nChain rule:\\n\\x00\\ng(f(x))\\n\\x01′\\n= (g ◦ f)′(x) = g′(f(x))f ′(x) (5.32)\\nHere, g ◦ f denotes function composition x 7→ f(x) 7→ g(f(x)).\\nExample 5.5 (Chain Rule)\\nLet us compute the derivative of the function h(x) = (2x + 1)4 using the\\nchain rule. With\\nh(x) = (2x + 1)4 = g(f(x)) , (5.33)\\nf(x) = 2x + 1 , (5.34)\\ng(f) = f 4 , (5.35)\\nwe obtain the derivatives of f and g as\\nf ′(x) = 2 , (5.36)\\ng′(f) = 4f 3 , (5.37)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='adc0daa2-4685-43f2-8ad1-a4d871082d5c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 151, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='146 Vector Calculus\\nsuch that the derivative of h is given as\\nh′(x) = g′(f)f ′(x) = (4f 3) · 2\\n(5.34)\\n= 4(2 x + 1)3 · 2 = 8(2x + 1)3 , (5.38)\\nwhere we used the chain rule (5.32) and substituted the definition of f\\nin (5.34) in g′(f).\\n5.2 Partial Differentiation and Gradients\\nDifferentiation as discussed in Section 5.1 applies to functions f of a\\nscalar variable x ∈ R. In the following, we consider the general case\\nwhere the function f depends on one or more variables x ∈ Rn, e.g.,\\nf(x) = f(x1, x2). The generalization of the derivative to functions of sev-\\neral variables is the gradient.\\nWe find the gradient of the function f with respect to x by varying one\\nvariable at a time and keeping the others constant. The gradient is then\\nthe collection of these partial derivatives.\\nDefinition 5.5 (Partial Derivative). For a function f : Rn → R, x 7→\\nf(x), x ∈ Rn of n variables x1, . . . , xn we define the partial derivatives aspartial derivative\\n∂f\\n∂x1\\n= lim\\nh→0\\nf(x1 + h, x2, . . . , xn) − f(x)\\nh\\n...\\n∂f\\n∂xn\\n= lim\\nh→0\\nf(x1, . . . , xn−1, xn + h) − f(x)\\nh\\n(5.39)\\nand collect them in the row vector\\n∇xf = gradf = df\\ndx =\\n\\x14 ∂f (x)\\n∂x1\\n∂f (x)\\n∂x2\\n· · · ∂f (x)\\n∂xn\\n\\x15\\n∈ R1×n , (5.40)\\nwhere n is the number of variables and 1 is the dimension of the image/\\nrange/codomain of f. Here, we defined the column vectorx = [x1, . . . , xn]⊤\\n∈ Rn. The row vector in (5.40) is called the gradient of f or the Jacobiangradient\\nJacobian and is the generalization of the derivative from Section 5.1.\\nRemark. This definition of the Jacobian is a special case of the general\\ndefinition of the Jacobian for vector-valued functions as the collection of\\npartial derivatives. We will get back to this in Section 5.3. ♢We can use results\\nfrom scalar\\ndifferentiation: Each\\npartial derivative is\\na derivative with\\nrespect to a scalar.\\nExample 5.6 (Partial Derivatives Using the Chain Rule)\\nFor f(x, y) = (x + 2y3)2, we obtain the partial derivatives\\n∂f (x, y)\\n∂x = 2(x + 2y3) ∂\\n∂x (x + 2y3) = 2(x + 2y3) , (5.41)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e9876fbe-b8a8-424e-b1f5-f743ebb94108', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 152, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Partial Differentiation and Gradients 147\\n∂f (x, y)\\n∂y = 2(x + 2y3) ∂\\n∂y (x + 2y3) = 12(x + 2y3)y2 . (5.42)\\nwhere we used the chain rule (5.32) to compute the partial derivatives.\\nRemark (Gradient as a Row Vector). It is not uncommon in the literature\\nto define the gradient vector as a column vector, following the conven-\\ntion that vectors are generally column vectors. The reason why we define\\nthe gradient vector as a row vector is twofold: First, we can consistently\\ngeneralize the gradient to vector-valued functions f : Rn → Rm (then\\nthe gradient becomes a matrix). Second, we can immediately apply the\\nmulti-variate chain rule without paying attention to the dimension of the\\ngradient. We will discuss both points in Section 5.3. ♢\\nExample 5.7 (Gradient)\\nFor f(x1, x2) = x2\\n1x2 + x1x3\\n2 ∈ R, the partial derivatives (i.e., the deriva-\\ntives of f with respect to x1 and x2) are\\n∂f (x1, x2)\\n∂x1\\n= 2x1x2 + x3\\n2 (5.43)\\n∂f (x1, x2)\\n∂x2\\n= x2\\n1 + 3x1x2\\n2 (5.44)\\nand the gradient is then\\ndf\\ndx =\\n\\x14 ∂f (x1, x2)\\n∂x1\\n∂f (x1, x2)\\n∂x2\\n\\x15\\n=\\n\\x02\\n2x1x2 + x3\\n2 x2\\n1 + 3x1x2\\n2\\n\\x03\\n∈ R1×2 .\\n(5.45)\\n5.2.1 Basic Rules of Partial Differentiation\\nProduct rule:\\n(f g)′ = f ′g + f g′,\\nSum rule:\\n(f + g)′ = f ′ + g′,\\nChain rule:\\n(g(f))′ = g′(f)f ′\\nIn the multivariate case, wherex ∈ Rn, the basic differentiation rules that\\nwe know from school (e.g., sum rule, product rule, chain rule; see also\\nSection 5.1.2) still apply . However, when we compute derivatives with re-\\nspect to vectors x ∈ Rn we need to pay attention: Our gradients now\\ninvolve vectors and matrices, and matrix multiplication is not commuta-\\ntive (Section 2.2.1), i.e., the order matters.\\nHere are the general product rule, sum rule, and chain rule:\\nProduct rule: ∂\\n∂x\\n\\x00\\nf(x)g(x)\\n\\x01\\n= ∂f\\n∂x g(x) + f(x) ∂g\\n∂x (5.46)\\nSum rule: ∂\\n∂x\\n\\x00\\nf(x) + g(x)\\n\\x01\\n= ∂f\\n∂x + ∂g\\n∂x (5.47)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c38b2b18-4944-4037-a6e9-204f541c1a29', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 153, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='148 Vector Calculus\\nChain rule: ∂\\n∂x(g ◦ f)(x) = ∂\\n∂x\\n\\x00\\ng(f(x))\\n\\x01\\n= ∂g\\n∂f\\n∂f\\n∂x (5.48)\\nLet us have a closer look at the chain rule. The chain rule (5.48) resem-This is only an\\nintuition, but not\\nmathematically\\ncorrect since the\\npartial derivative is\\nnot a fraction.\\nbles to some degree the rules for matrix multiplication where we said that\\nneighboring dimensions have to match for matrix multiplication to be de-\\nfined; see Section 2.2.1. If we go from left to right, the chain rule exhibits\\nsimilar properties: ∂f shows up in the “denominator” of the first factor\\nand in the “numerator” of the second factor. If we multiply the factors to-\\ngether, multiplication is defined, i.e., the dimensions of∂f match, and ∂f\\n“cancels”, such that∂g/∂ x remains.\\n5.2.2 Chain Rule\\nConsider a function f : R2 → R of two variables x1, x2. Furthermore,\\nx1(t) and x2(t) are themselves functions of t. To compute the gradient of\\nf with respect to t, we need to apply the chain rule (5.48) for multivariate\\nfunctions as\\ndf\\ndt =\\nh\\n∂f\\n∂x1\\n∂f\\n∂x2\\ni\"\\n∂x1(t)\\n∂t∂x2(t)\\n∂t\\n#\\n= ∂f\\n∂x1\\n∂x1\\n∂t + ∂f\\n∂x2\\n∂x2\\n∂t , (5.49)\\nwhere d denotes the gradient and ∂ partial derivatives.\\nExample 5.8\\nConsider f(x1, x2) = x2\\n1 + 2x2, where x1 = sin t and x2 = cos t, then\\ndf\\ndt = ∂f\\n∂x1\\n∂x1\\n∂t + ∂f\\n∂x2\\n∂x2\\n∂t (5.50a)\\n= 2 sint ∂ sin t\\n∂t + 2∂ cos t\\n∂t (5.50b)\\n= 2 sint cos t − 2 sint = 2 sint(cos t − 1) (5.50c)\\nis the corresponding derivative of f with respect to t.\\nIf f(x1, x2) is a function of x1 and x2, where x1(s, t) and x2(s, t) are\\nthemselves functions of two variables s and t, the chain rule yields the\\npartial derivatives\\n∂f\\n∂s = ∂f\\n∂x1\\n∂x1\\n∂s + ∂f\\n∂x2\\n∂x2\\n∂s , (5.51)\\n∂f\\n∂t = ∂f\\n∂x1\\n∂x1\\n∂t + ∂f\\n∂x2\\n∂x2\\n∂t , (5.52)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a793eb6-f3aa-4256-974c-445c737dddd7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 154, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.3 Gradients of Vector-Valued Functions 149\\nand the gradient is obtained by the matrix multiplication\\ndf\\nd(s, t) = ∂f\\n∂x\\n∂x\\n∂(s, t) =\\nh ∂f\\n∂x1\\n∂f\\n∂x2\\ni\\n| {z }\\n=\\n∂f\\n∂x\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂x1\\n∂s\\n∂x1\\n∂t\\n∂x2\\n∂s\\n∂x2\\n∂t\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n| {z }\\n=\\n∂x\\n∂(s, t)\\n. (5.53)\\nThis compact way of writing the chain rule as a matrix multiplication only The chain rule can\\nbe written as a\\nmatrix\\nmultiplication.\\nmakes sense if the gradient is defined as a row vector. Otherwise, we will\\nneed to start transposing gradients for the matrix dimensions to match.\\nThis may still be straightforward as long as the gradient is a vector or a\\nmatrix; however, when the gradient becomes a tensor (we will discuss this\\nin the following), the transpose is no longer a triviality .\\nRemark (Verifying the Correctness of a Gradient Implementation) . The\\ndefinition of the partial derivatives as the limit of the corresponding dif-\\nference quotient (see (5.39)) can be exploited when numerically checking\\nthe correctness of gradients in computer programs: When we compute Gradient checking\\ngradients and implement them, we can use finite differences to numer-\\nically test our computation and implementation: We choose the value h\\nto be small (e.g., h = 10−4) and compare the finite-difference approxima-\\ntion from (5.39) with our (analytic) implementation of the gradient. If the\\nerror is small, our gradient implementation is probably correct. “Small”\\ncould mean that\\nq P\\ni(dhi−d fi)2\\nP\\ni(dhi+d fi)2 < 10−6, where dhi is the finite-difference\\napproximation and d fi is the analytic gradient of f with respect to the ith\\nvariable xi. ♢\\n5.3 Gradients of Vector-Valued Functions\\nThus far, we discussed partial derivatives and gradients of functions f :\\nRn → R mapping to the real numbers. In the following, we will generalize\\nthe concept of the gradient to vector-valued functions (vector fields) f :\\nRn → Rm, where n ⩾ 1 and m > 1.\\nFor a function f : Rn → Rm and a vector x = [x1, . . . , xn]⊤ ∈ Rn, the\\ncorresponding vector of function values is given as\\nf(x) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nf1(x)\\n...\\nfm(x)\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ Rm . (5.54)\\nWriting the vector-valued function in this way allows us to view a vector-\\nvalued function f : Rn → Rm as a vector of functions [f1, . . . , fm]⊤,\\nfi : Rn → R that map onto R. The differentiation rules for every fi are\\nexactly the ones we discussed in Section 5.2.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c516c88-06b6-41c2-ad2b-2ff884ab780c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 155, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='150 Vector Calculus\\nTherefore, the partial derivative of a vector-valued function f : Rn →\\nRm with respect to xi ∈ R, i = 1, . . . n, is given as the vector\\n∂f\\n∂xi\\n=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂f1\\n∂xi\\n...\\n∂fm\\n∂xi\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nlimh→0\\nf1(x1,...,xi−1,xi+h,xi+1,...xn)−f1(x)\\nh\\n...\\nlimh→0\\nfm(x1,...,xi−1,xi+h,xi+1,...xn)−fm(x)\\nh\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ Rm .\\n(5.55)\\nFrom (5.40), we know that the gradient of f with respect to a vector is\\nthe row vector of the partial derivatives. In (5.55), every partial derivative\\n∂f /∂xi is itself a column vector. Therefore, we obtain the gradient of f :\\nRn → Rm with respect to x ∈ Rn by collecting these partial derivatives:\\ndf(x)\\ndx = ∂f(x)\\n∂x1\\n· · · ∂f(x)\\n∂xn\\n\\x14 \\x15\\n(5.56a)\\n=\\n∂f1(x)\\n∂x1\\n· · · ∂f1(x)\\n∂xn\\n... ...\\n∂fm(x)\\n∂x1\\n· · · ∂fm(x)\\n∂xn\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ Rm×n . (5.56b)\\nDefinition 5.6 (Jacobian). The collection of all first-order partial deriva-\\ntives of a vector-valued function f : Rn → Rm is called the Jacobian. TheJacobian\\nJacobian J is an m × n matrix, which we define and arrange as follows:The gradient of a\\nfunction\\nf : Rn → Rm is a\\nmatrix of size\\nm × n.\\nJ = ∇xf = df(x)\\ndx =\\n\\x14 ∂f(x)\\n∂x1\\n· · · ∂f(x)\\n∂xn\\n\\x15\\n(5.57)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f1(x)\\n∂x1\\n· · · ∂f1(x)\\n∂xn\\n... ...\\n∂fm(x)\\n∂x1\\n· · · ∂fm(x)\\n∂xn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, (5.58)\\nx =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nx1\\n...\\nxn\\n\\uf8f9\\n\\uf8fa\\uf8fb , J (i, j) = ∂fi\\n∂xj\\n. (5.59)\\nAs a special case of (5.58), a function f : Rn → R1, which maps a\\nvector x ∈ Rn onto a scalar (e.g., f(x) =Pn\\ni=1 xi), possesses a Jacobian\\nthat is a row vector (matrix of dimension 1 × n); see (5.40).\\nRemark. In this book, we use the numerator layout of the derivative, i.e.,numerator layout\\nthe derivative df /dx of f ∈ Rm with respect to x ∈ Rn is an m ×\\nn matrix, where the elements of f define the rows and the elements of\\nx define the columns of the corresponding Jacobian; see (5.58). There\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='29642613-e390-4ed1-8e77-fd23a1b6ddf5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 156, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.3 Gradients of Vector-Valued Functions 151\\nFigure 5.5 The\\ndeterminant of the\\nJacobian of f can\\nbe used to compute\\nthe magnifier\\nbetween the blue\\nand orange area.\\nb1\\nb2 c1 c2\\nf(·)\\nexists also thedenominator layout, which is the transpose of the numerator denominator layout\\nlayout. In this book, we will use the numerator layout. ♢\\nWe will see how the Jacobian is used in the change-of-variable method\\nfor probability distributions in Section 6.7. The amount of scaling due to\\nthe transformation of a variable is provided by the determinant.\\nIn Section 4.1, we saw that the determinant can be used to compute\\nthe area of a parallelogram. If we are given two vectors b1 = [1 , 0]⊤,\\nb2 = [0, 1]⊤ as the sides of the unit square (blue; see Figure 5.5), the area\\nof this square is\\n\\x0c\\x0c\\x0c\\x0cdet\\n\\x12\\x141 0\\n0 1\\n\\x15\\x13\\x0c\\x0c\\x0c\\x0c = 1 . (5.60)\\nIf we take a parallelogram with the sides c1 = [ −2, 1]⊤, c2 = [1 , 1]⊤\\n(orange in Figure 5.5), its area is given as the absolute value of the deter-\\nminant (see Section 4.1)\\n\\x0c\\x0c\\x0c\\x0cdet\\n\\x12\\x14−2 1\\n1 1\\n\\x15\\x13\\x0c\\x0c\\x0c\\x0c = | − 3| = 3 , (5.61)\\ni.e., the area of this is exactly three times the area of the unit square.\\nWe can find this scaling factor by finding a mapping that transforms the\\nunit square into the other square. In linear algebra terms, we effectively\\nperform a variable transformation from (b1, b2) to (c1, c2). In our case,\\nthe mapping is linear and the absolute value of the determinant of this\\nmapping gives us exactly the scaling factor we are looking for.\\nWe will describe two approaches to identify this mapping. First, we ex-\\nploit that the mapping is linear so that we can use the tools from Chapter 2\\nto identify this mapping. Second, we will find the mapping using partial\\nderivatives using the tools we have been discussing in this chapter.\\nApproach 1 To get started with the linear algebra approach, we\\nidentify both {b1, b2} and {c1, c2} as bases of R2 (see Section 2.6.1 for a\\nrecap). What we effectively perform is a change of basis from (b1, b2) to\\n(c1, c2), and we are looking for the transformation matrix that implements\\nthe basis change. Using results from Section 2.7.2, we identify the desired\\nbasis change matrix as\\nJ =\\n\\x14−2 1\\n1 1\\n\\x15\\n, (5.62)\\nsuch that J b1 = c1 and J b2 = c2. The absolute value of the determi-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2dc2a06c-2043-41cd-a7de-b1388a826a15', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 157, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='152 Vector Calculus\\nnant of J, which yields the scaling factor we are looking for, is given as\\n|det(J)| = 3, i.e., the area of the square spanned by(c1, c2) is three times\\ngreater than the area spanned by (b1, b2).\\nApproach 2 The linear algebra approach works for linear trans-\\nformations; for nonlinear transformations (which become relevant in Sec-\\ntion 6.7), we follow a more general approach using partial derivatives.\\nFor this approach, we consider a function f : R2 → R2 that performs a\\nvariable transformation. In our example, f maps the coordinate represen-\\ntation of any vector x ∈ R2 with respect to (b1, b2) onto the coordinate\\nrepresentation y ∈ R2 with respect to (c1, c2). We want to identify the\\nmapping so that we can compute how an area (or volume) changes when\\nit is being transformed by f. For this, we need to find out how f(x)\\nchanges if we modify x a bit. This question is exactly answered by the\\nJacobian matrix df\\ndx ∈ R2×2. Since we can write\\ny1 = −2x1 + x2 (5.63)\\ny2 = x1 + x2 (5.64)\\nwe obtain the functional relationship between x and y, which allows us\\nto get the partial derivatives\\n∂y1\\n∂x1\\n= −2 , ∂y1\\n∂x2\\n= 1 , ∂y2\\n∂x1\\n= 1 , ∂y2\\n∂x2\\n= 1 (5.65)\\nand compose the Jacobian as\\nJ =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂y1\\n∂x1\\n∂y1\\n∂x2\\n∂y2\\n∂x1\\n∂y2\\n∂x2\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\x14−2 1\\n1 1\\n\\x15\\n. (5.66)\\nThe Jacobian represents the coordinate transformation we are lookingGeometrically , the\\nJacobian\\ndeterminant gives\\nthe magnification/\\nscaling factor when\\nwe transform an\\narea or volume.\\nfor. It is exact if the coordinate transformation is linear (as in our case),\\nand (5.66) recovers exactly the basis change matrix in (5.62). If the co-\\nordinate transformation is nonlinear, the Jacobian approximates this non-\\nlinear transformation locally with a linear one. The absolute value of the\\nJacobian determinant |det(J)| is the factor by which areas or volumes are\\nJacobian\\ndeterminant\\nscaled when coordinates are transformed. Our case yields |det(J)| = 3.\\nThe Jacobian determinant and variable transformations will become\\nrelevant in Section 6.7 when we transform random variables and prob-\\nability distributions. These transformations are extremely relevant in ma-Figure 5.6\\nDimensionality of\\n(partial) derivatives.\\nf (x)\\nx\\n∂f\\n∂x\\nchine learning in the context of training deep neural networks using the\\nreparametrization trick, also called infinite perturbation analysis.\\nIn this chapter, we encountered derivatives of functions. Figure 5.6 sum-\\nmarizes the dimensions of those derivatives. If f : R → R the gradient is\\nsimply a scalar (top-left entry). For f : RD → R the gradient is a 1 × D\\nrow vector (top-right entry). For f : R → RE, the gradient is an E × 1\\ncolumn vector, and for f : RD → RE the gradient is an E × D matrix.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='29077bf5-6bac-4106-8ac0-fafc5bec92b7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 158, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.3 Gradients of Vector-Valued Functions 153\\nExample 5.9 (Gradient of a Vector-Valued Function)\\nWe are given\\nf(x) = Ax , f(x) ∈ RM , A ∈ RM ×N , x ∈ RN .\\nTo compute the gradient df /dx we first determine the dimension of\\ndf /dx: Since f : RN → RM, it follows that df /dx ∈ RM ×N. Second,\\nto compute the gradient we determine the partial derivatives of f with\\nrespect to every xj:\\nfi(x) =\\nNX\\nj=1\\nAijxj =⇒ ∂fi\\n∂xj\\n= Aij (5.67)\\nWe collect the partial derivatives in the Jacobian and obtain the gradient\\ndf\\ndx =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂f1\\n∂x1\\n· · · ∂f1\\n∂xN\\n... ...\\n∂fM\\n∂x1\\n· · · ∂fM\\n∂xN\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nA11 · · · A1N\\n... ...\\nAM1 · · · AM N\\n\\uf8f9\\n\\uf8fa\\uf8fb = A ∈ RM ×N . (5.68)\\nExample 5.10 (Chain Rule)\\nConsider the function h : R → R, h(t) = (f ◦ g)(t) with\\nf : R2 → R (5.69)\\ng : R → R2 (5.70)\\nf(x) = exp(x1x2\\n2) , (5.71)\\nx =\\n\\x14x1\\nx2\\n\\x15\\n= g(t) =\\n\\x14t cos t\\nt sin t\\n\\x15\\n(5.72)\\nand compute the gradient of h with respect to t. Since f : R2 → R and\\ng : R → R2 we note that\\n∂f\\n∂x ∈ R1×2 , ∂g\\n∂t ∈ R2×1 . (5.73)\\nThe desired gradient is computed by applying the chain rule:\\ndh\\ndt = ∂f\\n∂x\\n∂x\\n∂t =\\n\\x14 ∂f\\n∂x1\\n∂f\\n∂x2\\n\\x15\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂x1\\n∂t∂x2\\n∂t\\n\\uf8f9\\n\\uf8fa\\uf8fb (5.74a)\\n=\\n\\x02\\nexp(x1x2\\n2)x2\\n2 2 exp(x1x2\\n2)x1x2\\n\\x03\\x14cos t − t sin t\\nsin t + t cos t\\n\\x15\\n(5.74b)\\n= exp(x1x2\\n2)\\n\\x00\\nx2\\n2(cos t − t sin t) + 2x1x2(sin t + t cos t)\\n\\x01\\n, (5.74c)\\nwhere x1 = t cos t and x2 = t sin t; see (5.72).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4f376bb-4da3-4c48-901b-cb7401980eba', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 159, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='154 Vector Calculus\\nExample 5.11 (Gradient of a Least-Squares Loss in a Linear Model)\\nLet us consider the linear modelWe will discuss this\\nmodel in much\\nmore detail in\\nChapter 9 in the\\ncontext of linear\\nregression, where\\nwe need derivatives\\nof the least-squares\\nloss L with respect\\nto the parameters θ.\\ny = Φθ , (5.75)\\nwhere θ ∈ RD is a parameter vector, Φ ∈ RN ×D are input features and\\ny ∈ RN are the corresponding observations. We define the functions\\nL(e) := ∥e∥2 , (5.76)\\ne(θ) := y − Φθ . (5.77)\\nWe seek ∂L\\n∂θ , and we will use the chain rule for this purpose. L is called a\\nleast-squares loss function.least-squares loss\\nBefore we start our calculation, we determine the dimensionality of the\\ngradient as\\n∂L\\n∂θ ∈ R1×D . (5.78)\\nThe chain rule allows us to compute the gradient as\\n∂L\\n∂θ = ∂L\\n∂e\\n∂e\\n∂θ , (5.79)\\nwhere the dth element is given bydLdtheta =\\nnp.einsum(\\n’n,nd’,\\ndLde,dedtheta) ∂L\\n∂θ [1, d] =\\nNX\\nn=1\\n∂L\\n∂e [n] ∂e\\n∂θ [n, d] . (5.80)\\nWe know that ∥e∥2 = e⊤e (see Section 3.2) and determine\\n∂L\\n∂e = 2e⊤ ∈ R1×N . (5.81)\\nFurthermore, we obtain\\n∂e\\n∂θ = −Φ ∈ RN ×D , (5.82)\\nsuch that our desired derivative is\\n∂L\\n∂θ = −2e⊤Φ\\n(5.77)\\n= − 2(y⊤ − θ⊤Φ⊤)| {z }\\n1×N\\nΦ|{z}\\nN ×D\\n∈ R1×D . (5.83)\\nRemark. We would have obtained the same result without using the chain\\nrule by immediately looking at the function\\nL2(θ) := ∥y − Φθ∥2 = (y − Φθ)⊤(y − Φθ) . (5.84)\\nThis approach is still practical for simple functions like L2 but becomes\\nimpractical for deep function compositions. ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b56577e2-7047-49fd-9140-96e93084d96a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 160, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4 Gradients of Matrices 155\\nFigure 5.7\\nVisualization of\\ngradient\\ncomputation of a\\nmatrix with respect\\nto a vector. We are\\ninterested in\\ncomputing the\\ngradient of\\nA ∈ R4×2 with\\nrespect to a vector\\nx ∈ R3. We know\\nthat gradient\\ndA\\ndx ∈ R4×2×3. We\\nfollow two\\nequivalent\\napproaches to arrive\\nthere: (a) collating\\npartial derivatives\\ninto a Jacobian\\ntensor;\\n(b) flattening of the\\nmatrix into a vector,\\ncomputing the\\nJacobian matrix,\\nre-shaping into a\\nJacobian tensor.\\nA∈R4×2 x∈R3\\n∂A\\n∂x1\\n∈R4×2\\n∂A\\n∂x2\\n∈R4×2\\n∂A\\n∂x3\\n∈R4×2\\nx1\\nx2\\nx3\\ndA\\ndx ∈R4×2×3\\n4\\n2\\n3\\nPartial derivatives:\\ncollate\\n(a) Approach 1: We compute the partial derivative\\n∂A\\n∂x1\\n, ∂A\\n∂x2\\n, ∂A\\n∂x3\\n, each of which is a 4 × 2 matrix, and col-\\nlate them in a 4 × 2 × 3 tensor.\\nA∈R4×2 x∈R3\\nx1\\nx2\\nx3\\ndA\\ndx ∈R4×2×3\\nre-shape re-shapegradient\\nA∈R4×2 ˜A∈R8\\nd˜A\\ndx ∈R8×3\\n(b) Approach 2: We re-shape (flatten)A ∈ R4×2 into a vec-\\ntor ˜A ∈ R8. Then, we compute the gradient d ˜A\\ndx ∈ R8×3.\\nWe obtain the gradient tensor by re-shaping this gradient as\\nillustrated above.\\n5.4 Gradients of Matrices We can think of a\\ntensor as a\\nmultidimensional\\narray .\\nWe will encounter situations where we need to take gradients of matrices\\nwith respect to vectors (or other matrices), which results in a multidimen-\\nsional tensor. We can think of this tensor as a multidimensional array that\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0bc9d358-44b6-4287-90d6-452448285b04', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 161, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='156 Vector Calculus\\ncollects partial derivatives. For example, if we compute the gradient of an\\nm × n matrix A with respect to a p × q matrix B, the resulting Jacobian\\nwould be (m×n)×(p×q), i.e., a four-dimensional tensorJ, whose entries\\nare given as Jijkl = ∂Aij/∂Bkl.\\nSince matrices represent linear mappings, we can exploit the fact that\\nthere is a vector-space isomorphism (linear, invertible mapping) between\\nthe space Rm×n of m × n matrices and the space Rmn of mn vectors.\\nTherefore, we can re-shape our matrices into vectors of lengths mn and\\npq, respectively . The gradient using thesemn vectors results in a Jacobian\\nof size mn × pq. Figure 5.7 visualizes both approaches. In practical ap-Matrices can be\\ntransformed into\\nvectors by stacking\\nthe columns of the\\nmatrix\\n(“flattening”).\\nplications, it is often desirable to re-shape the matrix into a vector and\\ncontinue working with this Jacobian matrix: The chain rule (5.48) boils\\ndown to simple matrix multiplication, whereas in the case of a Jacobian\\ntensor, we will need to pay more attention to what dimensions we need\\nto sum out.\\nExample 5.12 (Gradient of Vectors with Respect to Matrices)\\nLet us consider the following example, where\\nf = Ax , f ∈ RM , A ∈ RM ×N , x ∈ RN (5.85)\\nand where we seek the gradientdf /dA. Let us start again by determining\\nthe dimension of the gradient as\\ndf\\ndA ∈ RM ×(M ×N) . (5.86)\\nBy definition, the gradient is the collection of the partial derivatives:\\ndf\\ndA =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂f1\\n∂A\\n...\\n∂fM\\n∂A\\n\\uf8f9\\n\\uf8fa\\uf8fb , ∂fi\\n∂A ∈ R1×(M ×N) . (5.87)\\nTo compute the partial derivatives, it will be helpful to explicitly write out\\nthe matrix vector multiplication:\\nfi =\\nNX\\nj=1\\nAijxj, i = 1, . . . , M , (5.88)\\nand the partial derivatives are then given as\\n∂fi\\n∂Aiq\\n= xq . (5.89)\\nThis allows us to compute the partial derivatives of fi with respect to a\\nrow of A, which is given as\\n∂fi\\n∂Ai,:\\n= x⊤ ∈ R1×1×N , (5.90)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02986d6d-d55b-4607-ac04-8ca2422405ad', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 162, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4 Gradients of Matrices 157\\n∂fi\\n∂Ak̸=i,:\\n= 0⊤ ∈ R1×1×N (5.91)\\nwhere we have to pay attention to the correct dimensionality . Since fi\\nmaps onto R and each row of A is of size 1 × N, we obtain a 1 × 1 × N-\\nsized tensor as the partial derivative of fi with respect to a row of A.\\nWe stack the partial derivatives (5.91) and get the desired gradient\\nin (5.87) via\\n∂fi\\n∂A =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0⊤\\n...\\n0⊤\\nx⊤\\n0⊤\\n...\\n0⊤\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ R1×(M ×N) . (5.92)\\nExample 5.13 (Gradient of Matrices with Respect to Matrices)\\nConsider a matrix R ∈ RM ×N and f : RM ×N → RN ×N with\\nf(R) = R⊤R =: K ∈ RN ×N , (5.93)\\nwhere we seek the gradient dK/dR.\\nTo solve this hard problem, let us first write down what we already\\nknow: The gradient has the dimensions\\ndK\\ndR ∈ R(N ×N)×(M ×N) , (5.94)\\nwhich is a tensor. Moreover,\\ndKpq\\ndR ∈ R1×M ×N (5.95)\\nfor p, q = 1 , . . . , N, where Kpq is the (p, q)th entry of K = f(R). De-\\nnoting the ith column of R by ri, every entry of K is given by the dot\\nproduct of two columns of R, i.e.,\\nKpq = r⊤\\np rq =\\nMX\\nm=1\\nRmpRmq . (5.96)\\nWhen we now compute the partial derivative ∂Kpq\\n∂Rij\\nwe obtain\\n∂Kpq\\n∂Rij\\n=\\nMX\\nm=1\\n∂\\n∂Rij\\nRmpRmq = ∂pqij , (5.97)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='84b12acb-585a-476d-b411-5d8f607846dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 163, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='158 Vector Calculus\\n∂pqij =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nRiq if j = p, p ̸= q\\nRip if j = q, p ̸= q\\n2Riq if j = p, p = q\\n0 otherwise\\n. (5.98)\\nFrom (5.94), we know that the desired gradient has the dimension (N ×\\nN) × (M × N), and every single entry of this tensor is given by ∂pqij\\nin (5.98), where p, q, j = 1, . . . , N and i = 1, . . . , M.\\n5.5 Useful Identities for Computing Gradients\\nIn the following, we list some useful gradients that are frequently required\\nin a machine learning context (Petersen and Pedersen, 2012). Here, we\\nuse tr(·) as the trace (see Definition 4.4), det(·) as the determinant (see\\nSection 4.1) and f(X)−1 as the inverse of f(X), assuming it exists.\\n∂\\n∂X f(X)⊤ =\\n\\x12 ∂f(X)\\n∂X\\n\\x13⊤\\n(5.99)\\n∂\\n∂X tr(f(X)) = tr\\n\\x12 ∂f(X)\\n∂X\\n\\x13\\n(5.100)\\n∂\\n∂X det(f(X)) = det(f(X))tr\\n\\x12\\nf(X)−1 ∂f(X)\\n∂X\\n\\x13\\n(5.101)\\n∂\\n∂X f(X)−1 = −f(X)−1 ∂f(X)\\n∂X f(X)−1 (5.102)\\n∂a⊤X −1b\\n∂X = −(X −1)⊤ab⊤(X −1)⊤ (5.103)\\n∂x⊤a\\n∂x = a⊤ (5.104)\\n∂a⊤x\\n∂x = a⊤ (5.105)\\n∂a⊤Xb\\n∂X = ab⊤ (5.106)\\n∂x⊤Bx\\n∂x = x⊤(B + B⊤) (5.107)\\n∂\\n∂s(x − As)⊤W (x − As) = −2(x − As)⊤W A for symmetric W\\n(5.108)\\nRemark. In this book, we only cover traces and transposes of matrices.\\nHowever, we have seen that derivatives can be higher-dimensional ten-\\nsors, in which case the usual trace and transpose are not defined. In these\\ncases, the trace of aD ×D ×E ×F tensor would be an E ×F -dimensional\\nmatrix. This is a special case of a tensor contraction. Similarly , when we\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60b0824b-3d14-4550-aa8d-8dd12b8aaf69', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 164, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.6 Backpropagation and Automatic Differentiation 159\\n“transpose” a tensor, we mean swapping the first two dimensions. Specif-\\nically , in (5.99) through (5.102), we require tensor-related computations\\nwhen we work with multivariate functions f(·) and compute derivatives\\nwith respect to matrices (and choose not to vectorize them as discussed in\\nSection 5.4). ♢\\n5.6 Backpropagation and Automatic Differentiation\\nA good discussion\\nabout\\nbackpropagation\\nand the chain rule is\\navailable at a blog\\nby Tim Vieira at\\nhttps://tinyurl.\\ncom/ycfm2yrw.\\nIn many machine learning applications, we find good model parameters\\nby performing gradient descent (Section 7.1), which relies on the fact\\nthat we can compute the gradient of a learning objective with respect\\nto the parameters of the model. For a given objective function, we can\\nobtain the gradient with respect to the model parameters using calculus\\nand applying the chain rule; see Section 5.2.2. We already had a taste in\\nSection 5.3 when we looked at the gradient of a squared loss with respect\\nto the parameters of a linear regression model.\\nConsider the function\\nf(x) =\\nq\\nx2 + exp(x2) + cos\\n\\x00\\nx2 + exp(x2)\\n\\x01\\n. (5.109)\\nBy application of the chain rule, and noting that differentiation is linear,\\nwe compute the gradient\\ndf\\ndx = 2x + 2x exp(x2)\\n2\\np\\nx2 + exp(x2) − sin\\n\\x00\\nx2 + exp(x2)\\n\\x01\\x00\\n2x + 2x exp(x2)\\n\\x01\\n= 2x\\n \\n1\\n2\\np\\nx2 + exp(x2) − sin\\n\\x00\\nx2 + exp(x2)\\n\\x01\\n!\\n\\x00\\n1 + exp(x2)\\n\\x01\\n.\\n(5.110)\\nWriting out the gradient in this explicit way is often impractical since it\\noften results in a very lengthy expression for a derivative. In practice,\\nit means that, if we are not careful, the implementation of the gradient\\ncould be significantly more expensive than computing the function, which\\nimposes unnecessary overhead. For training deep neural network mod-\\nels, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\\n1962; Rumelhart et al., 1986) is an efficient way to compute the gradient\\nof an error function with respect to the parameters of the model.\\n5.6.1 Gradients in a Deep Network\\nAn area where the chain rule is used to an extreme is deep learning, where\\nthe function value y is computed as a many-level function composition\\ny = (fK ◦ fK−1 ◦ · · · ◦ f1)(x) = fK(fK−1(· · ·(f1(x)) · · ·)) , (5.111)\\nwhere x are the inputs (e.g., images), y are the observations (e.g., class\\nlabels), and every function fi, i = 1, . . . , K, possesses its own parameters.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d242e0d-656c-4909-b80e-96ba4a6f4326', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 165, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='160 Vector Calculus\\nFigure 5.8 Forward\\npass in a multi-layer\\nneural network to\\ncompute the loss L\\nas a function of the\\ninputs x and the\\nparameters Ai, bi.\\nx fK\\nA0,b0 AK−1,bK−1\\nLfK−1\\nAK−2,bK−2\\nf1\\nA1,b1\\nIn neural networks with multiple layers, we have functions fi(xi−1) =We discuss the case,\\nwhere the activation\\nfunctions are\\nidentical in each\\nlayer to unclutter\\nnotation.\\nσ(Ai−1xi−1 + bi−1) in the ith layer. Here xi−1 is the output of layer i − 1\\nand σ an activation function, such as the logistic sigmoid 1\\n1+e−x , tanh or a\\nrectified linear unit (ReLU). In order to train these models, we require the\\ngradient of a loss function L with respect to all model parameters Aj, bj\\nfor j = 1, . . . , K. This also requires us to compute the gradient of L with\\nrespect to the inputs of each layer. For example, if we have inputs x and\\nobservations y and a network structure defined by\\nf 0 := x (5.112)\\nf i := σi(Ai−1f i−1 + bi−1) , i = 1, . . . , K , (5.113)\\nsee also Figure 5.8 for a visualization, we may be interested in finding\\nAj, bj for j = 0, . . . , K − 1, such that the squared loss\\nL(θ) = ∥y − f K(θ, x)∥2 (5.114)\\nis minimized, where θ = {A0, b0, . . . ,AK−1, bK−1}.\\nTo obtain the gradients with respect to the parameter set θ, we require\\nthe partial derivatives of L with respect to the parameters θj = {Aj, bj}\\nof each layer j = 0, . . . , K − 1. The chain rule allows us to determine the\\npartial derivatives asA more in-depth\\ndiscussion about\\ngradients of neural\\nnetworks can be\\nfound in Justin\\nDomke’s lecture\\nnotes\\nhttps://tinyurl.\\ncom/yalcxgtv.\\n∂L\\n∂θK−1\\n= ∂L\\n∂f K\\n∂f K\\n∂θK−1\\n(5.115)\\n∂L\\n∂θK−2\\n= ∂L\\n∂f K\\n∂f K\\n∂f K−1\\n∂f K−1\\n∂θK−2\\n(5.116)\\n∂L\\n∂θK−3\\n= ∂L\\n∂f K\\n∂f K\\n∂f K−1\\n∂f K−1\\n∂f K−2\\n∂f K−2\\n∂θK−3\\n(5.117)\\n∂L\\n∂θi\\n= ∂L\\n∂f K\\n∂f K\\n∂f K−1\\n· · · ∂f i+2\\n∂f i+1\\n∂f i+1\\n∂θi\\n(5.118)\\nThe orange terms are partial derivatives of the output of a layer with\\nrespect to its inputs, whereas the blue terms are partial derivatives of\\nthe output of a layer with respect to its parameters. Assuming, we have\\nalready computed the partial derivatives∂L/∂ θi+1, then most of the com-\\nputation can be reused to compute ∂L/∂ θi. The additional terms that we\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c09c1262-4c98-472b-a35d-803ef537f787', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 166, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.6 Backpropagation and Automatic Differentiation 161\\nFigure 5.9\\nBackward pass in a\\nmulti-layer neural\\nnetwork to compute\\nthe gradients of the\\nloss function.\\nx fK\\nA0,b0 AK−1,bK−1\\nLfK−1\\nAK−2,bK−2\\nf1\\nA1,b1\\nFigure 5.10 Simple\\ngraph illustrating\\nthe flow of data\\nfrom x to y via\\nsome intermediate\\nvariables a, b.\\nx a b y\\nneed to compute are indicated by the boxes. Figure 5.9 visualizes that the\\ngradients are passed backward through the network.\\n5.6.2 Automatic Differentiation\\nIt turns out that backpropagation is a special case of a general technique\\nin numerical analysis called automatic differentiation. We can think of au- automatic\\ndifferentiationtomatic differentation as a set of techniques to numerically (in contrast to\\nsymbolically) evaluate the exact (up to machine precision) gradient of a\\nfunction by working with intermediate variables and applying the chain\\nrule. Automatic differentiation applies a series of elementary arithmetic Automatic\\ndifferentiation is\\ndifferent from\\nsymbolic\\ndifferentiation and\\nnumerical\\napproximations of\\nthe gradient, e.g., by\\nusing finite\\ndifferences.\\noperations, e.g., addition and multiplication and elementary functions,\\ne.g., sin, cos, exp, log. By applying the chain rule to these operations, the\\ngradient of quite complicated functions can be computed automatically .\\nAutomatic differentiation applies to general computer programs and has\\nforward and reverse modes. Baydin et al. (2018) give a great overview of\\nautomatic differentiation in machine learning.\\nFigure 5.10 shows a simple graph representing the data flow from in-\\nputs x to outputs y via some intermediate variables a, b. If we were to\\ncompute the derivative dy/dx, we would apply the chain rule and obtain\\ndy\\ndx = dy\\ndb\\ndb\\nda\\nda\\ndx . (5.119)\\nIntuitively , the forward and reverse mode differ in the order of multipli- In the general case,\\nwe work with\\nJacobians, which\\ncan be vectors,\\nmatrices, or tensors.\\ncation. Due to the associativity of matrix multiplication, we can choose\\nbetween\\ndy\\ndx =\\n\\x12dy\\ndb\\ndb\\nda\\n\\x13 da\\ndx , (5.120)\\ndy\\ndx = dy\\ndb\\n\\x12 db\\nda\\nda\\ndx\\n\\x13\\n. (5.121)\\nEquation (5.120) would be the reverse mode because gradients are prop- reverse mode\\nagated backward through the graph, i.e., reverse to the data flow. Equa-\\ntion (5.121) would be the forward mode, where the gradients flow with forward mode\\nthe data from left to right through the graph.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b314863c-6685-49f8-ad88-5dadf5c74de7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 167, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='162 Vector Calculus\\nIn the following, we will focus on reverse mode automatic differentia-\\ntion, which is backpropagation. In the context of neural networks, where\\nthe input dimensionality is often much higher than the dimensionality of\\nthe labels, the reverse mode is computationally significantly cheaper than\\nthe forward mode. Let us start with an instructive example.\\nExample 5.14\\nConsider the function\\nf(x) =\\nq\\nx2 + exp(x2) + cos\\n\\x00\\nx2 + exp(x2)\\n\\x01\\n(5.122)\\nfrom (5.109). If we were to implement a function f on a computer, we\\nwould be able to save some computation by using intermediate variables:intermediate\\nvariables\\na = x2 , (5.123)\\nb = exp(a) , (5.124)\\nc = a + b , (5.125)\\nd = √c , (5.126)\\ne = cos(c) , (5.127)\\nf = d + e . (5.128)\\nFigure 5.11\\nComputation graph\\nwith inputs x,\\nfunction values f,\\nand intermediate\\nvariables a, b, c, d, e.\\nx (·)2 a\\nexp(·) b\\n+ c\\n√·\\ncos(·)\\nd\\ne\\n+ f\\nThis is the same kind of thinking process that occurs when applying\\nthe chain rule. Note that the preceding set of equations requires fewer\\noperations than a direct implementation of the function f(x) as defined\\nin (5.109). The corresponding computation graph in Figure 5.11 shows\\nthe flow of data and computations required to obtain the function value\\nf.\\nThe set of equations that include intermediate variables can be thought\\nof as a computation graph, a representation that is widely used in imple-\\nmentations of neural network software libraries. We can directly compute\\nthe derivatives of the intermediate variables with respect to their corre-\\nsponding inputs by recalling the definition of the derivative of elementary\\nfunctions. We obtain the following:\\n∂a\\n∂x = 2x (5.129)\\n∂b\\n∂a = exp(a) (5.130)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99277e9e-8034-45cd-a986-e44d551e2e78', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 168, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.6 Backpropagation and Automatic Differentiation 163\\n∂c\\n∂a = 1 = ∂c\\n∂b (5.131)\\n∂d\\n∂c = 1\\n2√c (5.132)\\n∂e\\n∂c = − sin(c) (5.133)\\n∂f\\n∂d = 1 = ∂f\\n∂e . (5.134)\\nBy looking at the computation graph in Figure 5.11, we can compute\\n∂f /∂x by working backward from the output and obtain\\n∂f\\n∂c = ∂f\\n∂d\\n∂d\\n∂c + ∂f\\n∂e\\n∂e\\n∂c (5.135)\\n∂f\\n∂b = ∂f\\n∂c\\n∂c\\n∂b (5.136)\\n∂f\\n∂a = ∂f\\n∂b\\n∂b\\n∂a + ∂f\\n∂c\\n∂c\\n∂a (5.137)\\n∂f\\n∂x = ∂f\\n∂a\\n∂a\\n∂x . (5.138)\\nNote that we implicitly applied the chain rule to obtain ∂f /∂x. By substi-\\ntuting the results of the derivatives of the elementary functions, we get\\n∂f\\n∂c = 1 · 1\\n2√c + 1 · (− sin(c)) (5.139)\\n∂f\\n∂b = ∂f\\n∂c · 1 (5.140)\\n∂f\\n∂a = ∂f\\n∂b exp(a) + ∂f\\n∂c · 1 (5.141)\\n∂f\\n∂x = ∂f\\n∂a · 2x . (5.142)\\nBy thinking of each of the derivatives above as a variable, we observe\\nthat the computation required for calculating the derivative is of similar\\ncomplexity as the computation of the function itself. This is quite counter-\\nintuitive since the mathematical expression for the derivative ∂f\\n∂x (5.110)\\nis significantly more complicated than the mathematical expression of the\\nfunction f(x) in (5.109).\\nAutomatic differentiation is a formalization of Example 5.14. Letx1, . . . , xd\\nbe the input variables to the function, xd+1, . . . , xD−1 be the intermediate\\nvariables, and xD the output variable. Then the computation graph can be\\nexpressed as follows:\\nFor i = d + 1, . . . , D : xi = gi(xPa(xi)) , (5.143)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1cffe453-23b8-459c-8770-9224aeb70e98', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 169, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='164 Vector Calculus\\nwhere the gi(·) are elementary functions and xPa(xi) are the parent nodes\\nof the variable xi in the graph. Given a function defined in this way , we\\ncan use the chain rule to compute the derivative of the function in a step-\\nby-step fashion. Recall that by definition f = xD and hence\\n∂f\\n∂xD\\n= 1 . (5.144)\\nFor other variables xi, we apply the chain rule\\n∂f\\n∂xi\\n=\\nX\\nxj:xi∈Pa(xj)\\n∂f\\n∂xj\\n∂xj\\n∂xi\\n=\\nX\\nxj:xi∈Pa(xj)\\n∂f\\n∂xj\\n∂gj\\n∂xi\\n, (5.145)\\nwhere Pa(xj) is the set of parent nodes of xj in the computation graph.\\nEquation (5.143) is the forward propagation of a function, whereas (5.145)Auto-differentiation\\nin reverse mode\\nrequires a parse\\ntree.\\nis the backpropagation of the gradient through the computation graph.\\nFor neural network training, we backpropagate the error of the prediction\\nwith respect to the label.\\nThe automatic differentiation approach above works whenever we have\\na function that can be expressed as a computation graph, where the ele-\\nmentary functions are differentiable. In fact, the function may not even be\\na mathematical function but a computer program. However, not all com-\\nputer programs can be automatically differentiated, e.g., if we cannot find\\ndifferential elementary functions. Programming structures, such as for\\nloops and if statements, require more care as well.\\n5.7 Higher-Order Derivatives\\nSo far, we have discussed gradients, i.e., first-order derivatives. Some-\\ntimes, we are interested in derivatives of higher order, e.g., when we want\\nto use Newton’s Method for optimization, which requires second-order\\nderivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed\\nthe Taylor series to approximate functions using polynomials. In the mul-\\ntivariate case, we can do exactly the same. In the following, we will do\\nexactly this. But let us start with some notation.\\nConsider a function f : R2 → R of two variables x, y. We use the\\nfollowing notation for higher-order partial derivatives (and for gradients):\\n∂2f\\n∂x2 is the second partial derivative of f with respect to x.\\n∂nf\\n∂xn is the nth partial derivative of f with respect to x.\\n∂2f\\n∂y∂x = ∂\\n∂y\\n\\x00 ∂f\\n∂x\\n\\x01\\nis the partial derivative obtained by first partial differ-\\nentiating with respect to x and then with respect to y.\\n∂2f\\n∂x∂y is the partial derivative obtained by first partial differentiating by\\ny and then x.\\nThe Hessian is the collection of all second-order partial derivatives.Hessian\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2d0ef3a-353b-4491-8efa-8b3471f1f9c1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 170, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Linearization and Multivariate Taylor Series 165\\nFigure 5.12 Linear\\napproximation of a\\nfunction. The\\noriginal function f\\nis linearized at\\nx0 = −2 using a\\nfirst-order Taylor\\nseries expansion.\\n−4 −2 0 2 4\\nx\\n−2\\n−1\\n0\\n1\\nf (x)\\nf (x)\\nf (x0) f (x0) + f′(x0)(x−x0)\\nIf f(x, y) is a twice (continuously) differentiable function, then\\n∂2f\\n∂x∂y = ∂2f\\n∂y∂x , (5.146)\\ni.e., the order of differentiation does not matter, and the corresponding\\nHessian matrix Hessian matrix\\nH =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n∂2f\\n∂x2\\n∂2f\\n∂x∂y\\n∂2f\\n∂x∂y\\n∂2f\\n∂y2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (5.147)\\nis symmetric. The Hessian is denoted as∇2\\nx,yf(x, y). Generally , forx ∈ Rn\\nand f : Rn → R, the Hessian is an n × n matrix. The Hessian measures\\nthe curvature of the function locally around (x, y).\\nRemark (Hessian of a Vector Field). If f : Rn → Rm is a vector field, the\\nHessian is an (m × n × n)-tensor. ♢\\n5.8 Linearization and Multivariate Taylor Series\\nThe gradient ∇f of a function f is often used for a locally linear approxi-\\nmation of f around x0:\\nf(x) ≈ f(x0) + (∇xf)(x0)(x − x0) . (5.148)\\nHere (∇xf)(x0) is the gradient of f with respect to x, evaluated at x0.\\nFigure 5.12 illustrates the linear approximation of a functionf at an input\\nx0. The original function is approximated by a straight line. This approx-\\nimation is locally accurate, but the farther we move away from x0 the\\nworse the approximation gets. Equation (5.148) is a special case of a mul-\\ntivariate Taylor series expansion of f at x0, where we consider only the\\nfirst two terms. We discuss the more general case in the following, which\\nwill allow for better approximations.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='79cb4476-a792-45aa-84f1-fe51d4c77f12', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 171, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='166 Vector Calculus\\nFigure 5.13\\nVisualizing outer\\nproducts. Outer\\nproducts of vectors\\nincrease the\\ndimensionality of\\nthe array by 1 per\\nterm. (a) The outer\\nproduct of two\\nvectors results in a\\nmatrix; (b) the\\nouter product of\\nthree vectors yields\\na third-order tensor.\\n(a) Given a vector δ ∈ R4, we obtain the outer product δ2 := δ ⊗ δ = δδ ⊤ ∈\\nR4×4 as a matrix.\\n(b) An outer product δ3 := δ ⊗ δ ⊗ δ ∈ R4×4×4 results in a third-order tensor (“three-\\ndimensional matrix”), i.e., an array with three indexes.\\nDefinition 5.7 (Multivariate Taylor Series). We consider a function\\nf : RD → R (5.149)\\nx 7→ f(x) , x ∈ RD , (5.150)\\nthat is smooth at x0. When we define the difference vector δ := x − x0,\\nthe multivariate Taylor series of f at (x0) is defined asmultivariate Taylor\\nseries\\nf(x) =\\n∞X\\nk=0\\nDk\\nxf(x0)\\nk! δk , (5.151)\\nwhere Dk\\nxf(x0) is the k-th (total) derivative of f with respect to x, eval-\\nuated at x0.\\nDefinition 5.8 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial\\nf at x0 contains the first n + 1 components of the series in (5.151) and is\\ndefined as\\nTn(x) =\\nnX\\nk=0\\nDk\\nxf(x0)\\nk! δk . (5.152)\\nIn (5.151) and (5.152), we used the slightly sloppy notation of δk,\\nwhich is not defined for vectors x ∈ RD, D > 1, and k > 1. Note that\\nboth Dk\\nxf and δk are k-th order tensors, i.e., k-dimensional arrays. TheA vector can be\\nimplemented as a\\none-dimensional\\narray , a matrix as a\\ntwo-dimensional\\narray .\\nkth-order tensor δk ∈ R\\nk times\\nz }| {\\nD×D×...×D is obtained as a k-fold outer product,\\ndenoted by ⊗, of the vector δ ∈ RD. For example,\\nδ2 := δ ⊗ δ = δδ ⊤ , δ2[i, j] = δ[i]δ[j] (5.153)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1df99e3-3f6e-45e9-8f8b-9ac3550e4ef4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 172, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Linearization and Multivariate Taylor Series 167\\nδ3 := δ ⊗ δ ⊗ δ , δ3[i, j, k] = δ[i]δ[j]δ[k] . (5.154)\\nFigure 5.13 visualizes two such outer products. In general, we obtain the\\nterms\\nDk\\nxf(x0)δk =\\nDX\\ni1=1\\n· · ·\\nDX\\nik=1\\nDk\\nxf(x0)[i1, . . . , ik]δ[i1] · · · δ[ik] (5.155)\\nin the Taylor series, where Dk\\nxf(x0)δk contains k-th order polynomials.\\nNow that we defined the Taylor series for vector fields, let us explicitly\\nwrite down the first terms Dk\\nxf(x0)δk of the Taylor series expansion for\\nk = 0, . . . ,3 and δ := x − x0: np.einsum(\\n’i,i’,Df1,d)\\nnp.einsum(\\n’ij,i,j’,\\nDf2,d,d)\\nnp.einsum(\\n’ijk,i,j,k’,\\nDf3,d,d,d)\\nk = 0 : D0\\nxf(x0)δ0 = f(x0) ∈ R (5.156)\\nk = 1 : D1\\nxf(x0)δ1 = ∇xf(x0)| {z }\\n1×D\\nδ|{z}\\nD×1\\n=\\nDX\\ni=1\\n∇xf(x0)[i]δ[i] ∈ R (5.157)\\nk = 2 : D2\\nxf(x0)δ2 = tr\\n\\x00\\nH(x0)| {z }\\nD×D\\nδ|{z}\\nD×1\\nδ⊤\\n|{z}\\n1×D\\n\\x01\\n= δ⊤H(x0)δ (5.158)\\n=\\nDX\\ni=1\\nDX\\nj=1\\nH[i, j]δ[i]δ[j] ∈ R (5.159)\\nk = 3 : D3\\nxf(x0)δ3 =\\nDX\\ni=1\\nDX\\nj=1\\nDX\\nk=1\\nD3\\nxf(x0)[i, j, k]δ[i]δ[j]δ[k] ∈ R\\n(5.160)\\nHere, H(x0) is the Hessian of f evaluated at x0.\\nExample 5.15 (Taylor Series Expansion of a Function with Two Vari-\\nables)\\nConsider the function\\nf(x, y) = x2 + 2xy + y3 . (5.161)\\nWe want to compute the Taylor series expansion of f at (x0, y0) = (1, 2).\\nBefore we start, let us discuss what to expect: The function in (5.161) is\\na polynomial of degree 3. We are looking for a Taylor series expansion,\\nwhich itself is a linear combination of polynomials. Therefore, we do not\\nexpect the Taylor series expansion to contain terms of fourth or higher\\norder to express a third-order polynomial. This means that it should be\\nsufficient to determine the first four terms of (5.151) for an exact alterna-\\ntive representation of (5.161).\\nTo determine the Taylor series expansion, we start with the constant\\nterm and the first-order derivatives, which are given by\\nf(1, 2) = 13 (5.162)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb459178-05eb-452b-a637-fb96f02d6f7a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 173, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='168 Vector Calculus\\n∂f\\n∂x = 2x + 2y =⇒ ∂f\\n∂x (1, 2) = 6 (5.163)\\n∂f\\n∂y = 2x + 3y2 =⇒ ∂f\\n∂y (1, 2) = 14 . (5.164)\\nTherefore, we obtain\\nD1\\nx,yf(1, 2) = ∇x,yf(1, 2) =\\nh\\n∂f\\n∂x (1, 2) ∂f\\n∂y (1, 2)\\ni\\n=\\n\\x02\\n6 14\\n\\x03\\n∈ R1×2\\n(5.165)\\nsuch that\\nD1\\nx,yf(1, 2)\\n1! δ =\\n\\x02\\n6 14\\n\\x03\\x14x − 1\\ny − 2\\n\\x15\\n= 6(x − 1) + 14(y − 2) . (5.166)\\nNote that D1\\nx,yf(1, 2)δ contains only linear terms, i.e., first-order polyno-\\nmials.\\nThe second-order partial derivatives are given by\\n∂2f\\n∂x2 = 2 = ⇒ ∂2f\\n∂x2 (1, 2) = 2 (5.167)\\n∂2f\\n∂y2 = 6y =⇒ ∂2f\\n∂y2 (1, 2) = 12 (5.168)\\n∂2f\\n∂y∂x = 2 = ⇒ ∂2f\\n∂y∂x (1, 2) = 2 (5.169)\\n∂2f\\n∂x∂y = 2 = ⇒ ∂2f\\n∂x∂y (1, 2) = 2 . (5.170)\\nWhen we collect the second-order partial derivatives, we obtain the Hes-\\nsian\\nH =\\n\" ∂2f\\n∂x2\\n∂2f\\n∂x∂y\\n∂2f\\n∂y∂x\\n∂2f\\n∂y 2\\n#\\n=\\n\\x142 2\\n2 6 y\\n\\x15\\n, (5.171)\\nsuch that\\nH(1, 2) =\\n\\x142 2\\n2 12\\n\\x15\\n∈ R2×2 . (5.172)\\nTherefore, the next term of the Taylor-series expansion is given by\\nD2\\nx,yf(1, 2)\\n2! δ2 = 1\\n2 δ⊤H(1, 2)δ (5.173a)\\n= 1\\n2\\n\\x02\\nx − 1 y − 2\\n\\x03\\x142 2\\n2 12\\n\\x15\\x14 x − 1\\ny − 2\\n\\x15\\n(5.173b)\\n= (x − 1)2 + 2(x − 1)(y − 2) + 6(y − 2)2 . (5.173c)\\nHere, D2\\nx,yf(1, 2)δ2 contains only quadratic terms, i.e., second-order poly-\\nnomials.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9f911ac7-6597-4e74-bbce-8fd4a3eeeba8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 174, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Linearization and Multivariate Taylor Series 169\\nThe third-order derivatives are obtained as\\nD3\\nx,yf =\\nh\\n∂H\\n∂x\\n∂H\\n∂y\\ni\\n∈ R2×2×2 , (5.174)\\nD3\\nx,yf[:, :, 1] = ∂H\\n∂x =\\n\" ∂3f\\n∂x3\\n∂3f\\n∂x2∂y\\n∂3f\\n∂x∂y∂x\\n∂3f\\n∂x∂y 2\\n#\\n, (5.175)\\nD3\\nx,yf[:, :, 2] = ∂H\\n∂y =\\n\" ∂3f\\n∂y∂x 2\\n∂3f\\n∂y∂x∂y\\n∂3f\\n∂y 2∂x\\n∂3f\\n∂y 3\\n#\\n. (5.176)\\nSince most second-order partial derivatives in the Hessian in (5.171) are\\nconstant, the only nonzero third-order partial derivative is\\n∂3f\\n∂y3 = 6 = ⇒ ∂3f\\n∂y3 (1, 2) = 6 . (5.177)\\nHigher-order derivatives and the mixed derivatives of degree 3 (e.g.,\\n∂f 3\\n∂x2∂y ) vanish, such that\\nD3\\nx,yf[:, :, 1] =\\n\\x140 0\\n0 0\\n\\x15\\n, D 3\\nx,yf[:, :, 2] =\\n\\x140 0\\n0 6\\n\\x15\\n(5.178)\\nand\\nD3\\nx,yf(1, 2)\\n3! δ3 = (y − 2)3 , (5.179)\\nwhich collects all cubic terms of the Taylor series. Overall, the (exact)\\nTaylor series expansion of f at (x0, y0) = (1, 2) is\\nf(x) = f(1, 2) +D1\\nx,yf(1, 2)δ + D2\\nx,yf(1, 2)\\n2! δ2 + D3\\nx,yf(1, 2)\\n3! δ3\\n(5.180a)\\n= f(1, 2) + ∂f (1, 2)\\n∂x (x − 1) + ∂f (1, 2)\\n∂y (y − 2)\\n+ 1\\n2!\\n\\x12 ∂2f(1, 2)\\n∂x2 (x − 1)2 + ∂2f(1, 2)\\n∂y2 (y − 2)2\\n+ 2∂2f(1, 2)\\n∂x∂y (x − 1)(y − 2)\\n\\x13\\n+ 1\\n6\\n∂3f(1, 2)\\n∂y3 (y − 2)3 (5.180b)\\n= 13 + 6(x − 1) + 14(y − 2)\\n+ (x − 1)2 + 6(y − 2)2 + 2(x − 1)(y − 2) + (y − 2)3 . (5.180c)\\nIn this case, we obtained an exact Taylor series expansion of the polyno-\\nmial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\\npolynomial in (5.161). In this particular example, this result is not sur-\\nprising since the original function was a third-order polynomial, which\\nwe expressed through a linear combination of constant terms, first-order,\\nsecond-order, and third-order polynomials in (5.180c).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ce66ee33-a6b3-4973-8316-e7ae3de0fd66', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 175, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='170 Vector Calculus\\n5.9 Further Reading\\nFurther details of matrix differentials, along with a short review of the\\nrequired linear algebra, can be found in Magnus and Neudecker (2007).\\nAutomatic differentiation has had a long history , and we refer to Griewank\\nand Walther (2003), Griewank and Walther (2008), and Elliott (2009)\\nand the references therein.\\nIn machine learning (and other disciplines), we often need to compute\\nexpectations, i.e., we need to solve integrals of the form\\nEx[f(x)] =\\nZ\\nf(x)p(x)dx . (5.181)\\nEven if p(x) is in a convenient form (e.g., Gaussian), this integral gen-\\nerally cannot be solved analytically . The Taylor series expansion of f is\\none way of finding an approximate solution: Assuming p(x) = N\\n\\x00\\nµ, Σ\\n\\x01\\nis Gaussian, then the first-order Taylor series expansion around µ locally\\nlinearizes the nonlinear function f. For linear functions, we can compute\\nthe mean (and the covariance) exactly if p(x) is Gaussian distributed (see\\nSection 6.5). This property is heavily exploited by the extended Kalmanextended Kalman\\nfilter filter (Maybeck, 1979) for online state estimation in nonlinear dynami-\\ncal systems (also called “state-space models”). Other deterministic ways\\nto approximate the integral in (5.181) are the unscented transform (Julierunscented transform\\nand Uhlmann, 1997), which does not require any gradients, or theLaplaceLaplace\\napproximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses\\na second-order Taylor series expansion (requiring the Hessian) for a local\\nGaussian approximation of p(x) around its mode.\\nExercises\\n5.1 Compute the derivative f ′(x) for\\nf(x) = log(x4) sin(x3) .\\n5.2 Compute the derivative f ′(x) of the logistic sigmoid\\nf(x) = 1\\n1 + exp(−x) .\\n5.3 Compute the derivative f ′(x) of the function\\nf(x) = exp(− 1\\n2σ2 (x − µ)2) ,\\nwhere µ, σ ∈ R are constants.\\n5.4 Compute the Taylor polynomials Tn, n = 0, . . . ,5 of f(x) = sin( x) + cos(x)\\nat x0 = 0.\\n5.5 Consider the following functions:\\nf1(x) = sin(x1) cos(x2) , x ∈ R2\\nf2(x, y) = x⊤y , x, y ∈ Rn\\nf3(x) = xx⊤ , x ∈ Rn\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='338582a0-de70-4066-8420-7338d88becaf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 176, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 171\\na. What are the dimensions of ∂fi\\n∂x ?\\nb. Compute the Jacobians.\\n5.6 Differentiate f with respect to t and g with respect to X, where\\nf(t) = sin(log(t⊤t)) , t ∈ RD\\ng(X) = tr(AXB) , A ∈ RD×E, X ∈ RE×F , B ∈ RF ×D ,\\nwhere tr(·) denotes the trace.\\n5.7 Compute the derivatives df /dx of the following functions by using the chain\\nrule. Provide the dimensions of every single partial derivative. Describe your\\nsteps in detail.\\na.\\nf(z) = log(1 + z) , z = x⊤x , x ∈ RD\\nb.\\nf(z) = sin(z) , z = Ax + b , A ∈ RE×D, x ∈ RD, b ∈ RE\\nwhere sin(·) is applied to every element of z.\\n5.8 Compute the derivatives df /dx of the following functions. Describe your\\nsteps in detail.\\na. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive.\\nf(z) = exp(− 1\\n2 z)\\nz = g(y) = y⊤S−1y\\ny = h(x) = x − µ\\nwhere x, µ ∈ RD, S ∈ RD×D.\\nb.\\nf(x) = tr(xx⊤ + σ2I) , x ∈ RD\\nHere tr(A) is the trace of A, i.e., the sum of the diagonal elements Aii.\\nHint: Explicitly write out the outer product.\\nc. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive. You do not need to compute the product of the partial derivatives\\nexplicitly .\\nf = tanh(z) ∈ RM\\nz = Ax + b, x ∈ RN , A ∈ RM ×N , b ∈ RM .\\nHere, tanh is applied to every component of z.\\n5.9 We define\\ng(x, z, ν) := log p(x, z) − log q(z, ν)\\nz := t(ϵ, ν)\\nfor differentiable functions p, q, t and x ∈ RD, z ∈ RE, ν ∈ RF , ϵ ∈ RG. By\\nusing the chain rule, compute the gradient\\nd\\ndν g(x, z, ν) .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ae272b4-3d1f-4ba8-8d44-8aa7d875b994', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 177, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6\\nProbability and Distributions\\nProbability , loosely speaking, concerns the study of uncertainty . Probabil-\\nity can be thought of as the fraction of times an event occurs, or as a degree\\nof belief about an event. We then would like to use this probability to mea-\\nsure the chance of something occurring in an experiment. As mentioned\\nin Chapter 1, we often quantify uncertainty in the data, uncertainty in the\\nmachine learning model, and uncertainty in the predictions produced by\\nthe model. Quantifying uncertainty requires the idea of arandom variable,random variable\\nwhich is a function that maps outcomes of random experiments to a set of\\nproperties that we are interested in. Associated with the random variable\\nis a function that measures the probability that a particular outcome (or\\nset of outcomes) will occur; this is called the probability distribution.probability\\ndistribution Probability distributions are used as a building block for other con-\\ncepts, such as probabilistic modeling (Section 8.4), graphical models (Sec-\\ntion 8.5), and model selection (Section 8.6). In the next section, we present\\nthe three concepts that define a probability space (the sample space, the\\nevents, and the probability of an event) and how they are related to a\\nfourth concept called the random variable. The presentation is deliber-\\nately slightly hand wavy since a rigorous presentation may occlude the\\nintuition behind the concepts. An outline of the concepts presented in this\\nchapter are shown in Figure 6.1.\\n6.1 Construction of a Probability Space\\nThe theory of probability aims at defining a mathematical structure to\\ndescribe random outcomes of experiments. For example, when tossing a\\nsingle coin, we cannot determine the outcome, but by doing a large num-\\nber of coin tosses, we can observe a regularity in the average outcome.\\nUsing this mathematical structure of probability , the goal is to perform\\nautomated reasoning, and in this sense, probability generalizes logical\\nreasoning (Jaynes, 2003).\\n6.1.1 Philosophical Issues\\nWhen constructing automated reasoning systems, classical Boolean logic\\ndoes not allow us to express certain forms of plausible reasoning. Consider\\n172\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ab8f9e18-d93b-4329-95d9-2742440bcd41', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 178, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 Construction of a Probability Space 173\\nFigure 6.1 A mind\\nmap of the concepts\\nrelated to random\\nvariables and\\nprobability\\ndistributions, as\\ndescribed in this\\nchapter.\\nRandom variable& distribution\\nSum ruleProduct rule\\nBayes’ Theorem\\nSummary statistics\\nMean Variance\\nTransformations\\nIndependence\\nInner product\\nGaussian\\nBernoulli\\nBeta\\nSufficient statistics\\nExponential family\\nChapter 9Regression\\nChapter 10Dimensionalityreduction\\nChapter 11Density estimation\\nProperty\\nSimilarity\\nExample\\nExample\\nConjugate\\nPropertyFinite\\nthe following scenario: We observe that A is false. We find B becomes\\nless plausible, although no conclusion can be drawn from classical logic.\\nWe observe that B is true. It seems A becomes more plausible. We use\\nthis form of reasoning daily . We are waiting for a friend, and consider\\nthree possibilities: H1, she is on time; H2, she has been delayed by traffic;\\nand H3, she has been abducted by aliens. When we observe our friend\\nis late, we must logically rule out H1. We also tend to consider H2 to be\\nmore likely , though we are not logically required to do so. Finally , we may\\nconsider H3 to be possible, but we continue to consider it quite unlikely .\\nHow do we conclude H2 is the most plausible answer? Seen in this way , “For plausible\\nreasoning it is\\nnecessary to extend\\nthe discrete true and\\nfalse values of truth\\nto continuous\\nplausibilities”\\n(Jaynes, 2003).\\nprobability theory can be considered a generalization of Boolean logic. In\\nthe context of machine learning, it is often applied in this way to formalize\\nthe design of automated reasoning systems. Further arguments about how\\nprobability theory is the foundation of reasoning systems can be found\\nin Pearl (1988).\\nThe philosophical basis of probability and how it should be somehow\\nrelated to what we think should be true (in the logical sense) was studied\\nby Cox (Jaynes, 2003). Another way to think about it is that if we are\\nprecise about our common sense we end up constructing probabilities.\\nE. T. Jaynes (1922–1998) identified three mathematical criteria, which\\nmust apply to all plausibilities:\\n1. The degrees of plausibility are represented by real numbers.\\n2. These numbers must be based on the rules of common sense.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='887ef1f9-4590-4ff6-b53f-42dcbec3891c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 179, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='174 Probability and Distributions\\n3. The resulting reasoning must be consistent, with the three following\\nmeanings of the word “consistent”:\\n(a) Consistency or non-contradiction: When the same result can be\\nreached through different means, the same plausibility value must\\nbe found in all cases.\\n(b) Honesty: All available data must be taken into account.\\n(c) Reproducibility: If our state of knowledge about two problems are\\nthe same, then we must assign the same degree of plausibility to\\nboth of them.\\nThe Cox–Jaynes theorem proves these plausibilities to be sufficient to\\ndefine the universal mathematical rules that apply to plausibility p, up to\\ntransformation by an arbitrary monotonic function. Crucially , these rules\\nare the rules of probability .\\nRemark. In machine learning and statistics, there are two major interpre-\\ntations of probability: the Bayesian and frequentist interpretations (Bishop,\\n2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil-\\nity to specify the degree of uncertainty that the user has about an event. It\\nis sometimes referred to as “subjective probability” or “degree of belief”.\\nThe frequentist interpretation considers the relative frequencies of events\\nof interest to the total number of events that occurred. The probability of\\nan event is defined as the relative frequency of the event in the limit when\\none has infinite data. ♢\\nSome machine learning texts on probabilistic models use lazy notation\\nand jargon, which is confusing. This text is no exception. Multiple distinct\\nconcepts are all referred to as “probability distribution”, and the reader\\nhas to often disentangle the meaning from the context. One trick to help\\nmake sense of probability distributions is to check whether we are trying\\nto model something categorical (a discrete random variable) or some-\\nthing continuous (a continuous random variable). The kinds of questions\\nwe tackle in machine learning are closely related to whether we are con-\\nsidering categorical or continuous models.\\n6.1.2 Probability and Random Variables\\nThere are three distinct ideas that are often confused when discussing\\nprobabilities. First is the idea of a probability space, which allows us to\\nquantify the idea of a probability . However, we mostly do not work directly\\nwith this basic probability space. Instead, we work with random variables\\n(the second idea), which transfers the probability to a more convenient\\n(often numerical) space. The third idea is the idea of a distribution or law\\nassociated with a random variable. We will introduce the first two ideas\\nin this section and expand on the third idea in Section 6.2.\\nModern probability is based on a set of axioms proposed by Kolmogorov\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0dfc975f-e28c-44dc-8fae-16575c6d79e9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 180, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 Construction of a Probability Space 175\\n(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-\\ncepts of sample space, event space, and probability measure. The prob-\\nability space models a real-world process (referred to as an experiment)\\nwith random outcomes.\\nThe sample space Ω\\nThe sample space is the set of all possible outcomes of the experiment, sample space\\nusually denoted by Ω. For example, two successive coin tosses have\\na sample space of {hh, tt, ht, th }, where “h” denotes “heads” and “t”\\ndenotes “tails”.\\nThe event space A\\nThe event space is the space of potential results of the experiment. A event space\\nsubset A of the sample space Ω is in the event space A if at the end\\nof the experiment we can observe whether a particular outcome ω ∈ Ω\\nis in A. The event space A is obtained by considering the collection of\\nsubsets of Ω, and for discrete probability distributions (Section 6.2.1)\\nA is often the power set of Ω.\\nThe probability P\\nWith each event A ∈ A, we associate a numberP (A) that measures the\\nprobability or degree of belief that the event will occur. P (A) is called\\nthe probability of A. probability\\nThe probability of a single event must lie in the interval [0, 1], and the\\ntotal probability over all outcomes in the sample space Ω must be 1, i.e.,\\nP (Ω) = 1. Given a probability space(Ω, A, P), we want to use it to model\\nsome real-world phenomenon. In machine learning, we often avoid explic-\\nitly referring to the probability space, but instead refer to probabilities on\\nquantities of interest, which we denote by T . In this book, we refer to T\\nas the target space and refer to elements of T as states. We introduce a target space\\nfunction X : Ω → T that takes an element ofΩ (an outcome) and returns\\na particular quantity of interest x, a value in T . This association/mapping\\nfrom Ω to T is called arandom variable. For example, in the case of tossing random variable\\ntwo coins and counting the number of heads, a random variable X maps\\nto the three possible outcomes: X(hh) = 2 , X(ht) = 1, X(th) = 1 , and\\nX(tt) = 0. In this particular case, T = {0, 1, 2}, and it is the probabilities\\non elements of T that we are interested in. For a finite sample spaceΩ and The name “random\\nvariable” is a great\\nsource of\\nmisunderstanding\\nas it is neither\\nrandom nor is it a\\nvariable. It is a\\nfunction.\\nfinite T , the function corresponding to a random variable is essentially a\\nlookup table. For any subset S ⊆ T , we associate PX(S) ∈ [0, 1] (the\\nprobability) to a particular event occurring corresponding to the random\\nvariable X. Example 6.1 provides a concrete illustration of the terminol-\\nogy .\\nRemark. The aforementioned sample space Ω unfortunately is referred\\nto by different names in different books. Another common name for Ω\\nis “state space” (Jacod and Protter, 2004), but state space is sometimes\\nreserved for referring to states in a dynamical system (Hasselblatt and\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='50a8b14c-f533-4bfd-9d32-7e87467f6786', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 181, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='176 Probability and Distributions\\nKatok, 2003). Other names sometimes used to describe Ω are: “sample\\ndescription space”, “possibility space,” and “event space”. ♢\\nExample 6.1\\nWe assume that the reader is already familiar with computing probabilitiesThis toy example is\\nessentially a biased\\ncoin flip example. of intersections and unions of sets of events. A gentler introduction to\\nprobability with many examples can be found in chapter 2 of Walpole\\net al. (2011).\\nConsider a statistical experiment where we model a funfair game con-\\nsisting of drawing two coins from a bag (with replacement). There are\\ncoins from USA (denoted as $) and UK (denoted as £) in the bag, and\\nsince we draw two coins from the bag, there are four outcomes in total.\\nThe state space or sample space Ω of this experiment is then ($, $), ($,\\n£), (£, $), (£, £). Let us assume that the composition of the bag of coins is\\nsuch that a draw returns at random a $ with probability 0.3.\\nThe event we are interested in is the total number of times the repeated\\ndraw returns $. Let us define a random variable X that maps the sample\\nspace Ω to T , which denotes the number of times we draw $ out of the\\nbag. We can see from the preceding sample space we can get zero $, one $,\\nor two $s, and thereforeT = {0, 1, 2}. The random variableX (a function\\nor lookup table) can be represented as a table like the following:\\nX(($, $)) = 2 (6.1)\\nX(($, £)) = 1 (6.2)\\nX((£, $)) = 1 (6.3)\\nX((£, £)) = 0 . (6.4)\\nSince we return the first coin we draw before drawing the second, this\\nimplies that the two draws are independent of each other, which we will\\ndiscuss in Section 6.4.5. Note that there are two experimental outcomes,\\nwhich map to the same event, where only one of the draws returns $.\\nTherefore, the probability mass function (Section 6.2.1) of X is given by\\nP (X = 2) = P (($, $))\\n= P ($) · P ($)\\n= 0.3 · 0.3 = 0.09 (6.5)\\nP (X = 1) = P (($, £) ∪ (£, $))\\n= P (($, £)) + P ((£, $))\\n= 0.3 · (1 − 0.3) + (1 − 0.3) · 0.3 = 0.42 (6.6)\\nP (X = 0) = P ((£, £))\\n= P (£) · P (£)\\n= (1 − 0.3) · (1 − 0.3) = 0.49 . (6.7)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bafc05ca-1693-4f70-807c-5f53798d3d0b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 182, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 Construction of a Probability Space 177\\nIn the calculation, we equated two different concepts, the probability\\nof the output of X and the probability of the samples in Ω. For example,\\nin (6.7) we say P (X = 0) = P ((£, £)). Consider the random variable\\nX : Ω → T and a subset S ⊆ T (for example, a single element of T ,\\nsuch as the outcome that one head is obtained when tossing two coins).\\nLet X −1(S) be the pre-image of S by X, i.e., the set of elements of Ω that\\nmap to S under X; {ω ∈ Ω : X(ω) ∈ S}. One way to understand the\\ntransformation of probability from events in Ω via the random variable\\nX is to associate it with the probability of the pre-image of S (Jacod and\\nProtter, 2004). For S ⊆ T , we have the notation\\nPX(S) = P (X ∈ S) = P (X −1(S)) = P ({ω ∈ Ω : X(ω) ∈ S}) . (6.8)\\nThe left-hand side of (6.8) is the probability of the set of possible outcomes\\n(e.g., number of $ = 1) that we are interested in. Via the random variable\\nX, which maps states to outcomes, we see in the right-hand side of (6.8)\\nthat this is the probability of the set of states (inΩ) that have the property\\n(e.g., $£, £$). We say that a random variable X is distributed according\\nto a particular probability distribution PX, which defines the probability\\nmapping between the event and the probability of the outcome of the\\nrandom variable. In other words, the functionPX or equivalently P ◦ X −1\\nis the law or distribution of random variable X. law\\ndistributionRemark. The target space, that is, the range T of the random variable X,\\nis used to indicate the kind of probability space, i.e., aT random variable.\\nWhen T is finite or countably infinite, this is called a discrete random\\nvariable (Section 6.2.1). For continuous random variables (Section 6.2.2),\\nwe only consider T = R or T = RD. ♢\\n6.1.3 Statistics\\nProbability theory and statistics are often presented together, but they con-\\ncern different aspects of uncertainty . One way of contrasting them is by the\\nkinds of problems that are considered. Using probability , we can consider\\na model of some process, where the underlying uncertainty is captured\\nby random variables, and we use the rules of probability to derive what\\nhappens. In statistics, we observe that something has happened and try\\nto figure out the underlying process that explains the observations. In this\\nsense, machine learning is close to statistics in its goals to construct a\\nmodel that adequately represents the process that generated the data. We\\ncan use the rules of probability to obtain a “best-fitting” model for some\\ndata.\\nAnother aspect of machine learning systems is that we are interested\\nin generalization error (see Chapter 8). This means that we are actually\\ninterested in the performance of our system on instances that we will\\nobserve in future, which are not identical to the instances that we have\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bcc42165-b025-40e9-a238-4e19f5bd638a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 183, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='178 Probability and Distributions\\nseen so far. This analysis of future performance relies on probability and\\nstatistics, most of which is beyond what will be presented in this chapter.\\nThe interested reader is encouraged to look at the books by Boucheron\\net al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more\\nabout statistics in Chapter 8.\\n6.2 Discrete and Continuous Probabilities\\nLet us focus our attention on ways to describe the probability of an event\\nas introduced in Section 6.1. Depending on whether the target space is dis-\\ncrete or continuous, the natural way to refer to distributions is different.\\nWhen the target space T is discrete, we can specify the probability that a\\nrandom variable X takes a particular value x ∈ T , denoted as P (X = x).\\nThe expression P (X = x) for a discrete random variable X is known as\\nthe probability mass function. When the target space T is continuous, e.g.,probability mass\\nfunction the real line R, it is more natural to specify the probability that a random\\nvariable X is in an interval, denoted by P (a ⩽ X ⩽ b) for a < b . By con-\\nvention, we specify the probability that a random variable X is less than\\na particular value x, denoted by P (X ⩽ x). The expression P (X ⩽ x) for\\na continuous random variable X is known as the cumulative distributioncumulative\\ndistribution function function. We will discuss continuous random variables in Section 6.2.2.\\nWe will revisit the nomenclature and contrast discrete and continuous\\nrandom variables in Section 6.2.3.\\nRemark. We will use the phraseunivariate distribution to refer to distribu-univariate\\ntions of a single random variable (whose states are denoted by non-bold\\nx). We will refer to distributions of more than one random variable as\\nmultivariate distributions, and will usually consider a vector of randommultivariate\\nvariables (whose states are denoted by bold x). ♢\\n6.2.1 Discrete Probabilities\\nWhen the target space is discrete, we can imagine the probability distri-\\nbution of multiple random variables as filling out a (multidimensional)\\narray of numbers. Figure 6.2 shows an example. The target space of the\\njoint probability is the Cartesian product of the target spaces of each of\\nthe random variables. We define the joint probability as the entry of bothjoint probability\\nvalues jointly\\nP (X = xi, Y = yj) = nij\\nN , (6.9)\\nwhere nij is the number of events with state xi and yj and N the total\\nnumber of events. The joint probability is the probability of the intersec-\\ntion of both events, that is, P (X = xi, Y = yj) = P (X = xi ∩ Y = yj).\\nFigure 6.2 illustrates theprobability mass function(pmf) of a discrete prob-probability mass\\nfunction ability distribution. For two random variables X and Y , the probability\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a76379c3-c6f8-4a4c-b211-ff46f4a9be4e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 184, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2 Discrete and Continuous Probabilities 179\\nFigure 6.2\\nVisualization of a\\ndiscrete bivariate\\nprobability mass\\nfunction, with\\nrandom variables X\\nand Y . This\\ndiagram is adapted\\nfrom Bishop (2006).\\nX\\nx1 x2 x3 x4 x5\\nY\\ny3\\ny2\\ny1\\nnij\\norj\\nciz}|{\\nthat X = x and Y = y is (lazily) written as p(x, y) and is called the joint\\nprobability . One can think of a probability as a function that takes state\\nx and y and returns a real number, which is the reason we write p(x, y).\\nThe marginal probability that X takes the value x irrespective of the value marginal probability\\nof random variable Y is (lazily) written as p(x). We write X ∼ p(x) to\\ndenote that the random variable X is distributed according to p(x). If we\\nconsider only the instances where X = x, then the fraction of instances\\n(the conditional probability) for which Y = y is written (lazily) as p(y | x). conditional\\nprobability\\nExample 6.2\\nConsider two random variables X and Y , where X has five possible states\\nand Y has three possible states, as shown in Figure 6.2. We denote by nij\\nthe number of events with state X = xi and Y = yj, and denote by\\nN the total number of events. The value ci is the sum of the individual\\nfrequencies for the ith column, that is, ci =P3\\nj=1 nij. Similarly , the value\\nrj is the row sum, that is, rj =P5\\ni=1 nij. Using these definitions, we can\\ncompactly express the distribution of X and Y .\\nThe probability distribution of each random variable, the marginal\\nprobability , can be seen as the sum over a row or column\\nP (X = xi) = ci\\nN =\\nP3\\nj=1 nij\\nN (6.10)\\nand\\nP (Y = yj) = rj\\nN =\\nP5\\ni=1 nij\\nN , (6.11)\\nwhere ci and rj are the ith column and jth row of the probability table,\\nrespectively . By convention, for discrete random variables with a finite\\nnumber of events, we assume that probabilties sum up to one, that is,\\n5X\\ni=1\\nP (X = xi) = 1 and\\n3X\\nj=1\\nP (Y = yj) = 1 . (6.12)\\nThe conditional probability is the fraction of a row or column in a par-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec80c273-9c71-4cdd-a8ca-c1706d6568d4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 185, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='180 Probability and Distributions\\nticular cell. For example, the conditional probability of Y given X is\\nP (Y = yj | X = xi) = nij\\nci\\n, (6.13)\\nand the conditional probability of X given Y is\\nP (X = xi | Y = yj) = nij\\nrj\\n. (6.14)\\nIn machine learning, we use discrete probability distributions to model\\ncategorical variables, i.e., variables that take a finite set of unordered val-categorical variable\\nues. They could be categorical features, such as the degree taken at uni-\\nversity when used for predicting the salary of a person, or categorical la-\\nbels, such as letters of the alphabet when doing handwriting recognition.\\nDiscrete distributions are also often used to construct probabilistic models\\nthat combine a finite number of continuous distributions (Chapter 11).\\n6.2.2 Continuous Probabilities\\nWe consider real-valued random variables in this section, i.e., we consider\\ntarget spaces that are intervals of the real line R. In this book, we pretend\\nthat we can perform operations on real random variables as if we have dis-\\ncrete probability spaces with finite states. However, this simplification is\\nnot precise for two situations: when we repeat something infinitely often,\\nand when we want to draw a point from an interval. The first situation\\narises when we discuss generalization errors in machine learning (Chap-\\nter 8). The second situation arises when we want to discuss continuous\\ndistributions, such as the Gaussian (Section 6.5). For our purposes, the\\nlack of precision allows for a briefer introduction to probability .\\nRemark. In continuous spaces, there are two additional technicalities,\\nwhich are counterintuitive. First, the set of all subsets (used to define\\nthe event space A in Section 6.1) is not well behaved enough. A needs\\nto be restricted to behave well under set complements, set intersections,\\nand set unions. Second, the size of a set (which in discrete spaces can be\\nobtained by counting the elements) turns out to be tricky . The size of a\\nset is called its measure. For example, the cardinality of discrete sets, themeasure\\nlength of an interval in R, and the volume of a region in Rd are all mea-\\nsures. Sets that behave well under set operations and additionally have\\na topology are called a Borel σ-algebra. Betancourt details a careful con-Borel σ-algebra\\nstruction of probability spaces from set theory without being bogged down\\nin technicalities; see https://tinyurl.com/yb3t6mfd. For a more pre-\\ncise construction, we refer to Billingsley (1995) and Jacod and Protter\\n(2004).\\nIn this book, we consider real-valued random variables with their cor-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe8c8c13-ba09-4b9d-9738-66cba6930ce9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 186, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2 Discrete and Continuous Probabilities 181\\nresponding Borel σ-algebra. We consider random variables with values in\\nRD to be a vector of real-valued random variables. ♢\\nDefinition 6.1 (Probability Density Function). A function f : RD → R is\\ncalled a probability density function (pdf) if probability density\\nfunction\\npdf1. ∀x ∈ RD : f(x) ⩾ 0\\n2. Its integral exists and\\nZ\\nRD\\nf(x)dx = 1 . (6.15)\\nFor probability mass functions (pmf) of discrete random variables, the\\nintegral in (6.15) is replaced with a sum (6.12).\\nObserve that the probability density function is any function f that is\\nnon-negative and integrates to one. We associate a random variable X\\nwith this function f by\\nP (a ⩽ X ⩽ b) =\\nZ b\\na\\nf(x)dx , (6.16)\\nwhere a, b ∈ R and x ∈ R are outcomes of the continuous random vari-\\nable X. States x ∈ RD are defined analogously by considering a vector\\nof x ∈ R. This association (6.16) is called the law or distribution of the law\\nrandom variable X. P (X = x) is a set of\\nmeasure zero.Remark. In contrast to discrete random variables, the probability of a con-\\ntinuous random variable X taking a particular value P (X = x) is zero.\\nThis is like trying to specify an interval in (6.16) where a = b. ♢\\nDefinition 6.2 (Cumulative Distribution Function). A cumulative distribu- cumulative\\ndistribution functiontion function (cdf) of a multivariate real-valued random variable X with\\nstates x ∈ RD is given by\\nFX(x) = P (X1 ⩽ x1, . . . , XD ⩽ xD) , (6.17)\\nwhere X = [ X1, . . . , XD]⊤, x = [ x1, . . . , xD]⊤, and the right-hand side\\nrepresents the probability that random variableXi takes the value smaller\\nthan or equal to xi.\\nThere are cdfs,\\nwhich do not have\\ncorresponding pdfs.\\nThe cdf can be expressed also as the integral of the probability density\\nfunction f(x) so that\\nFX(x) =\\nZ x1\\n−∞\\n· · ·\\nZ xD\\n−∞\\nf(z1, . . . , zD)dz1 · · ·dzD . (6.18)\\nRemark. We reiterate that there are in fact two distinct concepts when\\ntalking about distributions. First is the idea of a pdf (denoted by f(x)),\\nwhich is a nonnegative function that sums to one. Second is the law of a\\nrandom variable X, that is, the association of a random variable X with\\nthe pdf f(x). ♢\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dbc7552f-0657-4a4e-8bd0-e90ba6ce460c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 187, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='182 Probability and Distributions\\nFigure 6.3\\nExamples of\\n(a) discrete and\\n(b) continuous\\nuniform\\ndistributions. See\\nExample 6.3 for\\ndetails of the\\ndistributions.\\n−1 0 1 2\\nz\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nP (Z = z)\\n(a) Discrete distribution\\n−1 0 1 2\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\np(x) (b) Continuous distribution\\nFor most of this book, we will not use the notation f(x) and FX(x) as\\nwe mostly do not need to distinguish between the pdf and cdf. However,\\nwe will need to be careful about pdfs and cdfs in Section 6.7.\\n6.2.3 Contrasting Discrete and Continuous Distributions\\nRecall from Section 6.1.2 that probabilities are positive and the total prob-\\nability sums up to one. For discrete random variables (see (6.12)), this\\nimplies that the probability of each state must lie in the interval [0, 1].\\nHowever, for continuous random variables the normalization (see (6.15))\\ndoes not imply that the value of the density is less than or equal to 1 for\\nall values. We illustrate this in Figure 6.3 using the uniform distributionuniform distribution\\nfor both discrete and continuous random variables.\\nExample 6.3\\nWe consider two examples of the uniform distribution, where each state is\\nequally likely to occur. This example illustrates some differences between\\ndiscrete and continuous probability distributions.\\nLet Z be a discrete uniform random variable with three states {z =\\n−1.1, z = 0.3, z = 1.5}. The probability mass function can be representedThe actual values of\\nthese states are not\\nmeaningful here,\\nand we deliberately\\nchose numbers to\\ndrive home the\\npoint that we do not\\nwant to use (and\\nshould ignore) the\\nordering of the\\nstates.\\nas a table of probability values:\\nz\\nP(Z=z)\\n−1.1\\n13\\n0.3\\n13\\n1.5\\n13\\nAlternatively , we can think of this as a graph (Figure 6.3(a)), where we\\nuse the fact that the states can be located on the x-axis, and the y-axis\\nrepresents the probability of a particular state. The y-axis in Figure 6.3(a)\\nis deliberately extended so that is it the same as in Figure 6.3(b).\\nLet X be a continuous random variable taking values in the range0.9 ⩽\\nX ⩽ 1.6, as represented by Figure 6.3(b). Observe that the height of the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3cd75b4-39d5-4434-95db-6baac8fbe78a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 188, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\nTable 6.1\\nNomenclature for\\nprobability\\ndistributions.\\nType “Point probability” “Interval probability”\\nDiscrete P (X = x) Not applicable\\nProbability mass function\\nContinuous p(x) P (X ⩽ x)\\nProbability density function Cumulative distribution function\\ndensity can be greater than 1. However, it needs to hold that\\nZ 1.6\\n0.9\\np(x)dx = 1 . (6.19)\\nRemark. There is an additional subtlety with regards to discrete prob-\\nability distributions. The states z1, . . . , zd do not in principle have any\\nstructure, i.e., there is usually no way to compare them, for example\\nz1 = red , z2 = green , z3 = blue . However, in many machine learning\\napplications discrete states take numerical values, e.g., z1 = −1.1, z2 =\\n0.3, z3 = 1 .5, where we could say z1 < z 2 < z 3. Discrete states that as-\\nsume numerical values are particularly useful because we often consider\\nexpected values (Section 6.4.1) of random variables. ♢\\nUnfortunately , machine learning literature uses notation and nomen-\\nclature that hides the distinction between the sample space Ω, the target\\nspace T , and the random variable X. For a value x of the set of possible\\noutcomes of the random variable X, i.e., x ∈ T , p(x) denotes the prob- We think of the\\noutcome x as the\\nargument that\\nresults in the\\nprobability p(x).\\nability that random variable X has the outcome x. For discrete random\\nvariables, this is written as P (X = x), which is known as the probabil-\\nity mass function. The pmf is often referred to as the “distribution”. For\\ncontinuous variables, p(x) is called the probability density function (often\\nreferred to as a density). To muddy things even further, the cumulative\\ndistribution function P (X ⩽ x) is often also referred to as the “distribu-\\ntion”. In this chapter, we will use the notationX to refer to both univariate\\nand multivariate random variables, and denote the states by x and x re-\\nspectively . We summarize the nomenclature in Table 6.1.\\nRemark. We will be using the expression “probability distribution” not\\nonly for discrete probability mass functions but also for continuous proba-\\nbility density functions, although this is technically incorrect. In line with\\nmost machine learning literature, we also rely on context to distinguish\\nthe different uses of the phrase probability distribution. ♢\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem\\nWe think of probability theory as an extension to logical reasoning. As we\\ndiscussed in Section 6.1.1, the rules of probability presented here follow\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='48a67f98-5fd5-4a85-a743-3ac86473d53e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 189, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='184 Probability and Distributions\\nnaturally from fulfilling the desiderata (Jaynes, 2003, chapter 2). Prob-\\nabilistic modeling (Section 8.4) provides a principled foundation for de-\\nsigning machine learning methods. Once we have defined probability dis-\\ntributions (Section 6.2) corresponding to the uncertainties of the data and\\nour problem, it turns out that there are only two fundamental rules, the\\nsum rule and the product rule.\\nRecall from (6.9) that p(x, y) is the joint distribution of the two ran-\\ndom variables x, y. The distributions p(x) and p(y) are the correspond-\\ning marginal distributions, and p(y | x) is the conditional distribution of y\\ngiven x. Given the definitions of the marginal and conditional probability\\nfor discrete and continuous random variables in Section 6.2, we can now\\npresent the two fundamental rules in probability theory .These two rules\\narise\\nnaturally (Jaynes,\\n2003) from the\\nrequirements we\\ndiscussed in\\nSection 6.1.1.\\nThe first rule, the sum rule, states that\\nsum rule\\np(x) =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nX\\ny∈Y\\np(x, y) if y is discrete\\nZ\\nY\\np(x, y)dy if y is continuous\\n, (6.20)\\nwhere Y are the states of the target space of random variable Y . This\\nmeans that we sum out (or integrate out) the set of statesy of the random\\nvariable Y . The sum rule is also known as the marginalization property.marginalization\\nproperty The sum rule relates the joint distribution to a marginal distribution. In\\ngeneral, when the joint distribution contains more than two random vari-\\nables, the sum rule can be applied to any subset of the random variables,\\nresulting in a marginal distribution of potentially more than one random\\nvariable. More concretely , ifx = [x1, . . . , xD]⊤, we obtain the marginal\\np(xi) =\\nZ\\np(x1, . . . , xD)dx\\\\i (6.21)\\nby repeated application of the sum rule where we integrate/sum out all\\nrandom variables except xi, which is indicated by \\\\i, which reads “all\\nexcept i.”\\nRemark. Many of the computational challenges of probabilistic modeling\\nare due to the application of the sum rule. When there are many variables\\nor discrete variables with many states, the sum rule boils down to per-\\nforming a high-dimensional sum or integral. Performing high-dimensional\\nsums or integrals is generally computationally hard, in the sense that there\\nis no known polynomial-time algorithm to calculate them exactly . ♢\\nThe second rule, known as theproduct rule, relates the joint distributionproduct rule\\nto the conditional distribution via\\np(x, y) = p(y | x)p(x) . (6.22)\\nThe product rule can be interpreted as the fact that every joint distribu-\\ntion of two random variables can be factorized (written as a product)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf060c30-ac11-4c53-864f-45dc4de9880e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 190, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3 Sum Rule, Product Rule, and Bayes’ Theorem 185\\nof two other distributions. The two factors are the marginal distribu-\\ntion of the first random variable p(x), and the conditional distribution\\nof the second random variable given the first p(y | x). Since the ordering\\nof random variables is arbitrary in p(x, y), the product rule also implies\\np(x, y) = p(x | y)p(y). To be precise, (6.22) is expressed in terms of the\\nprobability mass functions for discrete random variables. For continuous\\nrandom variables, the product rule is expressed in terms of the probability\\ndensity functions (Section 6.2.3).\\nIn machine learning and Bayesian statistics, we are often interested in\\nmaking inferences of unobserved (latent) random variables given that we\\nhave observed other random variables. Let us assume we have some prior\\nknowledge p(x) about an unobserved random variable x and some rela-\\ntionship p(y | x) between x and a second random variable y, which we\\ncan observe. If we observe y, we can use Bayes’ theorem to draw some\\nconclusions about x given the observed values of y. Bayes’ theorem(also Bayes’ theorem\\nBayes’ ruleor Bayes’ law) Bayes’ rule\\nBayes’ law\\np(x | y)| {z }\\nposterior\\n=\\nlikelihood\\nz }| {\\np(y | x)\\nprior\\nz}|{\\np(x)\\np(y)|{z}\\nevidence\\n(6.23)\\nis a direct consequence of the product rule in (6.22) since\\np(x, y) = p(x | y)p(y) (6.24)\\nand\\np(x, y) = p(y | x)p(x) (6.25)\\nso that\\np(x | y)p(y) = p(y | x)p(x) ⇐ ⇒ p(x | y) = p(y | x)p(x)\\np(y) . (6.26)\\nIn (6.23), p(x) is the prior, which encapsulates our subjective prior prior\\nknowledge of the unobserved (latent) variable x before observing any\\ndata. We can choose any prior that makes sense to us, but it is critical to\\nensure that the prior has a nonzero pdf (or pmf) on all plausible x, even\\nif they are very rare.\\nThe likelihood p(y | x) describes how x and y are related, and in the likelihood\\nThe likelihood is\\nsometimes also\\ncalled the\\n“measurement\\nmodel”.\\ncase of discrete probability distributions, it is the probability of the data y\\nif we were to know the latent variable x. Note that the likelihood is not a\\ndistribution in x, but only in y. We call p(y | x) either the “likelihood of\\nx (given y)” or the “probability ofy given x” but never the likelihood of\\ny (MacKay, 2003).\\nThe posterior p(x | y) is the quantity of interest in Bayesian statistics posterior\\nbecause it expresses exactly what we are interested in, i.e., what we know\\nabout x after having observed y.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e0dd7a4d-7d38-4e00-b519-22bcb4141974', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 191, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='186 Probability and Distributions\\nThe quantity\\np(y) :=\\nZ\\np(y | x)p(x)dx = EX[p(y | x)] (6.27)\\nis the marginal likelihood/evidence. The right-hand side of (6.27) uses themarginal likelihood\\nevidence expectation operator which we define in Section 6.4.1. By definition, the\\nmarginal likelihood integrates the numerator of (6.23) with respect to the\\nlatent variable x. Therefore, the marginal likelihood is independent of\\nx, and it ensures that the posterior p(x | y) is normalized. The marginal\\nlikelihood can also be interpreted as the expected likelihood where we\\ntake the expectation with respect to the prior p(x). Beyond normalization\\nof the posterior, the marginal likelihood also plays an important role in\\nBayesian model selection, as we will discuss in Section 8.6. Due to the\\nintegration in (8.44), the evidence is often hard to compute.Bayes’ theorem is\\nalso called the\\n“probabilistic\\ninverse.”\\nBayes’ theorem (6.23) allows us to invert the relationship between x\\nand y given by the likelihood. Therefore, Bayes’ theorem is sometimes\\ncalled the probabilistic inverse. We will discuss Bayes’ theorem further inprobabilistic inverse\\nSection 8.4.\\nRemark. In Bayesian statistics, the posterior distribution is the quantity\\nof interest as it encapsulates all available information from the prior and\\nthe data. Instead of carrying the posterior around, it is possible to focus\\non some statistic of the posterior, such as the maximum of the posterior,\\nwhich we will discuss in Section 8.3. However, focusing on some statistic\\nof the posterior leads to loss of information. If we think in a bigger con-\\ntext, then the posterior can be used within a decision-making system, and\\nhaving the full posterior can be extremely useful and lead to decisions that\\nare robust to disturbances. For example, in the context of model-based re-\\ninforcement learning, Deisenroth et al. (2015) show that using the full\\nposterior distribution of plausible transition functions leads to very fast\\n(data/sample efficient) learning, whereas focusing on the maximum of\\nthe posterior leads to consistent failures. Therefore, having the full pos-\\nterior can be very useful for a downstream task. In Chapter 9, we will\\ncontinue this discussion in the context of linear regression. ♢\\n6.4 Summary Statistics and Independence\\nWe are often interested in summarizing sets of random variables and com-\\nparing pairs of random variables. A statistic of a random variable is a de-\\nterministic function of that random variable. The summary statistics of a\\ndistribution provide one useful view of how a random variable behaves,\\nand as the name suggests, provide numbers that summarize and charac-\\nterize the distribution. We describe the mean and the variance, two well-\\nknown summary statistics. Then we discuss two ways to compare a pair\\nof random variables: first, how to say that two random variables are inde-\\npendent; and second, how to compute an inner product between them.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='57f63bea-4d4b-4546-b46d-82fb35d842cc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 192, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Summary Statistics and Independence 187\\n6.4.1 Means and Covariances\\nMean and (co)variance are often useful to describe properties of probabil-\\nity distributions (expected values and spread). We will see in Section 6.6\\nthat there is a useful family of distributions (called the exponential fam-\\nily), where the statistics of the random variable capture all possible infor-\\nmation.\\nThe concept of the expected value is central to machine learning, and\\nthe foundational concepts of probability itself can be derived from the\\nexpected value (Whittle, 2000).\\nDefinition 6.3 (Expected Value). The expected value of a function g : R → expected value\\nR of a univariate continuous random variable X ∼ p(x) is given by\\nEX[g(x)] =\\nZ\\nX\\ng(x)p(x)dx . (6.28)\\nCorrespondingly , the expected value of a functiong of a discrete random\\nvariable X ∼ p(x) is given by\\nEX[g(x)] =\\nX\\nx∈X\\ng(x)p(x) , (6.29)\\nwhere X is the set of possible outcomes (the target space) of the random\\nvariable X.\\nIn this section, we consider discrete random variables to have numerical\\noutcomes. This can be seen by observing that the function g takes real\\nnumbers as inputs. The expected value\\nof a function of a\\nrandom variable is\\nsometimes referred\\nto as the law of the\\nunconscious\\nstatistician (Casella\\nand Berger, 2002,\\nSection 2.2).\\nRemark. We consider multivariate random variables X as a finite vector\\nof univariate random variables [X1, . . . , XD]⊤. For multivariate random\\nvariables, we define the expected value element wise\\nEX[g(x)] =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nEX1[g(x1)]\\n...\\nEXD[g(xD)]\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ RD , (6.30)\\nwhere the subscript EXd indicates that we are taking the expected value\\nwith respect to the dth element of the vector x. ♢\\nDefinition 6.3 defines the meaning of the notation EX as the operator\\nindicating that we should take the integral with respect to the probabil-\\nity density (for continuous distributions) or the sum over all states (for\\ndiscrete distributions). The definition of the mean (Definition 6.4), is a\\nspecial case of the expected value, obtained by choosing g to be the iden-\\ntity function.\\nDefinition 6.4 (Mean). The mean of a random variable X with states mean\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3e01a044-acd7-4025-83bd-4880adb1407b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 193, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='188 Probability and Distributions\\nx ∈ RD is an average and is defined as\\nEX[x] =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nEX1[x1]\\n...\\nEXD[xD]\\n\\uf8f9\\n\\uf8fa\\uf8fb ∈ RD , (6.31)\\nwhere\\nEXd[xd] :=\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\nZ\\nX\\nxdp(xd)dxd if X is a continuous random variable\\nX\\nxi∈X\\nxip(xd = xi) if X is a discrete random variable\\n(6.32)\\nfor d = 1 , . . . , D, where the subscript d indicates the corresponding di-\\nmension of x. The integral and sum are over the states X of the target\\nspace of the random variable X.\\nIn one dimension, there are two other intuitive notions of “average”,\\nwhich are the median and the mode. The median is the “middle” value ifmedian\\nwe sort the values, i.e.,50% of the values are greater than the median and\\n50% are smaller than the median. This idea can be generalized to contin-\\nuous values by considering the value where the cdf (Definition 6.2) is0.5.\\nFor distributions, which are asymmetric or have long tails, the median\\nprovides an estimate of a typical value that is closer to human intuition\\nthan the mean value. Furthermore, the median is more robust to outliers\\nthan the mean. The generalization of the median to higher dimensions is\\nnon-trivial as there is no obvious way to “sort” in more than one dimen-\\nsion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the mostmode\\nfrequently occurring value. For a discrete random variable, the mode is\\ndefined as the value of x having the highest frequency of occurrence. For\\na continuous random variable, the mode is defined as a peak in the density\\np(x). A particular density p(x) may have more than one mode, and fur-\\nthermore there may be a very large number of modes in high-dimensional\\ndistributions. Therefore, finding all the modes of a distribution can be\\ncomputationally challenging.\\nExample 6.4\\nConsider the two-dimensional distribution illustrated in Figure 6.4:\\np(x) = 0.4 N\\n\\x12\\nx\\n\\x0c\\x0c\\x0c\\x0c\\n\\x1410\\n2\\n\\x15\\n,\\n\\x141 0\\n0 1\\n\\x15\\x13\\n+ 0.6 N\\n\\x12\\nx\\n\\x0c\\x0c\\x0c\\x0c\\n\\x140\\n0\\n\\x15\\n,\\n\\x148.4 2 .0\\n2.0 1 .7\\n\\x15\\x13\\n.\\n(6.33)\\nWe will define the Gaussian distribution N\\n\\x00\\nµ, σ 2\\x01\\nin Section 6.5. Also\\nshown is its corresponding marginal distribution in each dimension. Ob-\\nserve that the distribution is bimodal (has two modes), but one of the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a70ef339-b919-4ff7-8b6a-8bf5de63255b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 194, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Summary Statistics and Independence 189\\nmarginal distributions is unimodal (has one mode). The horizontal bi-\\nmodal univariate distribution illustrates that the mean and median can\\nbe different from each other. While it is tempting to define the two-\\ndimensional median to be the concatenation of the medians in each di-\\nmension, the fact that we cannot define an ordering of two-dimensional\\npoints makes it difficult. When we say “cannot define an ordering”, we\\nmean that there is more than one way to define the relation < so that\\x143\\n0\\n\\x15\\n<\\n\\x142\\n3\\n\\x15\\n.\\nFigure 6.4\\nIllustration of the\\nmean, mode, and\\nmedian for a\\ntwo-dimensional\\ndataset, as well as\\nits marginal\\ndensities.\\nMean\\nModes\\nMedian\\nRemark. The expected value (Definition 6.3) is a linear operator. For ex-\\nample, given a real-valued function f(x) = ag(x)+ bh(x) where a, b ∈ R\\nand x ∈ RD, we obtain\\nEX[f(x)] =\\nZ\\nf(x)p(x)dx (6.34a)\\n=\\nZ\\n[ag(x) + bh(x)]p(x)dx (6.34b)\\n= a\\nZ\\ng(x)p(x)dx + b\\nZ\\nh(x)p(x)dx (6.34c)\\n= aEX[g(x)] + bEX[h(x)] . (6.34d)\\n♢\\nFor two random variables, we may wish to characterize their correspon-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56566811-7fd9-4d1c-ad14-2d2537ada397', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 195, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='190 Probability and Distributions\\ndence to each other. The covariance intuitively represents the notion of\\nhow dependent random variables are to one another.\\nDefinition 6.5 (Covariance (Univariate)) . The covariance between twocovariance\\nunivariate random variables X, Y ∈ R is given by the expected product\\nof their deviations from their respective means, i.e.,\\nCovX,Y [x, y] := EX,Y\\n\\x02\\n(x − EX[x])(y − EY [y])\\n\\x03\\n. (6.35)\\nTerminology: The\\ncovariance of\\nmultivariate random\\nvariables Cov[x, y]\\nis sometimes\\nreferred to as\\ncross-covariance,\\nwith covariance\\nreferring to\\nCov[x, x].\\nRemark. When the random variable associated with the expectation or\\ncovariance is clear by its arguments, the subscript is often suppressed (for\\nexample, EX[x] is often written as E[x]). ♢\\nBy using the linearity of expectations, the expression in Definition 6.5\\ncan be rewritten as the expected value of the product minus the product\\nof the expected values, i.e.,\\nCov[x, y] = E[xy] − E[x]E[y] . (6.36)\\nThe covariance of a variable with itselfCov[x, x] is called the variance andvariance\\nis denoted by VX[x]. The square root of the variance is called thestandardstandard deviation\\ndeviation and is often denoted by σ(x). The notion of covariance can be\\ngeneralized to multivariate random variables.\\nDefinition 6.6 (Covariance (Multivariate)). If we consider two multivari-\\nate random variables X and Y with states x ∈ RD and y ∈ RE respec-\\ntively , thecovariance between X and Y is defined ascovariance\\nCov[x, y] = E[xy⊤] − E[x]E[y]⊤ = Cov[y, x]⊤ ∈ RD×E . (6.37)\\nDefinition 6.6 can be applied with the same multivariate random vari-\\nable in both arguments, which results in a useful concept that intuitively\\ncaptures the “spread” of a random variable. For a multivariate random\\nvariable, the variance describes the relation between individual dimen-\\nsions of the random variable.\\nDefinition 6.7 (Variance). The variance of a random variable X withvariance\\nstates x ∈ RD and a mean vector µ ∈ RD is defined as\\nVX[x] = CovX[x, x] (6.38a)\\n= EX[(x − µ)(x − µ)⊤] = EX[xx⊤] − EX[x]EX[x]⊤ (6.38b)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nCov[x1, x1] Cov[ x1, x2] . . . Cov[x1, xD]\\nCov[x2, x1] Cov[ x2, x2] . . . Cov[x2, xD]\\n... ... ... ...\\nCov[xD, x1] . . . . . . Cov[xD, xD]\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb . (6.38c)\\nThe D × D matrix in (6.38c) is called the covariance matrix of the mul-covariance matrix\\ntivariate random variable X. The covariance matrix is symmetric and pos-\\nitive semidefinite and tells us something about the spread of the data. On\\nits diagonal, the covariance matrix contains the variances of themarginalsmarginal\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66ceca9e-dc2e-4cbc-8612-115534dcaa17', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 196, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Summary Statistics and Independence 191\\nFigure 6.5\\nTwo-dimensional\\ndatasets with\\nidentical means and\\nvariances along\\neach axis (colored\\nlines) but with\\ndifferent\\ncovariances.\\n−5 0 5\\nx\\n−2\\n0\\n2\\n4\\n6\\ny\\n(a) x and y are negatively correlated.\\n−5 0 5\\nx\\n−2\\n0\\n2\\n4\\n6\\ny\\n (b) x and y are positively correlated.\\np(xi) =\\nZ\\np(x1, . . . , xD)dx\\\\i , (6.39)\\nwhere “\\\\i” denotes “all variables but i”. The off-diagonal entries are the\\ncross-covariance terms Cov[xi, xj] for i, j = 1, . . . , D, i ̸= j. cross-covariance\\nRemark. In this book, we generally assume that covariance matrices are\\npositive definite to enable better intuition. We therefore do not discuss\\ncorner cases that result in positive semidefinite (low-rank) covariance ma-\\ntrices. ♢\\nWhen we want to compare the covariances between different pairs of\\nrandom variables, it turns out that the variance of each random variable\\naffects the value of the covariance. The normalized version of covariance\\nis called the correlation.\\nDefinition 6.8 (Correlation). The correlation between two random vari- correlation\\nables X, Y is given by\\ncorr[x, y] = Cov[x, y]p\\nV[x]V[y] ∈ [−1, 1] . (6.40)\\nThe correlation matrix is the covariance matrix of standardized random\\nvariables, x/σ(x). In other words, each random variable is divided by its\\nstandard deviation (the square root of the variance) in the correlation\\nmatrix.\\nThe covariance (and correlation) indicate how two random variables\\nare related; see Figure 6.5. Positive correlation corr[x, y] means that when\\nx grows, then y is also expected to grow. Negative correlation means that\\nas x increases, then y decreases.\\n6.4.2 Empirical Means and Covariances\\nThe definitions in Section 6.4.1 are often also called the population mean population mean\\nand covarianceand covariance, as it refers to the true statistics for the population. In ma-\\nchine learning, we need to learn from empirical observations of data. Con-\\nsider a random variable X. There are two conceptual steps to go from\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0fad9e9e-73ef-4882-a19d-1a11020c62c5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 197, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='192 Probability and Distributions\\npopulation statistics to the realization of empirical statistics. First, we use\\nthe fact that we have a finite dataset (of size N) to construct an empirical\\nstatistic that is a function of a finite number of identical random variables,\\nX1, . . . , XN. Second, we observe the data, that is, we look at the realiza-\\ntion x1, . . . , xN of each of the random variables and apply the empirical\\nstatistic.\\nSpecifically , for the mean (Definition 6.4), given a particular dataset we\\ncan obtain an estimate of the mean, which is called the empirical mean orempirical mean\\nsample mean. The same holds for the empirical covariance.sample mean\\nDefinition 6.9(Empirical Mean and Covariance). The empirical mean vec-empirical mean\\ntor is the arithmetic average of the observations for each variable, and it\\nis defined as\\n¯x := 1\\nN\\nNX\\nn=1\\nxn , (6.41)\\nwhere xn ∈ RD.\\nSimilar to the empirical mean, theempirical covariance matrix is aD×Dempirical covariance\\nmatrix\\nΣ := 1\\nN\\nNX\\nn=1\\n(xn − ¯x)(xn − ¯x)⊤. (6.42)\\nThroughout the\\nbook, we use the\\nempirical\\ncovariance, which is\\na biased estimate.\\nThe unbiased\\n(sometimes called\\ncorrected)\\ncovariance has the\\nfactor N − 1 in the\\ndenominator\\ninstead of N.\\nTo compute the statistics for a particular dataset, we would use the\\nrealizations (observations) x1, . . . ,xN and use (6.41) and (6.42). Em-\\npirical covariance matrices are symmetric, positive semidefinite (see Sec-\\ntion 3.2.3).\\n6.4.3 Three Expressions for the Variance\\nWe now focus on a single random variable X and use the preceding em-\\npirical formulas to derive three possible expressions for the variance. The\\nThe derivations are\\nexercises at the end\\nof this chapter.\\nfollowing derivation is the same for the population variance, except that\\nwe need to take care of integrals. The standard definition of variance, cor-\\nresponding to the definition of covariance (Definition 6.5), is the expec-\\ntation of the squared deviation of a random variable X from its expected\\nvalue µ, i.e.,\\nVX[x] := EX[(x − µ)2] . (6.43)\\nThe expectation in (6.43) and the mean µ = EX(x) are computed us-\\ning (6.32), depending on whether X is a discrete or continuous random\\nvariable. The variance as expressed in (6.43) is the mean of a new random\\nvariable Z := (X − µ)2.\\nWhen estimating the variance in (6.43) empirically , we need to resort\\nto a two-pass algorithm: one pass through the data to calculate the mean\\nµ using (6.41), and then a second pass using this estimate ˆµ calculate the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c017138-672f-44b3-9fbc-4028255ba92a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 198, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Summary Statistics and Independence 193\\nvariance. It turns out that we can avoid two passes by rearranging the\\nterms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula\\nfor varianceformula for variance:\\nVX[x] = EX[x2] − (EX[x])\\n2\\n. (6.44)\\nThe expression in (6.44) can be remembered as “the mean of the square\\nminus the square of the mean”. It can be calculated empirically in one pass\\nthrough data since we can accumulate xi (to calculate the mean) and x2\\ni\\nsimultaneously , wherexi is the ith observation. Unfortunately , if imple- If the two terms\\nin (6.44) are huge\\nand approximately\\nequal, we may\\nsuffer from an\\nunnecessary loss of\\nnumerical precision\\nin floating-point\\narithmetic.\\nmented in this way , it can be numerically unstable. The raw-score version\\nof the variance can be useful in machine learning, e.g., when deriving the\\nbias–variance decomposition (Bishop, 2006).\\nA third way to understand the variance is that it is a sum of pairwise dif-\\nferences between all pairs of observations. Consider a sample x1, . . . , xN\\nof realizations of random variable X, and we compute the squared differ-\\nence between pairs of xi and xj. By expanding the square, we can show\\nthat the sum of N 2 pairwise differences is the empirical variance of the\\nobservations:\\n1\\nN 2\\nNX\\ni,j=1\\n(xi − xj)2 = 2\\n\\uf8ee\\n\\uf8f0 1\\nN\\nNX\\ni=1\\nx2\\ni −\\n \\n1\\nN\\nNX\\ni=1\\nxi\\n!2\\uf8f9\\n\\uf8fb . (6.45)\\nWe see that (6.45) is twice the raw-score expression (6.44). This means\\nthat we can express the sum of pairwise distances (of which there are N 2\\nof them) as a sum of deviations from the mean (of which there areN). Ge-\\nometrically , this means that there is an equivalence between the pairwise\\ndistances and the distances from the center of the set of points. From a\\ncomputational perspective, this means that by computing the mean ( N\\nterms in the summation), and then computing the variance (again N\\nterms in the summation), we can obtain an expression (left-hand side\\nof (6.45)) that has N 2 terms.\\n6.4.4 Sums and Transformations of Random Variables\\nWe may want to model a phenomenon that cannot be well explained by\\ntextbook distributions (we introduce some in Sections 6.5 and 6.6), and\\nhence may perform simple manipulations of random variables (such as\\nadding two random variables).\\nConsider two random variables X, Y with states x, y ∈ RD. Then:\\nE[x + y] = E[x] + E[y] (6.46)\\nE[x − y] = E[x] − E[y] (6.47)\\nV[x + y] = V[x] + V[y] + Cov[x, y] + Cov[y, x] (6.48)\\nV[x − y] = V[x] + V[y] − Cov[x, y] − Cov[y, x] . (6.49)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ea00da85-b9bc-4e83-89e0-ef23c39c3ea1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 199, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='194 Probability and Distributions\\nMean and (co)variance exhibit some useful properties when it comes\\nto affine transformation of random variables. Consider a random variable\\nX with mean µ and covariance matrix Σ and a (deterministic) affine\\ntransformation y = Ax + b of x. Then y is itself a random variable\\nwhose mean vector and covariance matrix are given by\\nEY [y] = EX[Ax + b] = AEX[x] + b = Aµ + b , (6.50)\\nVY [y] = VX[Ax + b] = VX[Ax] = AVX[x]A⊤ = AΣA⊤ , (6.51)\\nrespectively . Furthermore,This can be shown\\ndirectly by using the\\ndefinition of the\\nmean and\\ncovariance.\\nCov[x, y] = E[x(Ax + b)⊤] − E[x]E[Ax + b]⊤ (6.52a)\\n= E[x]b⊤ + E[xx⊤]A⊤ − µb⊤ − µµ⊤A⊤ (6.52b)\\n= µb⊤ − µb⊤ +\\n\\x00\\nE[xx⊤] − µµ⊤\\x01\\nA⊤ (6.52c)\\n(6.38b)\\n= ΣA⊤ , (6.52d)\\nwhere Σ = E[xx⊤] − µµ⊤ is the covariance of X.\\n6.4.5 Statistical Independence\\nDefinition 6.10 (Independence). Two random variables X, Y are statis-statistical\\nindependence tically independent if and only if\\np(x, y) = p(x)p(y) . (6.53)\\nIntuitively , two random variablesX and Y are independent if the value\\nof y (once known) does not add any additional information about x (and\\nvice versa). If X, Y are (statistically) independent, then\\np(y | x) = p(y)\\np(x | y) = p(x)\\nVX,Y [x + y] = VX[x] + VY [y]\\nCovX,Y [x, y] = 0\\nThe last point may not hold in converse, i.e., two random variables can\\nhave covariance zero but are not statistically independent. To understand\\nwhy , recall that covariance measures only linear dependence. Therefore,\\nrandom variables that are nonlinearly dependent could have covariance\\nzero.\\nExample 6.5\\nConsider a random variable X with zero mean ( EX[x] = 0 ) and also\\nEX[x3] = 0 . Let y = x2 (hence, Y is dependent on X) and consider the\\ncovariance (6.36) between X and Y . But this gives\\nCov[x, y] = E[xy] − E[x]E[y] = E[x3] = 0 . (6.54)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8713a2a6-e8ea-4bbf-8bac-bde2644b5c0c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 200, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Summary Statistics and Independence 195\\nIn machine learning, we often consider problems that can be mod-\\neled as independent and identically distributed (i.i.d.) random variables, independent and\\nidentically\\ndistributed\\ni.i.d.\\nX1, . . . , XN. For more than two random variables, the word “indepen-\\ndent” (Definition 6.10) usually refers to mutually independent random\\nvariables, where all subsets are independent (see Pollard (2002, chap-\\nter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically\\ndistributed” means that all the random variables are from the same distri-\\nbution.\\nAnother concept that is important in machine learning is conditional\\nindependence.\\nDefinition 6.11 (Conditional Independence). Two random variables X\\nand Y are conditionally independent given Z if and only if conditionally\\nindependent\\np(x, y | z) = p(x | z)p(y | z) for all z ∈ Z , (6.55)\\nwhere Z is the set of states of random variable Z. We write X ⊥ ⊥Y | Z to\\ndenote that X is conditionally independent of Y given Z.\\nDefinition 6.11 requires that the relation in (6.55) must hold true for\\nevery value of z. The interpretation of (6.55) can be understood as “given\\nknowledge about z, the distribution of x and y factorizes”. Independence\\ncan be cast as a special case of conditional independence if we writeX ⊥ ⊥\\nY | ∅. By using the product rule of probability (6.22), we can expand the\\nleft-hand side of (6.55) to obtain\\np(x, y | z) = p(x | y, z)p(y | z) . (6.56)\\nBy comparing the right-hand side of (6.55) with (6.56), we see thatp(y | z)\\nappears in both of them so that\\np(x | y, z) = p(x | z) . (6.57)\\nEquation (6.57) provides an alternative definition of conditional indepen-\\ndence, i.e., X ⊥ ⊥Y | Z. This alternative presentation provides the inter-\\npretation “given that we knowz, knowledge about y does not change our\\nknowledge of x”.\\n6.4.6 Inner Products of Random Variables\\nRecall the definition of inner products from Section 3.2. We can define an Inner products\\nbetween\\nmultivariate random\\nvariables can be\\ntreated in a similar\\nfashion\\ninner product between random variables, which we briefly describe in this\\nsection. If we have two uncorrelated random variables X, Y , then\\nV[x + y] = V[x] + V[y] . (6.58)\\nSince variances are measured in squared units, this looks very much like\\nthe Pythagorean theorem for right triangles c2 = a2 + b2.\\nIn the following, we see whether we can find a geometric interpreta-\\ntion of the variance relation of uncorrelated random variables in (6.58).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='478e8817-7b98-4f8d-ace9-f5640fc0a52a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 201, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='196 Probability and Distributions\\nFigure 6.6\\nGeometry of\\nrandom variables. If\\nrandom variables X\\nand Y are\\nuncorrelated, they\\nare orthogonal\\nvectors in a\\ncorresponding\\nvector space, and\\nthe Pythagorean\\ntheorem applies.\\n√\\nvar[y]\\n√\\nvar[x]\\n√\\nvar[x + y] =\\n√\\nvar[x] + var[y]\\na\\nc\\nb\\nRandom variables can be considered vectors in a vector space, and we\\ncan define inner products to obtain geometric properties of random vari-\\nables (Eaton, 2007). If we define\\n⟨X, Y ⟩ := Cov[x, y] (6.59)\\nfor zero mean random variablesX and Y , we obtain an inner product. We\\nsee that the covariance is symmetric, positive definite, and linear in eitherCov[x, x] = 0 ⇐ ⇒\\nx = 0 argument. The length of a random variable is\\nCov[αx + z, y] =\\nα Cov[x, y] +\\nCov[z, y] for α ∈ R. ∥X∥ =\\nq\\nCov[x, x] =\\nq\\nV[x] = σ[x] , (6.60)\\ni.e., its standard deviation. The “longer” the random variable, the more\\nuncertain it is; and a random variable with length 0 is deterministic.\\nIf we look at the angle θ between two random variables X, Y , we get\\ncos θ = ⟨X, Y ⟩\\n∥X∥ ∥ Y ∥ = Cov[x, y]p\\nV[x]V[y] , (6.61)\\nwhich is the correlation (Definition 6.8) between the two random vari-\\nables. This means that we can think of correlation as the cosine of the\\nangle between two random variables when we consider them geometri-\\ncally . We know from Definition 3.7 thatX ⊥ Y ⇐ ⇒ ⟨X, Y ⟩ = 0. In our\\ncase, this means that X and Y are orthogonal if and only ifCov[x, y] = 0,\\ni.e., they are uncorrelated. Figure 6.6 illustrates this relationship.\\nRemark. While it is tempting to use the Euclidean distance (constructed\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5f85384c-1a34-4e61-a385-77b3e470958e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 202, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Gaussian Distribution 197\\nFigure 6.7\\nGaussian\\ndistribution of two\\nrandom variables x1\\nand x2.\\nx1\\n−1 0 1\\nx2\\n−5.0−2.50.02.55.07.5\\np(x1, x\\n2)\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\nfrom the preceding definition of inner products) to compare probability\\ndistributions, it is unfortunately not the best way to obtain distances be-\\ntween distributions. Recall that the probability mass (or density) is posi-\\ntive and needs to add up to 1. These constraints mean that distributions\\nlive on something called a statistical manifold. The study of this space of\\nprobability distributions is called information geometry . Computing dis-\\ntances between distributions are often done using Kullback-Leibler diver-\\ngence, which is a generalization of distances that account for properties of\\nthe statistical manifold. Just like the Euclidean distance is a special case of\\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\\ntwo more general classes of divergences called Bregman divergences and\\nf-divergences. The study of divergences is beyond the scope of this book,\\nand we refer for more details to the recent book by Amari (2016), one of\\nthe founders of the field of information geometry . ♢\\n6.5 Gaussian Distribution\\nThe Gaussian distribution is the most well-studied probability distribution\\nfor continuous-valued random variables. It is also referred to as thenormal normal distribution\\ndistribution. Its importance originates from the fact that it has many com- The Gaussian\\ndistribution arises\\nnaturally when we\\nconsider sums of\\nindependent and\\nidentically\\ndistributed random\\nvariables. This is\\nknown as the\\ncentral limit\\ntheorem (Grinstead\\nand Snell, 1997).\\nputationally convenient properties, which we will be discussing in the fol-\\nlowing. In particular, we will use it to define the likelihood and prior for\\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\\ndensity estimation (Chapter 11).\\nThere are many other areas of machine learning that also benefit from\\nusing a Gaussian distribution, for example Gaussian processes, variational\\ninference, and reinforcement learning. It is also widely used in other ap-\\nplication areas such as signal processing (e.g., Kalman filter), control (e.g.,\\nlinear quadratic regulator), and statistics (e.g., hypothesis testing).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='45e3265f-d67e-4758-8f0a-e95eee41e98a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 203, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='198 Probability and Distributions\\nFigure 6.8\\nGaussian\\ndistributions\\noverlaid with 100\\nsamples. (a) One-\\ndimensional case;\\n(b) two-dimensional\\ncase.\\n−5.0 −2.5 0.0 2.5 5.0 7.5\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n p(x)\\nMean\\nSample\\n2σ\\n(a) Univariate (one-dimensional) Gaussian;\\nThe red cross shows the mean and the red\\nline shows the extent of the variance.\\n−1 0 1\\nx1\\n−4\\n−2\\n0\\n2\\n4\\n6\\n8\\nx2\\nMean\\nSample\\n(b) Multivariate (two-dimensional) Gaus-\\nsian, viewed from top. The red cross shows\\nthe mean and the colored lines show the con-\\ntour lines of the density .\\nFor a univariate random variable, the Gaussian distribution has a den-\\nsity that is given by\\np(x | µ, σ2) = 1√\\n2πσ2\\nexp\\n\\x12\\n−(x − µ)2\\n2σ2\\n\\x13\\n. (6.62)\\nThe multivariate Gaussian distribution is fully characterized by a meanmultivariate\\nGaussian\\ndistribution\\nmean vector\\nvector µ and a covariance matrix Σ and defined as\\ncovariance matrix p(x | µ, Σ) = (2π)− D\\n2 |Σ|− 1\\n2 exp\\n\\x00\\n− 1\\n2(x − µ)⊤Σ−1(x − µ)\\n\\x01\\n, (6.63)\\nwhere x ∈ RD. We write p(x) = N\\n\\x00\\nx | µ, Σ\\n\\x01\\nor X ∼ N\\n\\x00\\nµ, Σ\\n\\x01\\n. Fig-Also known as a\\nmultivariate normal\\ndistribution.\\nure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\\ntour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\\nwith corresponding samples. The special case of the Gaussian with zero\\nmean and identity covariance, that is, µ = 0 and Σ = I, is referred to as\\nthe standard normal distribution.standard normal\\ndistribution Gaussians are widely used in statistical estimation and machine learn-\\ning as they have closed-form expressions for marginal and conditional dis-\\ntributions. In Chapter 9, we use these closed-form expressions extensively\\nfor linear regression. A major advantage of modeling with Gaussian ran-\\ndom variables is that variable transformations (Section 6.7) are often not\\nneeded. Since the Gaussian distribution is fully specified by its mean and\\ncovariance, we often can obtain the transformed distribution by applying\\nthe transformation to the mean and covariance of the random variable.\\n6.5.1 Marginals and Conditionals of Gaussians are Gaussians\\nIn the following, we present marginalization and conditioning in the gen-\\neral case of multivariate random variables. If this is confusing at first read-\\ning, the reader is advised to consider two univariate random variables in-\\nstead. Let X and Y be two multivariate random variables, that may have\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99c893ae-d300-42ca-83bf-8e7f85143488', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 204, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Gaussian Distribution 199\\ndifferent dimensions. To consider the effect of applying the sum rule of\\nprobability and the effect of conditioning, we explicitly write the Gaus-\\nsian distribution in terms of the concatenated states [x⊤ y⊤]⊤ so that\\np(x, y) = N\\n\\x12\\x14µx\\nµy\\n\\x15\\n,\\n\\x14Σxx Σxy\\nΣyx Σyy\\n\\x15\\x13\\n, (6.64)\\nwhere Σxx = Cov[ x, x] and Σyy = Cov[ y, y] are the marginal covari-\\nance matrices of x and y, respectively , andΣxy = Cov[x, y] is the cross-\\ncovariance matrix between x and y.\\nThe conditional distribution p(x | y) is also Gaussian (illustrated in Fig-\\nure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\\np(x | y) = N\\n\\x00\\nµx | y, Σx | y\\n\\x01\\n(6.65)\\nµx | y = µx + ΣxyΣ−1\\nyy (y − µy) (6.66)\\nΣx | y = Σxx − ΣxyΣ−1\\nyy Σyx . (6.67)\\nNote that in the computation of the mean in (6.66), the y-value is an\\nobservation and no longer random.\\nRemark. The conditional Gaussian distribution shows up in many places,\\nwhere we are interested in posterior distributions:\\nThe Kalman filter (Kalman, 1960), one of the most central algorithms\\nfor state estimation in signal processing, does nothing but computing\\nGaussian conditionals of joint distributions (Deisenroth and Ohlsson,\\n2011; S¨arkk¨a, 2013).\\nGaussian processes (Rasmussen and Williams, 2006), which are a prac-\\ntical implementation of a distribution over functions. In a Gaussian pro-\\ncess, we make assumptions of joint Gaussianity of random variables. By\\n(Gaussian) conditioning on observed data, we can determine a poste-\\nrior distribution over functions.\\nLatent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\\nphy, 2012), which include probabilistic principal component analysis\\n(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\\ntail in Section 10.7.\\n♢\\nThe marginal distribution p(x) of a joint Gaussian distribution p(x, y)\\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\\n(6.20) and given by\\np(x) =\\nZ\\np(x, y)dy = N\\n\\x00\\nx | µx, Σxx\\n\\x01\\n. (6.68)\\nThe corresponding result holds for p(y), which is obtained by marginaliz-\\ning with respect tox. Intuitively , looking at the joint distribution in (6.64),\\nwe ignore (i.e., integrate out) everything we are not interested in. This is\\nillustrated in Figure 6.9(b).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae998349-0c0f-44a2-a12c-5bbf85028b35', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 205, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='200 Probability and Distributions\\nExample 6.6\\nFigure 6.9\\n(a) Bivariate\\nGaussian;\\n(b) marginal of a\\njoint Gaussian\\ndistribution is\\nGaussian; (c) the\\nconditional\\ndistribution of a\\nGaussian is also\\nGaussian.\\n−1 0 1\\nx1\\n−4\\n−2\\n0\\n2\\n4\\n6\\n8\\nx2\\nx2 =−1\\n(a) Bivariate Gaussian.\\n−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\\nx1\\n0.0\\n0.2\\n0.4\\n0.6\\np(x1)\\nMean\\n2σ\\n(b) Marginal distribution.\\n−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\\nx1\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2 p(x1|x2 =−1)\\nMean\\n2σ (c) Conditional distribution.\\nConsider the bivariate Gaussian distribution (illustrated in Figure 6.9):\\np(x1, x2) = N\\n\\x12\\x140\\n2\\n\\x15\\n,\\n\\x140.3 −1\\n−1 5\\n\\x15\\x13\\n. (6.69)\\nWe can compute the parameters of the univariate Gaussian, conditioned\\non x2 = −1, by applying (6.66) and (6.67) to obtain the mean and vari-\\nance respectively . Numerically , this is\\nµx1 | x2=−1 = 0 + (−1) · 0.2 · (−1 − 2) = 0.6 (6.70)\\nand\\nσ2\\nx1 | x2=−1 = 0.3 − (−1) · 0.2 · (−1) = 0.1 . (6.71)\\nTherefore, the conditional Gaussian is given by\\np(x1 | x2 = −1) = N\\n\\x00\\n0.6, 0.1\\n\\x01\\n. (6.72)\\nThe marginal distribution p(x1), in contrast, can be obtained by apply-\\ning (6.68), which is essentially using the mean and variance of the random\\nvariable x1, giving us\\np(x1) = N\\n\\x00\\n0, 0.3\\n\\x01\\n. (6.73)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a3d9a07-f325-449b-a1cc-0ba7a4896bf7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 206, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Gaussian Distribution 201\\n6.5.2 Product of Gaussian Densities\\nFor linear regression (Chapter 9), we need to compute a Gaussian likeli-\\nhood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\\nWe apply Bayes’ Theorem to compute the posterior, which results in a mul-\\ntiplication of the likelihood and the prior, that is, the multiplication of two\\nGaussian densities. Theproduct of two GaussiansN\\n\\x00\\nx | a, A\\n\\x01\\nN\\n\\x00\\nx | b, B\\n\\x01\\nThe derivation is an\\nexercise at the end\\nof this chapter.\\nis a Gaussian distribution scaled by a c ∈ R, given by c N\\n\\x00\\nx | c, C\\n\\x01\\nwith\\nC = (A−1 + B−1)−1 (6.74)\\nc = C(A−1a + B−1b) (6.75)\\nc = (2π)− D\\n2 |A + B|− 1\\n2 exp\\n\\x00\\n− 1\\n2(a − b)⊤(A + B)−1(a − b)\\n\\x01\\n. (6.76)\\nThe scaling constant c itself can be written in the form of a Gaussian\\ndensity either in a or in b with an “inflated” covariance matrix A + B,\\ni.e., c = N\\n\\x00\\na | b, A + B\\n\\x01\\n= N\\n\\x00\\nb | a, A + B\\n\\x01\\n.\\nRemark. For notation convenience, we will sometimes use N\\n\\x00\\nx | m, S\\n\\x01\\nto describe the functional form of a Gaussian density even if x is not a\\nrandom variable. We have just done this in the preceding demonstration\\nwhen we wrote\\nc = N\\n\\x00\\na | b, A + B\\n\\x01\\n= N\\n\\x00\\nb | a, A + B\\n\\x01\\n. (6.77)\\nHere, neither a nor b are random variables. However, writingc in this way\\nis more compact than (6.76). ♢\\n6.5.3 Sums and Linear Transformations\\nIf X, Y are independent Gaussian random variables (i.e., the joint distri-\\nbution is given as p(x, y) = p(x)p(y)) with p(x) = N\\n\\x00\\nx | µx, Σx\\n\\x01\\nand\\np(y) = N\\n\\x00\\ny | µy, Σy\\n\\x01\\n, then x + y is also Gaussian distributed and given\\nby\\np(x + y) = N\\n\\x00\\nµx + µy, Σx + Σy\\n\\x01\\n. (6.78)\\nKnowing that p(x + y) is Gaussian, the mean and covariance matrix can\\nbe determined immediately using the results from (6.46) through (6.49).\\nThis property will be important when we consider i.i.d. Gaussian noise\\nacting on random variables, as is the case for linear regression (Chap-\\nter 9).\\nExample 6.7\\nSince expectations are linear operations, we can obtain the weighted sum\\nof independent Gaussian random variables\\np(ax + by) = N\\n\\x00\\naµx + bµy, a 2Σx + b2Σy\\n\\x01\\n. (6.79)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a58c5e1-991c-448a-9e84-00f9a61555c4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 207, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='202 Probability and Distributions\\nRemark. A case that will be useful in Chapter 11 is the weighted sum of\\nGaussian densities. This is different from the weighted sum of Gaussian\\nrandom variables. ♢\\nIn Theorem 6.12, the random variable x is from a density that is a\\nmixture of two densities p1(x) and p2(x), weighted by α. The theorem can\\nbe generalized to the multivariate random variable case, since linearity of\\nexpectations holds also for multivariate random variables. However, the\\nidea of a squared random variable needs to be replaced by xx⊤.\\nTheorem 6.12. Consider a mixture of two univariate Gaussian densities\\np(x) = αp1(x) + (1 − α)p2(x) , (6.80)\\nwhere the scalar 0 < α < 1 is the mixture weight, and p1(x) and p2(x) are\\nunivariate Gaussian densities (Equation (6.62)) with different parameters,\\ni.e., (µ1, σ2\\n1) ̸= (µ2, σ2\\n2).\\nThen the mean of the mixture density p(x) is given by the weighted sum\\nof the means of each random variable:\\nE[x] = αµ1 + (1 − α)µ2 . (6.81)\\nThe variance of the mixture density p(x) is given by\\nV[x] =\\n\\x02\\nασ2\\n1 + (1 − α)σ2\\n2\\n\\x03\\n+\\n\\x10\\x02\\nαµ2\\n1 + (1 − α)µ2\\n2\\n\\x03\\n− [αµ1 + (1 − α)µ2]\\n2\\n\\x11\\n.\\n(6.82)\\nProof The mean of the mixture density p(x) is given by the weighted\\nsum of the means of each random variable. We apply the definition of the\\nmean (Definition 6.4), and plug in our mixture (6.80), which yields\\nE[x] =\\nZ ∞\\n−∞\\nxp(x)dx (6.83a)\\n=\\nZ ∞\\n−∞\\n(αxp1(x) + (1 − α)xp2(x)) dx (6.83b)\\n= α\\nZ ∞\\n−∞\\nxp1(x)dx + (1 − α)\\nZ ∞\\n−∞\\nxp2(x)dx (6.83c)\\n= αµ1 + (1 − α)µ2 . (6.83d)\\nTo compute the variance, we can use the raw-score version of the vari-\\nance from (6.44), which requires an expression of the expectation of the\\nsquared random variable. Here we use the definition of an expectation of\\na function (the square) of a random variable (Definition 6.3),\\nE[x2] =\\nZ ∞\\n−∞\\nx2p(x)dx (6.84a)\\n=\\nZ ∞\\n−∞\\n\\x00\\nαx2p1(x) + (1 − α)x2p2(x)\\n\\x01\\ndx (6.84b)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='807b2deb-9746-4bbc-a9d9-bb7620246a9e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 208, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Gaussian Distribution 203\\n= α\\nZ ∞\\n−∞\\nx2p1(x)dx + (1 − α)\\nZ ∞\\n−∞\\nx2p2(x)dx (6.84c)\\n= α(µ2\\n1 + σ2\\n1) + (1 − α)(µ2\\n2 + σ2\\n2) , (6.84d)\\nwhere in the last equality , we again used the raw-score version of the\\nvariance (6.44) giving σ2 = E[x2] − µ2. This is rearranged such that the\\nexpectation of a squared random variable is the sum of the squared mean\\nand the variance.\\nTherefore, the variance is given by subtracting (6.83d) from (6.84d),\\nV[x] = E[x2] − (E[x])2 (6.85a)\\n= α(µ2\\n1 + σ2\\n1) + (1 − α)(µ2\\n2 + σ2\\n2) − (αµ1 + (1 − α)µ2)2 (6.85b)\\n=\\n\\x02\\nασ2\\n1 + (1 − α)σ2\\n2\\n\\x03\\n+\\n\\x10\\x02\\nαµ2\\n1 + (1 − α)µ2\\n2\\n\\x03\\n− [αµ1 + (1 − α)µ2]\\n2\\n\\x11\\n. (6.85c)\\nRemark. The preceding derivation holds for any density , but since the\\nGaussian is fully determined by the mean and variance, the mixture den-\\nsity can be determined in closed form. ♢\\nFor a mixture density , the individual components can be considered\\nto be conditional distributions (conditioned on the component identity).\\nEquation (6.85c) is an example of the conditional variance formula, also\\nknown as the law of total variance, which generally states that for two ran- law of total variance\\ndom variables X and Y it holds that VX[x] = EY [VX[x|y]]+VY [EX[x|y]],\\ni.e., the (total) variance of X is the expected conditional variance plus the\\nvariance of a conditional mean.\\nWe consider in Example 6.17 a bivariate standard Gaussian random\\nvariable X and performed a linear transformation Ax on it. The outcome\\nis a Gaussian random variable with mean zero and covariance AA⊤. Ob-\\nserve that adding a constant vector will change the mean of the distribu-\\ntion, without affecting its variance, that is, the random variable x + µ is\\nGaussian with mean µ and identity covariance. Hence, any linear/affine\\ntransformation of a Gaussian random variable is Gaussian distributed. Any linear/affine\\ntransformation of a\\nGaussian random\\nvariable is also\\nGaussian\\ndistributed.\\nConsider a Gaussian distributed random variable X ∼ N\\n\\x00\\nµ, Σ\\n\\x01\\n. For\\na given matrix A of appropriate shape, let Y be a random variable such\\nthat y = Ax is a transformed version of x. We can compute the mean of\\ny by exploiting that the expectation is a linear operator (6.50) as follows:\\nE[y] = E[Ax] = AE[x] = Aµ . (6.86)\\nSimilarly the variance of y can be found by using (6.51):\\nV[y] = V[Ax] = AV[x]A⊤ = AΣA⊤ . (6.87)\\nThis means that the random variable y is distributed according to\\np(y) = N\\n\\x00\\ny | Aµ, AΣA⊤\\x01\\n. (6.88)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1d1a8837-2603-4970-bc5a-9c47191e2cbf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 209, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='204 Probability and Distributions\\nLet us now consider the reverse transformation: when we know that a\\nrandom variable has a mean that is a linear transformation of another\\nrandom variable. For a given full rank matrixA ∈ RM ×N, where M ⩾ N,\\nlet y ∈ RM be a Gaussian random variable with mean Ax, i.e.,\\np(y) = N\\n\\x00\\ny | Ax, Σ\\n\\x01\\n. (6.89)\\nWhat is the corresponding probability distribution p(x)? If A is invert-\\nible, then we can write x = A−1y and apply the transformation in the\\nprevious paragraph. However, in general A is not invertible, and we use\\nan approach similar to that of the pseudo-inverse (3.57). That is, we pre-\\nmultiply both sides with A⊤ and then invert A⊤A, which is symmetric\\nand positive definite, giving us the relation\\ny = Ax ⇐ ⇒ (A⊤A)−1A⊤y = x . (6.90)\\nHence, x is a linear transformation of y, and we obtain\\np(x) = N\\n\\x00\\nx | (A⊤A)−1A⊤y, (A⊤A)−1A⊤ΣA(A⊤A)−1\\x01\\n. (6.91)\\n6.5.4 Sampling from Multivariate Gaussian Distributions\\nWe will not explain the subtleties of random sampling on a computer, and\\nthe interested reader is referred to Gentle (2004). In the case of a mul-\\ntivariate Gaussian, this process consists of three stages: first, we need a\\nsource of pseudo-random numbers that provide a uniform sample in the\\ninterval [0,1]; second, we use a non-linear transformation such as the\\nBox-M¨uller transform (Devroye, 1986) to obtain a sample from a univari-\\nate Gaussian; and third, we collate a vector of these samples to obtain a\\nsample from a multivariate standard normal N\\n\\x00\\n0, I\\n\\x01\\n.\\nFor a general multivariate Gaussian, that is, where the mean is non\\nzero and the covariance is not the identity matrix, we use the proper-\\nties of linear transformations of a Gaussian random variable. Assume we\\nare interested in generating samples xi, i = 1, . . . , n,from a multivariate\\nGaussian distribution with mean µ and covariance matrix Σ. We wouldTo compute the\\nCholesky\\nfactorization of a\\nmatrix, it is required\\nthat the matrix is\\nsymmetric and\\npositive definite\\n(Section 3.2.3).\\nCovariance matrices\\npossess this\\nproperty .\\nlike to construct the sample from a sampler that provides samples from\\nthe multivariate standard normal N\\n\\x00\\n0, I\\n\\x01\\n.\\nTo obtain samples from a multivariate normal N\\n\\x00\\nµ, Σ\\n\\x01\\n, we can use\\nthe properties of a linear transformation of a Gaussian random variable:\\nIf x ∼ N\\n\\x00\\n0, I\\n\\x01\\n, then y = Ax + µ, where AA⊤ = Σ is Gaussian dis-\\ntributed with mean µ and covariance matrix Σ. One convenient choice of\\nA is to use the Cholesky decomposition (Section 4.3) of the covariance\\nmatrix Σ = AA⊤. The Cholesky decomposition has the benefit that A is\\ntriangular, leading to efficient computation.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a6c94370-345b-45a8-b106-d23127a96a32', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 210, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Conjugacy and the Exponential Family 205\\n6.6 Conjugacy and the Exponential Family\\nMany of the probability distributions “with names” that we find in statis-\\ntics textbooks were discovered to model particular types of phenomena.\\nFor example, we have seen the Gaussian distribution in Section 6.5. The\\ndistributions are also related to each other in complex ways (Leemis and\\nMcQueston, 2008). For a beginner in the field, it can be overwhelming to\\nfigure out which distribution to use. In addition, many of these distribu-\\ntions were discovered at a time that statistics and computation were done “Computers” used to\\nbe a job description.by pencil and paper. It is natural to ask what are meaningful concepts\\nin the computing age (Efron and Hastie, 2016). In the previous section,\\nwe saw that many of the operations required for inference can be conve-\\nniently calculated when the distribution is Gaussian. It is worth recalling\\nat this point the desiderata for manipulating probability distributions in\\nthe machine learning context:\\n1. There is some “closure property” when applying the rules of probability ,\\ne.g., Bayes’ theorem. By closure, we mean that applying a particular\\noperation returns an object of the same type.\\n2. As we collect more data, we do not need more parameters to describe\\nthe distribution.\\n3. Since we are interested in learning from data, we want parameter es-\\ntimation to behave nicely .\\nIt turns out that the class of distributions called the exponential family exponential family\\nprovides the right balance of generality while retaining favorable compu-\\ntation and inference properties. Before we introduce the exponential fam-\\nily , let us see three more members of “named” probability distributions,\\nthe Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\\nple 6.10) distributions.\\nExample 6.8\\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\\ndistributionvariable X with state x ∈ {0, 1}. It is governed by a single continuous pa-\\nrameter µ ∈ [0, 1] that represents the probability of X = 1. The Bernoulli\\ndistribution Ber(µ) is defined as\\np(x | µ) = µx(1 − µ)1−x , x ∈ {0, 1} , (6.92)\\nE[x] = µ , (6.93)\\nV[x] = µ(1 − µ) , (6.94)\\nwhere E[x] and V[x] are the mean and variance of the binary random\\nvariable X.\\nAn example where the Bernoulli distribution can be used is when we\\nare interested in modeling the probability of “heads” when flipping a coin.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff3d329b-59bf-4f47-8a60-ac91ec2f6d10', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 211, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='206 Probability and Distributions\\nFigure 6.10\\nExamples of the\\nBinomial\\ndistribution for\\nµ ∈ {0.1, 0.4, 0.75}\\nand N = 15.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\\nNumber m of observations x = 1 in N = 15 experiments\\n0.0\\n0.1\\n0.2\\n0.3\\np(m)\\nµ = 0.1\\nµ = 0.4\\nµ = 0.75\\nRemark. The rewriting above of the Bernoulli distribution, where we use\\nBoolean variables as numerical 0 or 1 and express them in the exponents,\\nis a trick that is often used in machine learning textbooks. Another oc-\\ncurence of this is when expressing the Multinomial distribution. ♢\\nExample 6.9 (Binomial Distribution)\\nThe Binomial distribution is a generalization of the Bernoulli distributionBinomial\\ndistribution to a distribution over integers (illustrated in Figure 6.10). In particular,\\nthe Binomial can be used to describe the probability of observing m oc-\\ncurrences of X = 1 in a set of N samples from a Bernoulli distribution\\nwhere p(X = 1) = µ ∈ [0, 1]. The Binomial distribution Bin (N, µ) is\\ndefined as\\np(m | N, µ) =\\n \\nN\\nm\\n!\\nµm(1 − µ)N −m , (6.95)\\nE[m] = N µ , (6.96)\\nV[m] = N µ(1 − µ) , (6.97)\\nwhere E[m] and V[m] are the mean and variance of m, respectively .\\nAn example where the Binomial could be used is if we want to describe\\nthe probability of observing m “heads” in N coin-flip experiments if the\\nprobability for observing head in a single experiment is µ.\\nExample 6.10 (Beta Distribution)\\nWe may wish to model a continuous random variable on a finite interval.\\nThe Beta distribution is a distribution over a continuous random variableBeta distribution\\nµ ∈ [0, 1], which is often used to represent the probability for some binary\\nevent (e.g., the parameter governing the Bernoulli distribution). The Beta\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='549dd68b-adeb-442c-b372-04fe38e67505', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 212, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Conjugacy and the Exponential Family 207\\ndistribution Beta(α, β) (illustrated in Figure 6.11) itself is governed by\\ntwo parameters α > 0, β > 0 and is defined as\\np(µ | α, β) = Γ(α + β)\\nΓ(α)Γ(β) µα−1(1 − µ)β−1 (6.98)\\nE[µ] = α\\nα + β , V[µ] = αβ\\n(α + β)2(α + β + 1) (6.99)\\nwhere Γ(·) is the Gamma function defined as\\nΓ(t) :=\\nZ ∞\\n0\\nxt−1 exp(−x)dx, t > 0 . (6.100)\\nΓ(t + 1) = tΓ(t) . (6.101)\\nNote that the fraction of Gamma functions in (6.98) normalizes the Beta\\ndistribution.\\nFigure 6.11\\nExamples of the\\nBeta distribution for\\ndifferent values of α\\nand β.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nµ\\n0\\n2\\n4\\n6\\n8\\n10p(µ|α, β)\\nα = 0.5 = β\\nα = 1 = β\\nα = 2, β= 0.3\\nα = 4, β= 10\\nα = 5, β= 1\\nIntuitively ,α moves probability mass toward 1, whereas β moves prob-\\nability mass toward 0. There are some special cases (Murphy, 2012):\\nFor α = 1 = β, we obtain the uniform distribution U[0, 1].\\nFor α, β < 1, we get a bimodal distribution with spikes at 0 and 1.\\nFor α, β > 1, the distribution is unimodal.\\nFor α, β > 1 and α = β, the distribution is unimodal, symmetric, and\\ncentered in the interval [0, 1], i.e., the mode/mean is at 1\\n2.\\nRemark. There is a whole zoo of distributions with names, and they are\\nrelated in different ways to each other (Leemis and McQueston, 2008).\\nIt is worth keeping in mind that each named distribution is created for a\\nparticular reason, but may have other applications. Knowing the reason\\nbehind the creation of a particular distribution often allows insight into\\nhow to best use it. We introduced the preceding three distributions to be\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e11a9c6-3744-4ce5-a459-4c0db3751c0a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 213, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='208 Probability and Distributions\\nable to illustrate the concepts of conjugacy (Section 6.6.1) and exponen-\\ntial families (Section 6.6.3). ♢\\n6.6.1 Conjugacy\\nAccording to Bayes’ theorem (6.23), the posterior is proportional to the\\nproduct of the prior and the likelihood. The specification of the prior can\\nbe tricky for two reasons: First, the prior should encapsulate our knowl-\\nedge about the problem before we see any data. This is often difficult to\\ndescribe. Second, it is often not possible to compute the posterior distribu-\\ntion analytically . However, there are some priors that are computationally\\nconvenient: conjugate priors.conjugate prior\\nDefinition 6.13 (Conjugate Prior). A prior is conjugate for the likelihoodconjugate\\nfunction if the posterior is of the same form/type as the prior.\\nConjugacy is particularly convenient because we can algebraically cal-\\nculate our posterior distribution by updating the parameters of the prior\\ndistribution.\\nRemark. When considering the geometry of probability distributions, con-\\njugate priors retain the same distance structure as the likelihood (Agarwal\\nand Daum´e III, 2010). ♢\\nTo introduce a concrete example of conjugate priors, we describe in Ex-\\nample 6.11 the Binomial distribution (defined on discrete random vari-\\nables) and the Beta distribution (defined on continuous random vari-\\nables).\\nExample 6.11 (Beta-Binomial Conjugacy)\\nConsider a Binomial random variable x ∼ Bin(N, µ) where\\np(x | N, µ) =\\n \\nN\\nx\\n!\\nµx(1 − µ)N −x , x = 0, 1, . . . , N , (6.102)\\nis the probability of finding x times the outcome “heads” in N coin flips,\\nwhere µ is the probability of a “head”. We place a Beta prior on the pa-\\nrameter µ, that is, µ ∼ Beta(α, β), where\\np(µ | α, β) = Γ(α + β)\\nΓ(α)Γ(β) µα−1(1 − µ)β−1 . (6.103)\\nIf we now observe some outcome x = h, that is, we see h heads in N coin\\nflips, we compute the posterior distribution on µ as\\np(µ | x = h, N, α, β) ∝ p(x | N, µ)p(µ | α, β) (6.104a)\\n∝ µh(1 − µ)(N −h)µα−1(1 − µ)β−1 (6.104b)\\n= µh+α−1(1 − µ)(N −h)+β−1 (6.104c)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5148f08d-0ead-485f-b080-05723e4856b2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 214, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Conjugacy and the Exponential Family 209\\nTable 6.2 Examples\\nof conjugate priors\\nfor common\\nlikelihood functions.\\nLikelihood Conjugate prior Posterior\\nBernoulli Beta Beta\\nBinomial Beta Beta\\nGaussian Gaussian/inverse Gamma Gaussian/inverse Gamma\\nGaussian Gaussian/inverse Wishart Gaussian/inverse Wishart\\nMultinomial Dirichlet Dirichlet\\n∝ Beta(h + α, N − h + β) , (6.104d)\\ni.e., the posterior distribution is a Beta distribution as the prior, i.e., the\\nBeta prior is conjugate for the parameter µ in the Binomial likelihood\\nfunction.\\nIn the following example, we will derive a result that is similar to the\\nBeta-Binomial conjugacy result. Here we will show that the Beta distribu-\\ntion is a conjugate prior for the Bernoulli distribution.\\nExample 6.12 (Beta-Bernoulli Conjugacy)\\nLet x ∈ {0, 1} be distributed according to the Bernoulli distribution with\\nparameter θ ∈ [0, 1], that is, p(x = 1 | θ) = θ. This can also be expressed\\nas p(x | θ) = θx(1 − θ)1−x. Let θ be distributed according to a Beta distri-\\nbution with parameters α, β, that is, p(θ | α, β) ∝ θα−1(1 − θ)β−1.\\nMultiplying the Beta and the Bernoulli distributions, we get\\np(θ | x, α, β) ∝ p(x | θ)p(θ | α, β) (6.105a)\\n= θx(1 − θ)1−xθα−1(1 − θ)β−1 (6.105b)\\n= θα+x−1(1 − θ)β+(1−x)−1 (6.105c)\\n∝ p(θ | α + x, β + (1 − x)) . (6.105d)\\nThe last line is the Beta distribution with parameters (α + x, β + (1− x)).\\nTable 6.2 lists examples for conjugate priors for the parameters of some\\nstandard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is\\nconjugate for the\\nprecision (inverse\\nvariance) in the\\nunivariate Gaussian\\nlikelihood, and the\\nWishart prior is\\nconjugate for the\\nprecision matrix\\n(inverse covariance\\nmatrix) in the\\nmultivariate\\nGaussian likelihood.\\nMultinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found\\nin any statistical text, and are described in Bishop (2006), for example.\\nThe Beta distribution is the conjugate prior for the parameter µ in both\\nthe Binomial and the Bernoulli likelihood. For a Gaussian likelihood func-\\ntion, we can place a conjugate Gaussian prior on the mean. The reason\\nwhy the Gaussian likelihood appears twice in the table is that we need\\nto distinguish the univariate from the multivariate case. In the univariate\\n(scalar) case, the inverse Gamma is the conjugate prior for the variance.\\nIn the multivariate case, we use a conjugate inverse Wishart distribution\\nas a prior on the covariance matrix. The Dirichlet distribution is the conju-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='df99838e-7837-4e4d-80d2-8003c3d9f8e1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 215, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='210 Probability and Distributions\\ngate prior for the multinomial likelihood function. For further details, we\\nrefer to Bishop (2006).\\n6.6.2 Sufficient Statistics\\nRecall that a statistic of a random variable is a deterministic function of\\nthat random variable. For example, if x = [ x1, . . . , xN]⊤ is a vector of\\nunivariate Gaussian random variables, that is, xn ∼ N\\n\\x00\\nµ, σ 2\\x01\\n, then the\\nsample mean ˆµ = 1\\nN (x1 + · · · + xN) is a statistic. Sir Ronald Fisher dis-\\ncovered the notion of sufficient statistics: the idea that there are statisticssufficient statistics\\nthat will contain all available information that can be inferred from data\\ncorresponding to the distribution under consideration. In other words, suf-\\nficient statistics carry all the information needed to make inference about\\nthe population, that is, they are the statistics that are sufficient to repre-\\nsent the distribution.\\nFor a set of distributions parametrized byθ, let X be a random variable\\nwith distribution p(x | θ0) given an unknown θ0. A vector ϕ(x) of statistics\\nis called sufficient statistics for θ0 if they contain all possible informa-\\ntion about θ0. To be more formal about “contain all possible information”,\\nthis means that the probability of x given θ can be factored into a part\\nthat does not depend on θ, and a part that depends on θ only via ϕ(x).\\nThe Fisher-Neyman factorization theorem formalizes this notion, which\\nwe state in Theorem 6.14 without proof.\\nTheorem 6.14 (Fisher-Neyman). [Theorem 6.5 in Lehmann and Casella\\n(1998)] Let X have probability density function p(x | θ). Then the statisticsFisher-Neyman\\ntheorem ϕ(x) are sufficient for θ if and only if p(x | θ) can be written in the form\\np(x | θ) = h(x)gθ(ϕ(x)) , (6.106)\\nwhere h(x) is a distribution independent of θ and gθ captures all the depen-\\ndence on θ via sufficient statistics ϕ(x).\\nIf p(x | θ) does not depend onθ, then ϕ(x) is trivially a sufficient statistic\\nfor any function ϕ. The more interesting case is that p(x | θ) is dependent\\nonly on ϕ(x) and not x itself. In this case, ϕ(x) is a sufficient statistic for\\nθ.\\nIn machine learning, we consider a finite number of samples from a\\ndistribution. One could imagine that for simple distributions (such as the\\nBernoulli in Example 6.8) we only need a small number of samples to\\nestimate the parameters of the distributions. We could also consider the\\nopposite problem: If we have a set of data (a sample from an unknown\\ndistribution), which distribution gives the best fit? A natural question to\\nask is, as we observe more data, do we need more parameters θ to de-\\nscribe the distribution? It turns out that the answer is yes in general, and\\nthis is studied in non-parametric statistics (Wasserman, 2007). A converse\\nquestion is to consider which class of distributions have finite-dimensional\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='284750d2-86f6-48e3-b8df-68049d174b89', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 216, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Conjugacy and the Exponential Family 211\\nsufficient statistics, that is the number of parameters needed to describe\\nthem does not increase arbitrarily . The answer is exponential family dis-\\ntributions, described in the following section.\\n6.6.3 Exponential Family\\nThere are three possible levels of abstraction we can have when con-\\nsidering distributions (of discrete or continuous random variables). At\\nlevel one (the most concrete end of the spectrum), we have a particu-\\nlar named distribution with fixed parameters, for example a univariate\\nGaussian N\\n\\x00\\n0, 1\\n\\x01\\nwith zero mean and unit variance. In machine learning,\\nwe often use the second level of abstraction, that is, we fix the paramet-\\nric form (the univariate Gaussian) and infer the parameters from data. For\\nexample, we assume a univariate GaussianN\\n\\x00\\nµ, σ 2\\x01\\nwith unknown mean\\nµ and unknown variance σ2, and use a maximum likelihood fit to deter-\\nmine the best parameters (µ, σ2). We will see an example of this when\\nconsidering linear regression in Chapter 9. A third level of abstraction is\\nto consider families of distributions, and in this book, we consider the ex-\\nponential family . The univariate Gaussian is an example of a member of\\nthe exponential family . Many of the widely used statistical models, includ-\\ning all the “named” models in Table 6.2, are members of the exponential\\nfamily . They can all be unified into one concept (Brown, 1986).\\nRemark. A brief historical anecdote: Like many concepts in mathemat-\\nics and science, exponential families were independently discovered at\\nthe same time by different researchers. In the years 1935–1936, Edwin\\nPitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in\\nNew York independently showed that the exponential families are the only\\nfamilies that enjoy finite-dimensional sufficient statistics under repeated\\nindependent sampling (Lehmann and Casella, 1998). ♢\\nAn exponential family is a family of probability distributions, parame- exponential family\\nterized by θ ∈ RD, of the form\\np(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) , (6.107)\\nwhere ϕ(x) is the vector of sufficient statistics. In general, any inner prod-\\nuct (Section 3.2) can be used in (6.107), and for concreteness we will use\\nthe standard dot product here (⟨θ, ϕ(x)⟩ = θ⊤ϕ(x)). Note that the form\\nof the exponential family is essentially a particular expression of gθ(ϕ(x))\\nin the Fisher-Neyman theorem (Theorem 6.14).\\nThe factor h(x) can be absorbed into the dot product term by adding\\nanother entry ( log h(x)) to the vector of sufficient statistics ϕ(x), and\\nconstraining the corresponding parameter θ0 = 1. The term A(θ) is the\\nnormalization constant that ensures that the distribution sums up or inte-\\ngrates to one and is called the log-partition function. A good intuitive no- log-partition\\nfunctiontion of exponential families can be obtained by ignoring these two terms\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a940d262-f954-477f-9421-014c640de534', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 217, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='212 Probability and Distributions\\nand considering exponential families as distributions of the form\\np(x | θ) ∝ exp\\n\\x00\\nθ⊤ϕ(x)\\n\\x01\\n. (6.108)\\nFor this form of parametrization, the parameters θ are called the naturalnatural parameters\\nparameters. At first glance, it seems that exponential families are a mun-\\ndane transformation by adding the exponential function to the result of a\\ndot product. However, there are many implications that allow for conve-\\nnient modeling and efficient computation based on the fact that we can\\ncapture information about data in ϕ(x).\\nExample 6.13 (Gaussian as Exponential Family)\\nConsider the univariate Gaussian distributionN\\n\\x00\\nµ, σ 2\\x01\\n. Let ϕ(x) =\\n\\x14 x\\nx2\\n\\x15\\n.\\nThen by using the definition of the exponential family ,\\np(x | θ) ∝ exp(θ1x + θ2x2) . (6.109)\\nSetting\\nθ =\\n\\x14 µ\\nσ2 , − 1\\n2σ2\\n\\x15⊤\\n(6.110)\\nand substituting into (6.109), we obtain\\np(x | θ) ∝ exp\\n\\x12 µx\\nσ2 − x2\\n2σ2\\n\\x13\\n∝ exp\\n\\x12\\n− 1\\n2σ2 (x − µ)2\\n\\x13\\n. (6.111)\\nTherefore, the univariate Gaussian distribution is a member of the expo-\\nnential family with sufficient statistic ϕ(x) =\\n\\x14 x\\nx2\\n\\x15\\n, and natural parame-\\nters given by θ in (6.110).\\nExample 6.14 (Bernoulli as Exponential Family)\\nRecall the Bernoulli distribution from Example 6.8\\np(x | µ) = µx(1 − µ)1−x , x ∈ {0, 1}. (6.112)\\nThis can be written in exponential family form\\np(x | µ) = exp\\n\\x02\\nlog\\n\\x00\\nµx(1 − µ)1−x\\x01\\x03\\n(6.113a)\\n= exp [x log µ + (1 − x) log(1 − µ)] (6.113b)\\n= exp [x log µ − x log(1 − µ) + log(1 − µ)] (6.113c)\\n= exp\\nh\\nx log µ\\n1−µ + log(1 − µ)\\ni\\n. (6.113d)\\nThe last line (6.113d) can be identified as being in exponential family\\nform (6.107) by observing that\\nh(x) = 1 (6.114)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b5203b86-a2f7-4615-872d-ff1f49ae6938', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 218, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Conjugacy and the Exponential Family 213\\nθ = log µ\\n1−µ (6.115)\\nϕ(x) = x (6.116)\\nA(θ) = − log(1 − µ) = log(1 + exp(θ)). (6.117)\\nThe relationship between θ and µ is invertible so that\\nµ = 1\\n1 + exp(−θ) . (6.118)\\nThe relation (6.118) is used to obtain the right equality of (6.117).\\nRemark. The relationship between the original Bernoulli parameterµ and\\nthe natural parameter θ is known as the sigmoid or logistic function. Ob- sigmoid\\nserve that µ ∈ (0, 1) but θ ∈ R, and therefore the sigmoid function\\nsqueezes a real value into the range (0, 1). This property is useful in ma-\\nchine learning, for example it is used in logistic regression (Bishop, 2006,\\nsection 4.3.2), as well as as a nonlinear activation functions in neural net-\\nworks (Goodfellow et al., 2016, chapter 6). ♢\\nIt is often not obvious how to find the parametric form of the conjugate\\ndistribution of a particular distribution (for example, those in Table 6.2).\\nExponential families provide a convenient way to find conjugate pairs of\\ndistributions. Consider the random variable X is a member of the expo-\\nnential family (6.107):\\np(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) . (6.119)\\nEvery member of the exponential family has a conjugate prior (Brown,\\n1986)\\np(θ | γ) = hc(θ) exp\\n\\x12\\x1c\\x14γ1\\nγ2\\n\\x15\\n,\\n\\x14 θ\\n−A(θ)\\n\\x15\\x1d\\n− Ac(γ)\\n\\x13\\n, (6.120)\\nwhere γ =\\n\\x14γ1\\nγ2\\n\\x15\\nhas dimension dim(θ) + 1. The sufficient statistics of\\nthe conjugate prior are\\n\\x14 θ\\n−A(θ)\\n\\x15\\n. By using the knowledge of the general\\nform of conjugate priors for exponential families, we can derive functional\\nforms of conjugate priors corresponding to particular distributions.\\nExample 6.15\\nRecall the exponential family form of the Bernoulli distribution (6.113d)\\np(x | µ) = exp\\n\\x14\\nx log µ\\n1 − µ + log(1 − µ)\\n\\x15\\n. (6.121)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='80200405-13c1-408f-b275-db22efb2d6e5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 219, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='214 Probability and Distributions\\nThe canonical conjugate prior has the form\\np(µ | α, β) = µ\\n1 − µ exp\\n\\x14\\nα log µ\\n1 − µ + (β + α) log(1 − µ) − Ac(γ)\\n\\x15\\n,\\n(6.122)\\nwhere we defined γ := [ α, β + α]⊤ and hc(µ) := µ/(1 − µ). Equa-\\ntion (6.122) then simplifies to\\np(µ | α, β) = exp [(α − 1) logµ + (β − 1) log(1 − µ) − Ac(α, β)] .\\n(6.123)\\nPutting this in non-exponential family form yields\\np(µ | α, β) ∝ µα−1(1 − µ)β−1 , (6.124)\\nwhich we identify as the Beta distribution (6.98). In example 6.12, we\\nassumed that the Beta distribution is the conjugate prior of the Bernoulli\\ndistribution and showed that it was indeed the conjugate prior. In this\\nexample, we derived the form of the Beta distribution by looking at the\\ncanonical conjugate prior of the Bernoulli distribution in exponential fam-\\nily form.\\nAs mentioned in the previous section, the main motivation for expo-\\nnential families is that they have finite-dimensional sufficient statistics.\\nAdditionally , conjugate distributions are easy to write down, and the con-\\njugate distributions also come from an exponential family . From an infer-\\nence perspective, maximum likelihood estimation behaves nicely because\\nempirical estimates of sufficient statistics are optimal estimates of the pop-\\nulation values of sufficient statistics (recall the mean and covariance of a\\nGaussian). From an optimization perspective, the log-likelihood function\\nis concave, allowing for efficient optimization approaches to be applied\\n(Chapter 7).\\n6.7 Change of Variables/Inverse Transform\\nIt may seem that there are very many known distributions, but in reality\\nthe set of distributions for which we have names is quite limited. There-\\nfore, it is often useful to understand how transformed random variables\\nare distributed. For example, assuming that X is a random variable dis-\\ntributed according to the univariate normal distribution N\\n\\x00\\n0, 1\\n\\x01\\n, what is\\nthe distribution of X2? Another example, which is quite common in ma-\\nchine learning, is, given that X1 and X2 are univariate standard normal,\\nwhat is the distribution of 1\\n2(X1 + X2)?\\nOne option to work out the distribution of 1\\n2(X1 + X2) is to calculate\\nthe mean and variance of X1 and X2 and then combine them. As we saw\\nin Section 6.4.4, we can calculate the mean and variance of resulting ran-\\ndom variables when we consider affine transformations of random vari-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98831f52-741c-4c80-b0ed-9c2799d73233', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 220, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.7 Change of Variables/Inverse Transform 215\\nables. However, we may not be able to obtain the functional form of the\\ndistribution under transformations. Furthermore, we may be interested\\nin nonlinear transformations of random variables for which closed-form\\nexpressions are not readily available.\\nRemark (Notation). In this section, we will be explicit about random vari-\\nables and the values they take. Hence, recall that we use capital letters\\nX, Y to denote random variables and small letters x, y to denote the val-\\nues in the target spaceT that the random variables take. We will explicitly\\nwrite pmfs of discrete random variables X as P (X = x). For continuous\\nrandom variables X (Section 6.2.2), the pdf is written asf(x) and the cdf\\nis written as FX(x). ♢\\nWe will look at two approaches for obtaining distributions of transfor-\\nmations of random variables: a direct approach using the definition of a\\ncumulative distribution function and a change-of-variable approach that\\nuses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating\\nfunctions can also\\nbe used to study\\ntransformations of\\nrandom\\nvariables (Casella\\nand Berger, 2002,\\nchapter 2).\\nproach is widely used because it provides a “recipe” for attempting to\\ncompute the resulting distribution due to a transformation. We will ex-\\nplain the techniques for univariate random variables, and will only briefly\\nprovide the results for the general case of multivariate random variables.\\nTransformations of discrete random variables can be understood di-\\nrectly . Suppose that there is a discrete random variableX with pmf P (X =\\nx) (Section 6.2.1), and an invertible function U(x). Consider the trans-\\nformed random variable Y := U(X), with pmf P (Y = y). Then\\nP (Y = y) = P (U(X) = y) transformation of interest (6.125a)\\n= P (X = U −1(y)) inverse (6.125b)\\nwhere we can observe that x = U −1(y). Therefore, for discrete random\\nvariables, transformations directly change the individual events (with the\\nprobabilities appropriately transformed).\\n6.7.1 Distribution Function Technique\\nThe distribution function technique goes back to first principles, and uses\\nthe definition of a cdf FX(x) = P (X ⩽ x) and the fact that its differential\\nis the pdf f(x) (Wasserman, 2004, chapter 2). For a random variable X\\nand a function U, we find the pdf of the random variable Y := U(X) by\\n1. Finding the cdf:\\nFY (y) = P (Y ⩽ y) (6.126)\\n2. Differentiating the cdf FY (y) to get the pdf f(y).\\nf(y) = d\\ndy FY (y) . (6.127)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aaac8b87-4aa7-41e5-95b6-cbeaa5681c29', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 221, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='216 Probability and Distributions\\nWe also need to keep in mind that the domain of the random variable may\\nhave changed due to the transformation by U.\\nExample 6.16\\nLet X be a continuous random variable with probability density function\\non 0 ⩽ x ⩽ 1\\nf(x) = 3x2 . (6.128)\\nWe are interested in finding the pdf of Y = X2.\\nThe function f is an increasing function ofx, and therefore the resulting\\nvalue of y lies in the interval [0, 1]. We obtain\\nFY (y) = P (Y ⩽ y) definition of cdf (6.129a)\\n= P (X2 ⩽ y) transformation of interest (6.129b)\\n= P (X ⩽ y\\n1\\n2 ) inverse (6.129c)\\n= FX(y\\n1\\n2 ) definition of cdf (6.129d)\\n=\\nZ y\\n1\\n2\\n0\\n3t2dt cdf as a definite integral (6.129e)\\n=\\n\\x02\\nt3\\x03t=y\\n1\\n2\\nt=0 result of integration (6.129f)\\n= y\\n3\\n2 , 0 ⩽ y ⩽ 1 . (6.129g)\\nTherefore, the cdf of Y is\\nFY (y) = y\\n3\\n2 (6.130)\\nfor 0 ⩽ y ⩽ 1. To obtain the pdf, we differentiate the cdf\\nf(y) = d\\ndy FY (y) = 3\\n2 y\\n1\\n2 (6.131)\\nfor 0 ⩽ y ⩽ 1.\\nIn Example 6.16, we considered a strictly monotonically increasing func-\\ntion f(x) = 3x2. This means that we could compute an inverse function.Functions that have\\ninverses are called\\nbijective functions\\n(Section 2.7).\\nIn general, we require that the function of interest y = U(x) has an in-\\nverse x = U −1(y). A useful result can be obtained by considering the cu-\\nmulative distribution function FX(x) of a random variable X, and using\\nit as the transformation U(x). This leads to the following theorem.\\nTheorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let X be a\\ncontinuous random variable with a strictly monotonic cumulative distribu-\\ntion function FX(x). Then the random variable Y defined as\\nY := FX(X) (6.132)\\nhas a uniform distribution.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97b1106a-20b8-43cd-bc6a-57af190ebf64', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 222, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.7 Change of Variables/Inverse Transform 217\\nTheorem 6.15 is known as the probability integral transform , and it is probability integral\\ntransformused to derive algorithms for sampling from distributions by transforming\\nthe result of sampling from a uniform random variable (Bishop, 2006).\\nThe algorithm works by first generating a sample from a uniform distribu-\\ntion, then transforming it by the inverse cdf (assuming this is available)\\nto obtain a sample from the desired distribution. The probability integral\\ntransform is also used for hypothesis testing whether a sample comes from\\na particular distribution (Lehmann and Romano, 2005). The idea that the\\noutput of a cdf gives a uniform distribution also forms the basis of copu-\\nlas (Nelsen, 2006).\\n6.7.2 Change of Variables\\nThe distribution function technique in Section 6.7.1 is derived from first\\nprinciples, based on the definitions of cdfs and using properties of in-\\nverses, differentiation, and integration. This argument from first principles\\nrelies on two facts:\\n1. We can transform the cdf of Y into an expression that is a cdf of X.\\n2. We can differentiate the cdf to obtain the pdf.\\nLet us break down the reasoning step by step, with the goal of understand-\\ning the more general change-of-variables approach in Theorem 6.16. Change of variables\\nin probability relies\\non the\\nchange-of-variables\\nmethod in\\ncalculus (Tandra,\\n2014).\\nRemark. The name “change of variables” comes from the idea of chang-\\ning the variable of integration when faced with a difficult integral. For\\nunivariate functions, we use the substitution rule of integration,\\nZ\\nf(g(x))g′(x)dx =\\nZ\\nf(u)du , where u = g(x) . (6.133)\\nThe derivation of this rule is based on the chain rule of calculus (5.32) and\\nby applying twice the fundamental theorem of calculus. The fundamental\\ntheorem of calculus formalizes the fact that integration and differentiation\\nare somehow “inverses” of each other. An intuitive understanding of the\\nrule can be obtained by thinking (loosely) about small changes (differen-\\ntials) to the equation u = g(x), that is by considering ∆u = g′(x)∆x as a\\ndifferential of u = g(x). By substituting u = g(x), the argument inside the\\nintegral on the right-hand side of (6.133) becomesf(g(x)). By pretending\\nthat the term du can be approximated by du ≈ ∆u = g′(x)∆x, and that\\ndx ≈ ∆x, we obtain (6.133). ♢\\nConsider a univariate random variable X, and an invertible function\\nU, which gives us another random variable Y = U(X). We assume that\\nrandom variable X has states x ∈ [a, b]. By the definition of the cdf, we\\nhave\\nFY (y) = P (Y ⩽ y) . (6.134)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f842cbe-0f7f-4a93-9de7-a41b0ef5e5e3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 223, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='218 Probability and Distributions\\nWe are interested in a function U of the random variable\\nP (Y ⩽ y) = P (U(X) ⩽ y) , (6.135)\\nwhere we assume that the function U is invertible. An invertible function\\non an interval is either strictly increasing or strictly decreasing. In the case\\nthat U is strictly increasing, then its inverse U −1 is also strictly increasing.\\nBy applying the inverse U −1 to the arguments of P (U(X) ⩽ y), we obtain\\nP (U(X) ⩽ y) = P (U −1(U(X)) ⩽ U −1(y)) = P (X ⩽ U −1(y)) .\\n(6.136)\\nThe right-most term in (6.136) is an expression of the cdf ofX. Recall the\\ndefinition of the cdf in terms of the pdf\\nP (X ⩽ U −1(y)) =\\nZ U −1(y)\\na\\nf(x)dx . (6.137)\\nNow we have an expression of the cdf of Y in terms of x:\\nFY (y) =\\nZ U −1(y)\\na\\nf(x)dx . (6.138)\\nTo obtain the pdf, we differentiate (6.138) with respect to y:\\nf(y) = d\\ndy Fy(y) = d\\ndy\\nZ U −1(y)\\na\\nf(x)dx . (6.139)\\nNote that the integral on the right-hand side is with respect to x, but we\\nneed an integral with respect to y because we are differentiating with\\nrespect to y. In particular, we use (6.133) to get the substitution\\nZ\\nf(U −1(y))U −1′\\n(y)dy =\\nZ\\nf(x)dx where x = U −1(y) . (6.140)\\nUsing (6.140) on the right-hand side of (6.139) gives us\\nf(y) = d\\ndy\\nZ U −1(y)\\na\\nfx(U −1(y))U −1′\\n(y)dy . (6.141)\\nWe then recall that differentiation is a linear operator and we use the\\nsubscript x to remind ourselves that fx(U −1(y)) is a function of x and not\\ny. Invoking the fundamental theorem of calculus again gives us\\nf(y) = fx(U −1(y)) ·\\n\\x12 d\\ndy U −1(y)\\n\\x13\\n. (6.142)\\nRecall that we assumed thatU is a strictly increasing function. For decreas-\\ning functions, it turns out that we have a negative sign when we follow\\nthe same derivation. We introduce the absolute value of the differential to\\nhave the same expression for both increasing and decreasing U:\\nf(y) = fx(U −1(y)) ·\\n\\x0c\\x0c\\x0c\\x0c\\nd\\ndy U −1(y)\\n\\x0c\\x0c\\x0c\\x0c . (6.143)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f23c31a6-0cd0-47f5-b114-f146cfa44450', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 224, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.7 Change of Variables/Inverse Transform 219\\nThis is called the change-of-variable technique . The term\\n\\x0c\\x0c\\x0c d\\ndy U −1(y)\\n\\x0c\\x0c\\x0c in change-of-variable\\ntechnique(6.143) measures how much a unit volume changes when applying U\\n(see also the definition of the Jacobian in Section 5.3).\\nRemark. In comparison to the discrete case in (6.125b), we have an addi-\\ntional factor\\n\\x0c\\x0c\\x0c d\\ndy U −1(y)\\n\\x0c\\x0c\\x0c. The continuous case requires more care because\\nP (Y = y) = 0 for all y. The probability density function f(y) does not\\nhave a description as a probability of an event involving y. ♢\\nSo far in this section, we have been studying univariate change of vari-\\nables. The case for multivariate random variables is analogous, but com-\\nplicated by fact that the absolute value cannot be used for multivariate\\nfunctions. Instead, we use the determinant of the Jacobian matrix. Recall\\nfrom (5.58) that the Jacobian is a matrix of partial derivatives, and that\\nthe existence of a nonzero determinant shows that we can invert the Ja-\\ncobian. Recall the discussion in Section 4.1 that the determinant arises\\nbecause our differentials (cubes of volume) are transformed into paral-\\nlelepipeds by the Jacobian. Let us summarize preceding the discussion in\\nthe following theorem, which gives us a recipe for multivariate change of\\nvariables.\\nTheorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x) be the value\\nof the probability density of the multivariate continuous random variableX.\\nIf the vector-valued function y = U(x) is differentiable and invertible for\\nall values within the domain of x, then for corresponding values of y, the\\nprobability density of Y = U(X) is given by\\nf(y) = fx(U −1(y)) ·\\n\\x0c\\x0c\\x0c\\x0cdet\\n\\x12 ∂\\n∂y U −1(y)\\n\\x13\\x0c\\x0c\\x0c\\x0c . (6.144)\\nThe theorem looks intimidating at first glance, but the key point is that\\na change of variable of a multivariate random variable follows the pro-\\ncedure of the univariate change of variable. First we need to work out\\nthe inverse transform, and substitute that into the density of x. Then we\\ncalculate the determinant of the Jacobian and multiply the result. The\\nfollowing example illustrates the case of a bivariate random variable.\\nExample 6.17\\nConsider a bivariate random variable X with states x =\\n\\x14x1\\nx2\\n\\x15\\nand proba-\\nbility density function\\nf\\n\\x12\\x14x1\\nx2\\n\\x15\\x13\\n= 1\\n2π exp\\n \\n−1\\n2\\n\\x14x1\\nx2\\n\\x15⊤\\x14x1\\nx2\\n\\x15!\\n. (6.145)\\nWe use the change-of-variable technique from Theorem 6.16 to derive the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd0c7914-08e9-4639-a110-a041a5f274dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 225, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='220 Probability and Distributions\\neffect of a linear transformation (Section 2.7) of the random variable.\\nConsider a matrix A ∈ R2×2 defined as\\nA =\\n\\x14a b\\nc d\\n\\x15\\n. (6.146)\\nWe are interested in finding the probability density function of the trans-\\nformed bivariate random variable Y with states y = Ax.\\nRecall that for change of variables we require the inverse transformation\\nof x as a function of y. Since we consider linear transformations, the\\ninverse transformation is given by the matrix inverse (see Section 2.2.2).\\nFor 2 × 2 matrices, we can explicitly write out the formula, given by\\n\\x14x1\\nx2\\n\\x15\\n= A−1\\n\\x14y1\\ny2\\n\\x15\\n= 1\\nad − bc\\n\\x14 d −b\\n−c a\\n\\x15\\x14 y1\\ny2\\n\\x15\\n. (6.147)\\nObserve that ad − bc is the determinant (Section 4.1) of A. The corre-\\nsponding probability density function is given by\\nf(x) = f(A−1y) = 1\\n2π exp\\n\\x10\\n− 1\\n2 y⊤A−⊤A−1y\\n\\x11\\n. (6.148)\\nThe partial derivative of a matrix times a vector with respect to the vector\\nis the matrix itself (Section 5.5), and therefore\\n∂\\n∂y A−1y = A−1 . (6.149)\\nRecall from Section 4.1 that the determinant of the inverse is the inverse\\nof the determinant so that the determinant of the Jacobian matrix is\\ndet\\n\\x12 ∂\\n∂y A−1y\\n\\x13\\n= 1\\nad − bc . (6.150)\\nWe are now able to apply the change-of-variable formula from Theo-\\nrem 6.16 by multiplying (6.148) with (6.150), which yields\\nf(y) = f(x)\\n\\x0c\\x0c\\x0c\\x0cdet\\n\\x12 ∂\\n∂y A−1y\\n\\x13\\x0c\\x0c\\x0c\\x0c (6.151a)\\n= 1\\n2π exp\\n\\x10\\n− 1\\n2 y⊤A−⊤A−1y\\n\\x11\\n|ad − bc|−1. (6.151b)\\nWhile Example 6.17 is based on a bivariate random variable, which al-\\nlows us to easily compute the matrix inverse, the preceding relation holds\\nfor higher dimensions.\\nRemark. We saw in Section 6.5 that the densityf(x) in (6.148) is actually\\nthe standard Gaussian distribution, and the transformed density f(y) is a\\nbivariate Gaussian with covariance Σ = AA⊤. ♢\\nWe will use the ideas in this chapter to describe probabilistic modeling\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2eeff19d-db62-4bf6-9da8-4915eae21242', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 226, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.8 Further Reading 221\\nin Section 8.4, as well as introduce a graphical language in Section 8.5. We\\nwill see direct machine learning applications of these ideas in Chapters 9\\nand 11.\\n6.8 Further Reading\\nThis chapter is rather terse at times. Grinstead and Snell (1997) and\\nWalpole et al. (2011) provide more relaxed presentations that are suit-\\nable for self-study . Readers interested in more philosophical aspects of\\nprobability should consider Hacking (2001), whereas an approach that\\nis more related to software engineering is presented by Downey (2014).\\nAn overview of exponential families can be found in Barndorff-Nielsen\\n(2014). We will see more about how to use probability distributions to\\nmodel machine learning tasks in Chapter 8. Ironically , the recent surge\\nin interest in neural networks has resulted in a broader appreciation of\\nprobabilistic models. For example, the idea of normalizing flows (Jimenez\\nRezende and Mohamed, 2015) relies on change of variables for transform-\\ning random variables. An overview of methods for variational inference as\\napplied to neural networks is described in chapters 16 to 20 of the book\\nby Goodfellow et al. (2016).\\nWe side stepped a large part of the difficulty in continuous random vari-\\nables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\\n2002), and by assuming without construction that we have real numbers,\\nand ways of defining sets on real numbers as well as their appropriate fre-\\nquency of occurrence. These details do matter, for example, in the specifi-\\ncation of conditional probability p(y | x) for continuous random variables\\nx, y (Proschan and Presnell, 1998). The lazy notation hides the fact that\\nwe want to specify that X = x (which is a set of measure zero). Fur-\\nthermore, we are interested in the probability density function of y. A\\nmore precise notation would have to say Ey[f(y) | σ(x)], where we take\\nthe expectation over y of a test function f conditioned on the σ-algebra of\\nx. A more technical audience interested in the details of probability the-\\nory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,\\n2004; Grimmett and Welsh, 2014), including some very technical discus-\\nsions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel\\nand Doksum, 2006; C ¸inlar, 2011). An alternative way to approach proba-\\nbility is to start with the concept of expectation, and “work backward” to\\nderive the necessary properties of a probability space (Whittle, 2000). As\\nmachine learning allows us to model more intricate distributions on ever\\nmore complex types of data, a developer of probabilistic machine learn-\\ning models would have to understand these more technical aspects. Ma-\\nchine learning texts with a probabilistic modeling focus include the books\\nby MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar-\\nber (2012); Murphy (2012).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e1df819-1364-4c6a-adba-b07154fc3166', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 227, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='222 Probability and Distributions\\nExercises\\n6.1 Consider the following bivariate distribution p(x, y) of two discrete random\\nvariables X and Y .\\nX\\nx1 x2 x3 x4 x5\\nY\\ny3\\ny2\\ny1 0.010.020.030.1 0.1\\n0.050.1 0.050.070.2\\n0.1 0.050.030.050.04\\nCompute:\\na. The marginal distributions p(x) and p(y).\\nb. The conditional distributions p(x|Y = y1) and p(y|X = x3).\\n6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),\\n0.4 N\\n\\x12\\x14\\n10\\n2\\n\\x15\\n,\\n\\x14\\n1 0\\n0 1\\n\\x15\\x13\\n+ 0.6 N\\n\\x12\\x14\\n0\\n0\\n\\x15\\n,\\n\\x14\\n8.4 2 .0\\n2.0 1 .7\\n\\x15\\x13\\n.\\na. Compute the marginal distributions for each dimension.\\nb. Compute the mean, mode and median for each marginal distribution.\\nc. Compute the mean and mode for the two-dimensional distribution.\\n6.3 You have written a computer program that sometimes compiles and some-\\ntimes not (code does not change). You decide to model the apparent stochas-\\nticity (success vs. no success)x of the compiler using a Bernoulli distribution\\nwith parameter µ:\\np(x | µ) = µx(1 − µ)1−x , x ∈ {0, 1} .\\nChoose a conjugate prior for the Bernoulli likelihood and compute the pos-\\nterior distribution p(µ | x1, . . . , xN).\\n6.4 There are two bags. The first bag contains four mangos and two apples; the\\nsecond bag contains four mangos and four apples.\\nWe also have a biased coin, which shows “heads” with probability 0.6 and\\n“tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at\\nrandom from bag 1; otherwise we pick a fruit at random from bag 2.\\nYour friend flips the coin (you cannot see the result), picks a fruit at random\\nfrom the corresponding bag, and presents you a mango.\\nWhat is the probability that the mango was picked from bag 2?\\nHint: Use Bayes’ theorem.\\n6.5 Consider the time-series model\\nxt+1 = Axt + w , w ∼ N\\n\\x00\\n0, Q\\n\\x01\\nyt = Cxt + v , v ∼ N\\n\\x00\\n0, R\\n\\x01\\n,\\nwhere w, v are i.i.d. Gaussian noise variables. Further, assume that p(x0) =\\nN\\n\\x00\\nµ0, Σ0\\n\\x01.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06467ca7-d5ca-4a36-a126-0419b9a07fb3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 228, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 223\\na. What is the form of p(x0, x1, . . . ,xT )? Justify your answer (you do not\\nhave to explicitly compute the joint distribution).\\nb. Assume that p(xt | y1, . . . ,yt) = N\\n\\x00\\nµt, Σt\\n\\x01.\\n1. Compute p(xt+1 | y1, . . . ,yt).\\n2. Compute p(xt+1, yt+1 | y1, . . . ,yt).\\n3. At time t+1, we observe the valueyt+1 = ˆy. Compute the conditional\\ndistribution p(xt+1 | y1, . . . ,yt+1).\\n6.6 Prove the relationship in (6.44), which relates the standard definition of the\\nvariance to the raw-score expression for the variance.\\n6.7 Prove the relationship in (6.45), which relates the pairwise difference be-\\ntween examples in a dataset with the raw-score expression for the variance.\\n6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\\nponential family , see (6.107).\\n6.9 Express the Binomial distribution as an exponential family distribution. Also\\nexpress the Beta distribution is an exponential family distribution. Show that\\nthe product of the Beta and the Binomial distribution is also a member of\\nthe exponential family .\\n6.10 Derive the relationship in Section 6.5.2 in two ways:\\na. By completing the square\\nb. By expressing the Gaussian in its exponential family form\\nThe product of two Gaussians N\\n\\x00\\nx | a, A\\n\\x01\\nN\\n\\x00\\nx | b, B\\n\\x01 is an unnormalized\\nGaussian distribution c N\\n\\x00\\nx | c, C\\n\\x01 with\\nC = (A−1 + B−1)−1\\nc = C(A−1a + B−1b)\\nc = (2π)− D\\n2 | A + B | − 1\\n2 exp\\n\\x00\\n− 1\\n2(a − b)⊤(A + B)−1(a − b)\\n\\x01\\n.\\nNote that the normalizing constantc itself can be considered a (normalized)\\nGaussian distribution either in a or in b with an “inflated” covariance matrix\\nA + B, i.e., c = N\\n\\x00\\na | b, A + B\\n\\x01\\n= N\\n\\x00\\nb | a, A + B\\n\\x01.\\n6.11 Iterated Expectations.\\nConsider two random variablesx, y with joint distribution p(x, y). Show that\\nEX[x] = EY\\n\\x02EX[x | y]\\n\\x03\\n.\\nHere, EX[x | y] denotes the expected value of x under the conditional distri-\\nbution p(x | y).\\n6.12 Manipulation of Gaussian Random Variables.\\nConsider a Gaussian random variable x ∼ N\\n\\x00\\nx | µx, Σx\\n\\x01, where x ∈ RD.\\nFurthermore, we have\\ny = Ax + b + w ,\\nwhere y ∈ RE, A ∈ RE×D, b ∈ RE, and w ∼ N\\n\\x00\\nw | 0, Q\\n\\x01 is indepen-\\ndent Gaussian noise. “Independent” implies that x and w are independent\\nrandom variables and that Q is diagonal.\\na. Write down the likelihood p(y | x).\\nb. The distribution p(y) =\\nR\\np(y | x)p(x)dx is Gaussian. Compute the mean\\nµy and the covariance Σy. Derive your result in detail.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='daffdeff-b816-4be1-b401-879d0610fb95', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 229, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='224 Probability and Distributions\\nc. The random variable y is being transformed according to the measure-\\nment mapping\\nz = Cy + v ,\\nwhere z ∈ RF , C ∈ RF ×E, and v ∼ N\\n\\x00\\nv | 0, R\\n\\x01 is independent Gaus-\\nsian (measurement) noise.\\nWrite down p(z | y).\\nCompute p(z), i.e., the mean µz and the covariance Σz. Derive your\\nresult in detail.\\nd. Now, a value ˆy is measured. Compute the posterior distribution p(x | ˆy).\\nHint for solution: This posterior is also Gaussian, i.e., we need to de-\\ntermine only its mean and covariance matrix. Start by explicitly com-\\nputing the joint Gaussian p(x, y). This also requires us to compute the\\ncross-covariances Covx,y[x, y] and Covy,x[y, x]. Then apply the rules\\nfor Gaussian conditioning.\\n6.13 Probability Integral Transformation\\nGiven a continuous random variable X, with cdf FX(x), show that the ran-\\ndom variable Y := FX(X) is uniformly distributed (Theorem 6.15).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff67a25c-3c25-4e78-8f0b-6fd69f14bbbe', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 230, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7\\nContinuous Optimization\\nSince machine learning algorithms are implemented on a computer, the\\nmathematical formulations are expressed as numerical optimization meth-\\nods. This chapter describes the basic numerical methods for training ma-\\nchine learning models. Training a machine learning model often boils\\ndown to finding a good set of parameters. The notion of “good” is de-\\ntermined by the objective function or the probabilistic model, which we\\nwill see examples of in the second part of this book. Given an objective\\nfunction, finding the best value is done using optimization algorithms. Since we consider\\ndata and models in\\nRD, the\\noptimization\\nproblems we face\\nare continuous\\noptimization\\nproblems, as\\nopposed to\\ncombinatorial\\noptimization\\nproblems for\\ndiscrete variables.\\nThis chapter covers two main branches of continuous optimization (Fig-\\nure 7.1): unconstrained and constrained optimization. We will assume in\\nthis chapter that our objective function is differentiable (see Chapter 5),\\nhence we have access to a gradient at each location in the space to help us\\nfind the optimum value. By convention, most objective functions in ma-\\nchine learning are intended to be minimized, that is, the best value is the\\nminimum value. Intuitively finding the best value is like finding the val-\\nleys of the objective function, and the gradients point us uphill. The idea is\\nto move downhill (opposite to the gradient) and hope to find the deepest\\npoint. For unconstrained optimization, this is the only concept we need,\\nbut there are several design choices, which we discuss in Section 7.1. For\\nconstrained optimization, we need to introduce other concepts to man-\\nage the constraints (Section 7.2). We will also introduce a special class\\nof problems (convex optimization problems in Section 7.3) where we can\\nmake statements about reaching the global optimum.\\nConsider the function in Figure 7.2. The function has aglobal minimum global minimum\\naround x = −4.5, with a function value of approximately −47. Since\\nthe function is “smooth,” the gradients can be used to help find the min-\\nimum by indicating whether we should take a step to the right or left.\\nThis assumes that we are in the correct bowl, as there exists another local local minimum\\nminimum around x = 0.7. Recall that we can solve for all the stationary\\npoints of a function by calculating its derivative and setting it to zero. For Stationary points\\nare the real roots of\\nthe derivative, that\\nis, points that have\\nzero gradient.\\nℓ(x) = x4 + 7x3 + 5x2 − 17x + 3 , (7.1)\\nwe obtain the corresponding gradient as\\ndℓ(x)\\ndx = 4x3 + 21x2 + 10x − 17 . (7.2)\\n225\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31dbb678-2fb5-4948-a3bb-76c7537ee4e5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 231, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='226 Continuous Optimization\\nFigure 7.1 A mind\\nmap of the concepts\\nrelated to\\noptimization, as\\npresented in this\\nchapter. There are\\ntwo main ideas:\\ngradient descent\\nand convex\\noptimization.\\nContinuousoptimization\\nUnconstrainedoptimization\\nConstrainedoptimization\\nGradient descent\\nStepsize\\nMomentum\\nStochasticgradientdescent\\nLagrangemultipliers\\nConvex optimization& duality\\nConvex\\nConvex conjugate\\nLinearprogramming\\nQuadraticprogramming\\nChapter 10Dimension reduc.\\nChapter 11Density estimation\\nChapter 12Classification\\nSince this is a cubic equation, it has in general three solutions when set to\\nzero. In the example, two of them are minimums and one is a maximum\\n(around x = −1.4). To check whether a stationary point is a minimum\\nor maximum, we need to take the derivative a second time and check\\nwhether the second derivative is positive or negative at the stationary\\npoint. In our case, the second derivative is\\nd2ℓ(x)\\ndx2 = 12x2 + 42x + 10 . (7.3)\\nBy substituting our visually estimated values of x = −4.5, −1.4, 0.7, we\\nwill observe that as expected the middle point is a maximum\\n\\x10\\nd2ℓ(x)\\ndx2 < 0\\n\\x11\\nand the other two stationary points are minimums.\\nNote that we have avoided analytically solving for values of x in the\\nprevious discussion, although for low-order polynomials such as the pre-\\nceding we could do so. In general, we are unable to find analytic solu-\\ntions, and hence we need to start at some value, say x0 = −6, and follow\\nthe negative gradient. The negative gradient indicates that we should go\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21ac0065-3e81-4d94-af57-7f95fde4c7fc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 232, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Optimization Using Gradient Descent 227\\nFigure 7.2 Example\\nobjective function.\\nNegative gradients\\nare indicated by\\narrows, and the\\nglobal minimum is\\nindicated by the\\ndashed blue line.\\n−6 −5 −4 −3 −2 −1 0 1 2\\nValue of parameter\\n−60\\n−40\\n−20\\n0\\n20\\n40\\n60\\nObjective\\nx4 + 7x3 + 5x2−17x + 3\\nright, but not how far (this is called the step-size). Furthermore, if we According to the\\nAbel–Ruffini\\ntheorem, there is in\\ngeneral no algebraic\\nsolution for\\npolynomials of\\ndegree 5 or more\\n(Abel, 1826).\\nhad started at the right side (e.g., x0 = 0 ) the negative gradient would\\nhave led us to the wrong minimum. Figure 7.2 illustrates the fact that for\\nx > −1, the negative gradient points toward the minimum on the right of\\nthe figure, which has a larger objective value.\\nIn Section 7.3, we will learn about a class of functions, called convex\\nfunctions, that do not exhibit this tricky dependency on the starting point\\nof the optimization algorithm. For convex functions, all local minimums\\nare global minimum. It turns out that many machine learning objective For convex functions\\nall local minima are\\nglobal minimum.\\nfunctions are designed such that they are convex, and we will see an ex-\\nample in Chapter 12.\\nThe discussion in this chapter so far was about a one-dimensional func-\\ntion, where we are able to visualize the ideas of gradients, descent direc-\\ntions, and optimal values. In the rest of this chapter we develop the same\\nideas in high dimensions. Unfortunately , we can only visualize the con-\\ncepts in one dimension, but some concepts do not generalize directly to\\nhigher dimensions, therefore some care needs to be taken when reading.\\n7.1 Optimization Using Gradient Descent\\nWe now consider the problem of solving for the minimum of a real-valued\\nfunction\\nmin\\nx\\nf(x) , (7.4)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4ec07ef8-7128-449e-84ae-0f35aeafdded', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 233, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='228 Continuous Optimization\\nwhere f : Rd → R is an objective function that captures the machine\\nlearning problem at hand. We assume that our functionf is differentiable,\\nand we are unable to analytically find a solution in closed form.\\nGradient descent is a first-order optimization algorithm. To find a local\\nminimum of a function using gradient descent, one takes steps propor-\\ntional to the negative of the gradient of the function at the current point.\\nRecall from Section 5.1 that the gradient points in the direction of theWe use the\\nconvention of row\\nvectors for\\ngradients.\\nsteepest ascent. Another useful intuition is to consider the set of lines\\nwhere the function is at a certain value (f(x) = c for some value c ∈ R),\\nwhich are known as the contour lines. The gradient points in a direction\\nthat is orthogonal to the contour lines of the function we wish to optimize.\\nLet us consider multivariate functions. Imagine a surface (described by\\nthe function f(x)) with a ball starting at a particular location x0. When\\nthe ball is released, it will move downhill in the direction of steepest de-\\nscent. Gradient descent exploits the fact thatf(x0) decreases fastest if one\\nmoves from x0 in the direction of the negative gradient −((∇f)(x0))⊤ of\\nf at x0. We assume in this book that the functions are differentiable, and\\nrefer the reader to more general settings in Section 7.4. Then, if\\nx1 = x0 − γ((∇f)(x0))⊤ (7.5)\\nfor a small step-size γ ⩾ 0, then f(x1) ⩽ f(x0). Note that we use the\\ntranspose for the gradient since otherwise the dimensions will not work\\nout.\\nThis observation allows us to define a simple gradient descent algo-\\nrithm: If we want to find a local optimum f(x∗) of a function f : Rn →\\nR, x 7→ f(x), we start with an initial guess x0 of the parameters we wish\\nto optimize and then iterate according to\\nxi+1 = xi − γi((∇f)(xi))⊤ . (7.6)\\nFor suitable step-size γi, the sequence f(x0) ⩾ f(x1) ⩾ . . . converges to\\na local minimum.\\nExample 7.1\\nConsider a quadratic function in two dimensions\\nf\\n\\x12\\x14x1\\nx2\\n\\x15\\x13\\n= 1\\n2\\n\\x14x1\\nx2\\n\\x15⊤\\x142 1\\n1 20\\n\\x15\\x14 x1\\nx2\\n\\x15\\n−\\n\\x145\\n3\\n\\x15⊤\\x14x1\\nx2\\n\\x15\\n(7.7)\\nwith gradient\\n∇f\\n\\x12\\x14x1\\nx2\\n\\x15\\x13\\n=\\n\\x14x1\\nx2\\n\\x15⊤\\x142 1\\n1 20\\n\\x15\\n−\\n\\x145\\n3\\n\\x15⊤\\n. (7.8)\\nStarting at the initial location x0 = [−3, −1]⊤, we iteratively apply (7.6)\\nto obtain a sequence of estimates that converge to the minimum value\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='351b3985-1061-4ee5-b82e-a7ef54e02965', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 234, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Optimization Using Gradient Descent 229\\nFigure 7.3 Gradient\\ndescent on a\\ntwo-dimensional\\nquadratic surface\\n(shown as a\\nheatmap). See\\nExample 7.1 for a\\ndescription.\\n−4 −2 0 2 4\\nx1\\n−2\\n−1\\n0\\n1\\n2\\nx2\\n0.0\\n10.0\\n20.0\\n30.0\\n40.0\\n40.0\\n50.0\\n50.0\\n60.070.0\\n80.0\\n−15\\n0\\n15\\n30\\n45\\n60\\n75\\n90\\n(illustrated in Figure 7.3). We can see (both from the figure and by plug-\\nging x0 into (7.8) with γ = 0.085) that the negative gradient at x0 points\\nnorth and east, leading to x1 = [−1.98, 1.21]⊤. Repeating that argument\\ngives us x2 = [−1.32, −0.42]⊤, and so on.\\nRemark. Gradient descent can be relatively slow close to the minimum:\\nIts asymptotic rate of convergence is inferior to many other methods. Us-\\ning the ball rolling down the hill analogy , when the surface is a long, thin\\nvalley , the problem is poorly conditioned (Trefethen and Bau III, 1997).\\nFor poorly conditioned convex problems, gradient descent increasingly\\n“zigzags” as the gradients point nearly orthogonally to the shortest di-\\nrection to a minimum point; see Figure 7.3. ♢\\n7.1.1 Step-size\\nAs mentioned earlier, choosing a good step-size is important in gradient\\ndescent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\\ncalled the learning\\nrate.\\nstep-size is chosen too large, gradient descent can overshoot, fail to con-\\nverge, or even diverge. We will discuss the use of momentum in the next\\nsection. It is a method that smoothes out erratic behavior of gradient up-\\ndates and dampens oscillations.\\nAdaptive gradient methods rescale the step-size at each iteration, de-\\npending on local properties of the function. There are two simple heuris-\\ntics (Toussaint, 2012):\\nWhen the function value increases after a gradient step, the step-size\\nwas too large. Undo the step and decrease the step-size.\\nWhen the function value decreases the step could have been larger. Try\\nto increase the step-size.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='007367ba-d68e-4808-bf94-f860f00c206f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 235, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='230 Continuous Optimization\\nAlthough the “undo” step seems to be a waste of resources, using this\\nheuristic guarantees monotonic convergence.\\nExample 7.2 (Solving a Linear Equation System)\\nWhen we solve linear equations of the form Ax = b, in practice we solve\\nAx−b = 0 approximately by finding x∗ that minimizes the squared error\\n∥Ax − b∥2 = (Ax − b)⊤(Ax − b) (7.9)\\nif we use the Euclidean norm. The gradient of (7.9) with respect to x is\\n∇x = 2(Ax − b)⊤A . (7.10)\\nWe can use this gradient directly in a gradient descent algorithm. How-\\never, for this particular special case, it turns out that there is an analytic\\nsolution, which can be found by setting the gradient to zero. We will see\\nmore on solving squared error problems in Chapter 9.\\nRemark. When applied to the solution of linear systems of equationsAx =\\nb, gradient descent may converge slowly . The speed of convergence of gra-\\ndient descent is dependent on the condition number κ = σ(A)max\\nσ(A)min\\n, whichcondition number\\nis the ratio of the maximum to the minimum singular value (Section 4.5)\\nof A. The condition number essentially measures the ratio of the most\\ncurved direction versus the least curved direction, which corresponds to\\nour imagery that poorly conditioned problems are long, thin valleys: They\\nare very curved in one direction, but very flat in the other. Instead of di-\\nrectly solving Ax = b, one could instead solve P −1(Ax − b) = 0, where\\nP is called the preconditioner. The goal is to design P −1 such that P −1Apreconditioner\\nhas a better condition number, but at the same time P −1 is easy to com-\\npute. For further information on gradient descent, preconditioning, and\\nconvergence we refer to Boyd and Vandenberghe (2004, chapter 9). ♢\\n7.1.2 Gradient Descent With Momentum\\nAs illustrated in Figure 7.3, the convergence of gradient descent may be\\nvery slow if the curvature of the optimization surface is such that there\\nare regions that are poorly scaled. The curvature is such that the gradient\\ndescent steps hops between the walls of the valley and approaches the\\noptimum in small steps. The proposed tweak to improve convergence is\\nto give gradient descent some memory .Goh (2017) wrote\\nan intuitive blog\\npost on gradient\\ndescent with\\nmomentum.\\nGradient descent with momentum (Rumelhart et al., 1986) is a method\\nthat introduces an additional term to remember what happened in the\\nprevious iteration. This memory dampens oscillations and smoothes out\\nthe gradient updates. Continuing the ball analogy , the momentum term\\nemulates the phenomenon of a heavy ball that is reluctant to change di-\\nrections. The idea is to have a gradient update with memory to implement\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='977a4bb1-d736-420c-b49f-61284e7b541b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 236, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Optimization Using Gradient Descent 231\\na moving average. The momentum-based method remembers the update\\n∆xi at each iteration i and determines the next update as a linear combi-\\nnation of the current and previous gradients\\nxi+1 = xi − γi((∇f)(xi))⊤ + α∆xi (7.11)\\n∆xi = xi − xi−1 = α∆xi−1 − γi−1((∇f)(xi−1))⊤ , (7.12)\\nwhere α ∈ [0, 1]. Sometimes we will only know the gradient approxi-\\nmately . In such cases, the momentum term is useful since it averages out\\ndifferent noisy estimates of the gradient. One particularly useful way to\\nobtain an approximate gradient is by using a stochastic approximation,\\nwhich we discuss next.\\n7.1.3 Stochastic Gradient Descent\\nComputing the gradient can be very time consuming. However, often it is\\npossible to find a “cheap” approximation of the gradient. Approximating\\nthe gradient is still useful as long as it points in roughly the same direction\\nas the true gradient. stochastic gradient\\ndescentStochastic gradient descent (often shortened as SGD) is a stochastic ap-\\nproximation of the gradient descent method for minimizing an objective\\nfunction that is written as a sum of differentiable functions. The word\\nstochastic here refers to the fact that we acknowledge that we do not\\nknow the gradient precisely , but instead only know a noisy approxima-\\ntion to it. By constraining the probability distribution of the approximate\\ngradients, we can still theoretically guarantee that SGD will converge.\\nIn machine learning, given n = 1, . . . , N data points, we often consider\\nobjective functions that are the sum of the losses Ln incurred by each\\nexample n. In mathematical notation, we have the form\\nL(θ) =\\nNX\\nn=1\\nLn(θ) , (7.13)\\nwhere θ is the vector of parameters of interest, i.e., we want to findθ that\\nminimizes L. An example from regression (Chapter 9) is the negative log-\\nlikelihood, which is expressed as a sum over log-likelihoods of individual\\nexamples so that\\nL(θ) = −\\nNX\\nn=1\\nlog p(yn|xn, θ) , (7.14)\\nwhere xn ∈ RD are the training inputs, yn are the training targets, and θ\\nare the parameters of the regression model.\\nStandard gradient descent, as introduced previously , is a “batch” opti-\\nmization method, i.e., optimization is performed using the full training set\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12906e13-d4b2-4b33-8a93-af2de97c5b33', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 237, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='232 Continuous Optimization\\nby updating the vector of parameters according to\\nθi+1 = θi − γi(∇L(θi))⊤ = θi − γi\\nNX\\nn=1\\n(∇Ln(θi))⊤ (7.15)\\nfor a suitable step-size parameter γi. Evaluating the sum gradient may re-\\nquire expensive evaluations of the gradients from all individual functions\\nLn. When the training set is enormous and/or no simple formulas exist,\\nevaluating the sums of gradients becomes very expensive.\\nConsider the termPN\\nn=1(∇Ln(θi)) in (7.15). We can reduce the amount\\nof computation by taking a sum over a smaller set of Ln. In contrast to\\nbatch gradient descent, which uses all Ln for n = 1, . . . , N, we randomly\\nchoose a subset of Ln for mini-batch gradient descent. In the extreme\\ncase, we randomly select only a single Ln to estimate the gradient. The\\nkey insight about why taking a subset of data is sensible is to realize that\\nfor gradient descent to converge, we only require that the gradient is an\\nunbiased estimate of the true gradient. In fact the term PN\\nn=1(∇Ln(θi))\\nin (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\\nthe gradient. Therefore, any other unbiased empirical estimate of the ex-\\npected value, for example using any subsample of the data, would suffice\\nfor convergence of gradient descent.\\nRemark. When the learning rate decreases at an appropriate rate, and sub-\\nject to relatively mild assumptions, stochastic gradient descent converges\\nalmost surely to local minimum (Bottou, 1998). ♢\\nWhy should one consider using an approximate gradient? A major rea-\\nson is practical implementation constraints, such as the size of central\\nprocessing unit (CPU)/graphics processing unit (GPU) memory or limits\\non computational time. We can think of the size of the subset used to esti-\\nmate the gradient in the same way that we thought of the size of a sample\\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\\nwill provide accurate estimates of the gradient, reducing the variance in\\nthe parameter update. Furthermore, large mini-batches take advantage of\\nhighly optimized matrix operations in vectorized implementations of the\\ncost and gradient. The reduction in variance leads to more stable conver-\\ngence, but each gradient calculation will be more expensive.\\nIn contrast, small mini-batches are quick to estimate. If we keep the\\nmini-batch size small, the noise in our gradient estimate will allow us to\\nget out of some bad local optima, which we may otherwise get stuck in.\\nIn machine learning, optimization methods are used for training by min-\\nimizing an objective function on the training data, but the overall goal\\nis to improve generalization performance (Chapter 8). Since the goal in\\nmachine learning does not necessarily need a precise estimate of the min-\\nimum of the objective function, approximate gradients using mini-batch\\napproaches have been widely used. Stochastic gradient descent is very\\neffective in large-scale machine learning problems (Bottou et al., 2018),\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e5cf988-d51e-4f83-9206-0e901ff09f86', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 238, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2 Constrained Optimization and Lagrange Multipliers 233\\nFigure 7.4\\nIllustration of\\nconstrained\\noptimization. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side (indicated\\nby the circle). The\\nbox constraints\\n(−1 ⩽ x ⩽ 1 and\\n−1 ⩽ y ⩽ 1) require\\nthat the optimal\\nsolution is within\\nthe box, resulting in\\nan optimal value\\nindicated by the\\nstar.\\n−3 −2 −1 0 1 2 3\\nx1\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\nx2\\nsuch as training deep neural networks on millions of images (Dean et al.,\\n2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\\net al., 2015), or training of large-scale Gaussian process models (Hensman\\net al., 2013; Gal et al., 2014).\\n7.2 Constrained Optimization and Lagrange Multipliers\\nIn the previous section, we considered the problem of solving for the min-\\nimum of a function\\nmin\\nx\\nf(x) , (7.16)\\nwhere f : RD → R.\\nIn this section, we have additional constraints. That is, for real-valued\\nfunctions gi : RD → R for i = 1 , . . . , m, we consider the constrained\\noptimization problem (see Figure 7.4 for an illustration)\\nmin\\nx\\nf(x) (7.17)\\nsubject to gi(x) ⩽ 0 for all i = 1, . . . , m .\\nIt is worth pointing out that the functions f and gi could be non-convex\\nin general, and we will consider the convex case in the next section.\\nOne obvious, but not very practical, way of converting the constrained\\nproblem (7.17) into an unconstrained one is to use an indicator function\\nJ(x) = f(x) +\\nmX\\ni=1\\n1(gi(x)) , (7.18)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37bd4b65-1bc8-4017-a60d-12a019d3a59f', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 239, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='234 Continuous Optimization\\nwhere 1(z) is an infinite step function\\n1(z) =\\n(\\n0 if z ⩽ 0\\n∞ otherwise . (7.19)\\nThis gives infinite penalty if the constraint is not satisfied, and hence\\nwould provide the same solution. However, this infinite step function is\\nequally difficult to optimize. We can overcome this difficulty by introduc-\\ning Lagrange multipliers. The idea of Lagrange multipliers is to replace theLagrange multiplier\\nstep function with a linear function.\\nWe associate to problem (7.17) the Lagrangian by introducing the La-Lagrangian\\ngrange multipliers λi ⩾ 0 corresponding to each inequality constraint re-\\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\\nL(x, λ) = f(x) +\\nmX\\ni=1\\nλigi(x) (7.20a)\\n= f(x) + λ⊤g(x) , (7.20b)\\nwhere in the last line we have concatenated all constraints gi(x) into a\\nvector g(x), and all the Lagrange multipliers into a vector λ ∈ Rm.\\nWe now introduce the idea of Lagrangian duality . In general, duality\\nin optimization is the idea of converting an optimization problem in one\\nset of variables x (called the primal variables), into another optimization\\nproblem in a different set of variables λ (called the dual variables). We\\nintroduce two different approaches to duality: In this section, we discuss\\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality .\\nDefinition 7.1. The problem in (7.17)\\nmin\\nx\\nf(x) (7.21)\\nsubject to gi(x) ⩽ 0 for all i = 1, . . . , m\\nis known as the primal problem, corresponding to the primal variables x.primal problem\\nThe associated Lagrangian dual problem is given byLagrangian dual\\nproblem\\nmax\\nλ∈Rm\\nD(λ)\\nsubject to λ ⩾ 0 ,\\n(7.22)\\nwhere λ are the dual variables and D(λ) = minx∈Rd L(x, λ).\\nRemark. In the discussion of Definition 7.1, we use two concepts that are\\nalso of independent interest (Boyd and Vandenberghe, 2004).\\nFirst is the minimax inequality, which says that for any function withminimax inequality\\ntwo arguments φ(x, y), the maximin is less than the minimax, i.e.,\\nmax\\ny\\nmin\\nx\\nφ(x, y) ⩽ min\\nx\\nmax\\ny\\nφ(x, y) . (7.23)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='27c320a3-31b6-4c31-b927-5202f0f7fdf6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 240, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2 Constrained Optimization and Lagrange Multipliers 235\\nThis inequality can be proved by considering the inequality\\nFor all x, y min\\nx\\nφ(x, y) ⩽ max\\ny\\nφ(x, y) . (7.24)\\nNote that taking the maximum overy of the left-hand side of (7.24) main-\\ntains the inequality since the inequality is true for all y. Similarly , we can\\ntake the minimum overx of the right-hand side of (7.24) to obtain (7.23).\\nThe second concept is weak duality , which uses (7.23) to show that weak duality\\nprimal values are always greater than or equal to dual values. This is de-\\nscribed in more detail in (7.27). ♢\\nRecall that the difference between J(x) in (7.18) and the Lagrangian\\nin (7.20b) is that we have relaxed the indicator function to a linear func-\\ntion. Therefore, when λ ⩾ 0, the Lagrangian L(x, λ) is a lower bound of\\nJ(x). Hence, the maximum of L(x, λ) with respect to λ is\\nJ(x) = max\\nλ⩾0\\nL(x, λ) . (7.25)\\nRecall that the original problem was minimizing J(x),\\nmin\\nx∈Rd\\nmax\\nλ⩾0\\nL(x, λ) . (7.26)\\nBy the minimax inequality (7.23), it follows that swapping the order of\\nthe minimum and maximum results in a smaller value, i.e.,\\nmin\\nx∈Rd\\nmax\\nλ⩾0\\nL(x, λ) ⩾ max\\nλ⩾0\\nmin\\nx∈Rd\\nL(x, λ) . (7.27)\\nThis is also known as weak duality. Note that the inner part of the right- weak duality\\nhand side is the dual objective function D(λ) and the definition follows.\\nIn contrast to the original optimization problem, which has constraints,\\nminx∈Rd L(x, λ) is an unconstrained optimization problem for a given\\nvalue of λ. If solving minx∈Rd L(x, λ) is easy , then the overall problem is\\neasy to solve. We can see this by observing from (7.20b) that L(x, λ) is\\naffine with respect to λ. Therefore minx∈Rd L(x, λ) is a pointwise min-\\nimum of affine functions of λ, and hence D(λ) is concave even though\\nf(·) and gi(·) may be nonconvex. The outer problem, maximization over\\nλ, is the maximum of a concave function and can be efficiently computed.\\nAssuming f(·) and gi(·) are differentiable, we find the Lagrange dual\\nproblem by differentiating the Lagrangian with respect to x, setting the\\ndifferential to zero, and solving for the optimal value. We will discuss two\\nconcrete examples in Sections 7.3.1 and 7.3.2, where f(·) and gi(·) are\\nconvex.\\nRemark (Equality Constraints). Consider (7.17) with additional equality\\nconstraints\\nmin\\nx\\nf(x)\\nsubject to gi(x) ⩽ 0 for all i = 1, . . . , m\\nhj(x) = 0 for all j = 1, . . . , n .\\n(7.28)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bb7d81f-d21a-431b-be0c-19956797db09', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 241, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='236 Continuous Optimization\\nWe can model equality constraints by replacing them with two inequality\\nconstraints. That is for each equality constrainthj(x) = 0 we equivalently\\nreplace it by two constraints hj(x) ⩽ 0 and hj(x) ⩾ 0. It turns out that\\nthe resulting Lagrange multipliers are then unconstrained.\\nTherefore, we constrain the Lagrange multipliers corresponding to the\\ninequality constraints in (7.28) to be non-negative, and leave the La-\\ngrange multipliers corresponding to the equality constraints unconstrained.\\n♢\\n7.3 Convex Optimization\\nWe focus our attention of a particularly useful class of optimization prob-\\nlems, where we can guarantee global optimality . When f(·) is a convex\\nfunction, and when the constraints involvingg(·) and h(·) are convex sets,\\nthis is called a convex optimization problem. In this setting, we have strongconvex optimization\\nproblem\\nstrong duality\\nduality: The optimal solution of the dual problem is the same as the opti-\\nmal solution of the primal problem. The distinction between convex func-\\ntions and convex sets are often not strictly presented in machine learning\\nliterature, but one can often infer the implied meaning from context.\\nDefinition 7.2. A set C is a convex set if for any x, y ∈ C and for any scalarconvex set\\nθ with 0 ⩽ θ ⩽ 1, we have\\nθx + (1 − θ)y ∈ C . (7.29)\\nFigure 7.5 Example\\nof a convex set.\\n Convex sets are sets such that a straight line connecting any two ele-\\nments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\\nand nonconvex sets, respectively .\\nFigure 7.6 Example\\nof a nonconvex set.\\nConvex functions are functions such that a straight line between any\\ntwo points of the function lie above the function. Figure 7.2 shows a non-\\nconvex function, and Figure 7.3 shows a convex function. Another convex\\nfunction is shown in Figure 7.7.\\nDefinition 7.3. Let function f : RD → R be a function whose domain is a\\nconvex set. The function f is a convex function if for all x, y in the domain\\nconvex function\\nof f, and for any scalar θ with 0 ⩽ θ ⩽ 1, we have\\nf(θx + (1 − θ)y) ⩽ θf(x) + (1 − θ)f(y) . (7.30)\\nRemark. A concave function is the negative of a convex function. ♢\\nconcave function\\nThe constraints involving g(·) and h(·) in (7.28) truncate functions at a\\nscalar value, resulting in sets. Another relation between convex functions\\nand convex sets is to consider the set obtained by “filling in” a convex\\nfunction. A convex function is a bowl-like object, and we imagine pouring\\nwater into it to fill it up. This resulting filled-in set, called the epigraph ofepigraph\\nthe convex function, is a convex set.\\nIf a function f : Rn → R is differentiable, we can specify convexity in\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='836eca60-e9eb-4b47-9b06-6ce82c474aa1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 242, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Convex Optimization 237\\nFigure 7.7 Example\\nof a convex\\nfunction.\\n−3 −2 −1 0 1 2 3\\nx\\n0\\n10\\n20\\n30\\n40y\\ny = 3x2−5x + 2\\nterms of its gradient ∇xf(x) (Section 5.2). A function f(x) is convex if\\nand only if for any two points x, y it holds that\\nf(y) ⩾ f(x) + ∇xf(x)⊤(y − x) . (7.31)\\nIf we further know that a function f(x) is twice differentiable, that is, the\\nHessian (5.147) exists for all values in the domain of x, then the function\\nf(x) is convex if and only if ∇2\\nxf(x) is positive semidefinite (Boyd and\\nVandenberghe, 2004).\\nExample 7.3\\nThe negative entropy f(x) = x log2 x is convex for x > 0. A visualization\\nof the function is shown in Figure 7.8, and we can see that the function is\\nconvex. To illustrate the previous definitions of convexity , let us check the\\ncalculations for two points x = 2 and x = 4. Note that to prove convexity\\nof f(x) we would need to check for all points x ∈ R.\\nRecall Definition 7.3. Consider a point midway between the two points\\n(that is θ = 0.5); then the left-hand side is f(0.5 · 2 + 0.5 · 4) = 3 log2 3 ≈\\n4.75. The right-hand side is 0.5(2 log2 2) + 0.5(4 log2 4) = 1 + 4 = 5. And\\ntherefore the definition is satisfied.\\nSince f(x) is differentiable, we can alternatively use (7.31). Calculating\\nthe derivative of f(x), we obtain\\n∇x(x log2 x) = 1 · log2 x + x · 1\\nx loge 2 = log2 x + 1\\nloge 2 . (7.32)\\nUsing the same two test points x = 2 and x = 4 , the left-hand side of\\n(7.31) is given by f(4) = 8. The right-hand side is\\nf(x) + ∇⊤\\nx (y − x) = f(2) + ∇f(2) · (4 − 2) (7.33a)\\n= 2 + (1 + 1\\nloge 2) · 2 ≈ 6.9 . (7.33b)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a69f87a1-5f9f-46d7-b7cb-8e3f770b43c4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 243, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='238 Continuous Optimization\\nFigure 7.8 The\\nnegative entropy\\nfunction (which is\\nconvex) and its\\ntangent at x = 2.\\n0 1 2 3 4 5\\nx\\n0\\n5\\n10f (x)\\nx log2 x\\ntangent at x = 2\\nWe can check that a function or set is convex from first principles by\\nrecalling the definitions. In practice, we often rely on operations that pre-\\nserve convexity to check that a particular function or set is convex. Al-\\nthough the details are vastly different, this is again the idea of closure\\nthat we introduced in Chapter 2 for vector spaces.\\nExample 7.4\\nA nonnegative weighted sum of convex functions is convex. Observe that\\nif f is a convex function, and α ⩾ 0 is a nonnegative scalar, then the\\nfunction αf is convex. We can see this by multiplyingα to both sides of the\\nequation in Definition 7.3, and recalling that multiplying a nonnegative\\nnumber does not change the inequality .\\nIf f1 and f2 are convex functions, then we have by the definition\\nf1(θx + (1 − θ)y) ⩽ θf1(x) + (1 − θ)f1(y) (7.34)\\nf2(θx + (1 − θ)y) ⩽ θf2(x) + (1 − θ)f2(y) . (7.35)\\nSumming up both sides gives us\\nf1(θx + (1 − θ)y) + f2(θx + (1 − θ)y)\\n⩽ θf1(x) + (1 − θ)f1(y) + θf2(x) + (1 − θ)f2(y) , (7.36)\\nwhere the right-hand side can be rearranged to\\nθ(f1(x) + f2(x)) + (1 − θ)(f1(y) + f2(y)) , (7.37)\\ncompleting the proof that the sum of convex functions is convex.\\nCombining the preceding two facts, we see that αf1(x) + βf2(x) is\\nconvex for α, β ⩾ 0. This closure property can be extended using a sim-\\nilar argument for nonnegative weighted sums of more than two convex\\nfunctions.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='107caa61-faba-4e65-a5ad-56f09354cbee', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 244, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Convex Optimization 239\\nRemark. The inequality in (7.30) is sometimes called Jensen’s inequality. Jensen’s inequality\\nIn fact, a whole class of inequalities for taking nonnegative weighted sums\\nof convex functions are all called Jensen’s inequality . ♢\\nIn summary , a constrained optimization problem is called aconvex opti- convex optimization\\nproblemmization problem if\\nmin\\nx\\nf(x)\\nsubject to gi(x) ⩽ 0 for all i = 1, . . . , m\\nhj(x) = 0 for all j = 1, . . . , n ,\\n(7.38)\\nwhere all functions f(x) and gi(x) are convex functions, and all hj(x) =\\n0 are convex sets. In the following, we will describe two classes of convex\\noptimization problems that are widely used and well understood.\\n7.3.1 Linear Programming\\nConsider the special case when all the preceding functions are linear, i.e.,\\nmin\\nx∈Rd\\nc⊤x (7.39)\\nsubject to Ax ⩽ b ,\\nwhere A ∈ Rm×d and b ∈ Rm. This is known as a linear program. It has d linear program\\nLinear programs are\\none of the most\\nwidely used\\napproaches in\\nindustry .\\nvariables and m linear constraints. The Lagrangian is given by\\nL(x, λ) = c⊤x + λ⊤(Ax − b) , (7.40)\\nwhere λ ∈ Rm is the vector of non-negative Lagrange multipliers. Rear-\\nranging the terms corresponding to x yields\\nL(x, λ) = (c + A⊤λ)⊤x − λ⊤b . (7.41)\\nTaking the derivative of L(x, λ) with respect to x and setting it to zero\\ngives us\\nc + A⊤λ = 0 . (7.42)\\nTherefore, the dual Lagrangian is D(λ) = −λ⊤b. Recall we would like\\nto maximize D(λ). In addition to the constraint due to the derivative of\\nL(x, λ) being zero, we also have the fact that λ ⩾ 0, resulting in the\\nfollowing dual optimization problem It is convention to\\nminimize the primal\\nand maximize the\\ndual.\\nmax\\nλ∈Rm\\n− b⊤λ (7.43)\\nsubject to c + A⊤λ = 0\\nλ ⩾ 0 .\\nThis is also a linear program, but with m variables. We have the choice\\nof solving the primal (7.39) or the dual (7.43) program depending on\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d50950e7-e1d3-4cea-bec1-e2f2be31f349', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 245, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='240 Continuous Optimization\\nwhether m or d is larger. Recall thatd is the number of variables and m is\\nthe number of constraints in the primal linear program.\\nExample 7.5 (Linear Program)\\nConsider the linear program\\nmin\\nx∈R2\\n−\\n\\x145\\n3\\n\\x15⊤\\x14x1\\nx2\\n\\x15\\nsubject to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2 2\\n2 −4\\n−2 1\\n0 −1\\n0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\x14x1\\nx2\\n\\x15\\n⩽\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n33\\n8\\n5\\n−1\\n8\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(7.44)\\nwith two variables. This program is also shown in Figure 7.9. The objective\\nfunction is linear, resulting in linear contour lines. The constraint set in\\nstandard form is translated into the legend. The optimal value must lie in\\nthe shaded (feasible) region, and is indicated by the star.\\nFigure 7.9\\nIllustration of a\\nlinear program. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side. The\\noptimal value given\\nthe constraints are\\nshown by the star.\\n0 2 4 6 8 10 12 14 16\\nx1\\n0\\n2\\n4\\n6\\n8\\n10x2\\n2x2≤ 33−2x1\\n4x2≥ 2x1−8\\nx2≤ 2x1−5\\nx2≥ 1\\nx2≤ 8\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa7a9618-7400-4233-8136-2384055c22b3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 246, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Convex Optimization 241\\n7.3.2 Quadratic Programming\\nConsider the case of a convex quadratic objective function, where the con-\\nstraints are affine, i.e.,\\nmin\\nx∈Rd\\n1\\n2 x⊤Qx + c⊤x (7.45)\\nsubject to Ax ⩽ b ,\\nwhere A ∈ Rm×d, b ∈ Rm, and c ∈ Rd. The square symmetric matrixQ ∈\\nRd×d is positive definite, and therefore the objective function is convex.\\nThis is known as a quadratic program. Observe that it has d variables and\\nm linear constraints.\\nExample 7.6 (Quadratic Program)\\nConsider the quadratic program\\nmin\\nx∈R2\\n1\\n2\\n\\x14x1\\nx2\\n\\x15⊤\\x142 1\\n1 4\\n\\x15\\x14 x1\\nx2\\n\\x15\\n+\\n\\x145\\n3\\n\\x15⊤\\x14x1\\nx2\\n\\x15\\n(7.46)\\nsubject to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0\\n−1 0\\n0 1\\n0 −1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\x14x1\\nx2\\n\\x15\\n⩽\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (7.47)\\nof two variables. The program is also illustrated in Figure 7.4. The objec-\\ntive function is quadratic with a positive semidefinite matrix Q, resulting\\nin elliptical contour lines. The optimal value must lie in the shaded (feasi-\\nble) region, and is indicated by the star.\\nThe Lagrangian is given by\\nL(x, λ) = 1\\n2 x⊤Qx + c⊤x + λ⊤(Ax − b) (7.48a)\\n= 1\\n2 x⊤Qx + (c + A⊤λ)⊤x − λ⊤b , (7.48b)\\nwhere again we have rearranged the terms. Taking the derivative ofL(x, λ)\\nwith respect to x and setting it to zero gives\\nQx + (c + A⊤λ) = 0 . (7.49)\\nSince Q is positive definite and therefore invertible, we get\\nx = −Q−1(c + A⊤λ) . (7.50)\\nSubstituting (7.50) into the primal Lagrangian L(x, λ), we get the dual\\nLagrangian\\nD(λ) = −1\\n2(c + A⊤λ)⊤Q−1(c + A⊤λ) − λ⊤b . (7.51)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44e1eda3-5d77-4106-adf1-6be39c1c84bd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 247, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='242 Continuous Optimization\\nTherefore, the dual optimization problem is given by\\nmax\\nλ∈Rm\\n− 1\\n2(c + A⊤λ)⊤Q−1(c + A⊤λ) − λ⊤b\\nsubject to λ ⩾ 0 .\\n(7.52)\\nWe will see an application of quadratic programming in machine learning\\nin Chapter 12.\\n7.3.3 Legendre–Fenchel Transform and Convex Conjugate\\nLet us revisit the idea of duality from Section 7.2, without considering\\nconstraints. One useful fact about a convex set is that it can be equiva-\\nlently described by its supporting hyperplanes. A hyperplane is called a\\nsupporting hyperplane of a convex set if it intersects the convex set, andsupporting\\nhyperplane the convex set is contained on just one side of it. Recall that we can fill up\\na convex function to obtain the epigraph, which is a convex set. Therefore,\\nwe can also describe convex functions in terms of their supporting hyper-\\nplanes. Furthermore, observe that the supporting hyperplane just touches\\nthe convex function, and is in fact the tangent to the function at that point.\\nAnd recall that the tangent of a function f(x) at a given point x0 is the\\nevaluation of the gradient of that function at that point df(x)\\ndx\\n\\x0c\\x0c\\x0c\\nx=x0\\n. In\\nsummary , because convex sets can be equivalently described by their sup-\\nporting hyperplanes, convex functions can be equivalently described by a\\nfunction of their gradient. TheLegendre transform formalizes this concept.Legendre transform\\nPhysics students are\\noften introduced to\\nthe Legendre\\ntransform as\\nrelating the\\nLagrangian and the\\nHamiltonian in\\nclassical mechanics.\\nWe begin with the most general definition, which unfortunately has a\\ncounter-intuitive form, and look at special cases to relate the definition to\\nthe intuition described in the preceding paragraph. The Legendre-Fenchel\\nLegendre-Fenchel\\ntransform\\ntransform is a transformation (in the sense of a Fourier transform) from\\na convex differentiable function f(x) to a function that depends on the\\ntangents s(x) = ∇xf(x). It is worth stressing that this is a transformation\\nof the function f(·) and not the variable x or the function evaluated at x.\\nThe Legendre-Fenchel transform is also known as theconvex conjugate (forconvex conjugate\\nreasons we will see soon) and is closely related to duality (Hiriart-Urruty\\nand Lemar´echal, 2001, chapter 5).\\nDefinition 7.4. The convex conjugate of a function f : RD → R is aconvex conjugate\\nfunction f ∗ defined by\\nf ∗(s) = sup\\nx∈RD\\n(⟨s, x⟩ − f(x)) . (7.53)\\nNote that the preceding convex conjugate definition does not need the\\nfunction f to be convex nor differentiable. In Definition 7.4, we have used\\na general inner product (Section 3.2) but in the rest of this section we\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='00cc258f-f628-4632-ad48-7ff00b275dc4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 248, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Convex Optimization 243\\nwill consider the standard dot product between finite-dimensional vectors\\n(⟨s, x⟩ = s⊤x) to avoid too many technical details.\\nTo understand Definition 7.4 in a geometric fashion, consider a nice This derivation is\\neasiest to\\nunderstand by\\ndrawing the\\nreasoning as it\\nprogresses.\\nsimple one-dimensional convex and differentiable function, for example\\nf(x) = x2. Note that since we are looking at a one-dimensional problem,\\nhyperplanes reduce to a line. Consider a liney = sx+c. Recall that we are\\nable to describe convex functions by their supporting hyperplanes, so let\\nus try to describe this function f(x) by its supporting lines. Fix the gradi-\\nent of the line s ∈ R and for each point (x0, f(x0)) on the graph of f, find\\nthe minimum value of c such that the line still intersects (x0, f(x0)). Note\\nthat the minimum value of c is the place where a line with slope s “just\\ntouches” the function f(x) = x2. The line passing through (x0, f(x0))\\nwith gradient s is given by\\ny − f(x0) = s(x − x0) . (7.54)\\nThe y-intercept of this line is −sx0 + f(x0). The minimum of c for which\\ny = sx + c intersects with the graph of f is therefore\\ninf\\nx0\\n−sx0 + f(x0) . (7.55)\\nThe preceding convex conjugate is by convention defined to be the nega-\\ntive of this. The reasoning in this paragraph did not rely on the fact that\\nwe chose a one-dimensional convex and differentiable function, and holds\\nfor f : RD → R, which are nonconvex and non-differentiable. The classical\\nLegendre transform\\nis defined on convex\\ndifferentiable\\nfunctions in RD.\\nRemark. Convex differentiable functions such as the examplef(x) = x2 is\\na nice special case, where there is no need for the supremum, and there is\\na one-to-one correspondence between a function and its Legendre trans-\\nform. Let us derive this from first principles. For a convex differentiable\\nfunction, we know that at x0 the tangent touches f(x0) so that\\nf(x0) = sx0 + c . (7.56)\\nRecall that we want to describe the convex function f(x) in terms of its\\ngradient ∇xf(x), and that s = ∇xf(x0). We rearrange to get an expres-\\nsion for −c to obtain\\n− c = sx0 − f(x0) . (7.57)\\nNote that −c changes with x0 and therefore with s, which is why we can\\nthink of it as a function of s, which we call\\nf ∗(s) := sx0 − f(x0) . (7.58)\\nComparing (7.58) with Definition 7.4, we see that (7.58) is a special case\\n(without the supremum). ♢\\nThe conjugate function has nice properties; for example, for convex\\nfunctions, applying the Legendre transform again gets us back to the orig-\\ninal function. In the same way that the slope off(x) is s, the slope off ∗(s)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='932fe79b-d22c-4917-a59b-1e939a65d78e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 249, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='244 Continuous Optimization\\nis x. The following two examples show common uses of convex conjugates\\nin machine learning.\\nExample 7.7 (Convex Conjugates)\\nTo illustrate the application of convex conjugates, consider the quadratic\\nfunction\\nf(y) = λ\\n2 y⊤K −1y (7.59)\\nbased on a positive definite matrix K ∈ Rn×n. We denote the primal\\nvariable to be y ∈ Rn and the dual variable to be α ∈ Rn.\\nApplying Definition 7.4, we obtain the function\\nf ∗(α) = sup\\ny∈Rn\\n⟨y, α⟩ − λ\\n2 y⊤K −1y . (7.60)\\nSince the function is differentiable, we can find the maximum by taking\\nthe derivative and with respect to y setting it to zero.\\n∂\\n\\x02\\n⟨y, α⟩ − λ\\n2 y⊤K −1y\\n\\x03\\n∂y = (α − λK −1y)⊤ (7.61)\\nand hence when the gradient is zero we have y = 1\\nλ Kα. Substituting\\ninto (7.60) yields\\nf ∗(α) = 1\\nλ α⊤Kα − λ\\n2\\n\\x12 1\\nλ Kα\\n\\x13⊤\\nK −1\\n\\x12 1\\nλ Kα\\n\\x13\\n= 1\\n2λ α⊤Kα .\\n(7.62)\\nExample 7.8\\nIn machine learning, we often use sums of functions; for example, the ob-\\njective function of the training set includes a sum of the losses for each ex-\\nample in the training set. In the following, we derive the convex conjugate\\nof a sum of losses ℓ(t), where ℓ : R → R. This also illustrates the appli-\\ncation of the convex conjugate to the vector case. Let L(t) =Pn\\ni=1 ℓi(ti).\\nThen,\\nL∗(z) = sup\\nt∈Rn\\n⟨z, t⟩ −\\nnX\\ni=1\\nℓi(ti) (7.63a)\\n= sup\\nt∈Rn\\nnX\\ni=1\\nziti − ℓi(ti) definition of dot product (7.63b)\\n=\\nnX\\ni=1\\nsup\\nt∈Rn\\nziti − ℓi(ti) (7.63c)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b1526c2-360c-465c-82e3-cb4875c7d798', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 250, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Convex Optimization 245\\n=\\nnX\\ni=1\\nℓ∗\\ni (zi) . definition of conjugate (7.63d)\\nRecall that in Section 7.2 we derived a dual optimization problem using\\nLagrange multipliers. Furthermore, for convex optimization problems we\\nhave strong duality , that is the solutions of the primal and dual problem\\nmatch. The Legendre-Fenchel transform described here also can be used\\nto derive a dual optimization problem. Furthermore, when the function\\nis convex and differentiable, the supremum is unique. To further investi-\\ngate the relation between these two approaches, let us consider a linear\\nequality constrained convex optimization problem.\\nExample 7.9\\nLet f(y) and g(x) be convex functions, andA a real matrix of appropriate\\ndimensions such that Ax = y. Then\\nmin\\nx\\nf(Ax) + g(x) = min\\nAx=y\\nf(y) + g(x). (7.64)\\nBy introducing the Lagrange multiplier u for the constraints Ax = y,\\nmin\\nAx=y\\nf(y) + g(x) = min\\nx,y\\nmax\\nu\\nf(y) + g(x) + (Ax − y)⊤u (7.65a)\\n= max\\nu\\nmin\\nx,y\\nf(y) + g(x) + (Ax − y)⊤u , (7.65b)\\nwhere the last step of swapping max and min is due to the fact that f(y)\\nand g(x) are convex functions. By splitting up the dot product term and\\ncollecting x and y,\\nmax\\nu\\nmin\\nx,y\\nf(y) + g(x) + (Ax − y)⊤u (7.66a)\\n= max\\nu\\n\\x14\\nmin\\ny\\n−y⊤u + f(y)\\n\\x15\\n+\\nh\\nmin\\nx\\n(Ax)⊤u + g(x)\\ni\\n(7.66b)\\n= max\\nu\\n\\x14\\nmin\\ny\\n−y⊤u + f(y)\\n\\x15\\n+\\nh\\nmin\\nx\\nx⊤A⊤u + g(x)\\ni\\n(7.66c)\\nRecall the convex conjugate (Definition 7.4) and the fact that dot prod- For general inner\\nproducts, A⊤ is\\nreplaced by the\\nadjoint A∗.\\nucts are symmetric,\\nmax\\nu\\n\\x14\\nmin\\ny\\n−y⊤u + f(y)\\n\\x15\\n+\\nh\\nmin\\nx\\nx⊤A⊤u + g(x)\\ni\\n(7.67a)\\n= max\\nu\\n−f ∗(u) − g∗(−A⊤u) . (7.67b)\\nTherefore, we have shown that\\nmin\\nx\\nf(Ax) + g(x) = max\\nu\\n−f ∗(u) − g∗(−A⊤u) . (7.68)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a8e27770-3f5d-4fa0-80c1-f15dbc1ef6b4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 251, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='246 Continuous Optimization\\nThe Legendre-Fenchel conjugate turns out to be quite useful for ma-\\nchine learning problems that can be expressed as convex optimization\\nproblems. In particular, for convex loss functions that apply independently\\nto each example, the conjugate loss is a convenient way to derive a dual\\nproblem.\\n7.4 Further Reading\\nContinuous optimization is an active area of research, and we do not try\\nto provide a comprehensive account of recent advances.\\nFrom a gradient descent perspective, there are two major weaknesses\\nwhich each have their own set of literature. The first challenge is the fact\\nthat gradient descent is a first-order algorithm, and does not use infor-\\nmation about the curvature of the surface. When there are long valleys,\\nthe gradient points perpendicularly to the direction of interest. The idea\\nof momentum can be generalized to a general class of acceleration meth-\\nods (Nesterov, 2018). Conjugate gradient methods avoid the issues faced\\nby gradient descent by taking previous directions into account (Shewchuk,\\n1994). Second-order methods such as Newton methods use the Hessian to\\nprovide information about the curvature. Many of the choices for choos-\\ning step-sizes and ideas like momentum arise by considering the curvature\\nof the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\\nmethods such as L-BFGS try to use cheaper computational methods to ap-\\nproximate the Hessian (Nocedal and Wright, 2006). Recently there has\\nbeen interest in other metrics for computing descent directions, result-\\ning in approaches such as mirror descent (Beck and Teboulle, 2003) and\\nnatural gradient (Toussaint, 2012).\\nThe second challenge is to handle non-differentiable functions. Gradi-\\nent methods are not well defined when there are kinks in the function.\\nIn these cases, subgradient methods can be used (Shor, 1985). For fur-\\nther information and algorithms for optimizing non-differentiable func-\\ntions, we refer to the book by Bertsekas (1999). There is a vast amount\\nof literature on different approaches for numerically solving continuous\\noptimization problems, including algorithms for constrained optimization\\nproblems. Good starting points to appreciate this literature are the books\\nby Luenberger (1969) and Bonnans et al. (2006). A recent survey of con-\\ntinuous optimization is provided by Bubeck (2015).Hugo Gonc ¸alves’\\nblog is also a good\\nresource for an\\neasier introduction\\nto Legendre–Fenchel\\ntransforms:\\nhttps://tinyurl.\\ncom/ydaal7hj\\nModern applications of machine learning often mean that the size of\\ndatasets prohibit the use of batch gradient descent, and hence stochastic\\ngradient descent is the current workhorse of large-scale machine learning\\nmethods. Recent surveys of the literature include Hazan (2015) and Bot-\\ntou et al. (2018).\\nFor duality and convex optimization, the book by Boyd and Vanden-\\nberghe (2004) includes lectures and slides online. A more mathematical\\ntreatment is provided by Bertsekas (2009), and recent book by one of\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa34ddc9-e430-4d93-8b2c-2e214a3efbf9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 252, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 247\\nthe key researchers in the area of optimization is Nesterov (2018). Con-\\nvex optimization is based upon convex analysis, and the reader interested\\nin more foundational results about convex functions is referred to Rock-\\nafellar (1970), Hiriart-Urruty and Lemar ´echal (2001), and Borwein and\\nLewis (2006). Legendre–Fenchel transforms are also covered in the afore-\\nmentioned books on convex analysis, but a more beginner-friendly pre-\\nsentation is available at Zia et al. (2009). The role of Legendre–Fenchel\\ntransforms in the analysis of convex optimization algorithms is surveyed\\nin Polyak (2016).\\nExercises\\n7.1 Consider the univariate function\\nf(x) = x3 + 6x2 − 3x − 5.\\nFind its stationary points and indicate whether they are maximum, mini-\\nmum, or saddle points.\\n7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).\\nWrite down the update when we use a mini-batch size of one.\\n7.3 Consider whether the following statements are true or false:\\na. The intersection of any two convex sets is convex.\\nb. The union of any two convex sets is convex.\\nc. The difference of a convex set A from another convex set B is convex.\\n7.4 Consider whether the following statements are true or false:\\na. The sum of any two convex functions is convex.\\nb. The difference of any two convex functions is convex.\\nc. The product of any two convex functions is convex.\\nd. The maximum of any two convex functions is convex.\\n7.5 Express the following optimization problem as a standard linear program in\\nmatrix notation\\nmax\\nx∈R2, ξ ∈R\\np⊤x + ξ\\nsubject to the constraints that ξ ⩾ 0, x0 ⩽ 0 and x1 ⩽ 3.\\n7.6 Consider the linear program illustrated in Figure 7.9,\\nmin\\nx∈R2\\n−\\n\\x14\\n5\\n3\\n\\x15⊤\\x14\\nx1\\nx2\\n\\x15\\nsubject to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2 2\\n2 −4\\n−2 1\\n0 −1\\n0 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\x14\\nx1\\nx2\\n\\x15\\n⩽\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n33\\n8\\n5\\n−1\\n8\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nDerive the dual linear program using Lagrange duality .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e20a5a74-f784-49e0-8888-819ef2d03ade', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 253, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='248 Continuous Optimization\\n7.7 Consider the quadratic program illustrated in Figure 7.4,\\nmin\\nx∈R2\\n1\\n2\\n\\x14\\nx1\\nx2\\n\\x15⊤\\x14\\n2 1\\n1 4\\n\\x15\\x14\\nx1\\nx2\\n\\x15\\n+\\n\\x14\\n5\\n3\\n\\x15⊤\\x14\\nx1\\nx2\\n\\x15\\nsubject to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 0\\n−1 0\\n0 1\\n0 −1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\x14\\nx1\\nx2\\n\\x15\\n⩽\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nDerive the dual quadratic program using Lagrange duality .\\n7.8 Consider the following convex optimization problem\\nmin\\nw∈RD\\n1\\n2 w⊤w\\nsubject to w⊤x ⩾ 1 .\\nDerive the Lagrangian dual by introducing the Lagrange multiplier λ.\\n7.9 Consider the negative entropy of x ∈ RD,\\nf(x) =\\nDX\\nd=1\\nxd log xd .\\nDerive the convex conjugate function f ∗(s), by assuming the standard dot\\nproduct.\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.10 Consider the function\\nf(x) = 1\\n2 x⊤Ax + b⊤x + c ,\\nwhere A is strictly positive definite, which means that it is invertible. Derive\\nthe convex conjugate of f(x).\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.11 The hinge loss (which is the loss used by the support vector machine) is\\ngiven by\\nL(α) = max{0, 1 − α} ,\\nIf we are interested in applying gradient methods such as L-BFGS, and do\\nnot want to resort to subgradient methods, we need to smooth the kink in\\nthe hinge loss. Compute the convex conjugate of the hinge lossL∗(β) where\\nβ is the dual variable. Add a ℓ2 proximal term, and compute the conjugate\\nof the resulting function\\nL∗(β) + γ\\n2 β2 ,\\nwhere γ is a given hyperparameter.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3f3e7eb-d072-4d7b-811b-310a9a7c2e26', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 254, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Part II\\nCentral Machine Learning Problems\\n249\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f34b936b-c823-4e5d-a97a-d65b14629ac6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 255, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15100037-35a0-4176-b151-ce6d758e5c9a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 256, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8\\nWhen Models Meet Data\\nIn the first part of the book, we introduced the mathematics that form\\nthe foundations of many machine learning methods. The hope is that a\\nreader would be able to learn the rudimentary forms of the language of\\nmathematics from the first part, which we will now use to describe and\\ndiscuss machine learning. The second part of the book introduces four\\npillars of machine learning:\\nRegression (Chapter 9)\\nDimensionality reduction (Chapter 10)\\nDensity estimation (Chapter 11)\\nClassification (Chapter 12)\\nThe main aim of this part of the book is to illustrate how the mathematical\\nconcepts introduced in the first part of the book can be used to design\\nmachine learning algorithms that can be used to solve tasks within the\\nremit of the four pillars. We do not intend to introduce advanced machine\\nlearning concepts, but instead to provide a set of practical methods that\\nallow the reader to apply the knowledge they gained from the first part\\nof the book. It also provides a gateway to the wider machine learning\\nliterature for readers already familiar with the mathematics.\\n8.1 Data, Models, and Learning\\nIt is worth at this point, to pause and consider the problem that a ma-\\nchine learning algorithm is designed to solve. As discussed in Chapter 1,\\nthere are three major components of a machine learning system: data,\\nmodels, and learning. The main question of machine learning is “What do\\nwe mean by good models?”. The wordmodel has many subtleties, and we model\\nwill revisit it multiple times in this chapter. It is also not entirely obvious\\nhow to objectively define the word “good”. One of the guiding principles\\nof machine learning is that good models should perform well on unseen\\ndata. This requires us to define some performance metrics, such as accu-\\nracy or distance from ground truth, as well as figuring out ways to do well\\nunder these performance metrics. This chapter covers a few necessary bits\\nand pieces of mathematical and statistical language that are commonly\\n251\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='557bdfc4-b76c-4b77-baa3-30b0cf5af33c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 257, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='252 When Models Meet Data\\nTable 8.1 Example\\ndata from a\\nfictitious human\\nresource database\\nthat is not in a\\nnumerical format.\\nName Gender Degree Postcode Age Annual salary\\nAditya M MSc W21BG 36 89563\\nBob M PhD EC1A1BA 47 123543\\nChlo´e F BEcon SW1A1BH 26 23989\\nDaisuke M BSc SE207AT 68 138769\\nElisabeth F MBA SE10AA 33 113888\\nused to talk about machine learning models. By doing so, we briefly out-\\nline the current best practices for training a model such that the resulting\\npredictor does well on data that we have not yet seen.\\nAs mentioned in Chapter 1, there are two different senses in which we\\nuse the phrase “machine learning algorithm”: training and prediction. We\\nwill describe these ideas in this chapter, as well as the idea of selecting\\namong different models. We will introduce the framework of empirical\\nrisk minimization in Section 8.2, the principle of maximum likelihood in\\nSection 8.3, and the idea of probabilistic models in Section 8.4. We briefly\\noutline a graphical language for specifying probabilistic models in Sec-\\ntion 8.5 and finally discuss model selection in Section 8.6. The rest of this\\nsection expands upon the three main components of machine learning:\\ndata, models and learning.\\n8.1.1 Data as Vectors\\nWe assume that our data can be read by a computer, and represented ade-\\nquately in a numerical format. Data is assumed to be tabular (Figure 8.1),\\nwhere we think of each row of the table as representing a particular in-\\nstance or example, and each column to be a particular feature. In recentData is assumed to\\nbe in a tidy\\nformat (Wickham,\\n2014; Codd, 1990).\\nyears, machine learning has been applied to many types of data that do not\\nobviously come in the tabular numerical format, for example genomic se-\\nquences, text and image contents of a webpage, and social media graphs.\\nWe do not discuss the important and challenging aspects of identifying\\ngood features. Many of these aspects depend on domain expertise and re-\\nquire careful engineering, and, in recent years, they have been put under\\nthe umbrella of data science (Stray, 2016; Adhikari and DeNero, 2018).\\nEven when we have data in tabular format, there are still choices to be\\nmade to obtain a numerical representation. For example, in Table 8.1, the\\ngender column (a categorical variable) may be converted into numbers 0\\nrepresenting “Male” and 1 representing “Female”. Alternatively , the gen-\\nder could be represented by numbers −1, +1, respectively (as shown in\\nTable 8.2). Furthermore, it is often important to use domain knowledge\\nwhen constructing the representation, such as knowing that university\\ndegrees progress from bachelor’s to master’s to PhD or realizing that the\\npostcode provided is not just a string of characters but actually encodes\\nan area in London. In Table 8.2, we converted the data from Table 8.1\\nto a numerical format, and each postcode is represented as two numbers,\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e38945aa-ace4-40e1-aa4f-2902914320f8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 258, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1 Data, Models, and Learning 253\\nTable 8.2 Example\\ndata from a\\nfictitious human\\nresource database\\n(see Table 8.1),\\nconverted to a\\nnumerical format.\\nGender ID Degree Latitude Longitude Age Annual Salary\\n(in degrees) (in degrees) (in thousands)\\n-1 2 51.5073 0.1290 36 89.563\\n-1 3 51.5074 0.1275 47 123.543\\n+1 1 51.5071 0.1278 26 23.989\\n-1 1 51.5075 0.1281 68 138.769\\n+1 2 51.5074 0.1278 33 113.888\\na latitude and longitude. Even numerical data that could potentially be\\ndirectly read into a machine learning algorithm should be carefully con-\\nsidered for units, scaling, and constraints. Without additional information,\\none should shift and scale all columns of the dataset such that they have\\nan empirical mean of 0 and an empirical variance of 1. For the purposes\\nof this book, we assume that a domain expert already converted data ap-\\npropriately , i.e., each inputxn is a D-dimensional vector of real numbers,\\nwhich are calledfeatures, attributes, or covariates. We consider a dataset to feature\\nattribute\\ncovariate\\nbe of the form as illustrated by Table 8.2. Observe that we have dropped\\nthe Name column of Table 8.1 in the new numerical representation. There\\nare two main reasons why this is desirable: (1) we do not expect the iden-\\ntifier (the Name) to be informative for a machine learning task; and (2)\\nwe may wish to anonymize the data to help protect the privacy of the\\nemployees.\\nIn this part of the book, we will use N to denote the number of exam-\\nples in a dataset and index the examples with lowercase n = 1 , . . . , N.\\nWe assume that we are given a set of numerical data, represented as an\\narray of vectors (Table 8.2). Each row is a particular individual xn, often\\nreferred to as an example or data point in machine learning. The subscript example\\ndata pointn refers to the fact that this is the nth example out of a total of N exam-\\nples in the dataset. Each column represents a particular feature of interest\\nabout the example, and we index the features asd = 1, . . . , D. Recall that\\ndata is represented as vectors, which means that each example (each data\\npoint) is a D-dimensional vector. The orientation of the table originates\\nfrom the database community , but for some machine learning algorithms\\n(e.g., in Chapter 10) it is more convenient to represent examples as col-\\numn vectors.\\nLet us consider the problem of predicting annual salary from age, based\\non the data in Table 8.2. This is called a supervised learning problem\\nwhere we have a label yn (the salary) associated with each example xn label\\n(the age). The label yn has various other names, including target, re-\\nsponse variable, and annotation. A dataset is written as a set of example-\\nlabel pairs {(x1, y1), . . . ,(xn, yn), . . . ,(xN , yN)}. The table of examples\\n{x1, . . . ,xN } is often concatenated, and written as X ∈ RN ×D. Fig-\\nure 8.1 illustrates the dataset consisting of the two rightmost columns\\nof Table 8.2, where x = age and y = salary.\\nWe use the concepts introduced in the first part of the book to formalize\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='552acd24-af97-4aa9-a548-208fda0a726e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 259, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='254 When Models Meet Data\\nFigure 8.1 Toy data\\nfor linear regression.\\nTraining data in\\n(xn, yn) pairs from\\nthe rightmost two\\ncolumns of\\nTable 8.2. We are\\ninterested in the\\nsalary of a person\\naged sixty (x = 60)\\nillustrated as a\\nvertical dashed red\\nline, which is not\\npart of the training\\ndata.\\n0 10 20 30 40 50 60 70 80\\nx\\n0\\n25\\n50\\n75\\n100\\n125\\n150y\\n?\\nthe machine learning problems such as that in the previous paragraph.\\nRepresenting data as vectors xn allows us to use concepts from linear al-\\ngebra (introduced in Chapter 2). In many machine learning algorithms,\\nwe need to additionally be able to compare two vectors. As we will see in\\nChapters 9 and 12, computing the similarity or distance between two ex-\\namples allows us to formalize the intuition that examples with similar fea-\\ntures should have similar labels. The comparison of two vectors requires\\nthat we construct a geometry (explained in Chapter 3) and allows us to\\noptimize the resulting learning problem using techniques from Chapter 7.\\nSince we have vector representations of data, we can manipulate data to\\nfind potentially better representations of it. We will discuss finding good\\nrepresentations in two ways: finding lower-dimensional approximations\\nof the original feature vector, and using nonlinear higher-dimensional\\ncombinations of the original feature vector. In Chapter 10, we will see an\\nexample of finding a low-dimensional approximation of the original data\\nspace by finding the principal components. Finding principal components\\nis closely related to concepts of eigenvalue and singular value decomposi-\\ntion as introduced in Chapter 4. For the high-dimensional representation,\\nwe will see an explicit feature map ϕ(·) that allows us to represent in-feature map\\nputs xn using a higher-dimensional representation ϕ(xn). The main mo-\\ntivation for higher-dimensional representations is that we can construct\\nnew features as non-linear combinations of the original features, which in\\nturn may make the learning problem easier. We will discuss the feature\\nmap in Section 9.2 and show how this feature map leads to a kernel inkernel\\nSection 12.4. In recent years, deep learning methods (Goodfellow et al.,\\n2016) have shown promise in using the data itself to learn new good fea-\\ntures and have been very successful in areas, such as computer vision,\\nspeech recognition, and natural language processing. We will not cover\\nneural networks in this part of the book, but the reader is referred to\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='696e3bea-4e00-4956-8fe9-563d158e93dc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 260, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1 Data, Models, and Learning 255\\nFigure 8.2 Example\\nfunction (black solid\\ndiagonal line) and\\nits prediction at\\nx = 60, i.e.,\\nf(60) = 100.\\n0 10 20 30 40 50 60 70 80\\nx\\n0\\n25\\n50\\n75\\n100\\n125\\n150y\\nSection 5.6 for the mathematical description of backpropagation, a key\\nconcept for training neural networks.\\n8.1.2 Models as Functions\\nOnce we have data in an appropriate vector representation, we can get to\\nthe business of constructing a predictive function (known as a predictor). predictor\\nIn Chapter 1, we did not yet have the language to be precise about models.\\nUsing the concepts from the first part of the book, we can now introduce\\nwhat “model” means. We present two major approaches in this book: a\\npredictor as a function, and a predictor as a probabilistic model. We de-\\nscribe the former here and the latter in the next subsection.\\nA predictor is a function that, when given a particular input example\\n(in our case, a vector of features), produces an output. For now, consider\\nthe output to be a single number, i.e., a real-valued scalar output. This can\\nbe written as\\nf : RD → R , (8.1)\\nwhere the input vectorx is D-dimensional (has D features), and the func-\\ntion f then applied to it (written as f(x)) returns a real number. Fig-\\nure 8.2 illustrates a possible function that can be used to compute the\\nvalue of the prediction for input values x.\\nIn this book, we do not consider the general case of all functions, which\\nwould involve the need for functional analysis. Instead, we consider the\\nspecial case of linear functions\\nf(x) = θ⊤x + θ0 (8.2)\\nfor unknown θ and θ0. This restriction means that the contents of Chap-\\nters 2 and 3 suffice for precisely stating the notion of a predictor for\\nthe non-probabilistic (in contrast to the probabilistic view described next)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7ac49c00-213b-4c74-940f-9c83f3ebd378', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 261, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='256 When Models Meet Data\\nFigure 8.3 Example\\nfunction (black solid\\ndiagonal line) and\\nits predictive\\nuncertainty at\\nx = 60 (drawn as a\\nGaussian).\\n0 10 20 30 40 50 60 70 80\\nx\\n0\\n25\\n50\\n75\\n100\\n125\\n150y\\nview of machine learning. Linear functions strike a good balance between\\nthe generality of the problems that can be solved and the amount of back-\\nground mathematics that is needed.\\n8.1.3 Models as Probability Distributions\\nWe often consider data to be noisy observations of some true underlying\\neffect, and hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning the effect of noise. We often would also like to have predictors that\\nexpress some sort of uncertainty , e.g., to quantify the confidence we have\\nabout the value of the prediction for a particular test data point. As we\\nhave seen in Chapter 6, probability theory provides a language for quan-\\ntifying uncertainty . Figure 8.3 illustrates the predictive uncertainty of the\\nfunction as a Gaussian distribution.\\nInstead of considering a predictor as a single function, we could con-\\nsider predictors to be probabilistic models, i.e., models describing the dis-\\ntribution of possible functions. We limit ourselves in this book to the spe-\\ncial case of distributions with finite-dimensional parameters, which allows\\nus to describe probabilistic models without needing stochastic processes\\nand random measures. For this special case, we can think about prob-\\nabilistic models as multivariate probability distributions, which already\\nallow for a rich class of models.\\nWe will introduce how to use concepts from probability (Chapter 6) to\\ndefine machine learning models in Section 8.4, and introduce a graphical\\nlanguage for describing probabilistic models in a compact way in Sec-\\ntion 8.5.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a75d6b3-3b01-4a2d-a683-91d091834ae2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 262, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1 Data, Models, and Learning 257\\n8.1.4 Learning is Finding Parameters\\nThe goal of learning is to find a model and its corresponding parame-\\nters such that the resulting predictor will perform well on unseen data.\\nThere are conceptually three distinct algorithmic phases when discussing\\nmachine learning algorithms:\\n1. Prediction or inference\\n2. Training or parameter estimation\\n3. Hyperparameter tuning or model selection\\nThe prediction phase is when we use a trained predictor on previously un-\\nseen test data. In other words, the parameters and model choice is already\\nfixed and the predictor is applied to new vectors representing new input\\ndata points. As outlined in Chapter 1 and the previous subsection, we will\\nconsider two schools of machine learning in this book, corresponding to\\nwhether the predictor is a function or a probabilistic model. When we\\nhave a probabilistic model (discussed further in Section 8.4) the predic-\\ntion phase is called inference.\\nRemark. Unfortunately , there is no agreed upon naming for the different\\nalgorithmic phases. The word “inference” is sometimes also used to mean\\nparameter estimation of a probabilistic model, and less often may be also\\nused to mean prediction for non-probabilistic models. ♢\\nThe training or parameter estimation phase is when we adjust our pre-\\ndictive model based on training data. We would like to find good predic-\\ntors given training data, and there are two main strategies for doing so:\\nfinding the best predictor based on some measure of quality (sometimes\\ncalled finding a point estimate), or using Bayesian inference. Finding a\\npoint estimate can be applied to both types of predictors, but Bayesian\\ninference requires probabilistic models.\\nFor the non-probabilistic model, we follow the principle ofempirical risk empirical risk\\nminimizationminimization, which we describe in Section 8.2. Empirical risk minimiza-\\ntion directly provides an optimization problem for finding good parame-\\nters. With a statistical model, the principle of maximum likelihood is used maximum likelihood\\nto find a good set of parameters (Section 8.3). We can additionally model\\nthe uncertainty of parameters using a probabilistic model, which we will\\nlook at in more detail in Section 8.4.\\nWe use numerical methods to find good parameters that “fit” the data,\\nand most training methods can be thought of as hill-climbing approaches\\nto find the maximum of an objective, for example the maximum of a likeli-\\nhood. To apply hill-climbing approaches we use the gradients described in The convention in\\noptimization is to\\nminimize objectives.\\nHence, there is often\\nan extra minus sign\\nin machine learning\\nobjectives.\\nChapter 5 and implement numerical optimization approaches from Chap-\\nter 7.\\nAs mentioned in Chapter 1, we are interested in learning a model based\\non data such that it performs well on future data. It is not enough for\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5bcbc383-db8b-4e45-8f20-9902aa5ae0bc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 263, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='258 When Models Meet Data\\nthe model to only fit the training data well, the predictor needs to per-\\nform well on unseen data. We simulate the behavior of our predictor on\\nfuture unseen data using cross-validation (Section 8.2.4). As we will seecross-validation\\nin this chapter, to achieve the goal of performing well on unseen data,\\nwe will need to balance between fitting well on training data and finding\\n“simple” explanations of the phenomenon. This trade-off is achieved us-\\ning regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In\\nphilosophy , this is considered to be neither induction nor deduction, but\\nis called abduction. According to the Stanford Encyclopedia of Philosophy ,abduction\\nabduction is the process of inference to the best explanation (Douven,\\n2017).A good movie title is\\n“AI abduction”. We often need to make high-level modeling decisions about the struc-\\nture of the predictor, such as the number of components to use or the\\nclass of probability distributions to consider. The choice of the number of\\ncomponents is an example of a hyperparameter, and this choice can af-hyperparameter\\nfect the performance of the model significantly . The problem of choosing\\namong different models is called model selection , which we describe inmodel selection\\nSection 8.6. For non-probabilistic models, model selection is often done\\nusing nested cross-validation, which is described in Section 8.6.1. We alsonested\\ncross-validation use model selection to choose hyperparameters of our model.\\nRemark. The distinction between parameters and hyperparameters is some-\\nwhat arbitrary , and is mostly driven by the distinction between what can\\nbe numerically optimized versus what needs to use search techniques.\\nAnother way to consider the distinction is to consider parameters as the\\nexplicit parameters of a probabilistic model, and to consider hyperparam-\\neters (higher-level parameters) as parameters that control the distribution\\nof these explicit parameters. ♢\\nIn the following sections, we will look at three flavors of machine learn-\\ning: empirical risk minimization (Section 8.2), the principle of maximum\\nlikelihood (Section 8.3), and probabilistic modeling (Section 8.4).\\n8.2 Empirical Risk Minimization\\nAfter having all the mathematics under our belt, we are now in a posi-\\ntion to introduce what it means to learn. The “learning” part of machine\\nlearning boils down to estimating parameters based on training data.\\nIn this section, we consider the case of a predictor that is a function,\\nand consider the case of probabilistic models in Section 8.3. We describe\\nthe idea of empirical risk minimization, which was originally popularized\\nby the proposal of the support vector machine (described in Chapter 12).\\nHowever, its general principles are widely applicable and allow us to ask\\nthe question of what is learning without explicitly constructing probabilis-\\ntic models. There are four main design choices, which we will cover in\\ndetail in the following subsections:\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae21f794-c2a4-40a2-8fc0-f9b4f80b157b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 264, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2 Empirical Risk Minimization 259\\nSection 8.2.1 What is the set of functions we allow the predictor to take?\\nSection 8.2.2 How do we measure how well the predictor performs on\\nthe training data?\\nSection 8.2.3 How do we construct predictors from only training data\\nthat performs well on unseen test data?\\nSection 8.2.4 What is the procedure for searching over the space of mod-\\nels?\\n8.2.1 Hypothesis Class of Functions\\nAssume we are given N examples xn ∈ RD and corresponding scalar la-\\nbels yn ∈ R. We consider the supervised learning setting, where we obtain\\npairs (x1, y1), . . . ,(xN , yN). Given this data, we would like to estimate a\\npredictor f(·, θ) : RD → R, parametrized by θ. We hope to be able to find\\na good parameter θ∗ such that we fit the data well, that is,\\nf(xn, θ∗) ≈ yn for all n = 1, . . . , N . (8.3)\\nIn this section, we use the notationˆyn = f(xn, θ∗) to represent the output\\nof the predictor.\\nRemark. For ease of presentation, we will describe empirical risk mini-\\nmization in terms of supervised learning (where we have labels). This\\nsimplifies the definition of the hypothesis class and the loss function. It\\nis also common in machine learning to choose a parametrized class of\\nfunctions, for example affine functions. ♢\\nExample 8.1\\nWe introduce the problem of ordinary least-squares regression to illustrate\\nempirical risk minimization. A more comprehensive account of regression\\nis given in Chapter 9. When the label yn is real-valued, a popular choice\\nof function class for predictors is the set of affine functions. We choose a Affine functions are\\noften referred to as\\nlinear functions in\\nmachine learning.\\nmore compact notation for an affine function by concatenating an addi-\\ntional unit feature x(0) = 1 to xn, i.e., xn = [1, x(1)\\nn , x(2)\\nn , . . . , x(D)\\nn ]⊤. The\\nparameter vector is correspondingly θ = [θ0, θ1, θ2, . . . , θD]⊤, allowing us\\nto write the predictor as a linear function\\nf(xn, θ) = θ⊤xn . (8.4)\\nThis linear predictor is equivalent to the affine model\\nf(xn, θ) = θ0 +\\nDX\\nd=1\\nθdx(d)\\nn . (8.5)\\nThe predictor takes the vector of features representing a single example\\nxn as input and produces a real-valued output, i.e., f : RD+1 → R. The\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36f44564-b2e3-4376-b999-2c054a4ad488', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 265, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='260 When Models Meet Data\\nprevious figures in this chapter had a straight line as a predictor, which\\nmeans that we have assumed an affine function.\\nInstead of a linear function, we may wish to consider non-linear func-\\ntions as predictors. Recent advances in neural networks allow for efficient\\ncomputation of more complex non-linear function classes.\\nGiven the class of functions, we want to search for a good predictor.\\nWe now move on to the second ingredient of empirical risk minimization:\\nhow to measure how well the predictor fits the training data.\\n8.2.2 Loss Function for Training\\nConsider the label yn for a particular example; and the corresponding pre-\\ndiction ˆyn that we make based on xn. To define what it means to fit the\\ndata well, we need to specify aloss function ℓ(yn, ˆyn) that takes the groundloss function\\ntruth label and the prediction as input and produces a non-negative num-\\nber (referred to as the loss) representing how much error we have made\\non this particular prediction. Our goal for finding a good parameter vectorThe expression\\n“error” is often used\\nto mean loss.\\nθ∗ is to minimize the average loss on the set of N training examples.\\nOne assumption that is commonly made in machine learning is that\\nthe set of examples (x1, y1), . . . ,(xN , yN) is independent and identicallyindependent and\\nidentically\\ndistributed\\ndistributed. The word independent (Section 6.4.5) means that two data\\npoints (xi, yi) and (xj, yj) do not statistically depend on each other, mean-\\ning that the empirical mean is a good estimate of the population mean\\n(Section 6.4.1). This implies that we can use the empirical mean of the\\nloss on the training data. For a giventraining set {(x1, y1), . . . ,(xN , yN)},training set\\nwe introduce the notation of an example matrix X := [ x1, . . . ,xN]⊤ ∈\\nRN ×D and a label vector y := [ y1, . . . , yN]⊤ ∈ RN. Using this matrix\\nnotation the average loss is given by\\nRemp(f, X, y) = 1\\nN\\nNX\\nn=1\\nℓ(yn, ˆyn) , (8.6)\\nwhere ˆyn = f(xn, θ). Equation (8.6) is called the empirical risk and de-empirical risk\\npends on three arguments, the predictorf and the data X, y. This general\\nstrategy for learning is called empirical risk minimization.empirical risk\\nminimization\\nExample 8.2 (Least-Squares Loss)\\nContinuing the example of least-squares regression, we specify that we\\nmeasure the cost of making an error during training using the squared\\nloss ℓ(yn, ˆyn) = (yn − ˆyn)2. We wish to minimize the empirical risk (8.6),\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a0a69eb1-0ed1-4be7-9806-6ca4470870be', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 266, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2 Empirical Risk Minimization 261\\nwhich is the average of the losses over the data\\nmin\\nθ∈RD\\n1\\nN\\nNX\\nn=1\\n(yn − f(xn, θ))2, (8.7)\\nwhere we substituted the predictor ˆyn = f(xn, θ). By using our choice of\\na linear predictor f(xn, θ) = θ⊤xn, we obtain the optimization problem\\nmin\\nθ∈RD\\n1\\nN\\nNX\\nn=1\\n(yn − θ⊤xn)2 . (8.8)\\nThis equation can be equivalently expressed in matrix form\\nmin\\nθ∈RD\\n1\\nN ∥y − Xθ ∥\\n2\\n. (8.9)\\nThis is known as the least-squares problem. There exists a closed-form an- least-squares\\nproblemalytic solution for this by solving the normal equations, which we will\\ndiscuss in Section 9.2.\\nWe are not interested in a predictor that only performs well on the\\ntraining data. Instead, we seek a predictor that performs well (has low\\nrisk) on unseen test data. More formally , we are interested in finding a\\npredictor f (with parameters fixed) that minimizes the expected risk expected risk\\nRtrue(f) = Ex,y[ℓ(y, f(x))] , (8.10)\\nwhere y is the label and f(x) is the prediction based on the example x.\\nThe notation Rtrue(f) indicates that this is the true risk if we had access to\\nan infinite amount of data. The expectation is over the (infinite) set of all Another phrase\\ncommonly used for\\nexpected risk is\\n“population risk”.\\npossible data and labels. There are two practical questions that arise from\\nour desire to minimize expected risk, which we address in the following\\ntwo subsections:\\nHow should we change our training procedure to generalize well?\\nHow do we estimate expected risk from (finite) data?\\nRemark. Many machine learning tasks are specified with an associated\\nperformance measure, e.g., accuracy of prediction or root mean squared\\nerror. The performance measure could be more complex, be cost sensitive,\\nand capture details about the particular application. In principle, the de-\\nsign of the loss function for empirical risk minimization should correspond\\ndirectly to the performance measure specified by the machine learning\\ntask. In practice, there is often a mismatch between the design of the loss\\nfunction and the performance measure. This could be due to issues such\\nas ease of implementation or efficiency of optimization. ♢\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a28ac34c-93fb-47e9-ac45-32fdc282bf53', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 267, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='262 When Models Meet Data\\n8.2.3 Regularization to Reduce Overfitting\\nThis section describes an addition to empirical risk minimization that al-\\nlows it to generalize well (approximately minimizing expected risk). Re-\\ncall that the aim of training a machine learning predictor is so that we can\\nperform well on unseen data, i.e., the predictor generalizes well. We sim-\\nulate this unseen data by holding out a proportion of the whole dataset.\\nThis hold out set is referred to as thetest set. Given a sufficiently rich classtest set\\nEven knowing only\\nthe performance of\\nthe predictor on the\\ntest set leaks\\ninformation (Blum\\nand Hardt, 2015).\\nof functions for the predictor f, we can essentially memorize the training\\ndata to obtain zero empirical risk. While this is great to minimize the loss\\n(and therefore the risk) on the training data, we would not expect the\\npredictor to generalize well to unseen data. In practice, we have only a\\nfinite set of data, and hence we split our data into a training and a test\\nset. The training set is used to fit the model, and the test set (not seen\\nby the machine learning algorithm during training) is used to evaluate\\ngeneralization performance. It is important for the user to not cycle back\\nto a new round of training after having observed the test set. We use the\\nsubscripts train and test to denote the training and test sets, respectively .\\nWe will revisit this idea of using a finite dataset to evaluate expected risk\\nin Section 8.2.4.\\nIt turns out that empirical risk minimization can lead to overfitting, i.e.,overfitting\\nthe predictor fits too closely to the training data and does not general-\\nize well to new data (Mitchell, 1997). This general phenomenon of hav-\\ning very small average loss on the training set but large average loss on\\nthe test set tends to occur when we have little data and a complex hy-\\npothesis class. For a particular predictor f (with parameters fixed), the\\nphenomenon of overfitting occurs when the risk estimate from the train-\\ning dataRemp(f, Xtrain, ytrain) underestimates the expected riskRtrue(f).\\nSince we estimate the expected risk Rtrue(f) by using the empirical risk\\non the test set Remp(f, Xtest, ytest) if the test risk is much larger than\\nthe training risk, this is an indication of overfitting. We revisit the idea of\\noverfitting in Section 8.3.3.\\nTherefore, we need to somehow bias the search for the minimizer of\\nempirical risk by introducing a penalty term, which makes it harder for\\nthe optimizer to return an overly flexible predictor. In machine learning,\\nthe penalty term is referred to as regularization. Regularization is a wayregularization\\nto compromise between accurate solution of empirical risk minimization\\nand the size or complexity of the solution.\\nExample 8.3 (Regularized Least Squares)\\nRegularization is an approach that discourages complex or extreme solu-\\ntions to an optimization problem. The simplest regularization strategy is\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ef483c1-c3ac-467a-9c22-df2edd2e09c2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 268, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2 Empirical Risk Minimization 263\\nto replace the least-squares problem\\nmin\\nθ\\n1\\nN ∥y − Xθ ∥\\n2\\n. (8.11)\\nin the previous example with the “regularized” problem by adding a\\npenalty term involving only θ:\\nmin\\nθ\\n1\\nN ∥y − Xθ ∥\\n2\\n+ λ ∥θ∥\\n2\\n. (8.12)\\nThe additional term ∥θ∥\\n2\\nis called the regularizer, and the parameter regularizer\\nλ is the regularization parameter . The regularization parameter trades regularization\\nparameteroff minimizing the loss on the training set and the magnitude of the pa-\\nrameters θ. It often happens that the magnitude of the parameter values\\nbecomes relatively large if we run into overfitting (Bishop, 2006).\\nThe regularization term is sometimes called the penalty term, which bi- penalty term\\nases the vector θ to be closer to the origin. The idea of regularization also\\nappears in probabilistic models as the prior probability of the parameters.\\nRecall from Section 6.6 that for the posterior distribution to be of the same\\nform as the prior distribution, the prior and the likelihood need to be con-\\njugate. We will revisit this idea in Section 8.3.2. We will see in Chapter 12\\nthat the idea of the regularizer is equivalent to the idea of a large margin.\\n8.2.4 Cross-Validation to Assess the Generalization Performance\\nWe mentioned in the previous section that we measure the generalization\\nerror by estimating it by applying the predictor on test data. This data is\\nalso sometimes referred to as thevalidation set. The validation set is a sub- validation set\\nset of the available training data that we keep aside. A practical issue with\\nthis approach is that the amount of data is limited, and ideally we would\\nuse as much of the data available to train the model. This would require\\nus to keep our validation set V small, which then would lead to a noisy\\nestimate (with high variance) of the predictive performance. One solu-\\ntion to these contradictory objectives (large training set, large validation\\nset) is to use cross-validation. K-fold cross-validation effectively partitions cross-validation\\nthe data into K chunks, K − 1 of which form the training set R, and\\nthe last chunk serves as the validation set V (similar to the idea outlined\\npreviously). Cross-validation iterates through (ideally) all combinations\\nof assignments of chunks to R and V; see Figure 8.4. This procedure is\\nrepeated for all K choices for the validation set, and the performance of\\nthe model from the K runs is averaged.\\nWe partition our dataset into two setsD = R ∪ V, such that they do not\\noverlap (R ∩ V = ∅), where V is the validation set, and train our model\\non R. After training, we assess the performance of the predictor f on the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='55daa692-18f5-41d8-9c4d-de3a069797c2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 269, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='264 When Models Meet Data\\nFigure 8.4 K-fold\\ncross-validation.\\nThe dataset is\\ndivided into K = 5\\nchunks, K − 1 of\\nwhich serve as the\\ntraining set (blue)\\nand one as the\\nvalidation set\\n(orange hatch).\\nTraining\\nValidation\\nvalidation set V (e.g., by computing root mean square error (RMSE) of\\nthe trained model on the validation set). More precisely , for each partition\\nk the training data R(k) produces a predictor f (k), which is then applied\\nto validation set V(k) to compute the empirical risk R(f (k), V(k)). We cycle\\nthrough all possible partitionings of validation and training sets and com-\\npute the average generalization error of the predictor. Cross-validation\\napproximates the expected generalization error\\nEV[R(f, V)] ≈ 1\\nK\\nKX\\nk=1\\nR(f (k), V(k)) , (8.13)\\nwhere R(f (k), V(k)) is the risk (e.g., RMSE) on the validation set V(k) for\\npredictor f (k). The approximation has two sources: first, due to the finite\\ntraining set, which results in not the best possiblef (k); and second, due to\\nthe finite validation set, which results in an inaccurate estimation of the\\nrisk R(f (k), V(k)). A potential disadvantage of K-fold cross-validation is\\nthe computational cost of training the model K times, which can be bur-\\ndensome if the training cost is computationally expensive. In practice, it\\nis often not sufficient to look at the direct parameters alone. For example,\\nwe need to explore multiple complexity parameters (e.g., multiple regu-\\nlarization parameters), which may not be direct parameters of the model.\\nEvaluating the quality of the model, depending on these hyperparameters,\\nmay result in a number of training runs that is exponential in the number\\nof model parameters. One can use nested cross-validation (Section 8.6.1)\\nto search for good hyperparameters.\\nHowever, cross-validation is anembarrassingly parallel problem, i.e., lit-embarrassingly\\nparallel tle effort is needed to separate the problem into a number of parallel\\ntasks. Given sufficient computing resources (e.g., cloud computing, server\\nfarms), cross-validation does not require longer than a single performance\\nassessment.\\nIn this section, we saw that empirical risk minimization is based on the\\nfollowing concepts: the hypothesis class of functions, the loss function and\\nregularization. In Section 8.3, we will see the effect of using a probability\\ndistribution to replace the idea of loss functions and regularization.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='577fd9df-34e8-4403-810e-aaae9b9deac0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 270, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Parameter Estimation 265\\n8.2.5 Further Reading\\nDue to the fact that the original development of empirical risk minimiza-\\ntion (Vapnik, 1998) was couched in heavily theoretical language, many\\nof the subsequent developments have been theoretical. The area of study\\nis called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statistical learning\\ntheoryHastie et al., 2001; von Luxburg and Sch¨olkopf, 2011). A recent machine\\nlearning textbook that builds on the theoretical foundations and develops\\nefficient learning algorithms is Shalev-Shwartz and Ben-David (2014).\\nThe concept of regularization has its roots in the solution of ill-posed in-\\nverse problems (Neumaier, 1998). The approach presented here is called\\nTikhonov regularization, and there is a closely related constrained version Tikhonov\\nregularizationcalled Ivanov regularization. Tikhonov regularization has deep relation-\\nships to the bias-variance trade-off and feature selection (B ¨uhlmann and\\nVan De Geer, 2011). An alternative to cross-validation is bootstrap and\\njackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall,\\n1992).\\nThinking about empirical risk minimization (Section 8.2) as “probabil-\\nity free” is incorrect. There is an underlying unknown probability distri-\\nbution p(x, y) that governs the data generation. However, the approach\\nof empirical risk minimization is agnostic to that choice of distribution.\\nThis is in contrast to standard statistical approaches that explicitly re-\\nquire the knowledge of p(x, y). Furthermore, since the distribution is a\\njoint distribution on both examples x and labels y, the labels can be non-\\ndeterministic. In contrast to standard statistics we do not need to specify\\nthe noise distribution for the labels y.\\n8.3 Parameter Estimation\\nIn Section 8.2, we did not explicitly model our problem using probability\\ndistributions. In this section, we will see how to use probability distribu-\\ntions to model our uncertainty due to the observation process and our\\nuncertainty in the parameters of our predictors. In Section 8.3.1, we in-\\ntroduce the likelihood, which is analogous to the concept of loss functions\\n(Section 8.2.2) in empirical risk minimization. The concept of priors (Sec-\\ntion 8.3.2) is analogous to the concept of regularization (Section 8.2.3).\\n8.3.1 Maximum Likelihood Estimation\\nThe idea behind maximum likelihood estimation (MLE) is to define a func- maximum likelihood\\nestimationtion of the parameters that enables us to find a model that fits the data\\nwell. The estimation problem is focused on the likelihood function, or likelihood\\nmore precisely its negative logarithm. For data represented by a random\\nvariable x and for a family of probability densities p(x | θ) parametrized\\nby θ, the negative log-likelihood is given by negative\\nlog-likelihood\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='13367350-6bf4-41d8-a37d-f5a3750437dd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 271, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='266 When Models Meet Data\\nLx(θ) = − log p(x | θ) . (8.14)\\nThe notation Lx(θ) emphasizes the fact that the parameter θ is varying\\nand the datax is fixed. We very often drop the reference tox when writing\\nthe negative log-likelihood, as it is really a function of θ, and write it as\\nL(θ) when the random variable representing the uncertainty in the data\\nis clear from the context.\\nLet us interpret what the probability density p(x | θ) is modeling for a\\nfixed value of θ. It is a distribution that models the uncertainty of the data\\nfor a given parameter setting. For a given dataset x, the likelihood allows\\nus to express preferences about different settings of the parametersθ, and\\nwe can choose the setting that more “likely” has generated the data.\\nIn a complementary view, if we consider the data to be fixed (because\\nit has been observed), and we vary the parameters θ, what does L(θ) tell\\nus? It tells us how likely a particular setting of θ is for the observations x.\\nBased on this second view, the maximum likelihood estimator gives us the\\nmost likely parameter θ for the set of data.\\nWe consider the supervised learning setting, where we obtain pairs\\n(x1, y1), . . . ,(xN , yN) with xn ∈ RD and labels yn ∈ R. We are inter-\\nested in constructing a predictor that takes a feature vector xn as input\\nand produces a prediction yn (or something close to it), i.e., given a vec-\\ntor xn we want the probability distribution of the labelyn. In other words,\\nwe specify the conditional probability distribution of the labels given the\\nexamples for the particular parameter setting θ.\\nExample 8.4\\nThe first example that is often used is to specify that the conditional\\nprobability of the labels given the examples is a Gaussian distribution. In\\nother words, we assume that we can explain our observation uncertainty\\nby independent Gaussian noise (refer to Section 6.5) with zero mean,\\nεn ∼ N\\n\\x00\\n0, σ 2\\x01\\n. We further assume that the linear model x⊤\\nn θ is used for\\nprediction. This means we specify a Gaussian likelihood for each example\\nlabel pair (xn, yn),\\np(yn | xn, θ) = N\\n\\x00\\nyn | x⊤\\nn θ, σ 2\\x01\\n. (8.15)\\nAn illustration of a Gaussian likelihood for a given parameter θ is shown\\nin Figure 8.3. We will see in Section 9.2 how to explicitly expand the\\npreceding expression out in terms of the Gaussian distribution.\\nWe assume that the set of examples(x1, y1), . . . ,(xN , yN) are independentindependent and\\nidentically\\ndistributed\\nand identically distributed (i.i.d.). The word “independent” (Section 6.4.5)\\nimplies that the likelihood involving the whole dataset (Y = {y1, . . . , yN }\\nand X = {x1, . . . ,xN }) factorizes into a product of the likelihoods of\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dccf3c3a-f2f7-40f9-954d-2dc09b33674e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 272, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Parameter Estimation 267\\neach individual example\\np(Y | X , θ) =\\nNY\\nn=1\\np(yn | xn, θ) , (8.16)\\nwhere p(yn | xn, θ) is a particular distribution (which was Gaussian in Ex-\\nample 8.4). The expression “identically distributed” means that each term\\nin the product (8.16) is of the same distribution, and all of them share\\nthe same parameters. It is often easier from an optimization viewpoint to\\ncompute functions that can be decomposed into sums of simpler functions.\\nHence, in machine learning we often consider the negative log-likelihood Recall log(ab) =\\nlog(a) + log(b)\\nL(θ) = − log p(Y | X , θ) = −\\nNX\\nn=1\\nlog p(yn | xn, θ) . (8.17)\\nWhile it is temping to interpret the fact that θ is on the right of the condi-\\ntioning in p(yn|xn, θ) (8.15), and hence should be interpreted as observed\\nand fixed, this interpretation is incorrect. The negative log-likelihoodL(θ)\\nis a function of θ. Therefore, to find a good parameter vector θ that\\nexplains the data (x1, y1), . . . ,(xN , yN) well, minimize the negative log-\\nlikelihood L(θ) with respect to θ.\\nRemark. The negative sign in (8.17) is a historical artifact that is due\\nto the convention that we want to maximize likelihood, but numerical\\noptimization literature tends to study minimization of functions. ♢\\nExample 8.5\\nContinuing on our example of Gaussian likelihoods (8.15), the negative\\nlog-likelihood can be rewritten as\\nL(θ) = −\\nNX\\nn=1\\nlog p(yn | xn, θ) = −\\nNX\\nn=1\\nlog N\\n\\x00\\nyn | x⊤\\nn θ, σ 2\\x01\\n(8.18a)\\n= −\\nNX\\nn=1\\nlog 1√\\n2πσ2\\nexp\\n\\x12\\n−(yn − x⊤\\nn θ)2\\n2σ2\\n\\x13\\n(8.18b)\\n= −\\nNX\\nn=1\\nlog exp\\n\\x12\\n−(yn − x⊤\\nn θ)2\\n2σ2\\n\\x13\\n−\\nNX\\nn=1\\nlog 1√\\n2πσ2\\n(8.18c)\\n= 1\\n2σ2\\nNX\\nn=1\\n(yn − x⊤\\nn θ)2 −\\nNX\\nn=1\\nlog 1√\\n2πσ2\\n. (8.18d)\\nAs σ is given, the second term in (8.18d) is constant, and minimizingL(θ)\\ncorresponds to solving the least-squares problem (compare with (8.8))\\nexpressed in the first term.\\nIt turns out that for Gaussian likelihoods the resulting optimization\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d13a6426-c45d-4101-a819-b42a7f51f651', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 273, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='268 When Models Meet Data\\nFigure 8.5 For the\\ngiven data, the\\nmaximum likelihood\\nestimate of the\\nparameters results\\nin the black\\ndiagonal line. The\\norange square\\nshows the value of\\nthe maximum\\nlikelihood\\nprediction at\\nx = 60.\\n0 10 20 30 40 50 60 70 80\\nx\\n0\\n25\\n50\\n75\\n100\\n125\\n150y\\nFigure 8.6\\nComparing the\\npredictions with the\\nmaximum likelihood\\nestimate and the\\nMAP estimate at\\nx = 60. The prior\\nbiases the slope to\\nbe less steep and the\\nintercept to be\\ncloser to zero. In\\nthis example, the\\nbias that moves the\\nintercept closer to\\nzero actually\\nincreases the slope.\\n0 10 20 30 40 50 60 70 80\\nx\\n0\\n25\\n50\\n75\\n100\\n125\\n150y\\nMLE\\nMAP\\nproblem corresponding to maximum likelihood estimation has a closed-\\nform solution. We will see more details on this in Chapter 9. Figure 8.5\\nshows a regression dataset and the function that is induced by the maxi-\\nmum-likelihood parameters. Maximum likelihood estimation may suffer\\nfrom overfitting (Section 8.3.3), analogous to unregularized empirical risk\\nminimization (Section 8.2.3). For other likelihood functions, i.e., if we\\nmodel our noise with non-Gaussian distributions, maximum likelihood es-\\ntimation may not have a closed-form analytic solution. In this case, we\\nresort to numerical optimization methods discussed in Chapter 7.\\n8.3.2 Maximum A Posteriori Estimation\\nIf we have prior knowledge about the distribution of the parametersθ, we\\ncan multiply an additional term to the likelihood. This additional term is\\na prior probability distribution on parameters p(θ). For a given prior, after\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f0fcbff3-cdcc-4750-9431-24a23f06d4ca', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 274, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Parameter Estimation 269\\nobserving some data x, how should we update the distribution of θ? In\\nother words, how should we represent the fact that we have more specific\\nknowledge of θ after observing data x? Bayes’ theorem, as discussed in\\nSection 6.3, gives us a principled tool to update our probability distribu-\\ntions of random variables. It allows us to compute a posterior distribution posterior\\np(θ | x) (the more specific knowledge) on the parameters θ from general\\nprior statements (prior distribution) p(θ) and the function p(x | θ) that prior\\nlinks the parameters θ and the observed data x (called the likelihood): likelihood\\np(θ | x) = p(x | θ)p(θ)\\np(x) . (8.19)\\nRecall that we are interested in finding the parameter θ that maximizes\\nthe posterior. Since the distribution p(x) does not depend on θ, we can\\nignore the value of the denominator for the optimization and obtain\\np(θ | x) ∝ p(x | θ)p(θ) . (8.20)\\nThe preceding proportion relation hides the density of the data p(x),\\nwhich may be difficult to estimate. Instead of estimating the minimum\\nof the negative log-likelihood, we now estimate the minimum of the neg-\\native log-posterior, which is referred to as maximum a posteriori estima- maximum a\\nposteriori\\nestimation\\ntion (MAP estimation). An illustration of the effect of adding a zero-mean\\nMAP estimationGaussian prior is shown in Figure 8.6.\\nExample 8.6\\nIn addition to the assumption of Gaussian likelihood in the previous exam-\\nple, we assume that the parameter vector is distributed as a multivariate\\nGaussian with zero mean, i.e., p(θ) = N\\n\\x00\\n0, Σ\\n\\x01\\n, where Σ is the covari-\\nance matrix (Section 6.5). Note that the conjugate prior of a Gaussian\\nis also a Gaussian (Section 6.6.1), and therefore we expect the posterior\\ndistribution to also be a Gaussian. We will see the details of maximum a\\nposteriori estimation in Chapter 9.\\nThe idea of including prior knowledge about where good parameters\\nlie is widespread in machine learning. An alternative view, which we saw\\nin Section 8.2.3, is the idea of regularization, which introduces an addi-\\ntional term that biases the resulting parameters to be close to the origin.\\nMaximum a posteriori estimation can be considered to bridge the non-\\nprobabilistic and probabilistic worlds as it explicitly acknowledges the\\nneed for a prior distribution but it still only produces a point estimate\\nof the parameters.\\nRemark. The maximum likelihood estimate θML possesses the following\\nproperties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\\nAsymptotic consistency: The MLE converges to the true value in the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bac49cc-2b2d-4447-887b-ebd7100486eb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 275, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='270 When Models Meet Data\\nFigure 8.7 Model\\nfitting. In a\\nparametrized class\\nMθ of models, we\\noptimize the model\\nparameters θ to\\nminimize the\\ndistance to the true\\n(unknown) model\\nM ∗.\\nMθ\\nM ∗\\nMθ∗\\nMθ0\\nlimit of infinitely many observations, plus a random error that is ap-\\nproximately normal.\\nThe size of the samples necessary to achieve these properties can be\\nquite large.\\nThe error’s variance decays in 1/N, where N is the number of data\\npoints.\\nEspecially , in the “small” data regime, maximum likelihood estimation\\ncan lead to overfitting.\\n♢\\nThe principle of maximum likelihood estimation (and maximum a pos-\\nteriori estimation) uses probabilistic modeling to reason about the uncer-\\ntainty in the data and model parameters. However, we have not yet taken\\nprobabilistic modeling to its full extent. In this section, the resulting train-\\ning procedure still produces a point estimate of the predictor, i.e., training\\nreturns one single set of parameter values that represent the best predic-\\ntor. In Section 8.4, we will take the view that the parameter values should\\nalso be treated as random variables, and instead of estimating “best” val-\\nues of that distribution, we will use the full parameter distribution when\\nmaking predictions.\\n8.3.3 Model Fitting\\nConsider the setting where we are given a dataset, and we are interested\\nin fitting a parametrized model to the data. When we talk about “fit-\\nting”, we typically mean optimizing/learning model parameters so that\\nthey minimize some loss function, e.g., the negative log-likelihood. With\\nmaximum likelihood (Section 8.3.1) and maximum a posteriori estima-\\ntion (Section 8.3.2), we already discussed two commonly used algorithms\\nfor model fitting.\\nThe parametrization of the model defines a model class Mθ with which\\nwe can operate. For example, in a linear regression setting, we may define\\nthe relationship between inputs x and (noise-free) observations y to be\\ny = ax + b, where θ := {a, b} are the model parameters. In this case, the\\nmodel parameters θ describe the family of affine functions, i.e., straight\\nlines with slope a, which are offset from 0 by b. Assume the data comes\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42798c4b-82f0-4fd1-a4b4-90b4f9bd08e2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 276, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Parameter Estimation 271\\nFigure 8.8 Fitting\\n(by maximum\\nlikelihood) of\\ndifferent model\\nclasses to a\\nregression dataset.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\n(a) Overfitting\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (b) Underfitting.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (c) Fitting well.\\nfrom a model M ∗, which is unknown to us. For a given training dataset,\\nwe optimize θ so that Mθ is as close as possible to M ∗, where the “close-\\nness” is defined by the objective function we optimize (e.g., squared loss\\non the training data). Figure 8.7 illustrates a setting where we have a small\\nmodel class (indicated by the circle Mθ), and the data generation model\\nM ∗ lies outside the set of considered models. We begin our parameter\\nsearch at Mθ0. After the optimization, i.e., when we obtain the best pos-\\nsible parameters θ∗, we distinguish three different cases: (i) overfitting,\\n(ii) underfitting, and (iii) fitting well. We will give a high-level intuition\\nof what these three concepts mean.\\nRoughly speaking, overfitting refers to the situation where the para- overfitting\\nmetrized model class is too rich to model the dataset generated by M ∗,\\ni.e., Mθ could model much more complicated datasets. For instance, if the\\ndataset was generated by a linear function, and we define Mθ to be the\\nclass of seventh-order polynomials, we could model not only linear func-\\ntions, but also polynomials of degree two, three, etc. Models that over-\\nfit typically have a large number of parameters. An observation we often One way to detect\\noverfitting in\\npractice is to\\nobserve that the\\nmodel has low\\ntraining risk but\\nhigh test risk during\\ncross validation\\n(Section 8.2.4).\\nmake is that the overly flexible model classMθ uses all its modeling power\\nto reduce the training error. If the training data is noisy , it will therefore\\nfind some useful signal in the noise itself. This will cause enormous prob-\\nlems when we predict away from the training data. Figure 8.8(a) gives an\\nexample of overfitting in the context of regression where the model pa-\\nrameters are learned by means of maximum likelihood (see Section 8.3.1).\\nWe will discuss overfitting in regression more in Section 9.2.2.\\nWhen we run into underfitting, we encounter the opposite problem underfitting\\nwhere the model class Mθ is not rich enough. For example, if our dataset\\nwas generated by a sinusoidal function, but θ only parametrizes straight\\nlines, the best optimization procedure will not get us close to the true\\nmodel. However, we still optimize the parameters and find the best straight\\nline that models the dataset. Figure 8.8(b) shows an example of a model\\nthat underfits because it is insufficiently flexible. Models that underfit typ-\\nically have few parameters.\\nThe third case is when the parametrized model class is about right.\\nThen, our model fits well, i.e., it neither overfits nor underfits. This means\\nour model class is just rich enough to describe the dataset we are given.\\nFigure 8.8(c) shows a model that fits the given dataset fairly well. Ideally ,\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a79119ba-c8e4-41c0-aae5-ded554781de5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 277, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='272 When Models Meet Data\\nthis is the model class we would want to work with since it has good\\ngeneralization properties.\\nIn practice, we often define very rich model classes Mθ with many pa-\\nrameters, such as deep neural networks. To mitigate the problem of over-\\nfitting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).\\nWe will discuss how to choose the model class in Section 8.6.\\n8.3.4 Further Reading\\nWhen considering probabilistic models, the principle of maximum likeli-\\nhood estimation generalizes the idea of least-squares regression for linear\\nmodels, which we will discuss in detail in Chapter 9. When restricting\\nthe predictor to have linear form with an additional nonlinear function φ\\napplied to the output, i.e.,\\np(yn|xn, θ) = φ(θ⊤xn) , (8.21)\\nwe can consider other models for other prediction tasks, such as binary\\nclassification or modeling count data (McCullagh and Nelder, 1989). An\\nalternative view of this is to consider likelihoods that are from the ex-\\nponential family (Section 6.6). The class of models, which have linear\\ndependence between parameters and data, and have potentially nonlin-\\near transformation φ (called a link function), is referred to as generalizedlink function\\ngeneralized linear\\nmodel\\nlinear models (Agresti, 2002, chapter 4).\\nMaximum likelihood estimation has a rich history , and was originally\\nproposed by Sir Ronald Fisher in the 1930s. We will expand upon the idea\\nof a probabilistic model in Section 8.4. One debate among researchers\\nwho use probabilistic models, is the discussion between Bayesian and fre-\\nquentist statistics. As mentioned in Section 6.1.1, it boils down to the\\ndefinition of probability . Recall from Section 6.1 that one can consider\\nprobability to be a generalization (by allowing uncertainty) of logical rea-\\nsoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\\nlihood estimation is frequentist in nature, and the interested reader is\\npointed to Efron and Hastie (2016) for a balanced view of both Bayesian\\nand frequentist statistics.\\nThere are some probabilistic models where maximum likelihood esti-\\nmation may not be possible. The reader is referred to more advanced sta-\\ntistical textbooks, e.g., Casella and Berger (2002), for approaches, such as\\nmethod of moments, M-estimation, and estimating equations.\\n8.4 Probabilistic Modeling and Inference\\nIn machine learning, we are frequently concerned with the interpretation\\nand analysis of data, e.g., for prediction of future events and decision\\nmaking. To make this task more tractable, we often build models that\\ndescribe the generative process that generates the observed data.generative process\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='37f5a085-2ff2-406b-9b90-242fa1d9cb2a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 278, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.4 Probabilistic Modeling and Inference 273\\nFor example, we can describe the outcome of a coin-flip experiment\\n(“heads” or “tails”) in two steps. First, we define a parameter µ, which\\ndescribes the probability of “heads” as the parameter of a Bernoulli distri-\\nbution (Chapter 6); second, we can sample an outcome x ∈ { head, tail}\\nfrom the Bernoulli distribution p(x | µ) = Ber(µ). The parameter µ gives\\nrise to a specific dataset X and depends on the coin used. Since µ is un-\\nknown in advance and can never be observed directly , we need mecha-\\nnisms to learn something about µ given observed outcomes of coin-flip\\nexperiments. In the following, we will discuss how probabilistic modeling\\ncan be used for this purpose.\\n8.4.1 Probabilistic Models A probabilistic\\nmodel is specified\\nby the joint\\ndistribution of all\\nrandom variables.\\nProbabilistic models represent the uncertain aspects of an experiment as\\nprobability distributions. The benefit of using probabilistic models is that\\nthey offer a unified and consistent set of tools from probability theory\\n(Chapter 6) for modeling, inference, prediction, and model selection.\\nIn probabilistic modeling, the joint distribution p(x, θ) of the observed\\nvariables x and the hidden parameters θ is of central importance: It en-\\ncapsulates information from the following:\\nThe prior and the likelihood (product rule, Section 6.3).\\nThe marginal likelihood p(x), which will play an important role in\\nmodel selection (Section 8.6), can be computed by taking the joint dis-\\ntribution and integrating out the parameters (sum rule, Section 6.3).\\nThe posterior, which can be obtained by dividing the joint by the marginal\\nlikelihood.\\nOnly the joint distribution has this property . Therefore, a probabilistic\\nmodel is specified by the joint distribution of all its random variables.\\n8.4.2 Bayesian Inference\\nParameter\\nestimation can be\\nphrased as an\\noptimization\\nproblem.\\nA key task in machine learning is to take a model and the data to uncover\\nthe values of the model’s hidden variablesθ given the observed variables\\nx. In Section 8.3.1, we already discussed two ways for estimating model\\nparameters θ using maximum likelihood or maximum a posteriori esti-\\nmation. In both cases, we obtain a single-best value for θ so that the key\\nalgorithmic problem of parameter estimation is solving an optimization\\nproblem. Once these point estimates θ∗ are known, we use them to make\\npredictions. More specifically , the predictive distribution will bep(x | θ∗),\\nwhere we use θ∗ in the likelihood function.\\nAs discussed in Section 6.3, focusing solely on some statistic of the pos-\\nterior distribution (such as the parameter θ∗ that maximizes the poste-\\nrior) leads to loss of information, which can be critical in a system that\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36f0d95c-e6cf-4a90-98de-eacf9c9342bd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 279, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='274 When Models Meet Data\\nuses the prediction p(x | θ∗) to make decisions. These decision-making\\nsystems typically have different objective functions than the likelihood, aBayesian inference\\nis about learning the\\ndistribution of\\nrandom variables.\\nsquared-error loss or a mis-classification error. Therefore, having the full\\nposterior distribution around can be extremely useful and leads to more\\nrobust decisions. Bayesian inference is about finding this posterior distri-Bayesian inference\\nbution (Gelman et al., 2004). For a datasetX , a parameter prior p(θ), and\\na likelihood function, the posterior\\np(θ | X) = p(X | θ)p(θ)\\np(X ) , p (X ) =\\nZ\\np(X | θ)p(θ)dθ , (8.22)\\nis obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’Bayesian inference\\ninverts the\\nrelationship\\nbetween parameters\\nand the data.\\ntheorem to invert the relationship between the parameters θ and the data\\nX (given by the likelihood) to obtain the posterior distribution p(θ | X).\\nThe implication of having a posterior distribution on the parameters is\\nthat it can be used to propagate uncertainty from the parameters to the\\ndata. More specifically , with a distribution p(θ) on the parameters our\\npredictions will be\\np(x) =\\nZ\\np(x | θ)p(θ)dθ = Eθ[p(x | θ)] , (8.23)\\nand they no longer depend on the model parameters θ, which have been\\nmarginalized/integrated out. Equation (8.23) reveals that the prediction\\nis an average over all plausible parameter values θ, where the plausibility\\nis encapsulated by the parameter distribution p(θ).\\nHaving discussed parameter estimation in Section 8.3 and Bayesian in-\\nference here, let us compare these two approaches to learning. Parameter\\nestimation via maximum likelihood or MAP estimation yields a consistent\\npoint estimate θ∗ of the parameters, and the key computational problem\\nto be solved is optimization. In contrast, Bayesian inference yields a (pos-\\nterior) distribution, and the key computational problem to be solved is\\nintegration. Predictions with point estimates are straightforward, whereas\\npredictions in the Bayesian framework require solving another integration\\nproblem; see (8.23). However, Bayesian inference gives us a principled\\nway to incorporate prior knowledge, account for side information, and\\nincorporate structural knowledge, all of which is not easily done in the\\ncontext of parameter estimation. Moreover, the propagation of parameter\\nuncertainty to the prediction can be valuable in decision-making systems\\nfor risk assessment and exploration in the context of data-efficient learn-\\ning (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\\nWhile Bayesian inference is a mathematically principled framework for\\nlearning about parameters and making predictions, there are some prac-\\ntical challenges that come with it because of the integration problems we\\nneed to solve; see (8.22) and (8.23). More specifically , if we do not choose\\na conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22)\\nand (8.23) are not analytically tractable, and we cannot compute the pos-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e2ed4c0-30db-461a-8ae2-306516b2bad0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 280, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.4 Probabilistic Modeling and Inference 275\\nterior, the predictions, or the marginal likelihood in closed form. In these\\ncases, we need to resort to approximations. Here, we can use stochas-\\ntic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks\\net al., 1996), or deterministic approximations, such as the Laplace ap-\\nproximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-\\nference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-\\ntion (Minka, 2001a).\\nDespite these challenges, Bayesian inference has been successfully ap-\\nplied to a variety of problems, including large-scale topic modeling (Hoff-\\nman et al., 2013), click-through-rate prediction (Graepel et al., 2010),\\ndata-efficient reinforcement learning in control systems (Deisenroth et al.,\\n2015), online ranking systems (Herbrich et al., 2007), and large-scale rec-\\nommender systems. There are generic tools, such as Bayesian optimiza-\\ntion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that\\nare very useful ingredients for an efficient search of meta parameters of\\nmodels or algorithms.\\nRemark. In the machine learning literature, there can be a somewhat ar-\\nbitrary separation between (random) “variables” and “parameters”. While\\nparameters are estimated (e.g., via maximum likelihood), variables are\\nusually marginalized out. In this book, we are not so strict with this sep-\\naration because, in principle, we can place a prior on any parameter and\\nintegrate it out, which would then turn the parameter into a random vari-\\nable according to the aforementioned separation. ♢\\n8.4.3 Latent-Variable Models\\nIn practice, it is sometimes useful to have additional latent variables z latent variable\\n(besides the model parameters θ) as part of the model (Moustaki et al.,\\n2015). These latent variables are different from the model parameters\\nθ as they do not parametrize the model explicitly . Latent variables may\\ndescribe the data-generating process, thereby contributing to the inter-\\npretability of the model. They also often simplify the structure of the\\nmodel and allow us to define simpler and richer model structures. Sim-\\nplification of the model structure often goes hand in hand with a smaller\\nnumber of model parameters (Paquet, 2008; Murphy, 2012). Learning in\\nlatent-variable models (at least via maximum likelihood) can be done in a\\nprincipled way using the expectation maximization (EM) algorithm (Demp-\\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\\nare helpful, are principal component analysis for dimensionality reduc-\\ntion (Chapter 10), Gaussian mixture models for density estimation (Chap-\\nter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\\n(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,\\nand meta learning and task generalization (Hausman et al., 2018; Sæ-\\nmundsson et al., 2018). Although the introduction of these latent variables\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d6326dc-d0b5-4691-bbc4-3af69febf010', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 281, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='276 When Models Meet Data\\nmay make the model structure and the generative process easier, learning\\nin latent-variable models is generally hard, as we will see in Chapter 11.\\nSince latent-variable models also allow us to define the process that\\ngenerates data from parameters, let us have a look at this generative pro-\\ncess. Denoting data by x, the model parameters by θ and the latent vari-\\nables by z, we obtain the conditional distribution\\np(x | z, θ) (8.24)\\nthat allows us to generate data for any model parameters and latent vari-\\nables. Given that z are latent variables, we place a prior p(z) on them.\\nAs the models we discussed previously , models with latent variables\\ncan be used for parameter learning and inference within the frameworks\\nwe discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by\\nmeans of maximum likelihood estimation or Bayesian inference), we fol-\\nlow a two-step procedure. First, we compute the likelihood p(x | θ) of the\\nmodel, which does not depend on the latent variables. Second, we use this\\nlikelihood for parameter estimation or Bayesian inference, where we use\\nexactly the same expressions as in Sections 8.3 and 8.4.2, respectively .\\nSince the likelihood functionp(x | θ) is the predictive distribution of the\\ndata given the model parameters, we need to marginalize out the latent\\nvariables so that\\np(x | θ) =\\nZ\\np(x | z, θ)p(z)dz , (8.25)\\nwhere p(x | z, θ) is given in (8.24) and p(z) is the prior on the latent\\nvariables. Note that the likelihood must not depend on the latent variablesThe likelihood is a\\nfunction of the data\\nand the model\\nparameters, but is\\nindependent of the\\nlatent variables.\\nz, but it is only a function of the data x and the model parameters θ.\\nThe likelihood in (8.25) directly allows for parameter estimation via\\nmaximum likelihood. MAP estimation is also straightforward with an ad-\\nditional prior on the model parameters θ as discussed in Section 8.3.2.\\nMoreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\\nin a latent-variable model works in the usual way: We place a prior p(θ)\\non the model parameters and use Bayes’ theorem to obtain a posterior\\ndistribution\\np(θ | X) = p(X | θ)p(θ)\\np(X ) (8.26)\\nover the model parameters given a dataset X . The posterior in (8.26) can\\nbe used for predictions within a Bayesian inference framework; see (8.23).\\nOne challenge we have in this latent-variable model is that the like-\\nlihood p(X | θ) requires the marginalization of the latent variables ac-\\ncording to (8.25). Except when we choose a conjugate prior p(z) for\\np(x | z, θ), the marginalization in (8.25) is not analytically tractable, and\\nwe need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-\\nphy, 2012; Moustaki et al., 2015).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c5a7440-1477-4b58-bf3c-ade4140facb6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 282, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.4 Probabilistic Modeling and Inference 277\\nSimilar to the parameter posterior (8.26) we can compute a posterior\\non the latent variables according to\\np(z | X) = p(X | z)p(z)\\np(X ) , p (X | z) =\\nZ\\np(X | z, θ)p(θ)dθ , (8.27)\\nwhere p(z) is the prior on the latent variables and p(X | z) requires us to\\nintegrate out the model parameters θ.\\nGiven the difficulty of solving integrals analytically , it is clear that mar-\\nginalizing out both the latent variables and the model parameters at the\\nsame time is not possible in general (Bishop, 2006; Murphy, 2012). A\\nquantity that is easier to compute is the posterior distribution on the latent\\nvariables, but conditioned on the model parameters, i.e.,\\np(z | X , θ) = p(X | z, θ)p(z)\\np(X | θ) , (8.28)\\nwhere p(z) is the prior on the latent variables and p(X | z, θ) is given\\nin (8.24).\\nIn Chapters 10 and 11, we derive the likelihood functions for PCA and\\nGaussian mixture models, respectively . Moreover, we compute the poste-\\nrior distributions (8.28) on the latent variables for both PCA and Gaussian\\nmixture models.\\nRemark. In the following chapters, we may not be drawing such a clear\\ndistinction between latent variables z and uncertain model parameters θ\\nand call the model parameters “latent” or “hidden” as well because they\\nare unobserved. In Chapters 10 and 11, where we use the latent variables\\nz, we will pay attention to the difference as we will have two different\\ntypes of hidden variables: model parameters θ and latent variables z. ♢\\nWe can exploit the fact that all the elements of a probabilistic model are\\nrandom variables to define a unified language for representing them. In\\nSection 8.5, we will see a concise graphical language for representing the\\nstructure of probabilistic models. We will use this graphical language to\\ndescribe the probabilistic models in the subsequent chapters.\\n8.4.4 Further Reading\\nProbabilistic models in machine learning (Bishop, 2006; Barber, 2012;\\nMurphy, 2012) provide a way for users to capture uncertainty about data\\nand predictive models in a principled fashion. Ghahramani (2015) presents\\na short review of probabilistic models in machine learning. Given a proba-\\nbilistic model, we may be lucky enough to be able to compute parameters\\nof interest analytically . However, in general, analytic solutions are rare,\\nand computational methods such as sampling (Gilks et al., 1996; Brooks\\net al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='917bbdc9-8f9c-409b-a613-b93651a13c46', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 283, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='278 When Models Meet Data\\n2017) are used. Moustaki et al. (2015) and Paquet (2008) provide a good\\noverview of Bayesian inference in latent-variable models.\\nIn recent years, several programming languages have been proposed\\nthat aim to treat the variables defined in software as random variables\\ncorresponding to probability distributions. The objective is to be able to\\nwrite complex functions of probability distributions, while under the hood\\nthe compiler automatically takes care of the rules of Bayesian inference.\\nThis rapidly changing field is called probabilistic programming.probabilistic\\nprogramming\\n8.5 Directed Graphical Models\\nIn this section, we introduce a graphical language for specifying a prob-\\nabilistic model, called the directed graphical model. It provides a compactdirected graphical\\nmodel and succinct way to specify probabilistic models, and allows the reader to\\nvisually parse dependencies between random variables. A graphical model\\nvisually captures the way in which the joint distribution over all random\\nvariables can be decomposed into a product of factors depending only on\\na subset of these variables. In Section 8.4, we identified the joint distri-\\nbution of a probabilistic model as the key quantity of interest because it\\ncomprises information about the prior, the likelihood, and the posterior.\\nHowever, the joint distribution by itself can be quite complicated, andDirected graphical\\nmodels are also\\nknown as Bayesian\\nnetworks.\\nit does not tell us anything about structural properties of the probabilis-\\ntic model. For example, the joint distribution p(a, b, c) does not tell us\\nanything about independence relations. This is the point where graphical\\nmodels come into play . This section relies on the concepts of independence\\nand conditional independence, as described in Section 6.4.5.\\nIn a graphical model, nodes are random variables. In Figure 8.9(a), thegraphical model\\nnodes represent the random variables a, b, c. Edges represent probabilistic\\nrelations between variables, e.g., conditional probabilities.\\nRemark. Not every distribution can be represented in a particular choice of\\ngraphical model. A discussion of this can be found in Bishop (2006). ♢\\nProbabilistic graphical models have some convenient properties:\\nThey are a simple way to visualize the structure of a probabilistic model.\\nThey can be used to design or motivate new kinds of statistical models.\\nInspection of the graph alone gives us insight into properties, e.g., con-\\nditional independence.\\nComplex computations for inference and learning in statistical models\\ncan be expressed in terms of graphical manipulations.\\n8.5.1 Graph Semantics\\nDirected graphical models/Bayesian networks are a method for representingdirected graphical\\nmodel/Bayesian\\nnetwork\\nconditional dependencies in a probabilistic model. They provide a visual\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a0fb99fa-08f4-4990-b956-cbd68a4cd95a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 284, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.5 Directed Graphical Models 279\\ndescription of the conditional probabilities, hence, providing a simple lan-\\nguage for describing complex interdependence. The modular description With additional\\nassumptions, the\\narrows can be used\\nto indicate causal\\nrelationships (Pearl,\\n2009).\\nalso entails computational simplification. Directed links (arrows) between\\ntwo nodes (random variables) indicate conditional probabilities. For ex-\\nample, the arrow between a and b in Figure 8.9(a) gives the conditional\\nprobability p(b | a) of b given a.\\nFigure 8.9\\nExamples of\\ndirected graphical\\nmodels.\\na b\\nc\\n(a) Fully connected.\\nx1 x2\\nx3 x4\\nx5\\n(b) Not fully connected.\\nDirected graphical models can be derived from joint distributions if we\\nknow something about their factorization.\\nExample 8.7\\nConsider the joint distribution\\np(a, b, c) = p(c | a, b)p(b | a)p(a) (8.29)\\nof three random variables a, b, c. The factorization of the joint distribution\\nin (8.29) tells us something about the relationship between the random\\nvariables:\\nc depends directly on a and b.\\nb depends directly on a.\\na depends neither on b nor on c.\\nFor the factorization in (8.29), we obtain the directed graphical model in\\nFigure 8.9(a).\\nIn general, we can construct the corresponding directed graphical model\\nfrom a factorized joint distribution as follows:\\n1. Create a node for all random variables.\\n2. For each conditional distribution, we add a directed link (arrow) to\\nthe graph from the nodes corresponding to the variables on which the\\ndistribution is conditioned.\\nThe graph layout\\ndepends on the\\nfactorization of the\\njoint distribution.\\nThe graph layout depends on the choice of factorization of the joint dis-\\ntribution.\\nWe discussed how to get from a known factorization of the joint dis-\\ntribution to the corresponding directed graphical model. Now, we will do\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='50fbb8da-80ab-497b-83df-132f9c6490a3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 285, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='280 When Models Meet Data\\nexactly the opposite and describe how to extract the joint distribution of\\na set of random variables from a given graphical model.\\nExample 8.8\\nLooking at the graphical model in Figure 8.9(b), we exploit two proper-\\nties:\\nThe joint distribution p(x1, . . . , x5) we seek is the product of a set of\\nconditionals, one for each node in the graph. In this particular example,\\nwe will need five conditionals.\\nEach conditional depends only on the parents of the corresponding\\nnode in the graph. For example, x4 will be conditioned on x2.\\nThese two properties yield the desired factorization of the joint distribu-\\ntion\\np(x1, x2, x3, x4, x5) = p(x1)p(x5)p(x2 | x5)p(x3 | x1, x2)p(x4 | x2) . (8.30)\\nIn general, the joint distribution p(x) = p(x1, . . . , xK) is given as\\np(x) =\\nKY\\nk=1\\np(xk | Pak) , (8.31)\\nwhere Pak means “the parent nodes of xk”. Parent nodes of xk are nodes\\nthat have arrows pointing to xk.\\nWe conclude this subsection with a concrete example of the coin-flip\\nexperiment. Consider a Bernoulli experiment (Example 6.8) where the\\nprobability that the outcome x of this experiment is “heads” is\\np(x | µ) = Ber(µ) . (8.32)\\nWe now repeat this experimentN times and observe outcomes x1, . . . , xN\\nso that we obtain the joint distribution\\np(x1, . . . , xN | µ) =\\nNY\\nn=1\\np(xn | µ) . (8.33)\\nThe expression on the right-hand side is a product of Bernoulli distribu-\\ntions on each individual outcome because the experiments are indepen-\\ndent. Recall from Section 6.4.5 that statistical independence means that\\nthe distribution factorizes. To write the graphical model down for this set-\\nting, we make the distinction between unobserved/latent variables and\\nobserved variables. Graphically , observed variables are denoted by shaded\\nnodes so that we obtain the graphical model in Figure 8.10(a). We see\\nthat the single parameter µ is the same for all xn, n = 1 , . . . , N as the\\noutcomes xn are identically distributed. A more compact, but equivalent,\\ngraphical model for this setting is given in Figure 8.10(b), where we use\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a9da8322-503b-4822-b610-19bda5b208c8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 286, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.5 Directed Graphical Models 281\\nFigure 8.10\\nGraphical models\\nfor a repeated\\nBernoulli\\nexperiment.\\nµ\\nx1 xN\\n(a) Version with xn explicit.\\nµ\\nxn\\nn = 1, . . . , N\\n(b) Version with\\nplate notation.\\nµ\\nxn\\nβα\\nn = 1, . . . , N\\n(c) Hyperparameters α\\nand β on the latent µ.\\nthe plate notation. The plate (box) repeats everything inside (in this case, plate\\nthe observations xn) N times. Therefore, both graphical models are equiv-\\nalent, but the plate notation is more compact. Graphical models immedi-\\nately allow us to place a hyperprior on µ. A hyperprior is a second layer hyperprior\\nof prior distributions on the parameters of the first layer of priors. Fig-\\nure 8.10(c) places a Beta (α, β) prior on the latent variable µ. If we treat\\nα and β as deterministic parameters, i.e., not random variables, we omit\\nthe circle around it.\\n8.5.2 Conditional Independence and d-Separation\\nDirected graphical models allow us to find conditional independence (Sec-\\ntion 6.4.5) relationship properties of the joint distribution only by looking\\nat the graph. A concept called d-separation (Pearl, 1988) is key to this. d-separation\\nConsider a general directed graph in which A, B, C are arbitrary nonin-\\ntersecting sets of nodes (whose union may be smaller than the complete\\nset of nodes in the graph). We wish to ascertain whether a particular con-\\nditional independence statement, “ A is conditionally independent of B\\ngiven C”, denoted by\\nA ⊥ ⊥ B | C, (8.34)\\nis implied by a given directed acyclic graph. To do so, we consider all\\npossible trails (paths that ignore the direction of the arrows) from any\\nnode in A to any nodes in B. Any such path is said to be blocked if it\\nincludes any node such that either of the following are true:\\nThe arrows on the path meet either head to tail or tail to tail at the\\nnode, and the node is in the set C.\\nThe arrows meet head to head at the node, and neither the node nor\\nany of its descendants is in the set C.\\nIf all paths are blocked, then A is said to be d-separated from B by C,\\nand the joint distribution over all of the variables in the graph will satisfy\\nA ⊥ ⊥ B | C.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f0d6467-997e-477b-84ff-97964721f201', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 287, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='282 When Models Meet Data\\nFigure 8.12 Three\\ntypes of graphical\\nmodels: (a) Directed\\ngraphical models\\n(Bayesian\\nnetworks);\\n(b) Undirected\\ngraphical models\\n(Markov random\\nfields); (c) Factor\\ngraphs.\\na b\\nc\\n(a) Directed graphical model\\na b\\nc\\n(b) Undirected graphical\\nmodel\\na b\\nc\\n(c) Factor graph\\nExample 8.9 (Conditional Independence)\\nFigure 8.11\\nD-separation\\nexample.\\na b c\\nd\\ne\\nConsider the graphical model in Figure 8.11. Visual inspection gives us\\nb ⊥ ⊥d | a, c (8.35)\\na ⊥ ⊥c | b (8.36)\\nb ̸ ⊥ ⊥d | c (8.37)\\na ̸ ⊥ ⊥c | b, e (8.38)\\nDirected graphical models allow a compact representation of proba-\\nbilistic models, and we will see examples of directed graphical models in\\nChapters 9, 10, and 11. The representation, along with the concept of con-\\nditional independence, allows us to factorize the respective probabilistic\\nmodels into expressions that are easier to optimize.\\nThe graphical representation of the probabilistic model allows us to\\nvisually see the impact of design choices we have made on the structure\\nof the model. We often need to make high-level assumptions about the\\nstructure of the model. These modeling assumptions (hyperparameters)\\naffect the prediction performance, but cannot be selected directly using\\nthe approaches we have seen so far. We will discuss different ways to\\nchoose the structure in Section 8.6.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='71b9789a-7f60-4953-91c6-50d6ce216a04', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 288, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.6 Model Selection 283\\n8.5.3 Further Reading\\nAn introduction to probabilistic graphical models can be found in Bishop\\n(2006, chapter 8), and an extensive description of the different applica-\\ntions and corresponding algorithmic implications can be found in the book\\nby Koller and Friedman (2009). There are three main types of probabilistic\\ngraphical models:\\ndirected graphical\\nmodelDirected graphical models (Bayesian networks); see Figure 8.12(a)\\nBayesian network\\nundirected graphical\\nmodel\\nUndirected graphical models (Markov random fields); see Figure 8.12(b)\\nMarkov random\\nfield\\nfactor graph\\nFactor graphs; see Figure 8.12(c)\\nGraphical models allow for graph-based algorithms for inference and\\nlearning, e.g., via local message passing. Applications range from rank-\\ning in online games (Herbrich et al., 2007) and computer vision (e.g.,\\nimage segmentation, semantic labeling, image denoising, image restora-\\ntion (Kittler and F ¨oglein, 1984; Sucar and Gillies, 1994; Shotton et al.,\\n2006; Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solv-\\ning linear equation systems (Shental et al., 2008), and iterative Bayesian\\nstate estimation in signal processing (Bickson et al., 2007; Deisenroth and\\nMohamed, 2012).\\nOne topic that is particularly important in real applications that we do\\nnot discuss in this book is the idea of structured prediction (Bakir et al.,\\n2007; Nowozin et al., 2014), which allows machine learning models to\\ntackle predictions that are structured, for example sequences, trees, and\\ngraphs. The popularity of neural network models has allowed more flex-\\nible probabilistic models to be used, resulting in many useful applica-\\ntions of structured models (Goodfellow et al., 2016, chapter 16). In recent\\nyears, there has been a renewed interest in graphical models due to their\\napplications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\\nPeters et al., 2017; Rosenbaum, 2017).\\n8.6 Model Selection\\nIn machine learning, we often need to make high-level modeling decisions\\nthat critically influence the performance of the model. The choices we\\nmake (e.g., the functional form of the likelihood) influence the number\\nand type of free parameters in the model and thereby also the flexibility\\nand expressivity of the model. More complex models are more flexible in A polynomial\\ny = a0 +a1x+a2x2\\ncan also describe\\nlinear functions by\\nsetting a2 = 0, i.e.,\\nit is strictly more\\nexpressive than a\\nfirst-order\\npolynomial.\\nthe sense that they can be used to describe more datasets. For instance, a\\npolynomial of degree 1 (a line y = a0 + a1x) can only be used to describe\\nlinear relations between inputs x and observations y. A polynomial of\\ndegree 2 can additionally describe quadratic relationships between inputs\\nand observations.\\nOne would now think that very flexible models are generally preferable\\nto simple models because they are more expressive. A general problem\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e53b54cf-3a85-4eb1-b84e-e8bebb187c18', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 289, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='284 When Models Meet Data\\nFigure 8.13 Nested\\ncross-validation. We\\nperform two levels\\nof K-fold\\ncross-validation.\\nAll labeled data\\nAll training data Test data\\nTo train model Validation\\nis that at training time we can only use the training set to evaluate the\\nperformance of the model and learn its parameters. However, the per-\\nformance on the training set is not really what we are interested in. In\\nSection 8.3, we have seen that maximum likelihood estimation can lead\\nto overfitting, especially when the training dataset is small. Ideally , our\\nmodel (also) works well on the test set (which is not available at training\\ntime). Therefore, we need some mechanisms for assessing how a model\\ngeneralizes to unseen test data. Model selection is concerned with exactly\\nthis problem.\\n8.6.1 Nested Cross-Validation\\nWe have already seen an approach (cross-validation in Section 8.2.4) that\\ncan be used for model selection. Recall that cross-validation provides an\\nestimate of the generalization error by repeatedly splitting the dataset into\\ntraining and validation sets. We can apply this idea one more time, i.e.,\\nfor each split, we can perform another round of cross-validation. This is\\nsometimes referred to asnested cross-validation; see Figure 8.13. The innernested\\ncross-validation level is used to estimate the performance of a particular choice of model\\nor hyperparameter on a internal validation set. The outer level is used to\\nestimate generalization performance for the best choice of model chosen\\nby the inner loop. We can test different model and hyperparameter choices\\nin the inner loop. To distinguish the two levels, the set used to estimate\\nthe generalization performance is often called the test set and the set usedtest set\\nfor choosing the best model is called the validation set. The inner loopvalidation set\\nestimates the expected value of the generalization error for a given model\\n(8.39), by approximating it using the empirical error on the validation set,\\ni.e.,The standard error\\nis defined as σ√\\nK ,\\nwhere K is the\\nnumber of\\nexperiments and σ\\nis the standard\\ndeviation of the risk\\nof each experiment.\\nEV[R(V | M)] ≈ 1\\nK\\nKX\\nk=1\\nR(V(k) | M) , (8.39)\\nwhere R(V | M) is the empirical risk (e.g., root mean square error) on the\\nvalidation set V for model M. We repeat this procedure for all models and\\nchoose the model that performs best. Note that cross-validation not only\\ngives us the expected generalization error, but we can also obtain high-\\norder statistics, e.g., the standard error, an estimate of how uncertain the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='08c0aab5-ba27-4146-aa60-601ed10b87c2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 290, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.6 Model Selection 285\\nFigure 8.14\\nBayesian inference\\nembodies Occam’s\\nrazor. The\\nhorizontal axis\\ndescribes the space\\nof all possible\\ndatasets D. The\\nevidence (vertical\\naxis) evaluates how\\nwell a model\\npredicts available\\ndata. Since\\np(D | Mi) needs to\\nintegrate to 1, we\\nshould choose the\\nmodel with the\\ngreatest evidence.\\nAdapted\\nfrom MacKay\\n(2003).\\nEvidence\\nDC\\np(D |M1)\\np(D |M2)\\nmean estimate is. Once the model is chosen, we can evaluate the final\\nperformance on the test set.\\n8.6.2 Bayesian Model Selection\\nThere are many approaches to model selection, some of which are covered\\nin this section. Generally , they all attempt to trade off model complexity\\nand data fit. We assume that simpler models are less prone to overfitting\\nthan complex models, and hence the objective of model selection is to find\\nthe simplest model that explains the data reasonably well. This concept is\\nalso known as Occam’s razor. Occam’s razor\\nRemark. If we treat model selection as a hypothesis testing problem, we\\nare looking for the simplest hypothesis that is consistent with the data (Mur-\\nphy, 2012). ♢\\nOne may consider placing a prior on models that favors simpler models.\\nHowever, it is not necessary to do this: An “automatic Occam’s Razor” is\\nquantitatively embodied in the application of Bayesian probability (Smith\\nand Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\\nure 8.14, adapted from MacKay (2003), gives us the basic intuition why\\ncomplex and very expressive models may turn out to be a less probable\\nchoice for modeling a given dataset D. Let us think of the horizontal axis These predictions\\nare quantified by a\\nnormalized\\nprobability\\ndistribution on D,\\ni.e., it needs to\\nintegrate/sum to 1.\\nrepresenting the space of all possible datasets D. If we are interested in\\nthe posterior probability p(Mi | D) of model Mi given the data D, we can\\nemploy Bayes’ theorem. Assuming a uniform prior p(M) over all mod-\\nels, Bayes’ theorem rewards models in proportion to how much they pre-\\ndicted the data that occurred. This prediction of the data given model\\nMi, p(D | Mi), is called the evidence for Mi. A simple model M1 can only evidence\\npredict a small number of datasets, which is shown by p(D | M1); a more\\npowerful model M2 that has, e.g., more free parameters than M1, is able\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5f7d64a4-163d-4b99-bdd5-ac6674c52d62', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 291, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='286 When Models Meet Data\\nto predict a greater variety of datasets. This means, however, that M2\\ndoes not predict the datasets in region C as well as M1. Suppose that\\nequal prior probabilities have been assigned to the two models. Then, if\\nthe dataset falls into region C, the less powerful model M1 is the more\\nprobable model.\\nEarlier in this chapter, we argued that models need to be able to explain\\nthe data, i.e., there should be a way to generate data from a given model.\\nFurthermore, if the model has been appropriately learned from the data,\\nthen we expect that the generated data should be similar to the empirical\\ndata. For this, it is helpful to phrase model selection as a hierarchical\\ninference problem, which allows us to compute the posterior distribution\\nover models.\\nLet us consider a finite number of models M = {M1, . . . , MK}, where\\neach model Mk possesses parameters θk. In Bayesian model selection, weBayesian model\\nselection place a prior p(M) on the set of models. The corresponding generative\\ngenerative process process that allows us to generate data from this model is\\nFigure 8.15\\nIllustration of the\\nhierarchical\\ngenerative process\\nin Bayesian model\\nselection. We place\\na prior p(M) on the\\nset of models. For\\neach model, there is\\na distribution\\np(θ | M) on the\\ncorresponding\\nmodel parameters,\\nwhich is used to\\ngenerate the data D.\\nM\\nθ\\nD\\nMk ∼ p(M) (8.40)\\nθk ∼ p(θ | Mk) (8.41)\\nD ∼ p(D | θk) (8.42)\\nand illustrated in Figure 8.15. Given a training set D, we apply Bayes’\\ntheorem and compute the posterior distribution over models as\\np(Mk | D) ∝ p(Mk)p(D | Mk) . (8.43)\\nNote that this posterior no longer depends on the model parameters θk\\nbecause they have been integrated out in the Bayesian setting since\\np(D | Mk) =\\nZ\\np(D | θk)p(θk | Mk)dθk , (8.44)\\nwhere p(θk | Mk) is the prior distribution of the model parameters θk of\\nmodel Mk. The term (8.44) is referred to as themodel evidence or marginal\\nmodel evidence\\nmarginal likelihood\\nlikelihood. From the posterior in (8.43), we determine the MAP estimate\\nM ∗ = arg max\\nMk\\np(Mk | D) . (8.45)\\nWith a uniform prior p(Mk) = 1\\nK , which gives every model equal (prior)\\nprobability , determining the MAP estimate over models amounts to pick-\\ning the model that maximizes the model evidence (8.44).\\nRemark (Likelihood and Marginal Likelihood). There are some important\\ndifferences between a likelihood and a marginal likelihood (evidence):\\nWhile the likelihood is prone to overfitting, the marginal likelihood is typ-\\nically not as the model parameters have been marginalized out (i.e., we\\nno longer have to fit the parameters). Furthermore, the marginal likeli-\\nhood automatically embodies a trade-off between model complexity and\\ndata fit (Occam’s razor). ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a60972a6-0907-416e-9d21-1073f1d0398a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 292, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.6 Model Selection 287\\n8.6.3 Bayes Factors for Model Comparison\\nConsider the problem of comparing two probabilistic models M1, M2,\\ngiven a dataset D. If we compute the posteriors p(M1 | D) and p(M2 | D),\\nwe can compute the ratio of the posteriors\\np(M1 | D)\\np(M2 | D)| {z }\\nposterior odds\\n=\\np(D | M1)p(M1)\\np(D)\\np(D | M2)p(M2)\\np(D)\\n= p(M1)\\np(M2)| {z }\\nprior odds\\np(D | M1)\\np(D | M2)| {z }\\nBayes factor\\n. (8.46)\\nThe ratio of the posteriors is also called the posterior odds. The first frac- posterior odds\\ntion on the right-hand side of (8.46), the prior odds, measures how much prior odds\\nour prior (initial) beliefs favor M1 over M2. The ratio of the marginal like-\\nlihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\\nand measures how well the data D is predicted by M1 compared to M2.\\nRemark. The Jeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley\\nparadoxfavors the simpler model since the probability of the data under a complex\\nmodel with a diffuse prior will be very small” (Murphy, 2012). Here, a\\ndiffuse prior refers to a prior that does not favor specific models, i.e.,\\nmany models are a priori plausible under this prior. ♢\\nIf we choose a uniform prior over models, the prior odds term in (8.46)\\nis 1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\\nfactor)\\np(D | M1)\\np(D | M2) . (8.47)\\nIf the Bayes factor is greater than 1, we choose model M1, otherwise\\nmodel M2. In a similar way to frequentist statistics, there are guidelines\\non the size of the ratio that one should consider before ”significance” of\\nthe result (Jeffreys, 1961).\\nRemark (Computing the Marginal Likelihood) . The marginal likelihood\\nplays an important role in model selection: We need to compute Bayes\\nfactors (8.46) and posterior distributions over models (8.43).\\nUnfortunately , computing the marginal likelihood requires us to solve\\nan integral (8.44). This integration is generally analytically intractable,\\nand we will have to resort to approximation techniques, e.g., numerical\\nintegration (Stoer and Burlirsch, 2002), stochastic approximations using\\nMonte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O’Hagan,\\n1991; Rasmussen and Ghahramani, 2003).\\nHowever, there are special cases in which we can solve it. In Section 6.6.1,\\nwe discussed conjugate models. If we choose a conjugate parameter prior\\np(θ), we can compute the marginal likelihood in closed form. In Chap-\\nter 9, we will do exactly this in the context of linear regression. ♢\\nWe have seen a brief introduction to the basic concepts of machine\\nlearning in this chapter. For the rest of this part of the book we will see\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ad767e6-6166-4ccd-9ee7-0f243bd9ffce', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 293, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='288 When Models Meet Data\\nhow the three different flavors of learning in Sections 8.2, 8.3, and 8.4 are\\napplied to the four pillars of machine learning (regression, dimensionality\\nreduction, density estimation, and classification).\\n8.6.4 Further Reading\\nWe mentioned at the start of the section that there are high-level modeling\\nchoices that influence the performance of the model. Examples include the\\nfollowing:\\nThe degree of a polynomial in a regression setting\\nThe number of components in a mixture model\\nThe network architecture of a (deep) neural network\\nThe type of kernel in a support vector machine\\nThe dimensionality of the latent space in PCA\\nThe learning rate (schedule) in an optimization algorithm\\nIn parametric\\nmodels, the number\\nof parameters is\\noften related to the\\ncomplexity of the\\nmodel class.\\nRasmussen and Ghahramani (2001) showed that the automatic Occam’s\\nrazor does not necessarily penalize the number of parameters in a model,\\nbut it is active in terms of the complexity of functions. They also showed\\nthat the automatic Occam’s razor also holds for Bayesian nonparametric\\nmodels with many parameters, e.g., Gaussian processes.\\nIf we focus on the maximum likelihood estimate, there exist a number of\\nheuristics for model selection that discourage overfitting. They are called\\ninformation criteria, and we choose the model with the largest value. The\\nAkaike information criterion (AIC) (Akaike, 1974)Akaike information\\ncriterion\\nlog p(x | θ) − M (8.48)\\ncorrects for the bias of the maximum likelihood estimator by addition of\\na penalty term to compensate for the overfitting of more complex models\\nwith lots of parameters. Here, M is the number of model parameters. The\\nAIC estimates the relative information lost by a given model.\\nThe Bayesian information criterion (BIC) (Schwarz, 1978)Bayesian\\ninformation\\ncriterion log p(x) = log\\nZ\\np(x | θ)p(θ)dθ ≈ log p(x | θ) − 1\\n2 M log N (8.49)\\ncan be used for exponential family distributions. Here, N is the number\\nof data points and M is the number of parameters. BIC penalizes model\\ncomplexity more heavily than AIC.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b76ed1b2-1308-4947-9ab5-42fe62f9af57', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 294, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9\\nLinear Regression\\nIn the following, we will apply the mathematical concepts from Chap-\\nters 2, 5, 6, and 7 to solve linear regression (curve fitting) problems. In\\nregression, we aim to find a function f that maps inputs x ∈ RD to corre- regression\\nsponding function values f(x) ∈ R. We assume we are given a set of train-\\ning inputs xn and corresponding noisy observationsyn = f(xn)+ϵ, where\\nϵ is an i.i.d. random variable that describes measurement/observation\\nnoise and potentially unmodeled processes (which we will not consider\\nfurther in this chapter). Throughout this chapter, we assume zero-mean\\nGaussian noise. Our task is to find a function that not only models the\\ntraining data, but generalizes well to predicting function values at input\\nlocations that are not part of the training data (see Chapter 8). An il-\\nlustration of such a regression problem is given in Figure 9.1. A typical\\nregression setting is given in Figure 9.1(a): For some input values xn, we\\nobserve (noisy) function values yn = f(xn) + ϵ. The task is to infer the\\nfunction f that generated the data and generalizes well to function values\\nat new input locations. A possible solution is given in Figure 9.1(b), where\\nwe also show three distributions centered at the function values f(x) that\\nrepresent the noise in the data.\\nRegression is a fundamental problem in machine learning, and regres-\\nsion problems appear in a diverse range of research areas and applica-\\nFigure 9.1\\n(a) Dataset;\\n(b) possible solution\\nto the regression\\nproblem.\\n−4 −2 0 2 4\\nx\\n−0.4\\n−0.2\\n0.0\\n0.2\\n0.4\\ny\\n(a) Regression problem: observed noisy func-\\ntion values from which we wish to infer the\\nunderlying function that generated the data.\\n−4 −2 0 2 4\\nx\\n−0.4\\n−0.2\\n0.0\\n0.2\\n0.4\\ny\\n(b) Regression solution: possible function\\nthat could have generated the data (blue)\\nwith indication of the measurement noise of\\nthe function value at the corresponding in-\\nputs (orange distributions).\\n289\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='642d1661-19d7-4e0d-941e-78c805914ecb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 295, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='290 Linear Regression\\ntions, including time-series analysis (e.g., system identification), control\\nand robotics (e.g., reinforcement learning, forward/inverse model learn-\\ning), optimization (e.g., line searches, global optimization), and deep-\\nlearning applications (e.g., computer games, speech-to-text translation,\\nimage recognition, automatic video annotation). Regression is also a key\\ningredient of classification algorithms. Finding a regression function re-\\nquires solving a variety of problems, including the following:\\nChoice of the model (type) and the parametrization of the regres-\\nsion function. Given a dataset, what function classes (e.g., polynomi-Normally , the type\\nof noise could also\\nbe a “model choice”,\\nbut we fix the noise\\nto be Gaussian in\\nthis chapter.\\nals) are good candidates for modeling the data, and what particular\\nparametrization (e.g., degree of the polynomial) should we choose?\\nModel selection, as discussed in Section 8.6, allows us to compare var-\\nious models to find the simplest model that explains the training data\\nreasonably well.\\nFinding good parameters. Having chosen a model of the regression\\nfunction, how do we find good model parameters? Here, we will need to\\nlook at different loss/objective functions (they determine what a “good”\\nfit is) and optimization algorithms that allow us to minimize this loss.\\nOverfitting and model selection. Overfitting is a problem when the\\nregression function fits the training data “too well” but does not gen-\\neralize to unseen test data. Overfitting typically occurs if the underly-\\ning model (or its parametrization) is overly flexible and expressive; see\\nSection 8.6. We will look at the underlying reasons and discuss ways to\\nmitigate the effect of overfitting in the context of linear regression.\\nRelationship between loss functions and parameter priors.Loss func-\\ntions (optimization objectives) are often motivated and induced by prob-\\nabilistic models. We will look at the connection between loss functions\\nand the underlying prior assumptions that induce these losses.\\nUncertainty modeling. In any practical setting, we have access to only\\na finite, potentially large, amount of (training) data for selecting the\\nmodel class and the corresponding parameters. Given that this finite\\namount of training data does not cover all possible scenarios, we may\\nwant to describe the remaining parameter uncertainty to obtain a mea-\\nsure of confidence of the model’s prediction at test time; the smaller the\\ntraining set, the more important uncertainty modeling. Consistent mod-\\neling of uncertainty equips model predictions with confidence bounds.\\nIn the following, we will be using the mathematical tools from Chap-\\nters 3, 5, 6 and 7 to solve linear regression problems. We will discuss\\nmaximum likelihood and maximum a posteriori (MAP) estimation to find\\noptimal model parameters. Using these parameter estimates, we will have\\na brief look at generalization errors and overfitting. Toward the end of\\nthis chapter, we will discuss Bayesian linear regression, which allows us to\\nreason about model parameters at a higher level, thereby removing some\\nof the problems encountered in maximum likelihood and MAP estimation.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fe9780d3-39e6-4a6e-909e-4229e7613c1e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 296, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.1 Problem Formulation 291\\n9.1 Problem Formulation\\nBecause of the presence of observation noise, we will adopt a probabilis-\\ntic approach and explicitly model the noise using a likelihood function.\\nMore specifically , throughout this chapter, we consider a regression prob-\\nlem with the likelihood function\\np(y | x) = N\\n\\x00\\ny | f(x), σ 2\\x01\\n. (9.1)\\nHere, x ∈ RD are inputs and y ∈ R are noisy function values (targets).\\nWith (9.1), the functional relationship between x and y is given as\\ny = f(x) + ϵ , (9.2)\\nwhere ϵ ∼ N\\n\\x00\\n0, σ 2\\x01\\nis independent, identically distributed (i.i.d.) Gaus-\\nsian measurement noise with mean 0 and variance σ2. Our objective is\\nto find a function that is close (similar) to the unknown function f that\\ngenerated the data and that generalizes well.\\nIn this chapter, we focus on parametric models, i.e., we choose a para-\\nmetrized function and find parametersθ that “work well” for modeling the\\ndata. For the time being, we assume that the noise variance σ2 is known\\nand focus on learning the model parameters θ. In linear regression, we\\nconsider the special case that the parameters θ appear linearly in our\\nmodel. An example of linear regression is given by\\np(y | x, θ) = N\\n\\x00\\ny | x⊤θ, σ 2\\x01\\n(9.3)\\n⇐ ⇒y = x⊤θ + ϵ , ϵ ∼ N\\n\\x00\\n0, σ 2\\x01\\n, (9.4)\\nwhere θ ∈ RD are the parameters we seek. The class of functions de-\\nscribed by (9.4) are straight lines that pass through the origin. In (9.4),\\nwe chose a parametrization f(x) = x⊤θ. A Dirac delta (delta\\nfunction) is zero\\neverywhere except\\nat a single point,\\nand its integral is 1.\\nIt can be considered\\na Gaussian in the\\nlimit of σ2 → 0.\\nThe likelihood in (9.3) is the probability density function of y evalu-\\nlikelihood\\nated at x⊤θ. Note that the only source of uncertainty originates from the\\nobservation noise (as x and θ are assumed known in (9.3)). Without ob-\\nservation noise, the relationship between x and y would be deterministic\\nand (9.3) would be a Dirac delta.\\nExample 9.1\\nFor x, θ ∈ R the linear regression model in (9.4) describes straight lines\\n(linear functions), and the parameter θ is the slope of the line. Fig-\\nure 9.2(a) shows some example functions for different values of θ.\\nLinear regression\\nrefers to models that\\nare linear in the\\nparameters.\\nThe linear regression model in (9.3)–(9.4) is not only linear in the pa-\\nrameters, but also linear in the inputs x. Figure 9.2(a) shows examples\\nof such functions. We will see later that y = ϕ⊤(x)θ for nonlinear trans-\\nformations ϕ is also a linear regression model because “linear regression”\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='215cd01a-9842-428e-83d3-656fed6fb8bc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 297, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='292 Linear Regression\\nFigure 9.2 Linear\\nregression example.\\n(a) Example\\nfunctions that fall\\ninto this category;\\n(b) training set;\\n(c) maximum\\nlikelihood estimate.\\n−10 0 10\\nx\\n−20\\n0\\n20\\ny (a) Example functions (straight\\nlines) that can be described us-\\ning the linear model in (9.4).\\n−10 −5 0 5 10\\nx\\n−10\\n0\\n10\\ny\\n(b) Training set.\\n−10 −5 0 5 10\\nx\\n−10\\n0\\n10\\ny\\n (c) Maximum likelihood esti-\\nmate.\\nrefers to models that are “linear in the parameters”, i.e., models that de-\\nscribe a function by a linear combination of input features. Here, a “fea-\\nture” is a representation ϕ(x) of the inputs x.\\nIn the following, we will discuss in more detail how to find good pa-\\nrameters θ and how to evaluate whether a parameter set “works well”.\\nFor the time being, we assume that the noise variance σ2 is known.\\n9.2 Parameter Estimation\\nConsider the linear regression setting (9.4) and assume we are given a\\ntraining set D := {(x1, y1), . . . ,(xN , yN)} consisting of N inputs xn ∈training set\\nRD and corresponding observations/targets yn ∈ R, n = 1, . . . , N. TheFigure 9.3\\nProbabilistic\\ngraphical model for\\nlinear regression.\\nObserved random\\nvariables are\\nshaded,\\ndeterministic/\\nknown values are\\nwithout circles.\\nθ\\nyn\\nσ\\nxn\\nn = 1, . . . , N\\ncorresponding graphical model is given in Figure 9.3. Note that yi and yj\\nare conditionally independent given their respective inputs xi, xj so that\\nthe likelihood factorizes according to\\np(Y | X , θ) = p(y1, . . . , yN | x1, . . . ,xN , θ) (9.5a)\\n=\\nNY\\nn=1\\np(yn | xn, θ) =\\nNY\\nn=1\\nN\\n\\x00\\nyn | x⊤\\nn θ, σ 2\\x01\\n, (9.5b)\\nwhere we defined X := {x1, . . . ,xN } and Y := {y1, . . . , yN } as the sets\\nof training inputs and corresponding targets, respectively . The likelihood\\nand the factors p(yn | xn, θ) are Gaussian due to the noise distribution;\\nsee (9.3).\\nIn the following, we will discuss how to find optimal parameters θ∗ ∈\\nRD for the linear regression model (9.4). Once the parameters θ∗ are\\nfound, we can predict function values by using this parameter estimate\\nin (9.4) so that at an arbitrary test input x∗ the distribution of the corre-\\nsponding target y∗ is\\np(y∗ | x∗, θ∗) = N\\n\\x00\\ny∗ | x⊤\\n∗ θ∗, σ 2\\x01\\n. (9.6)\\nIn the following, we will have a look at parameter estimation by maxi-\\nmizing the likelihood, a topic that we already covered to some degree in\\nSection 8.3.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ce773f6c-987f-4512-bc88-c634219deb88', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 298, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2 Parameter Estimation 293\\n9.2.1 Maximum Likelihood Estimation\\nA widely used approach to finding the desired parametersθML is maximum maximum likelihood\\nestimationlikelihood estimation, where we find parameters θML that maximize the\\nlikelihood (9.5b). Intuitively , maximizing the likelihood means maximiz- Maximizing the\\nlikelihood means\\nmaximizing the\\npredictive\\ndistribution of the\\n(training) data\\ngiven the\\nparameters.\\ning the predictive distribution of the training data given the model param-\\neters. We obtain the maximum likelihood parameters as\\nθML ∈ arg max\\nθ\\np(Y | X , θ) . (9.7)\\nThe likelihood is not\\na probability\\ndistribution in the\\nparameters.\\nRemark. The likelihood p(y | x, θ) is not a probability distribution in θ: It\\nis simply a function of the parameters θ but does not integrate to 1 (i.e.,\\nit is unnormalized), and may not even be integrable with respect to θ.\\nHowever, the likelihood in (9.7) is a normalized probability distribution\\nin y. ♢\\nTo find the desired parameters θML that maximize the likelihood, we\\ntypically perform gradient ascent (or gradient descent on the negative\\nlikelihood). In the case of linear regression we consider here, however, Since the logarithm\\nis a (strictly)\\nmonotonically\\nincreasing function,\\nthe optimum of a\\nfunction f is\\nidentical to the\\noptimum of log f.\\na closed-form solution exists, which makes iterative gradient descent un-\\nnecessary . In practice, instead of maximizing the likelihood directly , we\\napply the log-transformation to the likelihood function and minimize the\\nnegative log-likelihood.\\nRemark (Log-Transformation). Since the likelihood (9.5b) is a product of\\nN Gaussian distributions, the log-transformation is useful since (a) it does\\nnot suffer from numerical underflow, and (b) the differentiation rules will\\nturn out simpler. More specifically , numerical underflow will be a prob-\\nlem when we multiply N probabilities, where N is the number of data\\npoints, since we cannot represent very small numbers, such as 10−256.\\nFurthermore, the log-transform will turn the product into a sum of log-\\nprobabilities such that the corresponding gradient is a sum of individual\\ngradients, instead of a repeated application of the product rule (5.46) to\\ncompute the gradient of a product of N terms. ♢\\nTo find the optimal parameters θML of our linear regression problem,\\nwe minimize the negative log-likelihood\\n− log p(Y | X , θ) = − log\\nNY\\nn=1\\np(yn | xn, θ) = −\\nNX\\nn=1\\nlog p(yn | xn, θ) , (9.8)\\nwhere we exploited that the likelihood (9.5b) factorizes over the number\\nof data points due to our independence assumption on the training set.\\nIn the linear regression model (9.4), the likelihood is Gaussian (due to\\nthe Gaussian additive noise term), such that we arrive at\\nlog p(yn | xn, θ) = − 1\\n2σ2 (yn − x⊤\\nn θ)2 + const , (9.9)\\nwhere the constant includes all terms independent ofθ. Using (9.9) in the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c2b0eafd-2423-4b9b-b357-09a4fdb889cd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 299, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='294 Linear Regression\\nnegative log-likelihood (9.8), we obtain (ignoring the constant terms)\\nL(θ) := 1\\n2σ2\\nNX\\nn=1\\n(yn − x⊤\\nn θ)2 (9.10a)\\n= 1\\n2σ2 (y − Xθ)⊤(y − Xθ) = 1\\n2σ2 ∥y − Xθ ∥\\n2\\n, (9.10b)\\nwhere we define the design matrix X := [ x1, . . . ,xN]⊤ ∈ RN ×D as theThe negative\\nlog-likelihood\\nfunction is also\\ncalled error function.\\ndesign matrix\\ncollection of training inputs and y := [y1, . . . , yN]⊤ ∈ RN as a vector that\\ncollects all training targets. Note that the nth row in the design matrix X\\ncorresponds to the training input xn. In (9.10b), we used the fact that the\\nThe squared error is\\noften used as a\\nmeasure of distance.\\nsum of squared errors between the observationsyn and the corresponding\\nmodel prediction x⊤\\nn θ equals the squared distance between y and Xθ.\\nRecall from\\nSection 3.1 that\\n∥x∥2 = x⊤x if we\\nchoose the dot\\nproduct as the inner\\nproduct.\\nWith (9.10b), we have now a concrete form of the negative log-likelihood\\nfunction we need to optimize. We immediately see that (9.10b) is quadratic\\nin θ. This means that we can find a unique global solution θML for mini-\\nmizing the negative log-likelihood L. We can find the global optimum by\\ncomputing the gradient of L, setting it to 0 and solving for θ.\\nUsing the results from Chapter 5, we compute the gradient of L with\\nrespect to the parameters as\\ndL\\ndθ = d\\ndθ\\n\\x12 1\\n2σ2 (y − Xθ)⊤(y − Xθ)\\n\\x13\\n(9.11a)\\n= 1\\n2σ2\\nd\\ndθ\\n\\x10\\ny⊤y − 2y⊤Xθ + θ⊤X ⊤Xθ\\n\\x11\\n(9.11b)\\n= 1\\nσ2 (−y⊤X + θ⊤X ⊤X) ∈ R1×D . (9.11c)\\nThe maximum likelihood estimator θML solves dL\\ndθ = 0⊤ (necessary opti-\\nmality condition) and we obtainIgnoring the\\npossibility of\\nduplicate data\\npoints, rk(X) = D\\nif N ⩾ D, i.e., we\\ndo not have more\\nparameters than\\ndata points.\\ndL\\ndθ = 0⊤ (9.11c)\\n⇐ ⇒ θ⊤\\nMLX ⊤X = y⊤X (9.12a)\\n⇐ ⇒ θ⊤\\nML = y⊤X(X ⊤X)−1 (9.12b)\\n⇐ ⇒ θML = (X ⊤X)−1X ⊤y . (9.12c)\\nWe could right-multiply the first equation by (X ⊤X)−1 because X ⊤X is\\npositive definite if rk(X) = D, where rk(X) denotes the rank of X.\\nRemark. Setting the gradient to0⊤ is a necessary and sufficient condition,\\nand we obtain a global minimum since the Hessian ∇2\\nθL(θ) = X ⊤X ∈\\nRD×D is positive definite. ♢\\nRemark. The maximum likelihood solution in (9.12c) requires us to solve\\na system of linear equations of the form Aθ = b with A = (X ⊤X) and\\nb = X ⊤y. ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f382722-d1a6-43a6-8a31-9bf0df08e26c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 300, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2 Parameter Estimation 295\\nExample 9.2 (Fitting Lines)\\nLet us have a look at Figure 9.2, where we aim to fit a straight linef(x) =\\nθx, where θ is an unknown slope, to a dataset using maximum likelihood\\nestimation. Examples of functions in this model class (straight lines) are\\nshown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we find\\nthe maximum likelihood estimate of the slope parameter θ using (9.12c)\\nand obtain the maximum likelihood linear function in Figure 9.2(c).\\nMaximum Likelihood Estimation with Features\\nSo far, we considered the linear regression setting described in (9.4),\\nwhich allowed us to fit straight lines to data using maximum likelihood\\nestimation. However, straight lines are not sufficiently expressive when it Linear regression\\nrefers to “linear-in-\\nthe-parameters”\\nregression models,\\nbut the inputs can\\nundergo any\\nnonlinear\\ntransformation.\\ncomes to fitting more interesting data. Fortunately , linear regression offers\\nus a way to fit nonlinear functions within the linear regression framework:\\nSince “linear regression” only refers to “linear in the parameters”, we can\\nperform an arbitrary nonlinear transformation ϕ(x) of the inputs x and\\nthen linearly combine the components of this transformation. The corre-\\nsponding linear regression model is\\np(y | x, θ) = N\\n\\x00\\ny | ϕ⊤(x)θ, σ 2\\x01\\n⇐ ⇒y = ϕ⊤(x)θ + ϵ =\\nK−1X\\nk=0\\nθkϕk(x) + ϵ ,\\n(9.13)\\nwhere ϕ : RD → RK is a (nonlinear) transformation of the inputs x and\\nϕk : RD → R is the kth component of the feature vector ϕ. Note that the feature vector\\nmodel parameters θ still appear only linearly .\\nExample 9.3 (Polynomial Regression)\\nWe are concerned with a regression problemy = ϕ⊤(x)θ+ϵ, where x ∈ R\\nand θ ∈ RK. A transformation that is often used in this context is\\nϕ(x) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nϕ0(x)\\nϕ1(x)\\n...\\nϕK−1(x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\nx\\nx2\\nx3\\n...\\nxK−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ RK . (9.14)\\nThis means that we “lift” the original one-dimensional input space into\\na K-dimensional feature space consisting of all monomials xk for k =\\n0, . . . , K − 1. With these features, we can model polynomials of degree\\n⩽ K−1 within the framework of linear regression: A polynomial of degree\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e46fcc72-40bc-4b00-b329-65298c74e92d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 301, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='296 Linear Regression\\nK − 1 is\\nf(x) =\\nK−1X\\nk=0\\nθkxk = ϕ⊤(x)θ , (9.15)\\nwhere ϕ is defined in (9.14) and θ = [θ0, . . . , θK−1]⊤ ∈ RK contains the\\n(linear) parameters θk.\\nLet us now have a look at maximum likelihood estimation of the param-\\neters θ in the linear regression model (9.13). We consider training inputs\\nxn ∈ RD and targets yn ∈ R, n = 1, . . . , N, and define the feature matrixfeature matrix\\n(design matrix) asdesign matrix\\nΦ :=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nϕ⊤(x1)\\n...\\nϕ⊤(xN)\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nϕ0(x1) · · · ϕK−1(x1)\\nϕ0(x2) · · · ϕK−1(x2)\\n... ...\\nϕ0(xN) · · · ϕK−1(xN)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb ∈ RN ×K , (9.16)\\nwhere Φij = ϕj(xi) and ϕj : RD → R.\\nExample 9.4 (Feature Matrix for Second-order Polynomials)\\nFor a second-order polynomial and N training points xn ∈ R, n =\\n1, . . . , N, the feature matrix is\\nΦ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x1 x2\\n1\\n1 x2 x2\\n2\\n... ... ...\\n1 xN x2\\nN\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb . (9.17)\\nWith the feature matrixΦ defined in (9.16), the negative log-likelihood\\nfor the linear regression model (9.13) can be written as\\n− log p(Y | X , θ) = 1\\n2σ2 (y − Φθ)⊤(y − Φθ) + const . (9.18)\\nComparing (9.18) with the negative log-likelihood in (9.10b) for the “fea-\\nture-free” model, we immediately see we just need to replace X with Φ.\\nSince both X and Φ are independent of the parameters θ that we wish to\\noptimize, we arrive immediately at the maximum likelihood estimatemaximum likelihood\\nestimate\\nθML = (Φ⊤Φ)−1Φ⊤y (9.19)\\nfor the linear regression problem with nonlinear features defined in (9.13).\\nRemark. When we were working without features, we required X ⊤X to\\nbe invertible, which is the case when rk(X) = D, i.e., the columns of X\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='df84fedc-bbaa-4c62-8beb-157ef554897d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 302, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2 Parameter Estimation 297\\nare linearly independent. In (9.19), we therefore require Φ⊤Φ ∈ RK×K\\nto be invertible. This is the case if and only if rk(Φ) = K. ♢\\nExample 9.5 (Maximum Likelihood Polynomial Fit)\\nFigure 9.4\\nPolynomial\\nregression:\\n(a) dataset\\nconsisting of\\n(xn, yn) pairs,\\nn = 1, . . . ,10;\\n(b) maximum\\nlikelihood\\npolynomial of\\ndegree 4.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(a) Regression dataset.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (b) Polynomial of degree 4 determined by max-\\nimum likelihood estimation.\\nConsider the dataset in Figure 9.4(a). The dataset consists of N = 10\\npairs (xn, yn), where xn ∼ U [−5, 5] and yn = − sin(xn/5) + cos(xn) + ϵ,\\nwhere ϵ ∼ N\\n\\x00\\n0, 0.22\\x01\\n.\\nWe fit a polynomial of degree 4 using maximum likelihood estimation,\\ni.e., parameters θML are given in (9.19). The maximum likelihood estimate\\nyields function values ϕ⊤(x∗)θML at any test location x∗. The result is\\nshown in Figure 9.4(b).\\nEstimating the Noise Variance\\nThus far, we assumed that the noise variance σ2 is known. However, we\\ncan also use the principle of maximum likelihood estimation to obtain the\\nmaximum likelihood estimator σ2\\nML for the noise variance. To do this, we\\nfollow the standard procedure: We write down the log-likelihood, com-\\npute its derivative with respect to σ2 > 0, set it to 0, and solve. The\\nlog-likelihood is given by\\nlog p(Y | X , θ, σ2) =\\nNX\\nn=1\\nlog N\\n\\x00\\nyn | ϕ⊤(xn)θ, σ 2\\x01\\n(9.20a)\\n=\\nNX\\nn=1\\n\\x12\\n−1\\n2 log(2π) − 1\\n2 log σ2 − 1\\n2σ2 (yn − ϕ⊤(xn)θ)2\\n\\x13\\n(9.20b)\\n= − N\\n2 log σ2 − 1\\n2σ2\\nNX\\nn=1\\n(yn − ϕ⊤(xn)θ)2\\n| {z }\\n=:s\\n+ const . (9.20c)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15ddb2c1-86be-4c26-9474-22e182470c37', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 303, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='298 Linear Regression\\nThe partial derivative of the log-likelihood with respect to σ2 is then\\n∂ log p(Y | X , θ, σ2)\\n∂σ2 = − N\\n2σ2 + 1\\n2σ4 s = 0 (9.21a)\\n⇐ ⇒ N\\n2σ2 = s\\n2σ4 (9.21b)\\nso that we identify\\nσ2\\nML = s\\nN = 1\\nN\\nNX\\nn=1\\n(yn − ϕ⊤(xn)θ)2 . (9.22)\\nTherefore, the maximum likelihood estimate of the noise variance is the\\nempirical mean of the squared distances between the noise-free function\\nvalues ϕ⊤(xn)θ and the corresponding noisy observations yn at input lo-\\ncations xn.\\n9.2.2 Overfitting in Linear Regression\\nWe just discussed how to use maximum likelihood estimation to fit lin-\\near models (e.g., polynomials) to data. We can evaluate the quality of\\nthe model by computing the error/loss incurred. One way of doing this\\nis to compute the negative log-likelihood (9.10b), which we minimized\\nto determine the maximum likelihood estimator. Alternatively , given that\\nthe noise parameter σ2 is not a free model parameter, we can ignore the\\nscaling by 1/σ2, so that we end up with a squared-error-loss function\\n∥y − Φθ∥\\n2\\n. Instead of using this squared loss, we often use theroot meanroot mean square\\nerror square error (RMSE)\\nRMSE\\nr\\n1\\nN ∥y − Φθ∥\\n2\\n=\\nvuut 1\\nN\\nNX\\nn=1\\n(yn − ϕ⊤(xn)θ)2 , (9.23)\\nwhich (a) allows us to compare errors of datasets with different sizes\\nand (b) has the same scale and the same units as the observed func-The RMSE is\\nnormalized. tion values yn. For example, if we fit a model that maps post-codes ( x\\nis given in latitude, longitude) to house prices ( y-values are EUR) then\\nthe RMSE is also measured in EUR, whereas the squared error is given\\nin EUR2. If we choose to include the factor σ2 from the original negativeThe negative\\nlog-likelihood is\\nunitless.\\nlog-likelihood (9.10b), then we end up with a unitless objective, i.e., in\\nthe preceding example, our objective would no longer be in EUR or EUR2.\\nFor model selection (see Section 8.6), we can use the RMSE (or the\\nnegative log-likelihood) to determine the best degree of the polynomial by\\nfinding the polynomial degree M that minimizes the objective. Given that\\nthe polynomial degree is a natural number, we can perform a brute-force\\nsearch and enumerate all (reasonable) values of M. For a training set of\\nsize N it is sufficient to test 0 ⩽ M ⩽ N − 1. For M < N , the maximum\\nlikelihood estimator is unique. For M ⩾ N, we have more parameters\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e87b060b-c5ef-4fdf-914d-a72d7d178b5d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 304, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2 Parameter Estimation 299\\nFigure 9.5\\nMaximum\\nlikelihood fits for\\ndifferent polynomial\\ndegrees M.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\n(a) M = 0\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (b) M = 1\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (c) M = 3\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\n(d) M = 4\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (e) M = 6\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE (f) M = 9\\nthan data points, and would need to solve an underdetermined system of\\nlinear equations ( Φ⊤Φ in (9.19) would also no longer be invertible) so\\nthat there are infinitely many possible maximum likelihood estimators.\\nFigure 9.5 shows a number of polynomial fits determined by maximum\\nlikelihood for the dataset from Figure 9.4(a) with N = 10 observations.\\nWe notice that polynomials of low degree (e.g., constants ( M = 0 ) or\\nlinear (M = 1)) fit the data poorly and, hence, are poor representations\\nof the true underlying function. For degrees M = 3 , . . . ,6, the fits look\\nplausible and smoothly interpolate the data. When we go to higher-degree The case of\\nM = N − 1 is\\nextreme in the sense\\nthat otherwise the\\nnull space of the\\ncorresponding\\nsystem of linear\\nequations would be\\nnon-trivial, and we\\nwould have\\ninfinitely many\\noptimal solutions to\\nthe linear regression\\nproblem.\\npolynomials, we notice that they fit the data better and better. In the ex-\\ntreme case of M = N − 1 = 9, the function will pass through every single\\ndata point. However, these high-degree polynomials oscillate wildly and\\nare a poor representation of the underlying function that generated the\\ndata, such that we suffer from overfitting.\\noverfitting\\nNote that the noise\\nvariance σ2 > 0.\\nRemember that the goal is to achieve good generalization by making\\naccurate predictions for new (unseen) data. We obtain some quantita-\\ntive insight into the dependence of the generalization performance on the\\npolynomial of degree M by considering a separate test set comprising200\\ndata points generated using exactly the same procedure used to generate\\nthe training set. As test inputs, we chose a linear grid of 200 points in the\\ninterval of [−5, 5]. For each choice ofM, we evaluate the RMSE (9.23) for\\nboth the training data and the test data.\\nLooking now at the test error, which is a qualitive measure of the gen-\\neralization properties of the corresponding polynomial, we notice that ini-\\ntially the test error decreases; see Figure 9.6 (orange). For fourth-order\\npolynomials, the test error is relatively low and stays relatively constant up\\nto degree 5. However, from degree6 onward the test error increases signif-\\nicantly , and high-order polynomials have very bad generalization proper-\\nties. In this particular example, this also is evident from the corresponding\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='34ed70ed-5bd4-40d2-b139-24ed4a9d54a3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 305, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='300 Linear Regression\\nFigure 9.6 Training\\nand test error.\\n0 2 4 6 8 10\\nDegree of polynomial\\n0\\n2\\n4\\n6\\n8\\n10RMSE\\nTraining error\\nTest error\\nmaximum likelihood fits in Figure 9.5. Note that the training error (bluetraining error\\ncurve in Figure 9.6) never increases when the degree of the polynomial in-\\ncreases. In our example, the best generalization (the point of the smallest\\ntest error) is obtained for a polynomial of degree M = 4.test error\\n9.2.3 Maximum A Posteriori Estimation\\nWe just saw that maximum likelihood estimation is prone to overfitting.\\nWe often observe that the magnitude of the parameter values becomes\\nrelatively large if we run into overfitting (Bishop, 2006).\\nTo mitigate the effect of huge parameter values, we can place a prior\\ndistribution p(θ) on the parameters. The prior distribution explicitly en-\\ncodes what parameter values are plausible (before having seen any data).\\nFor example, a Gaussian prior p(θ) = N\\n\\x00\\n0, 1\\n\\x01\\non a single parameter\\nθ encodes that parameter values are expected lie in the interval [−2, 2]\\n(two standard deviations around the mean value). Once a dataset X , Y\\nis available, instead of maximizing the likelihood we seek parameters that\\nmaximize the posterior distribution p(θ | X , Y). This procedure is called\\nmaximum a posteriori (MAP) estimation.maximum a\\nposteriori\\nMAP\\nThe posterior over the parameters θ, given the training data X , Y, is\\nobtained by applying Bayes’ theorem (Section 6.3) as\\np(θ | X , Y) = p(Y | X , θ)p(θ)\\np(Y | X ) . (9.24)\\nSince the posterior explicitly depends on the parameter prior p(θ), the\\nprior will have an effect on the parameter vector we find as the maximizer\\nof the posterior. We will see this more explicitly in the following. The\\nparameter vector θMAP that maximizes the posterior (9.24) is the MAP\\nestimate.\\nTo find the MAP estimate, we follow steps that are similar in flavor\\nto maximum likelihood estimation. We start with the log-transform and\\ncompute the log-posterior as\\nlog p(θ | X , Y) = log p(Y | X , θ) + logp(θ) + const , (9.25)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c1e3344-1dcc-4a58-8030-87cb52ea98d4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 306, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2 Parameter Estimation 301\\nwhere the constant comprises the terms that are independent ofθ. We see\\nthat the log-posterior in (9.25) is the sum of the log-likelihoodp(Y | X , θ)\\nand the log-priorlog p(θ) so that the MAP estimate will be a “compromise”\\nbetween the prior (our suggestion for plausible parameter values before\\nobserving data) and the data-dependent likelihood.\\nTo find the MAP estimate θMAP, we minimize the negative log-posterior\\ndistribution with respect to θ, i.e., we solve\\nθMAP ∈ arg min\\nθ\\n{− log p(Y | X , θ) − log p(θ)} . (9.26)\\nThe gradient of the negative log-posterior with respect to θ is\\n−d logp(θ | X , Y)\\ndθ = −d logp(Y | X , θ)\\ndθ − d logp(θ)\\ndθ , (9.27)\\nwhere we identify the first term on the right-hand side as the gradient of\\nthe negative log-likelihood from (9.11c).\\nWith a (conjugate) Gaussian priorp(θ) = N\\n\\x00\\n0, b 2I\\n\\x01\\non the parameters\\nθ, the negative log-posterior for the linear regression setting (9.13), we\\nobtain the negative log posterior\\n− log p(θ | X , Y) = 1\\n2σ2 (y − Φθ)⊤(y − Φθ) + 1\\n2b2 θ⊤θ + const . (9.28)\\nHere, the first term corresponds to the contribution from the log-likelihood,\\nand the second term originates from the log-prior. The gradient of the log-\\nposterior with respect to the parameters θ is then\\n−d logp(θ | X , Y)\\ndθ = 1\\nσ2 (θ⊤Φ⊤Φ − y⊤Φ) + 1\\nb2 θ⊤ . (9.29)\\nWe will find the MAP estimate θMAP by setting this gradient to 0⊤ and\\nsolving for θMAP. We obtain\\n1\\nσ2 (θ⊤Φ⊤Φ − y⊤Φ) + 1\\nb2 θ⊤ = 0⊤ (9.30a)\\n⇐ ⇒θ⊤\\n\\x12 1\\nσ2Φ⊤Φ + 1\\nb2 I\\n\\x13\\n− 1\\nσ2 y⊤Φ = 0⊤ (9.30b)\\n⇐ ⇒θ⊤\\n\\x12\\nΦ⊤Φ + σ2\\nb2 I\\n\\x13\\n= y⊤Φ (9.30c)\\n⇐ ⇒θ⊤ = y⊤Φ\\n\\x12\\nΦ⊤Φ + σ2\\nb2 I\\n\\x13−1\\n(9.30d)\\nso that the MAP estimate is (by transposing both sides of the last equality) Φ⊤Φ is symmetric,\\npositive semi\\ndefinite. The\\nadditional term\\nin (9.31) is strictly\\npositive definite so\\nthat the inverse\\nexists.\\nθMAP =\\n\\x12\\nΦ⊤Φ + σ2\\nb2 I\\n\\x13−1\\nΦ⊤y . (9.31)\\nComparing the MAP estimate in (9.31) with the maximum likelihood es-\\ntimate in (9.19), we see that the only difference between both solutions\\nis the additional term σ2\\nb2 I in the inverse matrix. This term ensures that\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='468e8c38-2622-4e78-818e-cbc26254fb32', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 307, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='302 Linear Regression\\nΦ⊤Φ + σ2\\nb2 I is symmetric and strictly positive definite (i.e., its inverse\\nexists and the MAP estimate is the unique solution of a system of linear\\nequations). Moreover, it reflects the impact of the regularizer.\\nExample 9.6 (MAP Estimation for Polynomial Regression)\\nIn the polynomial regression example from Section 9.2.1, we place a Gaus-\\nsian prior p(θ) = N\\n\\x00\\n0, I\\n\\x01\\non the parameters θ and determine the MAP\\nestimates according to (9.31). In Figure 9.7, we show both the maximum\\nlikelihood and the MAP estimates for polynomials of degree 6 (left) and\\ndegree 8 (right). The prior (regularizer) does not play a significant role\\nfor the low-degree polynomial, but keeps the function relatively smooth\\nfor higher-degree polynomials. Although the MAP estimate can push the\\nboundaries of overfitting, it is not a general solution to this problem, so\\nwe need a more principled approach to tackle overfitting.\\nFigure 9.7\\nPolynomial\\nregression:\\nmaximum likelihood\\nand MAP estimates.\\n(a) Polynomials of\\ndegree 6;\\n(b) polynomials of\\ndegree 8.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP\\n(a) Polynomials of degree 6.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP (b) Polynomials of degree 8.\\n9.2.4 MAP Estimation as Regularization\\nInstead of placing a prior distribution on the parameters θ, it is also pos-\\nsible to mitigate the effect of overfitting by penalizing the amplitude of\\nthe parameter by means of regularization. In regularized least squares, weregularization\\nregularized least\\nsquares\\nconsider the loss function\\n∥y − Φθ∥\\n2\\n+ λ ∥θ∥\\n2\\n2 , (9.32)\\nwhich we minimize with respect to θ (see Section 8.2.3). Here, the first\\nterm is a data-fit term (also called misfit term), which is proportional todata-fit term\\nmisfit term the negative log-likelihood; see (9.10b). The second term is called the\\nregularizer, and the regularization parameter λ ⩾ 0 controls the “strict-regularizer\\nregularization\\nparameter\\nness” of the regularization.\\nRemark. Instead of the Euclidean norm ∥·∥2, we can choose any p-norm\\n∥·∥p in (9.32). In practice, smaller values for p lead to sparser solutions.\\nHere, “sparse” means that many parameter values θd = 0, which is also\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='464be757-3bec-4f29-81fd-a15514e76b07', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 308, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3 Bayesian Linear Regression 303\\nuseful for variable selection. For p = 1 , the regularizer is called LASSO LASSO\\n(least absolute shrinkage and selection operator) and was proposed by Tib-\\nshirani (1996). ♢\\nThe regularizer λ ∥θ∥\\n2\\n2 in (9.32) can be interpreted as a negative log-\\nGaussian prior, which we use in MAP estimation; see (9.26). More specif-\\nically , with a Gaussian prior p(θ) = N\\n\\x00\\n0, b 2I\\n\\x01\\n, we obtain the negative\\nlog-Gaussian prior\\n− log p(θ) = 1\\n2b2 ∥θ∥\\n2\\n2 + const (9.33)\\nso that for λ = 1\\n2b2 the regularization term and the negative log-Gaussian\\nprior are identical.\\nGiven that the regularized least-squares loss function in (9.32) consists\\nof terms that are closely related to the negative log-likelihood plus a neg-\\native log-prior, it is not surprising that, when we minimize this loss, we\\nobtain a solution that closely resembles the MAP estimate in (9.31). More\\nspecifically , minimizing the regularized least-squares loss function yields\\nθRLS = (Φ⊤Φ + λI)−1Φ⊤y , (9.34)\\nwhich is identical to the MAP estimate in (9.31) for λ = σ2\\nb2 , where σ2 is\\nthe noise variance and b2 the variance of the (isotropic) Gaussian prior\\np(θ) = N\\n\\x00\\n0, b 2I\\n\\x01\\n. A point estimate is a\\nsingle specific\\nparameter value,\\nunlike a distribution\\nover plausible\\nparameter settings.\\nSo far, we have covered parameter estimation using maximum likeli-\\nhood and MAP estimation where we found point estimates θ∗ that op-\\ntimize an objective function (likelihood or posterior). We saw that both\\nmaximum likelihood and MAP estimation can lead to overfitting. In the\\nnext section, we will discuss Bayesian linear regression, where we use\\nBayesian inference (Section 8.4) to find a posterior distribution over the\\nunknown parameters, which we subsequently use to make predictions.\\nMore specifically , for predictions we will average over all plausible sets of\\nparameters instead of focusing on a point estimate.\\n9.3 Bayesian Linear Regression\\nPreviously , we looked at linear regression models where we estimated the\\nmodel parameters θ, e.g., by means of maximum likelihood or MAP esti-\\nmation. We discovered that MLE can lead to severe overfitting, in particu-\\nlar, in the small-data regime. MAP addresses this issue by placing a prior\\non the parameters that plays the role of a regularizer. Bayesian linear\\nregressionBayesian linear regression pushes the idea of the parameter prior a step\\nfurther and does not even attempt to compute a point estimate of the\\nparameters, but instead the full posterior distribution over the parameters\\nis taken into account when making predictions. This means we do not fit\\nany parameters, but we compute a mean over all plausible parameters\\nsettings (according to the posterior).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d67a2637-e3f5-4280-bb31-20ebb786755e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 309, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='304 Linear Regression\\n9.3.1 Model\\nIn Bayesian linear regression, we consider the model\\nprior p(θ) = N\\n\\x00\\nm0, S0\\n\\x01\\n,\\nlikelihood p(y | x, θ) = N\\n\\x00\\ny | ϕ⊤(x)θ, σ 2\\x01\\n,\\n(9.35)\\nwhere we now explicitly place a Gaussian prior p(θ) = N\\n\\x00\\nm0, S0\\n\\x01\\non θ,Figure 9.8\\nGraphical model for\\nBayesian linear\\nregression.\\nθ\\ny\\nσ\\nx\\nm0 S0\\nwhich turns the parameter vector into a random variable. This allows us\\nto write down the corresponding graphical model in Figure 9.8, where we\\nmade the parameters of the Gaussian prior on θ explicit. The full proba-\\nbilistic model, i.e., the joint distribution of observed and unobserved ran-\\ndom variables, y and θ, respectively , is\\np(y, θ | x) = p(y | x, θ)p(θ) . (9.36)\\n9.3.2 Prior Predictions\\nIn practice, we are usually not so much interested in the parameter values\\nθ themselves. Instead, our focus often lies in the predictions we make\\nwith those parameter values. In a Bayesian setting, we take the parameter\\ndistribution and average over all plausible parameter settings when we\\nmake predictions. More specifically , to make predictions at an input x∗,\\nwe integrate out θ and obtain\\np(y∗ | x∗) =\\nZ\\np(y∗ | x∗, θ)p(θ)dθ = Eθ[p(y∗ | x∗, θ)] , (9.37)\\nwhich we can interpret as the average prediction of y∗ | x∗, θ for all plau-\\nsible parameters θ according to the prior distribution p(θ). Note that pre-\\ndictions using the prior distribution only require us to specify the input\\nx∗, but no training data.\\nIn our model (9.35), we chose a conjugate (Gaussian) prior on θ so\\nthat the predictive distribution is Gaussian as well (and can be computed\\nin closed form): With the prior distributionp(θ) = N\\n\\x00\\nm0, S0\\n\\x01\\n, we obtain\\nthe predictive distribution as\\np(y∗ | x∗) = N\\n\\x00\\nϕ⊤(x∗)m0, ϕ⊤(x∗)S0ϕ(x∗) + σ2\\x01\\n, (9.38)\\nwhere we exploited that (i) the prediction is Gaussian due to conjugacy\\n(see Section 6.6) and the marginalization property of Gaussians (see Sec-\\ntion 6.5), (ii) the Gaussian noise is independent so that\\nV[y∗] = Vθ[ϕ⊤(x∗)θ] + Vϵ[ϵ] , (9.39)\\nand (iii) y∗ is a linear transformation of θ so that we can apply the rules\\nfor computing the mean and covariance of the prediction analytically by\\nusing (6.50) and (6.51), respectively . In (9.38), the termϕ⊤(x∗)S0ϕ(x∗)\\nin the predictive variance explicitly accounts for the uncertainty associated\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='650fb6f8-3226-4deb-990c-d77d6700ee2e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 310, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3 Bayesian Linear Regression 305\\nwith the parameters θ, whereas σ2 is the uncertainty contribution due to\\nthe measurement noise.\\nIf we are interested in predicting noise-free function values f(x∗) =\\nϕ⊤(x∗)θ instead of the noise-corrupted targets y∗ we obtain\\np(f(x∗)) = N\\n\\x00\\nϕ⊤(x∗)m0, ϕ⊤(x∗)S0ϕ(x∗)\\n\\x01\\n, (9.40)\\nwhich only differs from (9.38) in the omission of the noise variance σ2 in\\nthe predictive variance.\\nRemark (Distribution over Functions). Since we can represent the distri- The parameter\\ndistribution p(θ)\\ninduces a\\ndistribution over\\nfunctions.\\nbution p(θ) using a set of samples θi and every sample θi gives rise to a\\nfunction fi(·) = θ⊤\\ni ϕ(·), it follows that the parameter distribution p(θ)\\ninduces a distribution p(f(·)) over functions. Here we use the notation (·)\\nto explicitly denote a functional relationship. ♢\\nExample 9.7 (Prior over Functions)\\nFigure 9.9 Prior\\nover functions.\\n(a) Distribution over\\nfunctions\\nrepresented by the\\nmean function\\n(black line) and the\\nmarginal\\nuncertainties\\n(shaded),\\nrepresenting the\\n67% and 95%\\nconfidence bounds,\\nrespectively;\\n(b) samples from\\nthe prior over\\nfunctions, which are\\ninduced by the\\nsamples from the\\nparameter prior.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(a) Prior distribution over functions.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny (b) Samples from the prior distribution over\\nfunctions.\\nLet us consider a Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(θ) = N\\n\\x00\\n0, 1\\n4 I\\n\\x01\\n. Figure 9.9\\nvisualizes the induced prior distribution over functions (shaded area: dark\\ngray: 67% confidence bound; light gray: 95% confidence bound) induced\\nby this parameter prior, including some function samples from this prior.\\nA function sample is obtained by first sampling a parameter vector\\nθi ∼ p(θ) and then computing fi(·) = θ⊤\\ni ϕ(·). We used 200 input lo-\\ncations x∗ ∈ [−5, 5] to which we apply the feature function ϕ(·). The\\nuncertainty (represented by the shaded area) in Figure 9.9 is solely due to\\nthe parameter uncertainty because we considered the noise-free predictive\\ndistribution (9.40).\\nSo far, we looked at computing predictions using the parameter prior\\np(θ). However, when we have a parameter posterior (given some train-\\ning data X , Y), the same principles for prediction and inference hold\\nas in (9.37) – we just need to replace the prior p(θ) with the posterior\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='241ecc48-cdab-44a4-81a6-b7fdb7ecb69d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 311, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='306 Linear Regression\\np(θ | X , Y). In the following, we will derive the posterior distribution in\\ndetail before using it to make predictions.\\n9.3.3 Posterior Distribution\\nGiven a training set of inputs xn ∈ RD and corresponding observations\\nyn ∈ R, n = 1 , . . . , N, we compute the posterior over the parameters\\nusing Bayes’ theorem as\\np(θ | X , Y) = p(Y | X , θ)p(θ)\\np(Y | X ) , (9.41)\\nwhere X is the set of training inputs and Y the collection of correspond-\\ning training targets. Furthermore, p(Y | X , θ) is the likelihood, p(θ) the\\nparameter prior, and\\np(Y | X ) =\\nZ\\np(Y | X , θ)p(θ)dθ = Eθ[p(Y | X , θ)] (9.42)\\nthe marginal likelihood/evidence, which is independent of the parametersmarginal likelihood\\nevidence θ and ensures that the posterior is normalized, i.e., it integrates to 1. We\\nThe marginal\\nlikelihood is the\\nexpected likelihood\\nunder the parameter\\nprior.\\ncan think of the marginal likelihood as the likelihood averaged over all\\npossible parameter settings (with respect to the prior distribution p(θ)).\\nTheorem 9.1 (Parameter Posterior). In our model (9.35), the parameter\\nposterior (9.41) can be computed in closed form as\\np(θ | X , Y) = N\\n\\x00\\nθ | mN , SN\\n\\x01\\n, (9.43a)\\nSN = (S−1\\n0 + σ−2Φ⊤Φ)−1 , (9.43b)\\nmN = SN(S−1\\n0 m0 + σ−2Φ⊤y) , (9.43c)\\nwhere the subscript N indicates the size of the training set.\\nProof Bayes’ theorem tells us that the posterior p(θ | X , Y) is propor-\\ntional to the product of the likelihood p(Y | X , θ) and the prior p(θ):\\nPosterior p(θ | X , Y) = p(Y | X , θ)p(θ)\\np(Y | X ) (9.44a)\\nLikelihood p(Y | X , θ) = N\\n\\x00\\ny | Φθ, σ 2I\\n\\x01\\n(9.44b)\\nPrior p(θ) = N\\n\\x00\\nθ | m0, S0\\n\\x01\\n. (9.44c)\\nInstead of looking at the product of the prior and the likelihood, we\\ncan transform the problem into log-space and solve for the mean and\\ncovariance of the posterior by completing the squares.\\nThe sum of the log-prior and the log-likelihood is\\nlog N\\n\\x00\\ny | Φθ, σ 2I\\n\\x01\\n+ log N\\n\\x00\\nθ | m0, S0\\n\\x01\\n(9.45a)\\n= −1\\n2\\n\\x00\\nσ−2(y − Φθ)⊤(y − Φθ) + (θ − m0)⊤S−1\\n0 (θ − m0)\\n\\x01\\n+ const\\n(9.45b)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a5544722-1fb9-4314-83b0-5311c400014b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 312, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3 Bayesian Linear Regression 307\\nwhere the constant contains terms independent of θ. We will ignore the\\nconstant in the following. We now factorize (9.45b), which yields\\n− 1\\n2\\n\\x00\\nσ−2y⊤y − 2σ−2y⊤Φθ + θ⊤σ−2Φ⊤Φθ + θ⊤S−1\\n0 θ\\n− 2m⊤\\n0 S−1\\n0 θ + m⊤\\n0 S−1\\n0 m0\\n\\x01 (9.46a)\\n= − 1\\n2\\n\\x00\\nθ⊤(σ−2Φ⊤Φ + S−1\\n0 )θ − 2(σ−2Φ⊤y + S−1\\n0 m0)⊤θ\\n\\x01\\n+ const ,\\n(9.46b)\\nwhere the constant contains the black terms in (9.46a), which are inde-\\npendent of θ. The orange terms are terms that are linear in θ, and the\\nblue terms are the ones that are quadratic in θ. Inspecting (9.46b), we\\nfind that this equation is quadratic in θ. The fact that the unnormalized\\nlog-posterior distribution is a (negative) quadratic form implies that the\\nposterior is Gaussian, i.e.,\\np(θ | X , Y) = exp(log p(θ | X , Y)) ∝ exp(log p(Y | X , θ) + logp(θ))\\n(9.47a)\\n∝ exp\\n\\x10\\n− 1\\n2\\n\\x00\\nθ⊤(σ−2Φ⊤Φ + S−1\\n0 )θ − 2(σ−2Φ⊤y + S−1\\n0 m0)⊤θ\\n\\x01\\x11\\n,\\n(9.47b)\\nwhere we used (9.46b) in the last expression.\\nThe remaining task is it to bring this (unnormalized) Gaussian into the\\nform that is proportional to N\\n\\x00\\nθ | mN , SN\\n\\x01\\n, i.e., we need to identify the\\nmean mN and the covariance matrix SN. To do this, we use the concept\\nof completing the squares. The desired log-posterior is completing the\\nsquares\\nlog N\\n\\x00\\nθ | mN , SN\\n\\x01\\n= −1\\n2(θ − mN)⊤S−1\\nN (θ − mN) + const (9.48a)\\n= −1\\n2\\n\\x00\\nθ⊤S−1\\nN θ − 2m⊤\\nN S−1\\nN θ + m⊤\\nN S−1\\nN mN\\n\\x01\\n. (9.48b)\\nHere, we factorized the quadratic form (θ − mN)⊤S−1\\nN (θ − mN) into a Since p(θ | X , Y) =\\nN\\n\\x00\\nmN , SN\\n\\x01\\n, it\\nholds that\\nθMAP = mN .\\nterm that is quadratic inθ alone (blue), a term that is linear inθ (orange),\\nand a constant term (black). This allows us now to find SN and mN by\\nmatching the colored expressions in (9.46b) and (9.48b), which yields\\nS−1\\nN = Φ⊤σ−2IΦ + S−1\\n0 (9.49a)\\n⇐ ⇒SN = (σ−2Φ⊤Φ + S−1\\n0 )−1 (9.49b)\\nand\\nm⊤\\nN S−1\\nN = (σ−2Φ⊤y + S−1\\n0 m0)⊤ (9.50a)\\n⇐ ⇒mN = SN(σ−2Φ⊤y + S−1\\n0 m0) . (9.50b)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4242c04d-8133-4f31-b9d7-182dce49b847', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 313, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='308 Linear Regression\\nRemark (General Approach to Completing the Squares) . If we are given\\nan equation\\nx⊤Ax − 2a⊤x + const1 , (9.51)\\nwhere A is symmetric and positive definite, which we wish to bring into\\nthe form\\n(x − µ)⊤Σ(x − µ) + const2 , (9.52)\\nwe can do this by setting\\nΣ := A , (9.53)\\nµ := Σ−1a (9.54)\\nand const2 = const1 − µ⊤Σµ. ♢\\nWe can see that the terms inside the exponential in (9.47b) are of the\\nform (9.51) with\\nA := σ−2Φ⊤Φ + S−1\\n0 , (9.55)\\na := σ−2Φ⊤y + S−1\\n0 m0 . (9.56)\\nSince A, a can be difficult to identify in equations like (9.46a), it is of-\\nten helpful to bring these equations into the form (9.51) that decouples\\nquadratic term, linear terms, and constants, which simplifies finding the\\ndesired solution.\\n9.3.4 Posterior Predictions\\nIn (9.37), we computed the predictive distribution of y∗ at a test input\\nx∗ using the parameter prior p(θ). In principle, predicting with the pa-\\nrameter posterior p(θ | X , Y) is not fundamentally different given that\\nin our conjugate model the prior and posterior are both Gaussian (with\\ndifferent parameters). Therefore, by following the same reasoning as in\\nSection 9.3.2, we obtain the (posterior) predictive distribution\\np(y∗ | X , Y, x∗) =\\nZ\\np(y∗ | x∗, θ)p(θ | X , Y)dθ (9.57a)\\n=\\nZ\\nN\\n\\x00\\ny∗ | ϕ⊤(x∗)θ, σ 2\\x01\\nN\\n\\x00\\nθ | mN , SN\\n\\x01\\ndθ (9.57b)\\n= N\\n\\x00\\ny∗ | ϕ⊤(x∗)mN , ϕ⊤(x∗)SN ϕ(x∗) + σ2\\x01\\n. (9.57c)\\nThe term ϕ⊤(x∗)SN ϕ(x∗) reflects the posterior uncertainty associatedE[y∗ | X , Y, x∗] =\\nϕ⊤(x∗)mN =\\nϕ⊤(x∗)θMAP.\\nwith the parameters θ. Note that SN depends on the training inputs\\nthrough Φ; see (9.43b). The predictive mean ϕ⊤(x∗)mN coincides with\\nthe predictions made with the MAP estimate θMAP.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ebe0ed39-a9ae-4e50-b18a-ebb190e9ab9c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 314, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3 Bayesian Linear Regression 309\\nRemark (Marginal Likelihood and Posterior Predictive Distribution) . By\\nreplacing the integral in (9.57a), the predictive distribution can be equiv-\\nalently written as the expectation Eθ | X ,Y[p(y∗ | x∗, θ)], where the expec-\\ntation is taken with respect to the parameter posterior p(θ | X , Y).\\nWriting the posterior predictive distribution in this way highlights a\\nclose resemblance to the marginal likelihood (9.42). The key difference\\nbetween the marginal likelihood and the posterior predictive distribution\\nare (i) the marginal likelihood can be thought of predicting the training\\ntargets y and not the test targets y∗, and (ii) the marginal likelihood av-\\nerages with respect to the parameter prior and not the parameter poste-\\nrior. ♢\\nRemark (Mean and Variance of Noise-Free Function Values) . In many\\ncases, we are not interested in the predictive distribution p(y∗ | X , Y, x∗)\\nof a (noisy) observation y∗. Instead, we would like to obtain the distribu-\\ntion of the (noise-free) function values f(x∗) = ϕ⊤(x∗)θ. We determine\\nthe corresponding moments by exploiting the properties of means and\\nvariances, which yields\\nE[f(x∗) | X , Y] = Eθ[ϕ⊤(x∗)θ | X , Y] = ϕ⊤(x∗)Eθ[θ | X , Y]\\n= ϕ⊤(x∗)mN = m⊤\\nN ϕ(x∗) ,\\n(9.58)\\nVθ[f(x∗) | X , Y] = Vθ[ϕ⊤(x∗)θ | X , Y]\\n= ϕ⊤(x∗)Vθ[θ | X , Y]ϕ(x∗)\\n= ϕ⊤(x∗)SN ϕ(x∗) .\\n(9.59)\\nWe see that the predictive mean is the same as the predictive mean for\\nnoisy observations as the noise has mean 0, and the predictive variance\\nonly differs by σ2, which is the variance of the measurement noise: When\\nwe predict noisy function values, we need to include σ2 as a source of\\nuncertainty , but this term is not needed for noise-free predictions. Here,\\nthe only remaining uncertainty stems from the parameter posterior. ♢ Integrating out\\nparameters induces\\na distribution over\\nfunctions.\\nRemark (Distribution over Functions). The fact that we integrate out the\\nparameters θ induces a distribution over functions: If we sample θi ∼\\np(θ | X , Y) from the parameter posterior, we obtain a single function re-\\nalization θ⊤\\ni ϕ(·). The mean function, i.e., the set of all expected function mean function\\nvalues Eθ[f(·) | θ, X , Y], of this distribution over functions is m⊤\\nN ϕ(·).\\nThe (marginal) variance, i.e., the variance of the functionf(·), is given by\\nϕ⊤(·)SN ϕ(·). ♢\\nExample 9.8 (Posterior over Functions)\\nLet us revisit the Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(θ) = N\\n\\x00\\n0, 1\\n4 I\\n\\x01\\n. Figure 9.9\\nvisualizes the prior over functions induced by the parameter prior and\\nsample functions from this prior.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c00b2ed5-5045-4d32-9824-3dc17a036cdd', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 315, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='310 Linear Regression\\nFigure 9.10 shows the posterior over functions that we obtain via\\nBayesian linear regression. The training dataset is shown in panel (a);\\npanel (b) shows the posterior distribution over functions, including the\\nfunctions we would obtain via maximum likelihood and MAP estimation.\\nThe function we obtain using the MAP estimate also corresponds to the\\nposterior mean function in the Bayesian linear regression setting. Panel (c)\\nshows some plausible realizations (samples) of functions under that pos-\\nterior over functions.\\nFigure 9.10\\nBayesian linear\\nregression and\\nposterior over\\nfunctions.\\n(a) training data;\\n(b) posterior\\ndistribution over\\nfunctions;\\n(c) Samples from\\nthe posterior over\\nfunctions.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(a) Training data.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP\\nBLR (b) Posterior over functions rep-\\nresented by the marginal uncer-\\ntainties (shaded) showing the\\n67% and 95% predictive con-\\nfidence bounds, the maximum\\nlikelihood estimate (MLE) and\\nthe MAP estimate (MAP), the\\nlatter of which is identical to\\nthe posterior mean function.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(c) Samples from the posterior\\nover functions, which are in-\\nduced by the samples from the\\nparameter posterior.\\nFigure 9.11 shows some posterior distributions over functions induced\\nby the parameter posterior. For different polynomial degrees M, the left\\npanels show the maximum likelihood function θ⊤\\nMLϕ(·), the MAP func-\\ntion θ⊤\\nMAPϕ(·) (which is identical to the posterior mean function), and the\\n67% and 95% predictive confidence bounds obtained by Bayesian linear\\nregression, represented by the shaded areas.\\nThe right panels show samples from the posterior over functions: Here,\\nwe sampled parameters θi from the parameter posterior and computed\\nthe function ϕ⊤(x∗)θi, which is a single realization of a function under\\nthe posterior distribution over functions. For low-order polynomials, the\\nparameter posterior does not allow the parameters to vary much: The\\nsampled functions are nearly identical. When we make the model more\\nflexible by adding more parameters (i.e., we end up with a higher-order\\npolynomial), these parameters are not sufficiently constrained by the pos-\\nterior, and the sampled functions can be easily visually separated. We also\\nsee in the corresponding panels on the left how the uncertainty increases,\\nespecially at the boundaries.\\nAlthough for a seventh-order polynomial the MAP estimate yields a rea-\\nsonable fit, the Bayesian linear regression model additionally tells us that\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d006b07-1618-4214-a424-eb2b93b29ce8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 316, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3 Bayesian Linear Regression 311\\nFigure 9.11\\nBayesian linear\\nregression. Left\\npanels: Shaded\\nareas indicate the\\n67% (dark gray)\\nand 95% (light\\ngray) predictive\\nconfidence bounds.\\nThe mean of the\\nBayesian linear\\nregression model\\ncoincides with the\\nMAP estimate. The\\npredictive\\nuncertainty is the\\nsum of the noise\\nterm and the\\nposterior parameter\\nuncertainty , which\\ndepends on the\\nlocation of the test\\ninput. Right panels:\\nsampled functions\\nfrom the posterior\\ndistribution.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(a) Posterior distribution for polynomials of degree M = 3 (left) and samples from the pos-\\nterior over functions (right).\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(b) Posterior distribution for polynomials of degree M = 5 (left) and samples from the\\nposterior over functions (right).\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(c) Posterior distribution for polynomials of degree M = 7 (left) and samples from the pos-\\nterior over functions (right).\\nthe posterior uncertainty is huge. This information can be critical when\\nwe use these predictions in a decision-making system, where bad deci-\\nsions can have significant consequences (e.g., in reinforcement learning\\nor robotics).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dacf5d78-987c-49c3-b5f7-b9c3aa475cc7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 317, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='312 Linear Regression\\n9.3.5 Computing the Marginal Likelihood\\nIn Section 8.6.2, we highlighted the importance of the marginal likelihood\\nfor Bayesian model selection. In the following, we compute the marginal\\nlikelihood for Bayesian linear regression with a conjugate Gaussian prior\\non the parameters, i.e., exactly the setting we have been discussing in this\\nchapter.\\nJust to recap, we consider the following generative process:\\nθ ∼ N\\n\\x00\\nm0, S0\\n\\x01\\n(9.60a)\\nyn | xn, θ ∼ N\\n\\x00\\nx⊤\\nn θ, σ 2\\x01\\n, (9.60b)\\nn = 1, . . . , N. The marginal likelihood is given byThe marginal\\nlikelihood can be\\ninterpreted as the\\nexpected likelihood\\nunder the prior, i.e.,\\nEθ[p(Y | X , θ)].\\np(Y | X ) =\\nZ\\np(Y | X , θ)p(θ)dθ (9.61a)\\n=\\nZ\\nN\\n\\x00\\ny | Xθ , σ 2I\\n\\x01\\nN\\n\\x00\\nθ | m0, S0\\n\\x01\\ndθ , (9.61b)\\nwhere we integrate out the model parametersθ. We compute the marginal\\nlikelihood in two steps: First, we show that the marginal likelihood is\\nGaussian (as a distribution in y); second, we compute the mean and co-\\nvariance of this Gaussian.\\n1. The marginal likelihood is Gaussian: From Section 6.5.2, we know that\\n(i) the product of two Gaussian random variables is an (unnormalized)\\nGaussian distribution, and (ii) a linear transformation of a Gaussian\\nrandom variable is Gaussian distributed. In (9.61b), we require a linear\\ntransformation to bring N\\n\\x00\\ny | Xθ , σ 2I\\n\\x01\\ninto the form N\\n\\x00\\nθ | µ, Σ\\n\\x01\\nfor\\nsome µ, Σ. Once this is done, the integral can be solved in closed form.\\nThe result is the normalizing constant of the product of the two Gaus-\\nsians. The normalizing constant itself has Gaussian shape; see (6.76).\\n2. Mean and covariance. We compute the mean and covariance matrix\\nof the marginal likelihood by exploiting the standard results for means\\nand covariances of affine transformations of random variables; see Sec-\\ntion 6.4.4. The mean of the marginal likelihood is computed as\\nE[Y | X ] = Eθ,ϵ[Xθ + ϵ] = XEθ[θ] = Xm0 . (9.62)\\nNote that ϵ ∼ N\\n\\x00\\n0, σ 2I\\n\\x01\\nis a vector of i.i.d. random variables. The\\ncovariance matrix is given as\\nCov[Y|X ] = Covθ,ϵ[Xθ + ϵ] = Covθ[Xθ] + σ2I (9.63a)\\n= X Covθ[θ]X ⊤ + σ2I = XS 0X ⊤ + σ2I . (9.63b)\\nHence, the marginal likelihood is\\np(Y | X ) = (2π)− N\\n2 det(XS 0X ⊤ + σ2I)− 1\\n2 (9.64a)\\n· exp\\n\\x00\\n− 1\\n2(y − Xm0)⊤(XS 0X ⊤ + σ2I)−1(y − Xm0)\\n\\x01\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='febccc3f-a19a-4db4-812b-0c58690e4165', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 318, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.4 Maximum Likelihood as Orthogonal Projection 313\\nFigure 9.12\\nGeometric\\ninterpretation of\\nleast squares.\\n(a) Dataset;\\n(b) maximum\\nlikelihood solution\\ninterpreted as a\\nprojection.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\n(a) Regression dataset consisting of noisy ob-\\nservations yn (blue) of function values f(xn)\\nat input locations xn.\\n−4 −2 0 2 4\\nx\\n−4\\n−2\\n0\\n2\\n4\\ny\\nProjection\\nObservations\\nMaximum likelihood estimate\\n(b) The orange dots are the projections of\\nthe noisy observations (blue dots) onto the\\nline θMLx. The maximum likelihood solution to\\na linear regression problem finds a subspace\\n(line) onto which the overall projection er-\\nror (orange lines) of the observations is mini-\\nmized.\\n= N\\n\\x00\\ny | Xm0, XS 0X ⊤ + σ2I\\n\\x01\\n. (9.64b)\\nGiven the close connection with the posterior predictive distribution (see\\nRemark on Marginal Likelihood and Posterior Predictive Distribution ear-\\nlier in this section), the functional form of the marginal likelihood should\\nnot be too surprising.\\n9.4 Maximum Likelihood as Orthogonal Projection\\nHaving crunched through much algebra to derive maximum likelihood\\nand MAP estimates, we will now provide a geometric interpretation of\\nmaximum likelihood estimation. Let us consider a simple linear regression\\nsetting\\ny = xθ + ϵ, ϵ ∼ N\\n\\x00\\n0, σ 2\\x01\\n, (9.65)\\nin which we consider linear functions f : R → R that go through the\\norigin (we omit features here for clarity). The parameterθ determines the\\nslope of the line. Figure 9.12(a) shows a one-dimensional dataset.\\nWith a training data set {(x1, y1), . . . ,(xN , yN)} we recall the results\\nfrom Section 9.2.1 and obtain the maximum likelihood estimator for the\\nslope parameter as\\nθML = (X ⊤X)−1X ⊤y = X ⊤y\\nX ⊤X ∈ R , (9.66)\\nwhere X = [x1, . . . , xN]⊤ ∈ RN, y = [y1, . . . , yN]⊤ ∈ RN.\\nThis means for the training inputs X we obtain the optimal (maximum\\nlikelihood) reconstruction of the training targets as\\nXθML = X X ⊤y\\nX ⊤X = XX ⊤\\nX ⊤X y , (9.67)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c2a7e8ce-8e06-4d10-839b-d214449a6cb1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 319, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='314 Linear Regression\\ni.e., we obtain the approximation with the minimum least-squares error\\nbetween y and Xθ.\\nAs we are looking for a solution of y = Xθ, we can think of linear\\nregression as a problem for solving systems of linear equations. There-Linear regression\\ncan be thought of as\\na method for solving\\nsystems of linear\\nequations.\\nfore, we can relate to concepts from linear algebra and analytic geometry\\nthat we discussed in Chapters 2 and 3. In particular, looking carefully\\nat (9.67) we see that the maximum likelihood estimator θML in our ex-\\nample from (9.65) effectively does an orthogonal projection of y onto\\nthe one-dimensional subspace spanned by X. Recalling the results on or-Maximum\\nlikelihood linear\\nregression performs\\nan orthogonal\\nprojection.\\nthogonal projections from Section 3.8, we identify XX ⊤\\nX ⊤X as the projection\\nmatrix, θML as the coordinates of the projection onto the one-dimensional\\nsubspace of RN spanned by X and XθML as the orthogonal projection of\\ny onto this subspace.\\nTherefore, the maximum likelihood solution provides also a geometri-\\ncally optimal solution by finding the vectors in the subspace spanned by\\nX that are “closest” to the corresponding observations y, where “clos-\\nest” means the smallest (squared) distance of the function values yn to\\nxnθ. This is achieved by orthogonal projections. Figure 9.12(b) shows the\\nprojection of the noisy observations onto the subspace that minimizes the\\nsquared distance between the original dataset and its projection (note that\\nthe x-coordinate is fixed), which corresponds to the maximum likelihood\\nsolution.\\nIn the general linear regression case where\\ny = ϕ⊤(x)θ + ϵ, ϵ ∼ N\\n\\x00\\n0, σ 2\\x01\\n(9.68)\\nwith vector-valued features ϕ(x) ∈ RK, we again can interpret the maxi-\\nmum likelihood result\\ny ≈ ΦθML , (9.69)\\nθML = (Φ⊤Φ)−1Φ⊤y (9.70)\\nas a projection onto a K-dimensional subspace of RN, which is spanned\\nby the columns of the feature matrix Φ; see Section 3.8.2.\\nIf the feature functions ϕk that we use to construct the feature ma-\\ntrix Φ are orthonormal (see Section 3.7), we obtain a special case where\\nthe columns of Φ form an orthonormal basis (see Section 3.5), such that\\nΦ⊤Φ = I. This will then lead to the projection\\nΦ(Φ⊤Φ)−1Φ⊤y = ΦΦ⊤y =\\n KX\\nk=1\\nϕkϕ⊤\\nk\\n!\\ny (9.71)\\nso that the maximum likelihood projection is simply the sum of projections\\nof y onto the individual basis vectors ϕk, i.e., the columns of Φ. Further-\\nmore, the coupling between different features has disappeared due to the\\northogonality of the basis. Many popular basis functions in signal process-\\ning, such as wavelets and Fourier bases, are orthogonal basis functions.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2fd26bbc-e267-46b1-811d-1b3f437b5b3e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 320, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.5 Further Reading 315\\nWhen the basis is not orthogonal, one can convert a set of linearly inde-\\npendent basis functions to an orthogonal basis by using the Gram-Schmidt\\nprocess; see Section 3.8.3 and (Strang, 2003).\\n9.5 Further Reading\\nIn this chapter, we discussed linear regression for Gaussian likelihoods\\nand conjugate Gaussian priors on the parameters of the model. This al-\\nlowed for closed-form Bayesian inference. However, in some applications\\nwe may want to choose a different likelihood function. For example, in\\na binary classification setting, we observe only two possible (categorical) classification\\noutcomes, and a Gaussian likelihood is inappropriate in this setting. In-\\nstead, we can choose a Bernoulli likelihood that will return a probability of\\nthe predicted label to be 1 (or 0). We refer to the books by Barber (2012),\\nBishop (2006), and Murphy (2012) for an in-depth introduction to classifi-\\ncation problems. A different example where non-Gaussian likelihoods are\\nimportant is count data. Counts are non-negative integers, and in this case\\na Binomial or Poisson likelihood would be a better choice than a Gaussian.\\nAll these examples fall into the category ofgeneralized linear models, a flex- generalized linear\\nmodelible generalization of linear regression that allows for response variables\\nthat have error distributions other than a Gaussian distribution. The GLM Generalized linear\\nmodels are the\\nbuilding blocks of\\ndeep neural\\nnetworks.\\ngeneralizes linear regression by allowing the linear model to be related\\nto the observed values via a smooth and invertible function σ(·) that may\\nbe nonlinear so that y = σ(f(x)), where f(x) = θ⊤ϕ(x) is the linear\\nregression model from (9.13). We can therefore think of a generalized\\nlinear model in terms of function composition y = σ ◦ f, where f is a\\nlinear regression model and σ the activation function. Note that although\\nwe are talking about “generalized linear models”, the outputs y are no\\nlonger linear in the parameters θ. In logistic regression , we choose the logistic regression\\nlogistic sigmoid σ(f) = 1\\n1+exp(−f) ∈ [0, 1], which can be interpreted as the logistic sigmoid\\nprobability of observing y = 1 of a Bernoulli random variable y ∈ {0, 1}.\\nThe function σ(·) is called transfer function or activation function, and its transfer function\\nactivation functioninverse is called the canonical link function . From this perspective, it is\\ncanonical link\\nfunction\\nFor ordinary linear\\nregression the\\nactivation function\\nwould simply be the\\nidentity .\\nalso clear that generalized linear models are the building blocks of (deep)\\nfeedforward neural networks: If we consider a generalized linear model\\ny = σ(Ax + b), where A is a weight matrix and b a bias vector, we iden-\\ntify this generalized linear model as a single-layer neural network with\\nactivation function σ(·). We can now recursively compose these functions\\nvia\\nA great post on the\\nrelation between\\nGLMs and deep\\nnetworks is\\navailable at\\nhttps://tinyurl.\\ncom/glm-dnn.\\nxk+1 = f k(xk)\\nf k(xk) = σk(Akxk + bk) (9.72)\\nfor k = 0 , . . . , K − 1, where x0 are the input features and xK = y are\\nthe observed outputs, such that f K−1 ◦ · · · ◦ f 0 is a K-layer deep neural\\nnetwork. Therefore, the building blocks of this deep neural network are\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='61f7e3f0-f1c1-4039-b0a9-0e233dcc622c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 321, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='316 Linear Regression\\nthe generalized linear models defined in (9.72). Neural networks (Bishop,\\n1995; Goodfellow et al., 2016) are significantly more expressive and flexi-\\nble than linear regression models. However, maximum likelihood parame-\\nter estimation is a non-convex optimization problem, and marginalization\\nof the parameters in a fully Bayesian setting is analytically intractable.\\nWe briefly hinted at the fact that a distribution over parameters in-\\nduces a distribution over regression functions. Gaussian processes (Ras-Gaussian process\\nmussen and Williams, 2006) are regression models where the concept of\\na distribution over function is central. Instead of placing a distribution\\nover parameters, a Gaussian process places a distribution directly on the\\nspace of functions without the “detour” via the parameters. To do so, the\\nGaussian process exploits the kernel trick (Sch¨olkopf and Smola, 2002),kernel trick\\nwhich allows us to compute inner products between two function values\\nf(xi), f(xj) only by looking at the corresponding input xi, xj. A Gaus-\\nsian process is closely related to both Bayesian linear regression and sup-\\nport vector regression but can also be interpreted as a Bayesian neural\\nnetwork with a single hidden layer where the number of units tends to\\ninfinity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\\nprocesses can be found in MacKay (1998) and Rasmussen and Williams\\n(2006).\\nWe focused on Gaussian parameter priors in the discussions in this chap-\\nter, because they allow for closed-form inference in linear regression mod-\\nels. However, even in a regression setting with Gaussian likelihoods, we\\nmay choose a non-Gaussian prior. Consider a setting, where the inputs are\\nx ∈ RD and our training set is small and of size N ≪ D. This means that\\nthe regression problem is underdetermined. In this case, we can choose\\na parameter prior that enforces sparsity , i.e., a prior that tries to set as\\nmany parameters to 0 as possible (variable selection). This prior providesvariable selection\\na stronger regularizer than the Gaussian prior, which often leads to an in-\\ncreased prediction accuracy and interpretability of the model. The Laplace\\nprior is one example that is frequently used for this purpose. A linear re-\\ngression model with the Laplace prior on the parameters is equivalent to\\nlinear regression with L1 regularization ( LASSO) (Tibshirani, 1996). TheLASSO\\nLaplace distribution is sharply peaked at zero (its first derivative is discon-\\ntinuous) and it concentrates its probability mass closer to zero than the\\nGaussian distribution, which encourages parameters to be 0. Therefore,\\nthe nonzero parameters are relevant for the regression problem, which is\\nthe reason why we also speak of “variable selection”.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a7b1576c-2512-4b62-995b-509ae75dcdc9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 322, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10\\nDimensionality Reduction with Principal\\nComponent Analysis\\nWorking directly with high-dimensional data, such as images, comes with A 640 × 480 pixel\\ncolor image is a data\\npoint in a\\nmillion-dimensional\\nspace, where every\\npixel responds to\\nthree dimensions,\\none for each color\\nchannel (red, green,\\nblue).\\nsome difficulties: It is hard to analyze, interpretation is difficult, visualiza-\\ntion is nearly impossible, and (from a practical point of view) storage of\\nthe data vectors can be expensive. However, high-dimensional data often\\nhas properties that we can exploit. For example, high-dimensional data is\\noften overcomplete, i.e., many dimensions are redundant and can be ex-\\nplained by a combination of other dimensions. Furthermore, dimensions\\nin high-dimensional data are often correlated so that the data possesses an\\nintrinsic lower-dimensional structure. Dimensionality reduction exploits\\nstructure and correlation and allows us to work with a more compact rep-\\nresentation of the data, ideally without losing information. We can think\\nof dimensionality reduction as a compression technique, similar to jpeg or\\nmp3, which are compression algorithms for images and music.\\nIn this chapter, we will discuss principal component analysis (PCA), an principal component\\nanalysis\\nPCA\\nalgorithm for linear dimensionality reduction. PCA, proposed by Pearson\\ndimensionality\\nreduction\\n(1901) and Hotelling (1933), has been around for more than 100 years\\nand is still one of the most commonly used techniques for data compres-\\nsion and data visualization. It is also used for the identification of simple\\npatterns, latent factors, and structures of high-dimensional data. In the\\nFigure 10.1\\nIllustration:\\ndimensionality\\nreduction. (a) The\\noriginal dataset\\ndoes not vary much\\nalong the x2\\ndirection. (b) The\\ndata from (a) can be\\nrepresented using\\nthe x1-coordinate\\nalone with nearly no\\nloss.\\n−5.0 −2.5 0.0 2.5 5.0\\nx1\\n−4\\n−2\\n0\\n2\\n4\\nx2\\n(a) Dataset with x1 and x2 coordinates.\\n−5.0 −2.5 0.0 2.5 5.0\\nx1\\n−4\\n−2\\n0\\n2\\n4\\nx2\\n (b) Compressed dataset where only thex1 coor-\\ndinate is relevant.\\n317\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='102c98ac-1885-4be0-b5fc-bddbabfbb651', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 323, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='318 Dimensionality Reduction with Principal Component Analysis\\nsignal processing community , PCA is also known as the Karhunen-Lo`eveKarhunen-Lo`eve\\ntransform transform. In this chapter, we derive PCA from first principles, drawing on\\nour understanding of basis and basis change (Sections 2.6.1 and 2.7.2),\\nprojections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-\\ntions (Section 6.5), and constrained optimization (Section 7.2).\\nDimensionality reduction generally exploits a property of high-dimen-\\nsional data (e.g., images) that it often lies on a low-dimensional subspace.\\nFigure 10.1 gives an illustrative example in two dimensions. Although\\nthe data in Figure 10.1(a) does not quite lie on a line, the data does not\\nvary much in the x2-direction, so that we can express it as if it were on\\na line – with nearly no loss; see Figure 10.1(b). To describe the data in\\nFigure 10.1(b), only the x1-coordinate is required, and the data lies in a\\none-dimensional subspace of R2.\\n10.1 Problem Setting\\nIn PCA, we are interested in finding projections ˜xn of data points xn that\\nare as similar to the original data points as possible, but which have a sig-\\nnificantly lower intrinsic dimensionality . Figure 10.1 gives an illustration\\nof what this could look like.\\nMore concretely , we consider an i.i.d. datasetX = {x1, . . . ,xN }, xn ∈\\nRD, with mean 0 that possesses the data covariance matrix (6.42)data covariance\\nmatrix\\nS = 1\\nN\\nNX\\nn=1\\nxnx⊤\\nn . (10.1)\\nFurthermore, we assume there exists a low-dimensional compressed rep-\\nresentation (code)\\nzn = B⊤xn ∈ RM (10.2)\\nof xn, where we define the projection matrix\\nB := [b1, . . . ,bM] ∈ RD×M . (10.3)\\nWe assume that the columns ofB are orthonormal (Definition 3.7) so that\\nb⊤\\ni bj = 0 if and only if i ̸= j and b⊤\\ni bi = 1. We seek an M-dimensionalThe columns\\nb1, . . . ,bM of B\\nform a basis of the\\nM-dimensional\\nsubspace in which\\nthe projected data\\n˜x = BB ⊤x ∈ RD\\nlive.\\nsubspace U ⊆ RD, dim(U) = M < D onto which we project the data. We\\ndenote the projected data by ˜xn ∈ U, and their coordinates (with respect\\nto the basis vectors b1, . . . ,bM of U) by zn. Our aim is to find projections\\n˜xn ∈ RD (or equivalently the codes zn and the basis vectors b1, . . . ,bM)\\nso that they are as similar to the original data xn and minimize the loss\\ndue to compression.\\nExample 10.1 (Coordinate Representation/Code)\\nConsider R2 with the canonical basis e1 = [1 , 0]⊤, e2 = [0 , 1]⊤. From\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9f7ce86b-37df-4d44-a905-f86de8191e2c', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 324, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.1 Problem Setting 319\\nFigure 10.2\\nGraphical\\nillustration of PCA.\\nIn PCA, we find a\\ncompressed version\\nz of original data x.\\nThe compressed\\ndata can be\\nreconstructed into\\n˜x, which lives in the\\noriginal data space,\\nbut has an intrinsic\\nlower-dimensional\\nrepresentation than\\nx.\\nx ˜xz\\nOriginal\\nCompressed\\nReconstructed\\nRD RD\\nRM\\nChapter 2, we know that x ∈ R2 can be represented as a linear combina-\\ntion of these basis vectors, e.g.,\\n\\x145\\n3\\n\\x15\\n= 5e1 + 3e2 . (10.4)\\nHowever, when we consider vectors of the form\\n˜x =\\n\\x140\\nz\\n\\x15\\n∈ R2 , z ∈ R , (10.5)\\nthey can always be written as 0e1 + ze2. To represent these vectors it is\\nsufficient to remember/store the coordinate/code z of ˜x with respect to\\nthe e2 vector. The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors (see\\nSection 2.6.1).\\nMore precisely , the set of ˜x vectors (with the standard vector addition\\nand scalar multiplication) forms a vector subspace U (see Section 2.4)\\nwith dim(U) = 1 because U = span[e2].\\nIn Section 10.2, we will find low-dimensional representations that re-\\ntain as much information as possible and minimize the compression loss.\\nAn alternative derivation of PCA is given in Section 10.3, where we will\\nbe looking at minimizing the squared reconstruction error∥xn − ˜xn∥\\n2\\nbe-\\ntween the original data xn and its projection ˜xn.\\nFigure 10.2 illustrates the setting we consider in PCA, where z repre-\\nsents the lower-dimensional representation of the compressed data ˜x and\\nplays the role of a bottleneck, which controls how much information can\\nflow between x and ˜x. In PCA, we consider a linear relationship between\\nthe original data x and its low-dimensional code z so that z = B⊤x and\\n˜x = Bz for a suitable matrix B. Based on the motivation of thinking\\nof PCA as a data compression technique, we can interpret the arrows in\\nFigure 10.2 as a pair of operations representing encoders and decoders.\\nThe linear mapping represented by B can be thought of as a decoder,\\nwhich maps the low-dimensional code z ∈ RM back into the original data\\nspace RD. Similarly ,B⊤ can be thought of an encoder, which encodes the\\noriginal data x as a low-dimensional (compressed) code z.\\nThroughout this chapter, we will use the MNIST digits dataset as a re-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e5fd17e-8d32-4564-b452-5d5df6b68b6a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 325, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='320 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.3\\nExamples of\\nhandwritten digits\\nfrom the MNIST\\ndataset. http:\\n//yann.lecun.\\ncom/exdb/mnist/.\\noccurring example, which contains 60,000 examples of handwritten digits\\n0 through 9. Each digit is a grayscale image of size28 ×28, i.e., it contains\\n784 pixels so that we can interpret every image in this dataset as a vector\\nx ∈ R784. Examples of these digits are shown in Figure 10.3.\\n10.2 Maximum Variance Perspective\\nFigure 10.1 gave an example of how a two-dimensional dataset can be\\nrepresented using a single coordinate. In Figure 10.1(b), we chose to ig-\\nnore the x2-coordinate of the data because it did not add too much in-\\nformation so that the compressed data is similar to the original data in\\nFigure 10.1(a). We could have chosen to ignore the x1-coordinate, but\\nthen the compressed data had been very dissimilar from the original data,\\nand much information in the data would have been lost.\\nIf we interpret information content in the data as how “space filling”\\nthe dataset is, then we can describe the information contained in the data\\nby looking at the spread of the data. From Section 6.4.1, we know that the\\nvariance is an indicator of the spread of the data, and we can derive PCA as\\na dimensionality reduction algorithm that maximizes the variance in the\\nlow-dimensional representation of the data to retain as much information\\nas possible. Figure 10.4 illustrates this.\\nConsidering the setting discussed in Section 10.1, our aim is to find\\na matrix B (see (10.3)) that retains as much information as possible\\nwhen compressing data by projecting it onto the subspace spanned by\\nthe columns b1, . . . ,bM of B. Retaining most information after data com-\\npression is equivalent to capturing the largest amount of variance in the\\nlow-dimensional code (Hotelling, 1933).\\nRemark. (Centered Data) For the data covariance matrix in (10.1), we\\nassumed centered data. We can make this assumption without loss of gen-\\nerality: Let us assume that µ is the mean of the data. Using the properties\\nof the variance, which we discussed in Section 6.4.4, we obtain\\nVz[z] = Vx[B⊤(x − µ)] = Vx[B⊤x − B⊤µ] = Vx[B⊤x] , (10.6)\\ni.e., the variance of the low-dimensional code does not depend on the\\nmean of the data. Therefore, we assume without loss of generality that the\\ndata has mean 0 for the remainder of this section. With this assumption\\nthe mean of the low-dimensional code is also0 since Ez[z] = Ex[B⊤x] =\\nB⊤Ex[x] = 0. ♢\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='90a8379c-3d2b-483d-a5c0-d160bd4ddb80', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 326, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.2 Maximum Variance Perspective 321\\nFigure 10.4 PCA\\nfinds a\\nlower-dimensional\\nsubspace (line) that\\nmaintains as much\\nvariance (spread of\\nthe data) as possible\\nwhen the data\\n(blue) is projected\\nonto this subspace\\n(orange).\\n10.2.1 Direction with Maximal Variance\\nWe maximize the variance of the low-dimensional code using a sequential\\napproach. We start by seeking a single vectorb1 ∈ RD that maximizes the The vector b1 will\\nbe the first column\\nof the matrix B and\\ntherefore the first of\\nM orthonormal\\nbasis vectors that\\nspan the\\nlower-dimensional\\nsubspace.\\nvariance of the projected data, i.e., we aim to maximize the variance of\\nthe first coordinate z1 of z ∈ RM so that\\nV1 := V[z1] = 1\\nN\\nNX\\nn=1\\nz2\\n1n (10.7)\\nis maximized, where we exploited the i.i.d. assumption of the data and\\ndefined z1n as the first coordinate of the low-dimensional representation\\nzn ∈ RM of xn ∈ RD. Note that first component of zn is given by\\nz1n = b⊤\\n1 xn , (10.8)\\ni.e., it is the coordinate of the orthogonal projection of xn onto the one-\\ndimensional subspace spanned by b1 (Section 3.8). We substitute (10.8)\\ninto (10.7), which yields\\nV1 = 1\\nN\\nNX\\nn=1\\n(b⊤\\n1 xn)2 = 1\\nN\\nNX\\nn=1\\nb⊤\\n1 xnx⊤\\nn b1 (10.9a)\\n= b⊤\\n1\\n \\n1\\nN\\nNX\\nn=1\\nxnx⊤\\nn\\n!\\nb1 = b⊤\\n1 Sb1 , (10.9b)\\nwhere S is the data covariance matrix defined in (10.1). In (10.9a), we\\nhave used the fact that the dot product of two vectors is symmetric with\\nrespect to its arguments, that is, b⊤\\n1 xn = x⊤\\nn b1.\\nNotice that arbitrarily increasing the magnitude of the vector b1 in-\\ncreases V1, that is, a vector b1 that is two times longer can result in V1\\nthat is potentially four times larger. Therefore, we restrict all solutions to ∥b1∥2 = 1\\n⇐ ⇒ ∥b1∥ = 1.∥b1∥\\n2\\n= 1, which results in a constrained optimization problem in which\\nwe seek the direction along which the data varies most.\\nWith the restriction of the solution space to unit vectors the vector b1\\nthat points in the direction of maximum variance can be found by the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aac4ae8e-f472-4bc5-8bde-f16fbb908ae3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 327, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='322 Dimensionality Reduction with Principal Component Analysis\\nconstrained optimization problem\\nmax\\nb1\\nb⊤\\n1 Sb1\\nsubject to ∥b1∥\\n2\\n= 1 .\\n(10.10)\\nFollowing Section 7.2, we obtain the Lagrangian\\nL(b1, λ) = b⊤\\n1 Sb1 + λ1(1 − b⊤\\n1 b1) (10.11)\\nto solve this constrained optimization problem. The partial derivatives of\\nL with respect to b1 and λ1 are\\n∂L\\n∂b1\\n= 2b⊤\\n1 S − 2λ1b⊤\\n1 , ∂L\\n∂λ1\\n= 1 − b⊤\\n1 b1 , (10.12)\\nrespectively . Setting these partial derivatives to0 gives us the relations\\nSb1 = λ1b1 , (10.13)\\nb⊤\\n1 b1 = 1 . (10.14)\\nBy comparing this with the definition of an eigenvalue decomposition\\n(Section 4.4), we see that b1 is an eigenvector of the data covariance\\nmatrix S, and the Lagrange multiplier λ1 plays the role of the correspond-\\ning eigenvalue. This eigenvector property (10.13) allows us to rewrite ourThe quantity √λ1 is\\nalso called the\\nloading of the unit\\nvector b1 and\\nrepresents the\\nstandard deviation\\nof the data\\naccounted for by the\\nprincipal subspace\\nspan[b1].\\nvariance objective (10.10) as\\nV1 = b⊤\\n1 Sb1 = λ1b⊤\\n1 b1 = λ1 , (10.15)\\ni.e., the variance of the data projected onto a one-dimensional subspace\\nequals the eigenvalue that is associated with the basis vectorb1 that spans\\nthis subspace. Therefore, to maximize the variance of the low-dimensional\\ncode, we choose the basis vector associated with the largest eigenvalue\\nof the data covariance matrix. This eigenvector is called the first principalprincipal component\\ncomponent. We can determine the effect/contribution of the principal com-\\nponent b1 in the original data space by mapping the coordinate z1n back\\ninto data space, which gives us the projected data point\\n˜xn = b1z1n = b1b⊤\\n1 xn ∈ RD (10.16)\\nin the original data space.\\nRemark. Although ˜xn is a D-dimensional vector, it only requires a single\\ncoordinate z1n to represent it with respect to the basis vectorb1 ∈ RD. ♢\\n10.2.2 M-dimensional Subspace with Maximal Variance\\nAssume we have found the first m − 1 principal components as the m − 1\\neigenvectors of S that are associated with the largest m − 1 eigenvalues.\\nSince S is symmetric, the spectral theorem (Theorem 4.15) states that we\\ncan use these eigenvectors to construct an orthonormal eigenbasis of an\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e7c65a46-af00-47f8-a028-bbe637e6de90', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 328, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.2 Maximum Variance Perspective 323\\n(m − 1)-dimensional subspace of RD. Generally , themth principal com-\\nponent can be found by subtracting the effect of the first m − 1 principal\\ncomponents b1, . . . ,bm−1 from the data, thereby trying to find principal\\ncomponents that compress the remaining information. We then arrive at\\nthe new data matrix\\nˆX := X −\\nm−1X\\ni=1\\nbib⊤\\ni X = X − Bm−1X , (10.17)\\nwhere X = [ x1, . . . ,xN] ∈ RD×N contains the data points as column The matrix ˆX :=\\n[ˆx1, . . . , ˆxN] ∈\\nRD×N in (10.17)\\ncontains the\\ninformation in the\\ndata that has not yet\\nbeen compressed.\\nvectors and Bm−1 :=Pm−1\\ni=1 bib⊤\\ni is a projection matrix that projects onto\\nthe subspace spanned by b1, . . . ,bm−1.\\nRemark (Notation). Throughout this chapter, we do not follow the con-\\nvention of collecting data x1, . . . ,xN as the rows of the data matrix, but\\nwe define them to be the columns of X. This means that our data ma-\\ntrix X is a D × N matrix instead of the conventional N × D matrix. The\\nreason for our choice is that the algebra operations work out smoothly\\nwithout the need to either transpose the matrix or to redefine vectors as\\nrow vectors that are left-multiplied onto matrices. ♢\\nTo find the mth principal component, we maximize the variance\\nVm = V[zm] = 1\\nN\\nNX\\nn=1\\nz2\\nmn = 1\\nN\\nNX\\nn=1\\n(b⊤\\nm ˆxn)2 = b⊤\\nm ˆSbm , (10.18)\\nsubject to ∥bm∥\\n2\\n= 1 , where we followed the same steps as in (10.9b)\\nand defined ˆS as the data covariance matrix of the transformed dataset\\nˆX := {ˆx1, . . . ,ˆxN }. As previously , when we looked at the first principal\\ncomponent alone, we solve a constrained optimization problem and dis-\\ncover that the optimal solutionbm is the eigenvector of ˆS that is associated\\nwith the largest eigenvalue of ˆS.\\nIt turns out that bm is also an eigenvector of S. More generally , the sets\\nof eigenvectors of S and ˆS are identical. Since both S and ˆS are sym-\\nmetric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e.,\\nthere exist D distinct eigenvectors for both S and ˆS. Next, we show that\\nevery eigenvector of S is an eigenvector of ˆS. Assume we have already\\nfound eigenvectors b1, . . . ,bm−1 of ˆS. Consider an eigenvector bi of S,\\ni.e., Sbi = λibi. In general,\\nˆSbi = 1\\nN\\nˆX ˆX\\n⊤\\nbi = 1\\nN (X − Bm−1X)(X − Bm−1X)⊤bi (10.19a)\\n= (S − SB m−1 − Bm−1S + Bm−1SB m−1)bi . (10.19b)\\nWe distinguish between two cases. If i ⩾ m, i.e., bi is an eigenvector\\nthat is not among the firstm −1 principal components, then bi is orthogo-\\nnal to the firstm−1 principal components and Bm−1bi = 0. If i < m , i.e.,\\nbi is among the first m − 1 principal components, then bi is a basis vector\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3683b15-84de-4e6f-8bd0-768e5d1981f8', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 329, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='324 Dimensionality Reduction with Principal Component Analysis\\nof the principal subspace onto which Bm−1 projects. Since b1, . . . ,bm−1\\nare an ONB of this principal subspace, we obtain Bm−1bi = bi. The two\\ncases can be summarized as follows:\\nBm−1bi = bi if i < m , Bm−1bi = 0 if i ⩾ m . (10.20)\\nIn the case i ⩾ m, by using (10.20) in (10.19b), we obtain ˆSbi = (S −\\nBm−1S)bi = Sbi = λibi , i.e., bi is also an eigenvector of ˆS with eigen-\\nvalue λi. Specifically ,\\nˆSbm = Sbm = λmbm . (10.21)\\nEquation (10.21) reveals that bm is not only an eigenvector of S but also\\nof ˆS. Specifically ,λm is the largest eigenvalue of ˆS and λm is the mth\\nlargest eigenvalue of S, and both have the associated eigenvector bm.\\nIn the case i < m , by using (10.20) in (10.19b), we obtain\\nˆSbi = (S − SB m−1 − Bm−1S + Bm−1SB m−1)bi = 0 = 0bi (10.22)\\nThis means that b1, . . . ,bm−1 are also eigenvectors of ˆS, but they are as-\\nsociated with eigenvalue 0 so that b1, . . . ,bm−1 span the null space of ˆS.\\nOverall, every eigenvector of S is also an eigenvector of ˆS. However,\\nif the eigenvectors of S are part of the (m − 1) dimensional principal\\nsubspace, then the associated eigenvalue of ˆS is 0.This derivation\\nshows that there is\\nan intimate\\nconnection between\\nthe M-dimensional\\nsubspace with\\nmaximal variance\\nand the eigenvalue\\ndecomposition. We\\nwill revisit this\\nconnection in\\nSection 10.4.\\nWith the relation (10.21) and b⊤\\nmbm = 1, the variance of the data pro-\\njected onto the mth principal component is\\nVm = b⊤\\nmSbm\\n(10.21)\\n= λmb⊤\\nmbm = λm . (10.23)\\nThis means that the variance of the data, when projected onto an M-\\ndimensional subspace, equals the sum of the eigenvalues that are associ-\\nated with the corresponding eigenvectors of the data covariance matrix.\\nExample 10.2 (Eigenvalues of MNIST “8”)\\nFigure 10.5\\nProperties of the\\ntraining data of\\nMNIST “8”. (a)\\nEigenvalues sorted\\nin descending order;\\n(b) Variance\\ncaptured by the\\nprincipal\\ncomponents\\nassociated with the\\nlargest eigenvalues.\\n0 50 100 150 200\\nIndex\\n0\\n10\\n20\\n30\\n40\\n50Eigenvalue\\n (a) Eigenvalues (sorted in descending order) of\\nthe data covariance matrix of all digits “8” in\\nthe MNIST training set.\\n0 50 100 150 200\\nNumber of principal components\\n100\\n200\\n300\\n400\\n500Captured variance\\n(b) Variance captured by the principal compo-\\nnents.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0da9815a-177a-4e65-ab91-535310d3fcf4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 330, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.3 Projection Perspective 325\\nFigure 10.6\\nIllustration of the\\nprojection\\napproach: Find a\\nsubspace (line) that\\nminimizes the\\nlength of the\\ndifference vector\\nbetween projected\\n(orange) and\\noriginal (blue) data.\\nTaking all digits “8” in the MNIST training data, we compute the eigen-\\nvalues of the data covariance matrix. Figure 10.5(a) shows the200 largest\\neigenvalues of the data covariance matrix. We see that only a few of\\nthem have a value that differs significantly from 0. Therefore, most of\\nthe variance, when projecting data onto the subspace spanned by the cor-\\nresponding eigenvectors, is captured by only a few principal components,\\nas shown in Figure 10.5(b).\\nOverall, to find an M-dimensional subspace of RD that retains as much\\ninformation as possible, PCA tells us to choose the columns of the matrix\\nB in (10.3) as the M eigenvectors of the data covariance matrix S that\\nare associated with the M largest eigenvalues. The maximum amount of\\nvariance PCA can capture with the first M principal components is\\nVM =\\nMX\\nm=1\\nλm , (10.24)\\nwhere the λm are the M largest eigenvalues of the data covariance matrix\\nS. Consequently , the variance lost by data compression via PCA is\\nJM :=\\nDX\\nj=M+1\\nλj = VD − VM . (10.25)\\nInstead of these absolute quantities, we can define the relative variance\\ncaptured as VM\\nVD\\n, and the relative variance lost by compression as 1 − VM\\nVD\\n.\\n10.3 Projection Perspective\\nIn the following, we will derive PCA as an algorithm that directly mini-\\nmizes the average reconstruction error. This perspective allows us to in-\\nterpret PCA as implementing an optimal linear auto-encoder. We will draw\\nheavily from Chapters 2 and 3.\\nIn the previous section, we derived PCA by maximizing the variance\\nin the projected space to retain as much information as possible. In the\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a384356-0190-4d4f-9471-18cefd8f2c66', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 331, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='326 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.7\\nSimplified\\nprojection setting.\\n(a) A vector x ∈ R2\\n(red cross) shall be\\nprojected onto a\\none-dimensional\\nsubspace U ⊆ R2\\nspanned by b. (b)\\nshows the difference\\nvectors between x\\nand some\\ncandidates ˜x.\\n−1.0 −0.5 0.0 0.5 1.0 1.5 2.0\\nx1\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nx2\\nb\\nU\\n(a) Setting.\\n−1.0 −0.5 0.0 0.5 1.0 1.5 2.0\\nx1\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nx2\\nb\\nU (b) Differences x − ˜xi for 50 different ˜xi are\\nshown by the red lines.\\nfollowing, we will look at the difference vectors between the original data\\nxn and their reconstruction ˜xn and minimize this distance so that xn and\\n˜xn are as close as possible. Figure 10.6 illustrates this setting.\\n10.3.1 Setting and Objective\\nAssume an (ordered) orthonormal basis (ONB) B = (b1, . . . ,bD) of RD,\\ni.e., b⊤\\ni bj = 1 if and only if i = j and 0 otherwise.\\nFrom Section 2.5 we know that for a basis (b1, . . . ,bD) of RD any x ∈\\nRD can be written as a linear combination of the basis vectors of RD, i.e.,\\nVectors ˜x ∈ U could\\nbe vectors on a\\nplane in R3. The\\ndimensionality of\\nthe plane is 2, but\\nthe vectors still have\\nthree coordinates\\nwith respect to the\\nstandard basis of\\nR3.\\nx =\\nDX\\nd=1\\nζdbd =\\nMX\\nm=1\\nζmbm +\\nDX\\nj=M+1\\nζjbj (10.26)\\nfor suitable coordinates ζd ∈ R.\\nWe are interested in finding vectors ˜x ∈ RD, which live in lower-\\ndimensional subspace U ⊆ RD, dim(U) = M, so that\\n˜x =\\nMX\\nm=1\\nzmbm ∈ U ⊆ RD (10.27)\\nis as similar to x as possible. Note that at this point we need to assume\\nthat the coordinates zm of ˜x and ζm of x are not identical.\\nIn the following, we use exactly this kind of representation of ˜x to find\\noptimal coordinates z and basis vectors b1, . . . ,bM such that ˜x is as sim-\\nilar to the original data point x as possible, i.e., we aim to minimize the\\n(Euclidean) distance ∥x − ˜x∥. Figure 10.7 illustrates this setting.\\nWithout loss of generality , we assume that the datasetX = {x1, . . . ,xN },\\nxn ∈ RD, is centered at0, i.e., E[X ] = 0. Without the zero-mean assump-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='65b45d07-7bec-4501-b907-4db114c04f51', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 332, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.3 Projection Perspective 327\\ntion, we would arrive at exactly the same solution, but the notation would\\nbe substantially more cluttered.\\nWe are interested in finding the best linear projection ofX onto a lower-\\ndimensional subspace U of RD with dim(U) = M and orthonormal basis\\nvectors b1, . . . ,bM. We will call this subspace U the principal subspace. principal subspace\\nThe projections of the data points are denoted by\\n˜xn :=\\nMX\\nm=1\\nzmnbm = Bz n ∈ RD , (10.28)\\nwhere zn := [ z1n, . . . , zM n]⊤ ∈ RM is the coordinate vector of ˜xn with\\nrespect to the basis (b1, . . . ,bM). More specifically , we are interested in\\nhaving the ˜xn as similar to xn as possible.\\nThe similarity measure we use in the following is the squared distance\\n(Euclidean norm) ∥x − ˜x∥\\n2\\nbetween x and ˜x. We therefore define our ob-\\njective as minimizing the average squared Euclidean distance (reconstruction reconstruction error\\nerror) (Pearson, 1901)\\nJM := 1\\nN\\nNX\\nn=1\\n∥xn − ˜xn∥2 , (10.29)\\nwhere we make it explicit that the dimension of the subspace onto which\\nwe project the data is M. In order to find this optimal linear projection,\\nwe need to find the orthonormal basis of the principal subspace and the\\ncoordinates zn ∈ RM of the projections with respect to this basis.\\nTo find the coordinates zn and the ONB of the principal subspace, we\\nfollow a two-step approach. First, we optimize the coordinates zn for a\\ngiven ONB (b1, . . . ,bM); second, we find the optimal ONB.\\n10.3.2 Finding Optimal Coordinates\\nLet us start by finding the optimal coordinates z1n, . . . , zM n of the projec-\\ntions ˜xn for n = 1, . . . , N. Consider Figure 10.7(b), where the principal\\nsubspace is spanned by a single vector b. Geometrically speaking, finding\\nthe optimal coordinates z corresponds to finding the representation of the\\nlinear projection ˜x with respect to b that minimizes the distance between\\n˜x − x. From Figure 10.7(b), it is clear that this will be the orthogonal\\nprojection, and in the following we will show exactly this.\\nWe assume an ONB (b1, . . . ,bM) of U ⊆ RD. To find the optimal co-\\nordinates zm with respect to this basis, we require the partial derivatives\\n∂JM\\n∂zin\\n= ∂JM\\n∂˜xn\\n∂˜xn\\n∂zin\\n, (10.30a)\\n∂JM\\n∂˜xn\\n= − 2\\nN (xn − ˜xn)⊤ ∈ R1×D , (10.30b)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d29e87be-b5de-4290-ad52-d722ae94e7c7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 333, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='328 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.8\\nOptimal projection\\nof a vector x ∈ R2\\nonto a\\none-dimensional\\nsubspace\\n(continuation from\\nFigure 10.7).\\n(a) Distances\\n∥x − ˜x∥ for some\\n˜x ∈ U.\\n(b) Orthogonal\\nprojection and\\noptimal coordinates.\\n−1.0 −0.5 0.0 0.5 1.0 1.5 2.0\\nx1\\n1.25\\n1.50\\n1.75\\n2.00\\n2.25\\n2.50\\n2.75\\n3.00\\n3.25\\n∥x−˜x∥\\n(a) Distances ∥x − ˜x∥ for some ˜x = z1b ∈\\nU = span[b]; see panel (b) for the setting.\\n−1.0 −0.5 0.0 0.5 1.0 1.5 2.0\\nx1\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nx2\\nb\\nU\\n˜x\\n(b) The vector ˜x that minimizes the distance\\nin panel (a) is its orthogonal projection onto\\nU. The coordinate of the projection ˜x with\\nrespect to the basis vector b that spans U\\nis the factor we need to scale b in order to\\n“reach”˜x.\\n∂˜xn\\n∂zin\\n(10.28)\\n= ∂\\n∂zin\\n MX\\nm=1\\nzmnbm\\n!\\n= bi (10.30c)\\nfor i = 1, . . . , M, such that we obtain\\n∂JM\\n∂zin\\n(10.30b)\\n(10.30c)\\n= − 2\\nN (xn − ˜xn)⊤bi\\n(10.28)\\n= − 2\\nN\\n \\nxn −\\nMX\\nm=1\\nzmnbm\\n!⊤\\nbi\\n(10.31a)\\nONB\\n= − 2\\nN (x⊤\\nn bi − zinb⊤\\ni bi) = − 2\\nN (x⊤\\nn bi − zin) . (10.31b)\\nsince b⊤\\ni bi = 1. Setting this partial derivative to 0 yields immediately theThe coordinates of\\nthe optimal\\nprojection of xn\\nwith respect to the\\nbasis vectors\\nb1, . . . ,bM are the\\ncoordinates of the\\northogonal\\nprojection of xn\\nonto the principal\\nsubspace.\\noptimal coordinates\\nzin = x⊤\\nn bi = b⊤\\ni xn (10.32)\\nfor i = 1 , . . . , M and n = 1 , . . . , N. This means that the optimal co-\\nordinates zin of the projection ˜xn are the coordinates of the orthogonal\\nprojection (see Section 3.8) of the original data point xn onto the one-\\ndimensional subspace that is spanned by bi. Consequently:\\nThe optimal linear projection ˜xn of xn is an orthogonal projection.\\nThe coordinates of ˜xn with respect to the basis (b1, . . . ,bM) are the\\ncoordinates of the orthogonal projection of xn onto the principal sub-\\nspace.\\nAn orthogonal projection is the best linear mapping given the objec-\\ntive (10.29).\\nThe coordinates ζm of x in (10.26) and the coordinateszm of ˜x in (10.27)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0979f2f4-f337-4b20-944d-a01ebeab78e4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 334, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.3 Projection Perspective 329\\nmust be identical for m = 1, . . . , M since U ⊥ = span[bM+1, . . . ,bD] is\\nthe orthogonal complement (see Section 3.6) of U = span[b1, . . . ,bM].\\nRemark (Orthogonal Projections with Orthonormal Basis Vectors). Let us\\nbriefly recap orthogonal projections from Section 3.8. If(b1, . . . ,bD) is an\\northonormal basis of RD then b⊤\\nj x is the\\ncoordinate of the\\northogonal\\nprojection of x onto\\nthe subspace\\nspanned by bj.\\n˜x = bj(b⊤\\nj bj)−1b⊤\\nj x = bjb⊤\\nj x ∈ RD (10.33)\\nis the orthogonal projection ofx onto the subspace spanned by thejth ba-\\nsis vector, andzj = b⊤\\nj x is the coordinate of this projection with respect to\\nthe basis vector bj that spans that subspace sincezjbj = ˜x. Figure 10.8(b)\\nillustrates this setting.\\nMore generally , if we aim to project onto an M-dimensional subspace\\nof RD, we obtain the orthogonal projection of x onto the M-dimensional\\nsubspace with orthonormal basis vectors b1, . . . ,bM as\\n˜x = B(B⊤B|{z}\\n=I\\n)−1B⊤x = BB ⊤x , (10.34)\\nwhere we defined B := [ b1, . . . ,bM] ∈ RD×M. The coordinates of this\\nprojection with respect to the ordered basis (b1, . . . ,bM) are z := B⊤x\\nas discussed in Section 3.8.\\nWe can think of the coordinates as a representation of the projected\\nvector in a new coordinate system defined by (b1, . . . ,bM). Note that al-\\nthough ˜x ∈ RD, we only need M coordinates z1, . . . , zM to represent\\nthis vector; the other D − M coordinates with respect to the basis vectors\\n(bM+1, . . . ,bD) are always 0. ♢\\nSo far we have shown that for a given ONB we can find the optimal\\ncoordinates of ˜x by an orthogonal projection onto the principal subspace.\\nIn the following, we will determine what the best basis is.\\n10.3.3 Finding the Basis of the Principal Subspace\\nTo determine the basis vectors b1, . . . ,bM of the principal subspace, we\\nrephrase the loss function (10.29) using the results we have so far. This\\nwill make it easier to find the basis vectors. To reformulate the loss func-\\ntion, we exploit our results from before and obtain\\n˜xn =\\nMX\\nm=1\\nzmnbm\\n(10.32)\\n=\\nMX\\nm=1\\n(x⊤\\nn bm)bm . (10.35)\\nWe now exploit the symmetry of the dot product, which yields\\n˜xn =\\n MX\\nm=1\\nbmb⊤\\nm\\n!\\nxn . (10.36)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6dbbb8ee-5bf6-41e2-b129-6da4524bb325', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 335, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='330 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.9\\nOrthogonal\\nprojection and\\ndisplacement\\nvectors. When\\nprojecting data\\npoints xn (blue)\\nonto subspace U1,\\nwe obtain ˜xn\\n(orange). The\\ndisplacement vector\\n˜xn − xn lies\\ncompletely in the\\northogonal\\ncomplement U2 of\\nU1.\\n−5 0 5\\nx1\\n−6\\n−4\\n−2\\n0\\n2\\n4\\n6\\nx2\\nU\\nU⊥\\nSince we can generally write the original data point xn as a linear combi-\\nnation of all basis vectors, it holds that\\nxn =\\nDX\\nd=1\\nzdnbd\\n(10.32)\\n=\\nDX\\nd=1\\n(x⊤\\nn bd)bd =\\n DX\\nd=1\\nbdb⊤\\nd\\n!\\nxn (10.37a)\\n=\\n MX\\nm=1\\nbmb⊤\\nm\\n!\\nxn +\\n DX\\nj=M+1\\nbjb⊤\\nj\\n!\\nxn , (10.37b)\\nwhere we split the sum with D terms into a sum over M and a sum\\nover D − M terms. With this result, we find that the displacement vector\\nxn − ˜xn, i.e., the difference vector between the original data point and its\\nprojection, is\\nxn − ˜xn =\\n DX\\nj=M+1\\nbjb⊤\\nj\\n!\\nxn (10.38a)\\n=\\nDX\\nj=M+1\\n(x⊤\\nn bj)bj . (10.38b)\\nThis means the difference is exactly the projection of the data point onto\\nthe orthogonal complement of the principal subspace: We identify the ma-\\ntrixPD\\nj=M+1 bjb⊤\\nj in (10.38a) as the projection matrix that performs this\\nprojection. Hence the displacement vector xn − ˜xn lies in the subspace\\nthat is orthogonal to the principal subspace as illustrated in Figure 10.9.\\nRemark (Low-Rank Approximation). In (10.38a), we saw that the projec-\\ntion matrix, which projects x onto ˜x, is given by\\nMX\\nm=1\\nbmb⊤\\nm = BB ⊤ . (10.39)\\nBy construction as a sum of rank-one matrices bmb⊤\\nm we see that BB ⊤ is\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='16d740ac-f3ac-4cf8-b2c8-b6f225bc6f29', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 336, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.3 Projection Perspective 331\\nsymmetric and has rankM. Therefore, the average squared reconstruction\\nerror can also be written as\\n1\\nN\\nNX\\nn=1\\n∥xn − ˜xn∥\\n2\\n= 1\\nN\\nNX\\nn=1\\n\\r\\r\\rxn − BB ⊤xn\\n\\r\\r\\r\\n2\\n(10.40a)\\n= 1\\nN\\nNX\\nn=1\\n\\r\\r\\r(I − BB ⊤)xn\\n\\r\\r\\r\\n2\\n. (10.40b)\\nFinding orthonormal basis vectors b1, . . . ,bM, which minimize the differ- PCA finds the best\\nrank-M\\napproximation of\\nthe identity matrix.\\nence between the original data xn and their projections ˜xn, is equivalent\\nto finding the best rank- M approximation BB ⊤ of the identity matrix I\\n(see Section 4.6). ♢\\nNow we have all the tools to reformulate the loss function (10.29).\\nJM = 1\\nN\\nNX\\nn=1\\n∥xn − ˜xn∥2 (10.38b)\\n= 1\\nN\\nNX\\nn=1\\n\\r\\r\\r\\r\\r\\nDX\\nj=M+1\\n(b⊤\\nj xn)bj\\n\\r\\r\\r\\r\\r\\n2\\n. (10.41)\\nWe now explicitly compute the squared norm and exploit the fact that the\\nbj form an ONB, which yields\\nJM = 1\\nN\\nNX\\nn=1\\nDX\\nj=M+1\\n(b⊤\\nj xn)2 = 1\\nN\\nNX\\nn=1\\nDX\\nj=M+1\\nb⊤\\nj xnb⊤\\nj xn (10.42a)\\n= 1\\nN\\nNX\\nn=1\\nDX\\nj=M+1\\nb⊤\\nj xnx⊤\\nn bj , (10.42b)\\nwhere we exploited the symmetry of the dot product in the last step to\\nwrite b⊤\\nj xn = x⊤\\nn bj. We now swap the sums and obtain\\nJM =\\nDX\\nj=M+1\\nb⊤\\nj\\n \\n1\\nN\\nNX\\nn=1\\nxnx⊤\\nn\\n!\\n| {z }\\n=:S\\nbj =\\nDX\\nj=M+1\\nb⊤\\nj Sbj (10.43a)\\n=\\nDX\\nj=M+1\\ntr(b⊤\\nj Sbj) =\\nDX\\nj=M+1\\ntr(Sbjb⊤\\nj ) = tr\\n\\x10\\x10 DX\\nj=M+1\\nbjb⊤\\nj\\n\\x11\\n| {z }\\nprojection matrix\\nS\\n\\x11\\n,\\n(10.43b)\\nwhere we exploited the property that the trace operator tr(·) (see (4.18))\\nis linear and invariant to cyclic permutations of its arguments. Since we\\nassumed that our dataset is centered, i.e., E[X ] = 0, we identify S as the\\ndata covariance matrix. Since the projection matrix in (10.43b) is con-\\nstructed as a sum of rank-one matrices bjb⊤\\nj it itself is of rank D − M.\\nEquation (10.43a) implies that we can formulate the average squared\\nreconstruction error equivalently as the covariance matrix of the data,\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02419417-589b-4301-bddc-c7d55adf4d42', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 337, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='332 Dimensionality Reduction with Principal Component Analysis\\nprojected onto the orthogonal complement of the principal subspace. Min-\\nimizing the average squared reconstruction error is therefore equivalent toMinimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nminimizing the\\nprojection of the\\ndata covariance\\nmatrix onto the\\northogonal\\ncomplement of the\\nprincipal subspace.\\nminimizing the variance of the data when projected onto the subspace we\\nignore, i.e., the orthogonal complement of the principal subspace. Equiva-\\nlently , we maximize the variance of the projection that we retain in the\\nprincipal subspace, which links the projection loss immediately to the\\nmaximum-variance formulation of PCA discussed in Section 10.2. But this\\nthen also means that we will obtain the same solution that we obtained\\nMinimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nmaximizing the\\nvariance of the\\nprojected data.\\nfor the maximum-variance perspective. Therefore, we omit a derivation\\nthat is identical to the one presented in Section 10.2 and summarize the\\nresults from earlier in the light of the projection perspective.\\nThe average squared reconstruction error, when projecting onto theM-\\ndimensional principal subspace, is\\nJM =\\nDX\\nj=M+1\\nλj , (10.44)\\nwhere λj are the eigenvalues of the data covariance matrix. Therefore,\\nto minimize (10.44) we need to select the smallest D − M eigenvalues,\\nwhich then implies that their corresponding eigenvectors are the basis of\\nthe orthogonal complement of the principal subspace. Consequently , this\\nmeans that the basis of the principal subspace comprises the eigenvectors\\nb1, . . . ,bM that are associated with the largest M eigenvalues of the data\\ncovariance matrix.\\nExample 10.3 (MNIST Digits Embedding)\\nFigure 10.10\\nEmbedding of\\nMNIST digits 0\\n(blue) and 1\\n(orange) in a\\ntwo-dimensional\\nprincipal subspace\\nusing PCA. Four\\nembeddings of the\\ndigits “0” and “1” in\\nthe principal\\nsubspace are\\nhighlighted in red\\nwith their\\ncorresponding\\noriginal digit.\\nFigure 10.10 visualizes the training data of the MMIST digits “0” and “1”\\nembedded in the vector subspace spanned by the first two principal com-\\nponents. We observe a relatively clear separation between “0”s (blue dots)\\nand “1”s (orange dots), and we see the variation within each individual\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9bf857d3-e4e9-4b6d-a185-b2e2256c8c22', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 338, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.4 Eigenvector Computation and Low-Rank Approximations 333\\ncluster. Four embeddings of the digits “0” and “1” in the principal subspace\\nare highlighted in red with their corresponding original digit. The figure\\nreveals that the variation within the set of “0” is significantly greater than\\nthe variation within the set of “1”.\\n10.4 Eigenvector Computation and Low-Rank Approximations\\nIn the previous sections, we obtained the basis of the principal subspace\\nas the eigenvectors that are associated with the largest eigenvalues of the\\ndata covariance matrix\\nS = 1\\nN\\nNX\\nn=1\\nxnx⊤\\nn = 1\\nN XX ⊤ , (10.45)\\nX = [x1, . . . ,xN] ∈ RD×N . (10.46)\\nNote that X is a D × N matrix, i.e., it is the transpose of the “typical”\\ndata matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\\nthe corresponding eigenvectors) of S, we can follow two approaches: Use\\neigendecomposition\\nor SVD to compute\\neigenvectors.\\nWe perform an eigendecomposition (see Section 4.2) and compute the\\neigenvalues and eigenvectors of S directly .\\nWe use a singular value decomposition (see Section 4.5). Since S is\\nsymmetric and factorizes into XX ⊤ (ignoring the factor 1\\nN ), the eigen-\\nvalues of S are the squared singular values of X.\\nMore specifically , the SVD ofX is given by\\nX|{z}\\nD×N\\n= U|{z}\\nD×D\\nΣ|{z}\\nD×N\\nV ⊤\\n|{z}\\nN ×N\\n, (10.47)\\nwhere U ∈ RD×D and V ⊤ ∈ RN ×N are orthogonal matrices and Σ ∈\\nRD×N is a matrix whose only nonzero entries are the singular valuesσii ⩾\\n0. It then follows that\\nS = 1\\nN XX ⊤ = 1\\nN UΣ V ⊤V|{z}\\n=I N\\nΣ⊤U ⊤ = 1\\nN UΣΣ⊤U ⊤ . (10.48)\\nWith the results from Section 4.5, we get that the columns of U are the The columns of U\\nare the eigenvectors\\nof S.\\neigenvectors of XX ⊤ (and therefore S). Furthermore, the eigenvalues\\nλd of S are related to the singular values of X via\\nλd = σ2\\nd\\nN . (10.49)\\nThis relationship between the eigenvalues of S and the singular values\\nof X provides the connection between the maximum variance view (Sec-\\ntion 10.2) and the singular value decomposition.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0bb8bb35-5a37-4009-aee5-5045c0973da4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 339, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='334 Dimensionality Reduction with Principal Component Analysis\\n10.4.1 PCA Using Low-Rank Matrix Approximations\\nTo maximize the variance of the projected data (or minimize the average\\nsquared reconstruction error), PCA chooses the columns of U in (10.48)\\nto be the eigenvectors that are associated with the M largest eigenvalues\\nof the data covariance matrixS so that we identifyU as the projection ma-\\ntrix B in (10.3), which projects the original data onto a lower-dimensional\\nsubspace of dimension M. The Eckart-Young theorem (Theorem 4.25 inEckart-Young\\ntheorem Section 4.6) offers a direct way to estimate the low-dimensional represen-\\ntation. Consider the best rank-M approximation\\n˜X M := argminrk(A)⩽M ∥X − A∥2 ∈ RD×N (10.50)\\nof X, where ∥·∥2 is the spectral norm defined in (4.93). The Eckart-Young\\ntheorem states that ˜X M is given by truncating the SVD at the top- M\\nsingular value. In other words, we obtain\\n˜X M = U M|{z}\\nD×M\\nΣM|{z}\\nM ×M\\nV ⊤\\nM|{z}\\nM ×N\\n∈ RD×N (10.51)\\nwith orthogonal matrices U M := [ u1, . . . ,uM] ∈ RD×M and V M :=\\n[v1, . . . ,vM] ∈ RN ×M and a diagonal matrix ΣM ∈ RM ×M whose diago-\\nnal entries are the M largest singular values of X.\\n10.4.2 Practical Aspects\\nFinding eigenvalues and eigenvectors is also important in other funda-\\nmental machine learning methods that require matrix decompositions. In\\ntheory , as we discussed in Section 4.2, we can solve for the eigenvalues as\\nroots of the characteristic polynomial. However, for matrices larger than\\n4×4 this is not possible because we would need to find the roots of a poly-\\nnomial of degree 5 or higher. However, the Abel-Ruffini theorem (Ruffini,Abel-Ruffini\\ntheorem 1799; Abel, 1826) states that there exists no algebraic solution to this\\nproblem for polynomials of degree 5 or more. Therefore, in practice, wenp.linalg.eigh\\nor\\nnp.linalg.svd\\nsolve for eigenvalues or singular values using iterative methods, which are\\nimplemented in all modern packages for linear algebra.\\nIn many applications (such as PCA presented in this chapter), we only\\nrequire a few eigenvectors. It would be wasteful to compute the full de-\\ncomposition, and then discard all eigenvectors with eigenvalues that are\\nbeyond the first few. It turns out that if we are interested in only the first\\nfew eigenvectors (with the largest eigenvalues), then iterative processes,\\nwhich directly optimize these eigenvectors, are computationally more effi-\\ncient than a full eigendecomposition (or SVD). In the extreme case of only\\nneeding the first eigenvector, a simple method called the power iterationpower iteration\\nis very efficient. Power iteration chooses a random vectorx0 that is not in\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a178cbf-12b2-4021-a13f-f6f8ab81ed5e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 340, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.5 PCA in High Dimensions 335\\nthe null space of S and follows the iteration\\nxk+1 = Sxk\\n∥Sxk∥ , k = 0, 1, . . . . (10.52)\\nThis means the vector xk is multiplied by S in every iteration and then If S is invertible, it\\nis sufficient to\\nensure that x0 ̸= 0.\\nnormalized, i.e., we always have ∥xk∥ = 1. This sequence of vectors con-\\nverges to the eigenvector associated with the largest eigenvalue of S. The\\noriginal Google PageRank algorithm (Page et al., 1999) uses such an al-\\ngorithm for ranking web pages based on their hyperlinks.\\n10.5 PCA in High Dimensions\\nIn order to do PCA, we need to compute the data covariance matrix. In D\\ndimensions, the data covariance matrix is a D × D matrix. Computing the\\neigenvalues and eigenvectors of this matrix is computationally expensive\\nas it scales cubically in D. Therefore, PCA, as we discussed earlier, will be\\ninfeasible in very high dimensions. For example, if ourxn are images with\\n10,000 pixels (e.g., 100 × 100 pixel images), we would need to compute\\nthe eigendecomposition of a 10,000 × 10,000 covariance matrix. In the\\nfollowing, we provide a solution to this problem for the case that we have\\nsubstantially fewer data points than dimensions, i.e., N ≪ D.\\nAssume we have a centered dataset x1, . . . ,xN, xn ∈ RD. Then the\\ndata covariance matrix is given as\\nS = 1\\nN XX ⊤ ∈ RD×D , (10.53)\\nwhere X = [x1, . . . ,xN] is a D × N matrix whose columns are the data\\npoints.\\nWe now assume that N ≪ D, i.e., the number of data points is smaller\\nthan the dimensionality of the data. If there are no duplicate data points,\\nthe rank of the covariance matrix S is N, so it has D − N + 1many eigen-\\nvalues that are 0. Intuitively , this means that there are some redundancies.\\nIn the following, we will exploit this and turn theD×D covariance matrix\\ninto an N × N covariance matrix whose eigenvalues are all positive.\\nIn PCA, we ended up with the eigenvector equation\\nSbm = λmbm , m = 1, . . . , M , (10.54)\\nwhere bm is a basis vector of the principal subspace. Let us rewrite this\\nequation a bit: With S defined in (10.53), we obtain\\nSbm = 1\\nN XX ⊤bm = λmbm . (10.55)\\nWe now multiply X ⊤ ∈ RN ×D from the left-hand side, which yields\\n1\\nN X ⊤X| {z }\\nN ×N\\nX ⊤bm| {z }\\n=:cm\\n= λmX ⊤bm ⇐ ⇒ 1\\nN X ⊤Xc m = λmcm , (10.56)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b8973161-6578-47e9-ae10-367dcf01be19', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 341, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='336 Dimensionality Reduction with Principal Component Analysis\\nand we get a new eigenvector/eigenvalue equation: λm remains eigen-\\nvalue, which confirms our results from Section 4.5.3 that the nonzero\\neigenvalues of XX ⊤ equal the nonzero eigenvalues of X ⊤X. We obtain\\nthe eigenvector of the matrix 1\\nN X ⊤X ∈ RN ×N associated with λm as\\ncm := X ⊤bm. Assuming we have no duplicate data points, this matrix\\nhas rank N and is invertible. This also implies that 1\\nN X ⊤X has the same\\n(nonzero) eigenvalues as the data covariance matrixS. But this is now an\\nN × N matrix, so that we can compute the eigenvalues and eigenvectors\\nmuch more efficiently than for the originalD × D data covariance matrix.\\nNow that we have the eigenvectors of 1\\nN X ⊤X, we are going to re-\\ncover the original eigenvectors, which we still need for PCA. Currently ,\\nwe know the eigenvectors of 1\\nN X ⊤X. If we left-multiply our eigenvalue/\\neigenvector equation with X, we get\\n1\\nN XX ⊤\\n| {z }\\nS\\nXc m = λmXc m (10.57)\\nand we recover the data covariance matrix again. This now also means\\nthat we recover Xc m as an eigenvector of S.\\nRemark. If we want to apply the PCA algorithm that we discussed in Sec-\\ntion 10.6, we need to normalize the eigenvectors Xc m of S so that they\\nhave norm 1. ♢\\n10.6 Key Steps of PCA in Practice\\nIn the following, we will go through the individual steps of PCA using a\\nrunning example, which is summarized in Figure 10.11. We are given a\\ntwo-dimensional dataset (Figure 10.11(a)), and we want to use PCA to\\nproject it onto a one-dimensional subspace.\\n1. Mean subtraction We start by centering the data by computing the\\nmean µ of the dataset and subtracting it from every single data point.\\nThis ensures that the dataset has mean0 (Figure 10.11(b)). Mean sub-\\ntraction is not strictly necessary but reduces the risk of numerical prob-\\nlems.\\n2. Standardization Divide the data points by the standard deviationσd\\nof the dataset for every dimension d = 1, . . . , D. Now the data is unit\\nfree, and it has variance 1 along each axis, which is indicated by the\\ntwo arrows in Figure 10.11(c). This step completes thestandardizationstandardization\\nof the data.\\n3. Eigendecomposition of the covariance matrix Compute the data\\ncovariance matrix and its eigenvalues and corresponding eigenvectors.\\nSince the covariance matrix is symmetric, the spectral theorem (The-\\norem 4.15) states that we can find an ONB of eigenvectors. In Fig-\\nure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31453bdc-04df-4434-9ace-5017be1139d5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 342, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.6 Key Steps of PCA in Practice 337\\nFigure 10.11 Steps\\nof PCA. (a) Original\\ndataset;\\n(b) centering;\\n(c) divide by\\nstandard deviation;\\n(d) eigendecomposi-\\ntion; (e) projection;\\n(f) mapping back to\\noriginal data space.\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n(a) Original dataset.\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n (b) Step 1: Centering by sub-\\ntracting the mean from each\\ndata point.\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n(c) Step 2: Dividing by the\\nstandard deviation to make\\nthe data unit free. Data has\\nvariance 1 along each axis.\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n(d) Step 3: Compute eigenval-\\nues and eigenvectors (arrows)\\nof the data covariance matrix\\n(ellipse).\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n(e) Step 4: Project data onto\\nthe principal subspace.\\n0 5\\nx1\\n−2.5\\n0.0\\n2.5\\n5.0\\nx2\\n(f) Undo the standardization\\nand move projected data back\\ninto the original data space\\nfrom (a).\\nresponding eigenvalue. The longer vector spans the principal subspace,\\nwhich we denote by U. The data covariance matrix is represented by\\nthe ellipse.\\n4. Projection We can project any data pointx∗ ∈ RD onto the principal\\nsubspace: To get this right, we need to standardize x∗ using the mean\\nµd and standard deviation σd of the training data in thedth dimension,\\nrespectively , so that\\nx(d)\\n∗ ← x(d)\\n∗ − µd\\nσd\\n, d = 1, . . . , D , (10.58)\\nwhere x(d)\\n∗ is the dth component of x∗. We obtain the projection as\\n˜x∗ = BB ⊤x∗ (10.59)\\nwith coordinates\\nz∗ = B⊤x∗ (10.60)\\nwith respect to the basis of the principal subspace. Here, B is the ma-\\ntrix that contains the eigenvectors that are associated with the largest\\neigenvalues of the data covariance matrix as columns. PCA returns the\\ncoordinates (10.60), not the projections x∗.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='afce3b01-fa3a-4ad1-a119-be51cae532ac', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 343, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='338 Dimensionality Reduction with Principal Component Analysis\\nHaving standardized our dataset, (10.59) only yields the projections in\\nthe context of the standardized dataset. To obtain our projection in the\\noriginal data space (i.e., before standardization), we need to undo the\\nstandardization (10.58) and multiply by the standard deviation before\\nadding the mean so that we obtain\\n˜x(d)\\n∗ ← ˜x(d)\\n∗ σd + µd , d = 1, . . . , D . (10.61)\\nFigure 10.11(f) illustrates the projection in the original data space.\\nExample 10.4 (MNIST Digits: Reconstruction)\\nIn the following, we will apply PCA to the MNIST digits dataset, which\\ncontains 60,000 examples of handwritten digits 0 through 9. Each digit is\\nan image of size28×28, i.e., it contains784 pixels so that we can interpret\\nevery image in this dataset as a vector x ∈ R784. Examples of these digits\\nare shown in Figure 10.3.\\nFigure 10.12 Effect\\nof increasing the\\nnumber of principal\\ncomponents on\\nreconstruction.\\nOriginal\\nPCs: 1\\nPCs: 10\\nPCs: 100\\nPCs: 500\\nFor illustration purposes, we apply PCA to a subset of the MNIST digits,\\nand we focus on the digit “8”. We used 5,389 training images of the digit\\n“8” and determined the principal subspace as detailed in this chapter. We\\nthen used the learned projection matrix to reconstruct a set of test im-\\nages, which is illustrated in Figure 10.12. The first row of Figure 10.12\\nshows a set of four original digits from the test set. The following rows\\nshow reconstructions of exactly these digits when using a principal sub-\\nspace of dimensions 1, 10, 100, and 500, respectively . We see that even\\nwith a single-dimensional principal subspace we get a halfway decent re-\\nconstruction of the original digits, which, however, is blurry and generic.\\nWith an increasing number of principal components (PCs), the reconstruc-\\ntions become sharper and more details are accounted for. With 500 prin-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d0044b0a-9ebb-41c9-b70a-7297312c0bc9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 344, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.7 Latent Variable Perspective 339\\ncipal components, we effectively obtain a near-perfect reconstruction. If\\nwe were to choose 784 PCs, we would recover the exact digit without any\\ncompression loss.\\nFigure 10.13 shows the average squared reconstruction error, which is\\n1\\nN\\nNX\\nn=1\\n∥xn − ˜xn∥\\n2\\n=\\nDX\\ni=M+1\\nλi , (10.62)\\nas a function of the number M of principal components. We can see that\\nthe importance of the principal components drops off rapidly , and only\\nmarginal gains can be achieved by adding more PCs. This matches exactly\\nour observation in Figure 10.5, where we discovered that the most of the\\nvariance of the projected data is captured by only a few principal compo-\\nnents. With about550 PCs, we can essentially fully reconstruct the training\\ndata that contains the digit “8” (some pixels around the boundaries show\\nno variation across the dataset as they are always black).\\nFigure 10.13\\nAverage squared\\nreconstruction error\\nas a function of the\\nnumber of principal\\ncomponents. The\\naverage squared\\nreconstruction error\\nis the sum of the\\neigenvalues in the\\northogonal\\ncomplement of the\\nprincipal subspace.\\n0 200 400 600 800\\nNumber of PCs\\n0\\n100\\n200\\n300\\n400\\n500Average squared reconstruction error\\n10.7 Latent Variable Perspective\\nIn the previous sections, we derived PCA without any notion of a prob-\\nabilistic model using the maximum-variance and the projection perspec-\\ntives. On the one hand, this approach may be appealing as it allows us to\\nsidestep all the mathematical difficulties that come with probability the-\\nory , but on the other hand, a probabilistic model would offer us more flex-\\nibility and useful insights. More specifically , a probabilistic model would\\nCome with a likelihood function, and we can explicitly deal with noisy\\nobservations (which we did not even discuss earlier)\\nAllow us to do Bayesian model comparison via the marginal likelihood\\nas discussed in Section 8.6\\nView PCA as a generative model, which allows us to simulate new data\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2e4635ea-748b-48bd-b938-e7dbb860e861', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 345, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='340 Dimensionality Reduction with Principal Component Analysis\\nAllow us to make straightforward connections to related algorithms\\nDeal with data dimensions that are missing at random by applying\\nBayes’ theorem\\nGive us a notion of the novelty of a new data point\\nGive us a principled way to extend the model, e.g., to a mixture of PCA\\nmodels\\nHave the PCA we derived in earlier sections as a special case\\nAllow for a fully Bayesian treatment by marginalizing out the model\\nparameters\\nBy introducing a continuous-valued latent variable z ∈ RM it is possible\\nto phrase PCA as a probabilistic latent-variable model. Tipping and Bishop\\n(1999) proposed this latent-variable model as probabilistic PCA (PPCA).probabilistic PCA\\nPPCA PPCA addresses most of the aforementioned issues, and the PCA solution\\nthat we obtained by maximizing the variance in the projected space or\\nby minimizing the reconstruction error is obtained as the special case of\\nmaximum likelihood estimation in a noise-free setting.\\n10.7.1 Generative Process and Probabilistic Model\\nIn PPCA, we explicitly write down the probabilistic model for linear di-\\nmensionality reduction. For this we assume a continuous latent variable\\nz ∈ RM with a standard-normal prior p(z) = N\\n\\x00\\n0, I\\n\\x01\\nand a linear rela-\\ntionship between the latent variables and the observed x data where\\nx = Bz + µ + ϵ ∈ RD , (10.63)\\nwhere ϵ ∼ N\\n\\x00\\n0, σ 2I\\n\\x01\\nis Gaussian observation noise and B ∈ RD×M\\nand µ ∈ RD describe the linear/affine mapping from latent to observed\\nvariables. Therefore, PPCA links latent and observed variables via\\np(x|z, B, µ, σ2) = N\\n\\x00\\nx | Bz + µ, σ 2I\\n\\x01\\n. (10.64)\\nOverall, PPCA induces the following generative process:\\nzn ∼ N\\n\\x00\\nz | 0, I\\n\\x01\\n(10.65)\\nxn | zn ∼ N\\n\\x00\\nx | Bz n + µ, σ 2I\\n\\x01\\n(10.66)\\nTo generate a data point that is typical given the model parameters, we\\nfollow an ancestral sampling scheme: We first sample a latent variable znancestral sampling\\nfrom p(z). Then we use zn in (10.64) to sample a data point conditioned\\non the sampled zn, i.e., xn ∼ p(x | zn, B, µ, σ2).\\nThis generative process allows us to write down the probabilistic model\\n(i.e., the joint distribution of all random variables; see Section 8.4) as\\np(x, z|B, µ, σ2) = p(x|z, B, µ, σ2)p(z) , (10.67)\\nwhich immediately gives rise to the graphical model in Figure 10.14 using\\nthe results from Section 8.5.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a24ed1cb-1ae7-4c32-8c2a-cf70f590e922', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 346, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.7 Latent Variable Perspective 341\\nFigure 10.14\\nGraphical model for\\nprobabilistic PCA.\\nThe observations xn\\nexplicitly depend on\\ncorresponding\\nlatent variables\\nzn ∼ N\\n\\x00\\n0, I\\n\\x01\\n. The\\nmodel parameters\\nB, µ and the\\nlikelihood\\nparameter σ are\\nshared across the\\ndataset.\\nxn\\nB\\nzn\\nσ\\nµ\\nn = 1, . . . , N\\nRemark. Note the direction of the arrow that connects the latent variables\\nz and the observed data x: The arrow points from z to x, which means\\nthat the PPCA model assumes a lower-dimensional latent causez for high-\\ndimensional observations x. In the end, we are obviously interested in\\nfinding something out about z given some observations. To get there we\\nwill apply Bayesian inference to “invert” the arrow implicitly and go from\\nobservations to latent variables. ♢\\nExample 10.5 (Generating New Data Using Latent Variables)\\nFigure 10.15\\nGenerating new\\nMNIST digits. The\\nlatent variables z\\ncan be used to\\ngenerate new data\\n˜x = Bz. The closer\\nwe stay to the\\ntraining data, the\\nmore realistic the\\ngenerated data.\\nFigure 10.15 shows the latent coordinates of the MNIST digits “8” found\\nby PCA when using a two-dimensional principal subspace (blue dots). We\\ncan query any vector z∗ in this latent space and generate an image ˜x∗ =\\nBz ∗ that resembles the digit “8”. We show eight of such generated images\\nwith their corresponding latent space representation. Depending on where\\nwe query the latent space, the generated images look different (shape,\\nrotation, size, etc.). If we query away from the training data, we see more\\nand more artifacts, e.g., the top-left and top-right digits. Note that the\\nintrinsic dimensionality of these generated images is only two.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='61c7a9b1-e351-46fa-962c-81b8df0be932', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 347, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='342 Dimensionality Reduction with Principal Component Analysis\\n10.7.2 Likelihood and Joint DistributionThe likelihood does\\nnot depend on the\\nlatent variables z.\\nUsing the results from Chapter 6, we obtain the likelihood of this proba-\\nbilistic model by integrating out the latent variable z (see Section 8.4.3)\\nso that\\np(x | B, µ, σ2) =\\nZ\\np(x | z, B, µ, σ2)p(z)dz (10.68a)\\n=\\nZ\\nN\\n\\x00\\nx | Bz + µ, σ 2I\\n\\x01\\nN\\n\\x00\\nz | 0, I\\n\\x01\\ndz . (10.68b)\\nFrom Section 6.5, we know that the solution to this integral is a Gaussian\\ndistribution with mean\\nEx[x] = Ez[Bz + µ] + Eϵ[ϵ] = µ (10.69)\\nand with covariance matrix\\nV[x] = Vz[Bz + µ] + Vϵ[ϵ] = Vz[Bz] + σ2I (10.70a)\\n= BVz[z]B⊤ + σ2I = BB ⊤ + σ2I . (10.70b)\\nThe likelihood in (10.68b) can be used for maximum likelihood or MAP\\nestimation of the model parameters.\\nRemark. We cannot use the conditional distribution in (10.64) for maxi-\\nmum likelihood estimation as it still depends on the latent variables. The\\nlikelihood function we require for maximum likelihood (or MAP) estima-\\ntion should only be a function of the data x and the model parameters,\\nbut must not depend on the latent variables. ♢\\nFrom Section 6.5, we know that a Gaussian random variable z and\\na linear/affine transformation x = Bz of it are jointly Gaussian dis-\\ntributed. We already know the marginals p(z) = N\\n\\x00\\nz | 0, I\\n\\x01\\nand p(x) =\\nN\\n\\x00\\nx | µ, BB ⊤ + σ2I\\n\\x01\\n. The missing cross-covariance is given as\\nCov[x, z] = Covz[Bz + µ] = B Covz[z, z] = B . (10.71)\\nTherefore, the probabilistic model of PPCA, i.e., the joint distribution of\\nlatent and observed random variables is explicitly given by\\np(x, z | B, µ, σ2) = N\\n\\x12\\x14x\\nz\\n\\x15\\x0c\\x0c\\x0c\\x0c\\n\\x14µ\\n0\\n\\x15\\n,\\n\\x14BB ⊤ + σ2I B\\nB⊤ I\\n\\x15\\x13\\n, (10.72)\\nwith a mean vector of length D + M and a covariance matrix of size\\n(D + M) × (D + M).\\n10.7.3 Posterior Distribution\\nThe joint Gaussian distribution p(x, z | B, µ, σ2) in (10.72) allows us to\\ndetermine the posterior distribution p(z | x) immediately by applying the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4e391e9d-b5b0-4edf-9790-b0fb4681dfd0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 348, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.8 Further Reading 343\\nrules of Gaussian conditioning from Section 6.5.1. The posterior distribu-\\ntion of the latent variable given an observation x is then\\np(z | x) = N\\n\\x00\\nz | m, C\\n\\x01\\n, (10.73)\\nm = B⊤(BB ⊤ + σ2I)−1(x − µ) , (10.74)\\nC = I − B⊤(BB ⊤ + σ2I)−1B . (10.75)\\nNote that the posterior covariance does not depend on the observed data\\nx. For a new observation x∗ in data space, we use (10.73) to determine\\nthe posterior distribution of the corresponding latent variable z∗. The co-\\nvariance matrix C allows us to assess how confident the embedding is. A\\ncovariance matrix C with a small determinant (which measures volumes)\\ntells us that the latent embedding z∗ is fairly certain. If we obtain a pos-\\nterior distribution p(z∗ | x∗) with much variance, we may be faced with\\nan outlier. However, we can explore this posterior distribution to under-\\nstand what other data points x are plausible under this posterior. To do\\nthis, we exploit the generative process underlying PPCA, which allows us\\nto explore the posterior distribution on the latent variables by generating\\nnew data that is plausible under this posterior:\\n1. Sample a latent variable z∗ ∼ p(z | x∗) from the posterior distribution\\nover the latent variables (10.73).\\n2. Sample a reconstructed vector ˜x∗ ∼ p(x | z∗, B, µ, σ2) from (10.64).\\nIf we repeat this process many times, we can explore the posterior dis-\\ntribution (10.73) on the latent variables z∗ and its implications on the\\nobserved data. The sampling process effectively hypothesizes data, which\\nis plausible under the posterior distribution.\\n10.8 Further Reading\\nWe derived PCA from two perspectives: (a) maximizing the variance in the\\nprojected space; (b) minimizing the average reconstruction error. How-\\never, PCA can also be interpreted from different perspectives. Let us recap\\nwhat we have done: We took high-dimensional data x ∈ RD and used\\na matrix B⊤ to find a lower-dimensional representation z ∈ RM. The\\ncolumns of B are the eigenvectors of the data covariance matrixS that are\\nassociated with the largest eigenvalues. Once we have a low-dimensional\\nrepresentation z, we can get a high-dimensional version of it (in the orig-\\ninal data space) as x ≈ ˜x = Bz = BB ⊤x ∈ RD, where BB ⊤ is a\\nprojection matrix.\\nWe can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder\\nure 10.16. An auto-encoder encodes the dataxn ∈ RD to a code zn ∈ RM code\\nand decodes it to a ˜xn similar to xn. The mapping from the data to the\\ncode is called theencoder, and the mapping from the code back to the orig- encoder\\ninal data space is called thedecoder. If we consider linear mappings where decoder\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0d960353-0686-4983-86d5-5f9fdf0964d0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 349, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='344 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.16 PCA\\ncan be viewed as a\\nlinear auto-encoder.\\nIt encodes the\\nhigh-dimensional\\ndata x into a\\nlower-dimensional\\nrepresentation\\n(code) z ∈ RM and\\ndecodes z using a\\ndecoder. The\\ndecoded vector ˜x is\\nthe orthogonal\\nprojection of the\\noriginal data x onto\\nthe M-dimensional\\nprincipal subspace.\\nB⊤\\nx ˜xz B\\nEncoder Decoder\\nOriginal\\nCodeRD RD\\nRM\\nthe code is given by zn = B⊤xn ∈ RM and we are interested in minimiz-\\ning the average squared error between the data xn and its reconstruction\\n˜xn = Bz n, n = 1, . . . , N, we obtain\\n1\\nN\\nNX\\nn=1\\n∥xn − ˜xn∥2 = 1\\nN\\nNX\\nn=1\\n\\r\\r\\rxn − BB ⊤xn\\n\\r\\r\\r\\n2\\n. (10.76)\\nThis means we end up with the same objective function as in (10.29) that\\nwe discussed in Section 10.3 so that we obtain the PCA solution when we\\nminimize the squared auto-encoding loss. If we replace the linear map-\\nping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.\\nA prominent example of this is a deep auto-encoder where the linear func-\\ntions are replaced with deep neural networks. In this context, the encoder\\nis also known as a recognition network or inference network, whereas therecognition network\\ninference network decoder is also called a generator.\\ngenerator Another interpretation of PCA is related to information theory . We can\\nthink of the code as a smaller or compressed version of the original data\\npoint. When we reconstruct our original data using the code, we do not\\nget the exact data point back, but a slightly distorted or noisy version\\nof it. This means that our compression is “lossy”. Intuitively , we wantThe code is a\\ncompressed version\\nof the original data.\\nto maximize the correlation between the original data and the lower-\\ndimensional code. More formally , this is related to the mutual information.\\nWe would then get the same solution to PCA we discussed in Section 10.3\\nby maximizing the mutual information, a core concept in information the-\\nory (MacKay, 2003).\\nIn our discussion on PPCA, we assumed that the parameters of the\\nmodel, i.e., B, µ, and the likelihood parameter σ2, are known. Tipping\\nand Bishop (1999) describe how to derive maximum likelihood estimates\\nfor these parameters in the PPCA setting (note that we use a different\\nnotation in this chapter). The maximum likelihood parameters, when pro-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3c203fd-32d5-49bb-9511-b9f504b2b9ab', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 350, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.8 Further Reading 345\\njecting D-dimensional data onto an M-dimensional subspace, are\\nµML = 1\\nN\\nNX\\nn=1\\nxn , (10.77)\\nBML = T (Λ − σ2I)\\n1\\n2 R , (10.78)\\nσ2\\nML = 1\\nD − M\\nDX\\nj=M+1\\nλj , (10.79)\\nwhere T ∈ RD×M contains M eigenvectors of the data covariance matrix, The matrix Λ − σ2I\\nin (10.78) is\\nguaranteed to be\\npositive semidefinite\\nas the smallest\\neigenvalue of the\\ndata covariance\\nmatrix is bounded\\nfrom below by the\\nnoise variance σ2.\\nΛ = diag(λ1, . . . , λM) ∈ RM ×M is a diagonal matrix with the eigenvalues\\nassociated with the principal axes on its diagonal, and R ∈ RM ×M is\\nan arbitrary orthogonal matrix. The maximum likelihood solution BML is\\nunique up to an arbitrary orthogonal transformation, e.g., we can right-\\nmultiply BML with any rotation matrix R so that (10.78) essentially is a\\nsingular value decomposition (see Section 4.5). An outline of the proof is\\ngiven by Tipping and Bishop (1999).\\nThe maximum likelihood estimate for µ given in (10.77) is the sample\\nmean of the data. The maximum likelihood estimator for the observation\\nnoise variance σ2 given in (10.79) is the average variance in the orthog-\\nonal complement of the principal subspace, i.e., the average leftover vari-\\nance that we cannot capture with the first M principal components is\\ntreated as observation noise.\\nIn the noise-free limit where σ → 0, PPCA and PCA provide identical\\nsolutions: Since the data covariance matrix S is symmetric, it can be di-\\nagonalized (see Section 4.4), i.e., there exists a matrix T of eigenvectors\\nof S so that\\nS = T ΛT −1 . (10.80)\\nIn the PPCA model, the data covariance matrix is the covariance matrix of\\nthe Gaussian likelihoodp(x | B, µ, σ2), which isBB ⊤+σ2I, see (10.70b).\\nFor σ → 0, we obtain BB ⊤ so that this data covariance must equal the\\nPCA data covariance (and its factorization given in (10.80)) so that\\nCov[X ] = T ΛT −1 = BB ⊤ ⇐ ⇒ B = T Λ\\n1\\n2 R , (10.81)\\ni.e., we obtain the maximum likelihood estimate in (10.78) for σ = 0 .\\nFrom (10.78) and (10.80), it becomes clear that (P)PCA performs a de-\\ncomposition of the data covariance matrix.\\nIn a streaming setting, where data arrives sequentially , it is recom-\\nmended to use the iterative expectation maximization (EM) algorithm for\\nmaximum likelihood estimation (Roweis, 1998).\\nTo determine the dimensionality of the latent variables (the length of\\nthe code, the dimensionality of the lower-dimensional subspace onto which\\nwe project the data), Gavish and Donoho (2014) suggest the heuristic\\nthat, if we can estimate the noise variance σ2 of the data, we should\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='18c8f790-e4be-492e-b1c6-ad840d3f1536', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 351, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='346 Dimensionality Reduction with Principal Component Analysis\\ndiscard all singular values smaller than 4σ\\n√\\nD√\\n3 . Alternatively , we can use\\n(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-\\nteria (discussed in Section 8.6.2) to determine a good estimate of the\\nintrinsic dimensionality of the data (Minka, 2001b).\\nSimilar to our discussion on linear regression in Chapter 9, we can place\\na prior distribution on the parameters of the model and integrate them\\nout. By doing so, we (a) avoid point estimates of the parameters and the\\nissues that come with these point estimates (see Section 8.6) and (b) al-\\nlow for an automatic selection of the appropriate dimensionalityM of the\\nlatent space. In this Bayesian PCA, which was proposed by Bishop (1999),Bayesian PCA\\na prior p(µ, B, σ2) is placed on the model parameters. The generative\\nprocess allows us to integrate the model parameters out instead of condi-\\ntioning on them, which addresses overfitting issues. Since this integration\\nis analytically intractable, Bishop (1999) proposes to use approximate in-\\nference methods, such as MCMC or variational inference. We refer to the\\nwork by Gilks et al. (1996) and Blei et al. (2017) for more details on these\\napproximate inference techniques.\\nIn PPCA, we considered the linear model p(xn | zn) = N\\n\\x00\\nxn | Bz n +\\nµ, σ 2I\\n\\x01\\nwith prior p(zn) = N\\n\\x00\\n0, I\\n\\x01\\n, where all observation dimensions\\nare affected by the same amount of noise. If we allow each observation\\ndimension d to have a different variance σ2\\nd, we obtain factor analysisfactor analysis\\n(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA\\ngives the likelihood some more flexibility than PPCA, but still forces the\\ndata to be explained by the model parameters B, µ.However, FA noAn overly flexible\\nlikelihood would be\\nable to explain more\\nthan just the noise.\\nlonger allows for a closed-form maximum likelihood solution so that we\\nneed to use an iterative scheme, such as the expectation maximization\\nalgorithm, to estimate the model parameters. While in PPCA all station-\\nary points are global optima, this no longer holds for FA. Compared to\\nPPCA, FA does not change if we scale the data, but it does return different\\nsolutions if we rotate the data.\\nAn algorithm that is also closely related to PCA is independent com-independent\\ncomponent analysis ponent analysis (ICA (Hyvarinen et al., 2001)). Starting again with the\\nICA latent-variable perspective p(xn | zn) = N\\n\\x00\\nxn | Bz n + µ, σ 2I\\n\\x01\\nwe now\\nchange the prior on zn to non-Gaussian distributions. ICA can be used\\nfor blind-source separation. Imagine you are in a busy train station withblind-source\\nseparation many people talking. Your ears play the role of microphones, and they\\nlinearly mix different speech signals in the train station. The goal of blind-\\nsource separation is to identify the constituent parts of the mixed signals.\\nAs discussed previously in the context of maximum likelihood estimation\\nfor PPCA, the original PCA solution is invariant to any rotation. Therefore,\\nPCA can identify the best lower-dimensional subspace in which the sig-\\nnals live, but not the signals themselves (Murphy, 2012). ICA addresses\\nthis issue by modifying the prior distribution p(z) on the latent sources\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cee70139-6f3b-47e1-bd6a-cb92638d9dc4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 352, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.8 Further Reading 347\\nto require non-Gaussian priors p(z). We refer to the books by Hyvarinen\\net al. (2001) and Murphy (2012) for more details on ICA.\\nPCA, factor analysis, and ICA are three examples for dimensionality re-\\nduction with linear models. Cunningham and Ghahramani (2015) provide\\na broader survey of linear dimensionality reduction.\\nThe (P)PCA model we discussed here allows for several important ex-\\ntensions. In Section 10.5, we explained how to do PCA when the in-\\nput dimensionality D is significantly greater than the number N of data\\npoints. By exploiting the insight that PCA can be performed by computing\\n(many) inner products, this idea can be pushed to the extreme by consid-\\nering infinite-dimensional features. The kernel trick is the basis of kernel kernel trick\\nkernel PCAPCA and allows us to implicitly compute inner products between infinite-\\ndimensional features (Sch¨olkopf et al., 1998; Sch¨olkopf and Smola, 2002).\\nThere are nonlinear dimensionality reduction techniques that are de-\\nrived from PCA (Burges (2010) provides a good overview). The auto-\\nencoder perspective of PCA that we discussed previously in this section\\ncan be used to render PCA as a special case of a deep auto-encoder. In the deep auto-encoder\\ndeep auto-encoder, both the encoder and the decoder are represented by\\nmultilayer feedforward neural networks, which themselves are nonlinear\\nmappings. If we set the activation functions in these neural networks to be\\nthe identity , the model becomes equivalent to PCA. A different approach to\\nnonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process\\nlatent-variable\\nmodel\\nmodel (GP-LVM) proposed by Lawrence (2005). The GP-LVM starts off with\\nGP-LVMthe latent-variable perspective that we used to derive PPCA and replaces\\nthe linear relationship between the latent variablesz and the observations\\nx with a Gaussian process (GP). Instead of estimating the parameters of\\nthe mapping (as we do in PPCA), the GP-LVM marginalizes out the model\\nparameters and makes point estimates of the latent variables z. Similar\\nto Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM\\n(2010) maintains a distribution on the latent variablesz and uses approx-\\nimate inference to integrate them out as well.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='68426c39-adcb-47f0-aaf4-a8e55a4cb460', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 353, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11\\nDensity Estimation with Gaussian Mixture\\nModels\\nIn earlier chapters, we covered already two fundamental problems in\\nmachine learning: regression (Chapter 9) and dimensionality reduction\\n(Chapter 10). In this chapter, we will have a look at a third pillar of ma-\\nchine learning: density estimation. On our journey , we introduce impor-\\ntant concepts, such as the expectation maximization (EM) algorithm and\\na latent variable perspective of density estimation with mixture models.\\nWhen we apply machine learning to data we often aim to represent\\ndata in some way . A straightforward way is to take the data points them-\\nselves as the representation of the data; see Figure 11.1 for an example.\\nHowever, this approach may be unhelpful if the dataset is huge or if we\\nare interested in representing characteristics of the data. In density esti-\\nmation, we represent the data compactly using a density from a paramet-\\nric family , e.g., a Gaussian or Beta distribution. For example, we may be\\nlooking for the mean and variance of a dataset in order to represent the\\ndata compactly using a Gaussian distribution. The mean and variance can\\nbe found using tools we discussed in Section 8.3: maximum likelihood or\\nmaximum a posteriori estimation. We can then use the mean and variance\\nof this Gaussian to represent the distribution underlying the data, i.e., we\\nthink of the dataset to be a typical realization from this distribution if we\\nwere to sample from it.\\nFigure 11.1\\nTwo-dimensional\\ndataset that cannot\\nbe meaningfully\\nrepresented by a\\nGaussian.\\n−5 0 5\\nx1\\n−4\\n−2\\n0\\n2\\n4\\nx2\\n348\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='05188d6c-b158-421f-a230-affdf2f852fc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 354, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.1 Gaussian Mixture Model 349\\nIn practice, the Gaussian (or similarly all other distributions we encoun-\\ntered so far) have limited modeling capabilities. For example, a Gaussian\\napproximation of the density that generated the data in Figure 11.1 would\\nbe a poor approximation. In the following, we will look at a more ex-\\npressive family of distributions, which we can use for density estimation:\\nmixture models. mixture model\\nMixture models can be used to describe a distribution p(x) by a convex\\ncombination of K simple (base) distributions\\np(x) =\\nKX\\nk=1\\nπkpk(x) (11.1)\\n0 ⩽ πk ⩽ 1 ,\\nKX\\nk=1\\nπk = 1 , (11.2)\\nwhere the components pk are members of a family of basic distributions,\\ne.g., Gaussians, Bernoullis, or Gammas, and the πk are mixture weights. mixture weight\\nMixture models are more expressive than the corresponding base distri-\\nbutions because they allow for multimodal data representations, i.e., they\\ncan describe datasets with multiple “clusters”, such as the example in Fig-\\nure 11.1.\\nWe will focus on Gaussian mixture models (GMMs), where the basic\\ndistributions are Gaussians. For a given dataset, we aim to maximize the\\nlikelihood of the model parameters to train the GMM. For this purpose,\\nwe will use results from Chapter 5, Chapter 6, and Section 7.2. However,\\nunlike other applications we discussed earlier (linear regression or PCA),\\nwe will not find a closed-form maximum likelihood solution. Instead, we\\nwill arrive at a set of dependent simultaneous equations, which we can\\nonly solve iteratively .\\n11.1 Gaussian Mixture Model\\nA Gaussian mixture model is a density model where we combine a finite Gaussian mixture\\nmodelnumber of K Gaussian distributions N\\n\\x00\\nx | µk, Σk\\n\\x01\\nso that\\np(x | θ) =\\nKX\\nk=1\\nπkN\\n\\x00\\nx | µk, Σk\\n\\x01\\n(11.3)\\n0 ⩽ πk ⩽ 1 ,\\nKX\\nk=1\\nπk = 1 , (11.4)\\nwhere we defined θ := {µk, Σk, πk : k = 1, . . . , K} as the collection of\\nall parameters of the model. This convex combination of Gaussian distri-\\nbution gives us significantly more flexibility for modeling complex densi-\\nties than a simple Gaussian distribution (which we recover from (11.3) for\\nK = 1). An illustration is given in Figure 11.2, displaying the weighted\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2af697d6-1593-4f0d-b61d-3e02c7997ef0', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 355, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='350 Density Estimation with Gaussian Mixture Models\\nFigure 11.2\\nGaussian mixture\\nmodel. The\\nGaussian mixture\\ndistribution (black)\\nis composed of a\\nconvex combination\\nof Gaussian\\ndistributions and is\\nmore expressive\\nthan any individual\\ncomponent. Dashed\\nlines represent the\\nweighted Gaussian\\ncomponents.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nComponent 1\\nComponent 2\\nComponent 3\\nGMM density\\ncomponents and the mixture density , which is given as\\np(x | θ) = 0.5N\\n\\x00\\nx | − 2, 1\\n2\\n\\x01\\n+ 0.2N\\n\\x00\\nx | 1, 2\\n\\x01\\n+ 0.3N\\n\\x00\\nx | 4, 1\\n\\x01\\n. (11.5)\\n11.2 Parameter Learning via Maximum Likelihood\\nAssume we are given a dataset X = {x1, . . . ,xN }, where xn, n =\\n1, . . . , N, are drawn i.i.d. from an unknown distribution p(x). Our ob-\\njective is to find a good approximation/representation of this unknown\\ndistribution p(x) by means of a GMM with K mixture components. The\\nparameters of the GMM are the K means µk, the covariances Σk, and\\nmixture weights πk. We summarize all these free parameters in θ :=\\n{πk, µk, Σk : k = 1, . . . , K}.\\nExample 11.1 (Initial Setting)\\nFigure 11.3 Initial\\nsetting: GMM\\n(black) with\\nmixture three\\nmixture components\\n(dashed) and seven\\ndata points (discs).\\n−5 0 5 10 15\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\nThroughout this chapter, we will have a simple running example that\\nhelps us illustrate and visualize important concepts.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1baaa23e-90f4-401e-ac69-86ac6a21b8df', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 356, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2 Parameter Learning via Maximum Likelihood 351\\nWe consider a one-dimensional dataset X = {−3, −2.5, −1, 0, 2, 4, 5}\\nconsisting of seven data points and wish to find a GMM with K = 3\\ncomponents that models the density of the data. We initialize the mixture\\ncomponents as\\np1(x) = N\\n\\x00\\nx | − 4, 1\\n\\x01\\n(11.6)\\np2(x) = N\\n\\x00\\nx | 0, 0.2\\n\\x01\\n(11.7)\\np3(x) = N\\n\\x00\\nx | 8, 3\\n\\x01\\n(11.8)\\nand assign them equal weights π1 = π2 = π3 = 1\\n3. The corresponding\\nmodel (and the data points) are shown in Figure 11.3.\\nIn the following, we detail how to obtain a maximum likelihood esti-\\nmate θML of the model parameters θ. We start by writing down the like-\\nlihood, i.e., the predictive distribution of the training data given the pa-\\nrameters. We exploit our i.i.d. assumption, which leads to the factorized\\nlikelihood\\np(X | θ) =\\nNY\\nn=1\\np(xn | θ) , p (xn | θ) =\\nKX\\nk=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\n, (11.9)\\nwhere every individual likelihood term p(xn | θ) is a Gaussian mixture\\ndensity . Then we obtain the log-likelihood as\\nlog p(X | θ) =\\nNX\\nn=1\\nlog p(xn | θ) =\\nNX\\nn=1\\nlog\\nKX\\nk=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\n| {z }\\n=:L\\n. (11.10)\\nWe aim to find parametersθ∗\\nML that maximize the log-likelihood L defined\\nin (11.10). Our “normal” procedure would be to compute the gradient\\ndL/dθ of the log-likelihood with respect to the model parameters θ, set\\nit to 0, and solve for θ. However, unlike our previous examples for max-\\nimum likelihood estimation (e.g., when we discussed linear regression in\\nSection 9.2), we cannot obtain a closed-form solution. However, we can\\nexploit an iterative scheme to find good model parametersθML, which will\\nturn out to be the EM algorithm for GMMs. The key idea is to update one\\nmodel parameter at a time while keeping the others fixed.\\nRemark. If we were to consider a single Gaussian as the desired density ,\\nthe sum over k in (11.10) vanishes, and the log can be applied directly to\\nthe Gaussian component, such that we get\\nlog N\\n\\x00\\nx | µ, Σ\\n\\x01\\n= − D\\n2 log(2π) − 1\\n2 log det(Σ) − 1\\n2(x − µ)⊤Σ−1(x − µ).\\n(11.11)\\nThis simple form allows us to find closed-form maximum likelihood esti-\\nmates of µ and Σ, as discussed in Chapter 8. In (11.10), we cannot move\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='57c56244-9c68-419c-a791-29359b546650', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 357, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='352 Density Estimation with Gaussian Mixture Models\\nthe log into the sum over k so that we cannot obtain a simple closed-form\\nmaximum likelihood solution. ♢\\nAny local optimum of a function exhibits the property that its gradi-\\nent with respect to the parameters must vanish (necessary condition); see\\nChapter 7. In our case, we obtain the following necessary conditions when\\nwe optimize the log-likelihood in (11.10) with respect to the GMM param-\\neters µk, Σk, πk:\\n∂L\\n∂µk\\n= 0⊤ ⇐ ⇒\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂µk\\n= 0⊤ , (11.12)\\n∂L\\n∂Σk\\n= 0 ⇐ ⇒\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂Σk\\n= 0 , (11.13)\\n∂L\\n∂πk\\n= 0 ⇐ ⇒\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂πk\\n= 0 . (11.14)\\nFor all three necessary conditions, by applying the chain rule (see Sec-\\ntion 5.2.2), we require partial derivatives of the form\\n∂ log p(xn | θ)\\n∂θ = 1\\np(xn | θ)\\n∂p(xn | θ)\\n∂θ , (11.15)\\nwhere θ = {µk, Σk, πk, k = 1, . . . , K} are the model parameters and\\n1\\np(xn | θ) = 1\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01 . (11.16)\\nIn the following, we will compute the partial derivatives (11.12) through\\n(11.14). But before we do this, we introduce a quantity that will play a\\ncentral role in the remainder of this chapter: responsibilities.\\n11.2.1 Responsibilities\\nWe define the quantity\\nrnk := πkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01 (11.17)\\nas the responsibility of the kth mixture component for the nth data point.responsibility\\nThe responsibility rnk of the kth mixture component for data point xn is\\nproportional to the likelihood\\np(xn | πk, µk, Σk) = πkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\n(11.18)\\nof the mixture component given the data point. Therefore, mixture com-rn follows a\\nBoltzmann/Gibbs\\ndistribution.\\nponents have a high responsibility for a data point when the data point\\ncould be a plausible sample from that mixture component. Note that\\nrn := [ rn1, . . . , rnK]⊤ ∈ RK is a (normalized) probability vector, i.e.,\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='697cedb6-9af3-443b-8029-eccde6d647b3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 358, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2 Parameter Learning via Maximum Likelihood 353\\nP\\nk rnk = 1 with rnk ⩾ 0. This probability vector distributes probabil-\\nity mass among the K mixture components, and we can think of rn as a\\n“soft assignment” ofxn to the K mixture components. Therefore, the re- The responsibility\\nrnk is the\\nprobability that the\\nkth mixture\\ncomponent\\ngenerated the nth\\ndata point.\\nsponsibility rnk from (11.17) represents the probability that xn has been\\ngenerated by the kth mixture component.\\nExample 11.2 (Responsibilities)\\nFor our example from Figure 11.3, we compute the responsibilities rnk\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1.0 0 .0 0 .0\\n1.0 0 .0 0 .0\\n0.057 0 .943 0 .0\\n0.001 0 .999 0 .0\\n0.0 0 .066 0 .934\\n0.0 0 .0 1 .0\\n0.0 0 .0 1 .0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ RN ×K . (11.19)\\nHere the nth row tells us the responsibilities of all mixture components\\nfor xn. The sum of all K responsibilities for a data point (sum of every\\nrow) is 1. The kth column gives us an overview of the responsibility of\\nthe kth mixture component. We can see that the third mixture component\\n(third column) is not responsible for any of the first four data points, but\\ntakes much responsibility of the remaining data points. The sum of all\\nentries of a column gives us the values Nk, i.e., the total responsibility of\\nthe kth mixture component. In our example, we get N1 = 2 .058, N 2 =\\n2.008, N 3 = 2.934.\\nIn the following, we determine the updates of the model parameters\\nµk, Σk, πk for given responsibilities. We will see that the update equa-\\ntions all depend on the responsibilities, which makes a closed-form solu-\\ntion to the maximum likelihood estimation problem impossible. However,\\nfor given responsibilities we will be updating one model parameter at a\\ntime, while keeping the others fixed. After this, we will recompute the\\nresponsibilities. Iterating these two steps will eventually converge to a lo-\\ncal optimum and is a specific instantiation of the EM algorithm. We will\\ndiscuss this in some more detail in Section 11.3.\\n11.2.2 Updating the Means\\nTheorem 11.1 (Update of the GMM Means). The update of the mean pa-\\nrameters µk, k = 1, . . . , K, of the GMM is given by\\nµnew\\nk =\\nPN\\nn=1 rnkxn\\nPN\\nn=1 rnk\\n, (11.20)\\nwhere the responsibilities rnk are defined in (11.17).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ea1ff87-5b3a-406f-98aa-c618eac522db', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 359, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='354 Density Estimation with Gaussian Mixture Models\\nRemark. The update of the means µk of the individual mixture compo-\\nnents in (11.20) depends on all means, covariance matrices Σk, and mix-\\nture weights πk via rnk given in (11.17). Therefore, we cannot obtain a\\nclosed-form solution for all µk at once. ♢\\nProof From (11.15), we see that the gradient of the log-likelihood with\\nrespect to the mean parameters µk, k = 1, . . . , K, requires us to compute\\nthe partial derivative\\n∂p(xn | θ)\\n∂µk\\n=\\nKX\\nj=1\\nπj\\n∂N\\n\\x00\\nxn | µj, Σj\\n\\x01\\n∂µk\\n= πk\\n∂N\\n\\x00\\nxn | µk, Σk\\n\\x01\\n∂µk\\n(11.21a)\\n= πk(xn − µk)⊤Σ−1\\nk N\\n\\x00\\nxn | µk, Σk\\n\\x01\\n, (11.21b)\\nwhere we exploited that only the kth mixture component depends on µk.\\nWe use our result from (11.21b) in (11.15) and put everything together\\nso that the desired partial derivative of L with respect to µk is given as\\n∂L\\n∂µk\\n=\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂µk\\n=\\nNX\\nn=1\\n1\\np(xn | θ)\\n∂p(xn | θ)\\n∂µk\\n(11.22a)\\n=\\nNX\\nn=1\\n(xn − µk)⊤Σ−1\\nk\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01\\n| {z }\\n=rnk\\n(11.22b)\\n=\\nNX\\nn=1\\nrnk(xn − µk)⊤Σ−1\\nk . (11.22c)\\nHere we used the identity from (11.16) and the result of the partial deriva-\\ntive in (11.21b) to get to (11.22b). The values rnk are the responsibilities\\nwe defined in (11.17).\\nWe now solve (11.22c) for µnew\\nk so that ∂L(µnew\\nk )\\n∂µk\\n= 0⊤ and obtain\\nNX\\nn=1\\nrnkxn =\\nNX\\nn=1\\nrnkµnew\\nk ⇐ ⇒ µnew\\nk =\\nPN\\nn=1 rnkxn\\nPN\\nn=1 rnk\\n= 1\\nNk\\nNX\\nn=1\\nrnkxn ,\\n(11.23)\\nwhere we defined\\nNk :=\\nNX\\nn=1\\nrnk (11.24)\\nas the total responsibility of the kth mixture component for the entire\\ndataset. This concludes the proof of Theorem 11.1.\\nIntuitively , (11.20) can be interpreted as an importance-weighted Monte\\nCarlo estimate of the mean, where the importance weights of data point\\nxn are the responsibilities rnk of the kth cluster for xn, k = 1 , . . . , K.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d479c36-7b62-4a66-a7da-5b6b8bcd8139', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 360, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2 Parameter Learning via Maximum Likelihood 355\\nTherefore, the mean µk is pulled toward a data point xn with strength Figure 11.4 Update\\nof the mean\\nparameter of\\nmixture component\\nin a GMM. The\\nmean µ is being\\npulled toward\\nindividual data\\npoints with the\\nweights given by the\\ncorresponding\\nresponsibilities.\\nr1\\nr2\\nr3x1\\nx2 x3\\nµ\\ngiven by rnk. The means are pulled stronger toward data points for which\\nthe corresponding mixture component has a high responsibility , i.e., a high\\nlikelihood. Figure 11.4 illustrates this. We can also interpret the mean up-\\ndate in (11.20) as the expected value of all data points under the distri-\\nbution given by\\nrk := [r1k, . . . , rN k]⊤/Nk , (11.25)\\nwhich is a normalized probability vector, i.e.,\\nµk ← Erk[X ] . (11.26)\\nExample 11.3 (Mean Updates)\\nFigure 11.5 Effect\\nof updating the\\nmean values in a\\nGMM. (a) GMM\\nbefore updating the\\nmean values;\\n(b) GMM after\\nupdating the mean\\nvalues µk while\\nretaining the\\nvariances and\\nmixture weights.\\n−5 0 5 10 15\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density(a) GMM density and individual components\\nprior to updating the mean values.\\n−5 0 5 10 15\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(b) GMM density and individual components\\nafter updating the mean values.\\nIn our example from Figure 11.3, the mean values are updated as fol-\\nlows:\\nµ1 : −4 → −2.7 (11.27)\\nµ2 : 0 → −0.4 (11.28)\\nµ3 : 8 → 3.7 (11.29)\\nHere we see that the means of the first and third mixture component\\nmove toward the regime of the data, whereas the mean of the second\\ncomponent does not change so dramatically . Figure 11.5 illustrates this\\nchange, where Figure 11.5(a) shows the GMM density prior to updating\\nthe means and Figure 11.5(b) shows the GMM density after updating the\\nmean values µk.\\nThe update of the mean parameters in (11.20) look fairly straight-\\nforward. However, note that the responsibilities rnk are a function of\\nπj, µj, Σj for all j = 1, . . . , K, such that the updates in (11.20) depend\\non all parameters of the GMM, and a closed-form solution, which we ob-\\ntained for linear regression in Section 9.2 or PCA in Chapter 10, cannot\\nbe obtained.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a3b27ef-5092-4d29-9a8d-6ca29eea3cdf', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 361, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='356 Density Estimation with Gaussian Mixture Models\\n11.2.3 Updating the Covariances\\nTheorem 11.2 (Updates of the GMM Covariances). The update of the co-\\nvariance parameters Σk, k = 1, . . . , K of the GMM is given by\\nΣnew\\nk = 1\\nNk\\nNX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤ , (11.30)\\nwhere rnk and Nk are defined in (11.17) and (11.24), respectively.\\nProof To prove Theorem 11.2, our approach is to compute the partial\\nderivatives of the log-likelihood L with respect to the covariances Σk, set\\nthem to 0, and solve for Σk. We start with our general approach\\n∂L\\n∂Σk\\n=\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂Σk\\n=\\nNX\\nn=1\\n1\\np(xn | θ)\\n∂p(xn | θ)\\n∂Σk\\n. (11.31)\\nWe already know 1/p(xn | θ) from (11.16). To obtain the remaining par-\\ntial derivative ∂p(xn | θ)/∂Σk, we write down the definition of the Gaus-\\nsian distribution p(xn | θ) (see (11.9)) and drop all terms but the kth. We\\nthen obtain\\n∂p(xn | θ)\\n∂Σk\\n(11.32a)\\n= ∂\\n∂Σk\\n\\x12\\nπk(2π)− D\\n2 det(Σk)− 1\\n2 exp\\n\\x00\\n− 1\\n2(xn − µk)⊤Σ−1\\nk (xn − µk)\\n\\x01\\x13\\n(11.32b)\\n= πk(2π)− D\\n2\\n\\x14 ∂\\n∂Σk\\ndet(Σk)− 1\\n2 exp\\n\\x00\\n− 1\\n2(xn − µk)⊤Σ−1\\nk (xn − µk)\\n\\x01\\n+ det(Σk)− 1\\n2\\n∂\\n∂Σk\\nexp\\n\\x00\\n− 1\\n2(xn − µk)⊤Σ−1\\nk (xn − µk)\\n\\x01\\x15\\n. (11.32c)\\nWe now use the identities\\n∂\\n∂Σk\\ndet(Σk)− 1\\n2 (5.101)\\n= −1\\n2 det(Σk)− 1\\n2Σ−1\\nk , (11.33)\\n∂\\n∂Σk\\n(xn − µk)⊤Σ−1\\nk (xn − µk)\\n(5.103)\\n= −Σ−1\\nk (xn − µk)(xn − µk)⊤Σ−1\\nk\\n(11.34)\\nand obtain (after some rearranging) the desired partial derivative required\\nin (11.31) as\\n∂p(xn | θ)\\n∂Σk\\n= πk N\\n\\x00\\nxn | µk, Σk\\n\\x01\\n·\\n\\x02\\n− 1\\n2(Σ−1\\nk − Σ−1\\nk (xn − µk)(xn − µk)⊤Σ−1\\nk )\\n\\x03\\n. (11.35)\\nPutting everything together, the partial derivative of the log-likelihood\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c62aa67-394d-43dc-a30c-c10b50d519cc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 362, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2 Parameter Learning via Maximum Likelihood 357\\nwith respect to Σk is given by\\n∂L\\n∂Σk\\n=\\nNX\\nn=1\\n∂ log p(xn | θ)\\n∂Σk\\n=\\nNX\\nn=1\\n1\\np(xn | θ)\\n∂p(xn | θ)\\n∂Σk\\n(11.36a)\\n=\\nNX\\nn=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01\\n| {z }\\n=rnk\\n·\\n\\x02\\n− 1\\n2(Σ−1\\nk − Σ−1\\nk (xn − µk)(xn − µk)⊤Σ−1\\nk )\\n\\x03\\n(11.36b)\\n= −1\\n2\\nNX\\nn=1\\nrnk(Σ−1\\nk − Σ−1\\nk (xn − µk)(xn − µk)⊤Σ−1\\nk ) (11.36c)\\n= −1\\n2Σ−1\\nk\\nNX\\nn=1\\nrnk\\n| {z }\\n=Nk\\n+ 1\\n2Σ−1\\nk\\n NX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤\\n!\\nΣ−1\\nk .\\n(11.36d)\\nWe see that the responsibilities rnk also appear in this partial derivative.\\nSetting this partial derivative to 0, we obtain the necessary optimality\\ncondition\\nNkΣ−1\\nk = Σ−1\\nk\\n NX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤\\n!\\nΣ−1\\nk (11.37a)\\n⇐ ⇒NkI =\\n NX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤\\n!\\nΣ−1\\nk . (11.37b)\\nBy solving for Σk, we obtain\\nΣnew\\nk = 1\\nNk\\nNX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤ , (11.38)\\nwhere rk is the probability vector defined in (11.25). This gives us a sim-\\nple update rule for Σk for k = 1, . . . , K and proves Theorem 11.2.\\nSimilar to the update of µk in (11.20), we can interpret the update of\\nthe covariance in (11.30) as an importance-weighted expected value of\\nthe square of the centered data ˜Xk := {x1 − µk, . . . ,xN − µk}.\\nExample 11.4 (Variance Updates)\\nIn our example from Figure 11.3, the variances are updated as follows:\\nσ2\\n1 : 1 → 0.14 (11.39)\\nσ2\\n2 : 0.2 → 0.44 (11.40)\\nσ2\\n3 : 3 → 1.53 (11.41)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6fc87f89-1c06-47e5-92b2-9c85ae442a27', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 363, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='358 Density Estimation with Gaussian Mixture Models\\nHere we see that the variances of the first and third component shrink\\nsignificantly , whereas the variance of the second component increases\\nslightly .\\nFigure 11.6 illustrates this setting. Figure 11.6(a) is identical (but\\nzoomed in) to Figure 11.5(b) and shows the GMM density and its indi-\\nvidual components prior to updating the variances. Figure 11.6(b) shows\\nthe GMM density after updating the variances.\\nFigure 11.6 Effect\\nof updating the\\nvariances in a GMM.\\n(a) GMM before\\nupdating the\\nvariances; (b) GMM\\nafter updating the\\nvariances while\\nretaining the means\\nand mixture\\nweights.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(a) GMM density and individual components\\nprior to updating the variances.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(b) GMM density and individual components\\nafter updating the variances.\\nSimilar to the update of the mean parameters, we can interpret (11.30)\\nas a Monte Carlo estimate of the weighted covariance of data points xn\\nassociated with the kth mixture component, where the weights are the\\nresponsibilities rnk. As with the updates of the mean parameters, this up-\\ndate depends on all πj, µj, Σj, j = 1, . . . , K, through the responsibilities\\nrnk, which prohibits a closed-form solution.\\n11.2.4 Updating the Mixture Weights\\nTheorem 11.3 (Update of the GMM Mixture Weights). The mixture weights\\nof the GMM are updated as\\nπnew\\nk = Nk\\nN , k = 1, . . . , K , (11.42)\\nwhere N is the number of data points and Nk is defined in (11.24).\\nProof To find the partial derivative of the log-likelihood with respect\\nto the weight parameters πk, k = 1 , . . . , K, we account for the con-\\nstraintP\\nk πk = 1 by using Lagrange multipliers (see Section 7.2). The\\nLagrangian is\\nL = L + λ\\n KX\\nk=1\\nπk − 1\\n!\\n(11.43a)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bd8844ca-ae1a-4626-963f-40e39a530bc1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 364, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2 Parameter Learning via Maximum Likelihood 359\\n=\\nNX\\nn=1\\nlog\\nKX\\nk=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\n+ λ\\n KX\\nk=1\\nπk − 1\\n!\\n, (11.43b)\\nwhere L is the log-likelihood from (11.10) and the second term encodes\\nfor the equality constraint that all the mixture weights need to sum up to\\n1. We obtain the partial derivative with respect to πk as\\n∂L\\n∂πk\\n=\\nNX\\nn=1\\nN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01 + λ (11.44a)\\n= 1\\nπk\\nNX\\nn=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01\\n| {z }\\n=Nk\\n+λ = Nk\\nπk\\n+ λ , (11.44b)\\nand the partial derivative with respect to the Lagrange multiplier λ as\\n∂L\\n∂λ =\\nKX\\nk=1\\nπk − 1 . (11.45)\\nSetting both partial derivatives to 0 (necessary condition for optimum)\\nyields the system of equations\\nπk = − Nk\\nλ , (11.46)\\n1 =\\nKX\\nk=1\\nπk . (11.47)\\nUsing (11.46) in (11.47) and solving for πk, we obtain\\nKX\\nk=1\\nπk = 1 ⇐ ⇒ −\\nKX\\nk=1\\nNk\\nλ = 1 ⇐ ⇒ −N\\nλ = 1 ⇐ ⇒ λ = −N .\\n(11.48)\\nThis allows us to substitute −N for λ in (11.46) to obtain\\nπnew\\nk = Nk\\nN , (11.49)\\nwhich gives us the update for the weight parameters πk and proves Theo-\\nrem 11.3.\\nWe can identify the mixture weight in (11.42) as the ratio of the to-\\ntal responsibility of the kth cluster and the number of data points. Since\\nN = P\\nk Nk, the number of data points can also be interpreted as the\\ntotal responsibility of all mixture components together, such thatπk is the\\nrelative importance of the kth mixture component for the dataset.\\nRemark. Since Nk =PN\\ni=1 rnk, the update equation (11.42) for the mix-\\nture weights πk also depends on all πj, µj, Σj, j = 1 , . . . , K via the re-\\nsponsibilities rnk. ♢\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6048091c-4e5c-4a9d-8ea7-58e6077aa466', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 365, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='360 Density Estimation with Gaussian Mixture Models\\nExample 11.5 (Weight Parameter Updates)\\nFigure 11.7 Effect\\nof updating the\\nmixture weights in a\\nGMM. (a) GMM\\nbefore updating the\\nmixture weights;\\n(b) GMM after\\nupdating the\\nmixture weights\\nwhile retaining the\\nmeans and\\nvariances. Note the\\ndifferent scales of\\nthe vertical axes.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(a) GMM density and individual components\\nprior to updating the mixture weights.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(b) GMM density and individual components\\nafter updating the mixture weights.\\nIn our running example from Figure 11.3, the mixture weights are up-\\ndated as follows:\\nπ1 : 1\\n3 → 0.29 (11.50)\\nπ2 : 1\\n3 → 0.29 (11.51)\\nπ3 : 1\\n3 → 0.42 (11.52)\\nHere we see that the third component gets more weight/importance,\\nwhile the other components become slightly less important. Figure 11.7\\nillustrates the effect of updating the mixture weights. Figure 11.7(a) is\\nidentical to Figure 11.6(b) and shows the GMM density and its individual\\ncomponents prior to updating the mixture weights. Figure 11.7(b) shows\\nthe GMM density after updating the mixture weights.\\nOverall, having updated the means, the variances, and the weights\\nonce, we obtain the GMM shown in Figure 11.7(b). Compared with the\\ninitialization shown in Figure 11.3, we can see that the parameter updates\\ncaused the GMM density to shift some of its mass toward the data points.\\nAfter updating the means, variances, and weights once, the GMM fit\\nin Figure 11.7(b) is already remarkably better than its initialization from\\nFigure 11.3. This is also evidenced by the log-likelihood values, which\\nincreased from −28.3 (initialization) to −14.4 after a full update cycle.\\n11.3 EM Algorithm\\nUnfortunately , the updates in (11.20), (11.30), and (11.42) do not consti-\\ntute a closed-form solution for the updates of the parameters µk, Σk, πk\\nof the mixture model because the responsibilities rnk depend on those pa-\\nrameters in a complex way . However, the results suggest a simpleiterative\\nscheme for finding a solution to the parameters estimation problem via\\nmaximum likelihood. The expectation maximization algorithm ( EM algo-EM algorithm\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8988d6ea-7572-4e5c-bd0d-02cfa8df5a1e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 366, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.3 EM Algorithm 361\\nrithm) was proposed by Dempster et al. (1977) and is a general iterative\\nscheme for learning parameters (maximum likelihood or MAP) in mixture\\nmodels and, more generally , latent-variable models.\\nIn our example of the Gaussian mixture model, we choose initial values\\nfor µk, Σk, πk and alternate until convergence between\\nE-step: Evaluate the responsibilities rnk (posterior probability of data\\npoint n belonging to mixture component k).\\nM-step: Use the updated responsibilities to reestimate the parameters\\nµk, Σk, πk.\\nEvery step in the EM algorithm increases the log-likelihood function (Neal\\nand Hinton, 1999). For convergence, we can check the log-likelihood or\\nthe parameters directly . A concrete instantiation of the EM algorithm for\\nestimating the parameters of a GMM is as follows:\\n1. Initialize µk, Σk, πk.\\n2. E-step: Evaluate responsibilities rnk for every data point xn using cur-\\nrent parameters πk, µk, Σk:\\nrnk = πkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nP\\nj πjN\\n\\x00\\nxn | µj, Σj\\n\\x01 . (11.53)\\n3. M-step: Reestimate parameters πk, µk, Σk using the current responsi-\\nbilities rnk (from E-step): Having updated the\\nmeans µk\\nin (11.54), they are\\nsubsequently used\\nin (11.55) to update\\nthe corresponding\\ncovariances.\\nµk = 1\\nNk\\nNX\\nn=1\\nrnkxn , (11.54)\\nΣk = 1\\nNk\\nNX\\nn=1\\nrnk(xn − µk)(xn − µk)⊤ , (11.55)\\nπk = Nk\\nN . (11.56)\\nExample 11.6 (GMM Fit)\\nFigure 11.8 EM\\nalgorithm applied to\\nthe GMM from\\nFigure 11.2. (a)\\nFinal GMM fit;\\n(b) negative\\nlog-likelihood as a\\nfunction of the EM\\niteration.\\n−5 0 5 10 15\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nπ1N (x|µ1, σ2\\n1)\\nπ2N (x|µ2, σ2\\n2)\\nπ3N (x|µ3, σ2\\n3)\\nGMM density\\n(a) Final GMM fit. After five iterations, the EM\\nalgorithm converges and returns this GMM.\\n0 1 2 3 4 5\\nIteration\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n28Negative log-likelihood\\n(b) Negative log-likelihood as a function of the\\nEM iterations.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7002e414-ff55-40d4-8f24-6e597756ee1d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 367, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='362 Density Estimation with Gaussian Mixture Models\\nFigure 11.9\\nIllustration of the\\nEM algorithm for\\nfitting a Gaussian\\nmixture model with\\nthree components to\\na two-dimensional\\ndataset. (a) Dataset;\\n(b) negative\\nlog-likelihood\\n(lower is better) as\\na function of the EM\\niterations. The red\\ndots indicate the\\niterations for which\\nthe mixture\\ncomponents of the\\ncorresponding GMM\\nfits are shown in (c)\\nthrough (f). The\\nyellow discs indicate\\nthe means of the\\nGaussian mixture\\ncomponents.\\nFigure 11.10(a)\\nshows the final\\nGMM fit.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n(a) Dataset.\\n0 20 40 60\\nEM iteration\\n104\\n4×103\\n6×103\\nNegative log-likelihood (b) Negative log-likelihood.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n(c) EM initialization.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n (d) EM after one iteration.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n(e) EM after 10 iterations.\\n−10 −5 0 5 10\\nx1\\n−10\\n−5\\n0\\n5\\n10\\nx2\\n (f) EM after 62 iterations.\\nWhen we run EM on our example from Figure 11.3, we obtain the final\\nresult shown in Figure 11.8(a) after five iterations, and Figure 11.8(b)\\nshows how the negative log-likelihood evolves as a function of the EM\\niterations. The final GMM is given as\\np(x) = 0.29N\\n\\x00\\nx | − 2.75, 0.06\\n\\x01\\n+ 0.28N\\n\\x00\\nx | − 0.50, 0.25\\n\\x01\\n+ 0.43N\\n\\x00\\nx | 3.64, 1.63\\n\\x01\\n.\\n(11.57)\\nWe applied the EM algorithm to the two-dimensional dataset shown\\nin Figure 11.1 with K = 3 mixture components. Figure 11.9 illustrates\\nsome steps of the EM algorithm and shows the negative log-likelihood as\\na function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3efbfd6b-aa34-4f37-aa20-65b8674e9479', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 368, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.4 Latent-Variable Perspective 363\\nFigure 11.10 GMM\\nfit and\\nresponsibilities\\nwhen EM converges.\\n(a) GMM fit when\\nEM converges;\\n(b) each data point\\nis colored according\\nto the\\nresponsibilities of\\nthe mixture\\ncomponents.\\n−5 0 5\\nx1\\n−6\\n−4\\n−2\\n0\\n2\\n4\\n6\\nx2\\n(a) GMM fit after 62 iterations.\\n−5 0 5\\nx1\\n−6\\n−4\\n−2\\n0\\n2\\n4\\n6\\nx2 (b) Dataset colored according to the respon-\\nsibilities of the mixture components.\\nthe corresponding final GMM fit. Figure 11.10(b) visualizes the final re-\\nsponsibilities of the mixture components for the data points. The dataset is\\ncolored according to the responsibilities of the mixture components when\\nEM converges. While a single mixture component is clearly responsible\\nfor the data on the left, the overlap of the two data clusters on the right\\ncould have been generated by two mixture components. It becomes clear\\nthat there are data points that cannot be uniquely assigned to a single\\ncomponent (either blue or yellow), such that the responsibilities of these\\ntwo clusters for those points are around 0.5.\\n11.4 Latent-Variable Perspective\\nWe can look at the GMM from the perspective of a discrete latent-variable\\nmodel, i.e., where the latent variable z can attain only a finite set of val-\\nues. This is in contrast to PCA, where the latent variables were continuous-\\nvalued numbers in RM.\\nThe advantages of the probabilistic perspective are that (i) it will jus-\\ntify some ad hoc decisions we made in the previous sections, (ii) it allows\\nfor a concrete interpretation of the responsibilities as posterior probabil-\\nities, and (iii) the iterative algorithm for updating the model parameters\\ncan be derived in a principled manner as the EM algorithm for maximum\\nlikelihood parameter estimation in latent-variable models.\\n11.4.1 Generative Process and Probabilistic Model\\nTo derive the probabilistic model for GMMs, it is useful to think about the\\ngenerative process, i.e., the process that allows us to generate data, using\\na probabilistic model.\\nWe assume a mixture model with K components and that a data point\\nx can be generated by exactly one mixture component. We introduce a\\nbinary indicator variable zk ∈ {0, 1} with two states (see Section 6.2) that\\nindicates whether the kth mixture component generated that data point\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e0e3f4c-97cd-4d53-bb5b-29d89704f25b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 369, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='364 Density Estimation with Gaussian Mixture Models\\nso that\\np(x | zk = 1) = N\\n\\x00\\nx | µk, Σk\\n\\x01\\n. (11.58)\\nWe define z := [ z1, . . . , zK]⊤ ∈ RK as a probability vector consisting of\\nK −1 many 0s and exactly one1. For example, forK = 3, a valid z would\\nbe z = [ z1, z2, z3]⊤ = [0 , 1, 0]⊤, which would select the second mixture\\ncomponent since z2 = 1.\\nRemark. Sometimes this kind of probability distribution is called “multi-\\nnoulli”, a generalization of the Bernoulli distribution to more than two\\nvalues (Murphy, 2012). ♢\\nThe properties of z imply thatPK\\nk=1 zk = 1. Therefore, z is a one-hotone-hot encoding\\nencoding (also: 1-of-K representation).1-of-K\\nrepresentation Thus far, we assumed that the indicator variables zk are known. How-\\never, in practice, this is not the case, and we place a prior distribution\\np(z) = π = [π1, . . . , πK]⊤ ,\\nKX\\nk=1\\nπk = 1 , (11.59)\\non the latent variable z. Then the kth entry\\nπk = p(zk = 1) (11.60)\\nof this probability vector describes the probability that the kth mixture\\ncomponent generated data point x.Figure 11.11\\nGraphical model for\\na GMM with a single\\ndata point.\\nπ\\nz\\nxΣk\\nµk\\nk= 1, . . . , K\\nRemark (Sampling from a GMM). The construction of this latent-variable\\nmodel (see the corresponding graphical model in Figure 11.11) lends it-\\nself to a very simple sampling procedure (generative process) to generate\\ndata:\\n1. Sample z(i) ∼ p(z).\\n2. Sample x(i) ∼ p(x | z(i) = 1).\\nIn the first step, we select a mixture component i (via the one-hot encod-\\ning z) at random according to p(z) = π; in the second step we draw a\\nsample from the corresponding mixture component. When we discard the\\nsamples of the latent variable so that we are left with the x(i), we have\\nvalid samples from the GMM. This kind of sampling, where samples of\\nrandom variables depend on samples from the variable’s parents in the\\ngraphical model, is called ancestral sampling. ♢ancestral sampling\\nGenerally , a probabilistic model is defined by the joint distribution of\\nthe data and the latent variables (see Section 8.4). With the prior p(z)\\ndefined in (11.59) and (11.60) and the conditional p(x | z) from (11.58),\\nwe obtain all K components of this joint distribution via\\np(x, zk = 1) = p(x | zk = 1)p(zk = 1) = πkN\\n\\x00\\nx | µk, Σk\\n\\x01\\n(11.61)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e2f28265-da56-42e1-96e7-c0ea3e6b25ed', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 370, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.4 Latent-Variable Perspective 365\\nfor k = 1, . . . , K, so that\\np(x, z) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\np(x, z1 = 1)\\n...\\np(x, zK = 1)\\n\\uf8f9\\n\\uf8fa\\uf8fb =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nπ1N\\n\\x00\\nx | µ1, Σ1\\n\\x01\\n...\\nπKN\\n\\x00\\nx | µK, ΣK\\n\\x01\\n\\uf8f9\\n\\uf8fa\\uf8fb , (11.62)\\nwhich fully specifies the probabilistic model.\\n11.4.2 Likelihood\\nTo obtain the likelihood p(x | θ) in a latent-variable model, we need to\\nmarginalize out the latent variables (see Section 8.4.3). In our case, this\\ncan be done by summing out all latent variables from the joint p(x, z)\\nin (11.62) so that\\np(x | θ) =\\nX\\nz\\np(x | θ, z)p(z | θ) , θ := {µk, Σk, πk : k = 1, . . . , K} .\\n(11.63)\\nWe now explicitly condition on the parametersθ of the probabilistic model,\\nwhich we previously omitted. In (11.63), we sum over allK possible one-\\nhot encodings of z, which is denoted by P\\nz. Since there is only a single\\nnonzero single entry in each z there are only K possible configurations/\\nsettings of z. For example, if K = 3, then z can have the configurations\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fb ,\\n\\uf8ee\\n\\uf8f0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb . (11.64)\\nSumming over all possible configurations of z in (11.63) is equivalent to\\nlooking at the nonzero entry of the z-vector and writing\\np(x | θ) =\\nX\\nz\\np(x | θ, z)p(z | θ) (11.65a)\\n=\\nKX\\nk=1\\np(x | θ, zk = 1)p(zk = 1 | θ) (11.65b)\\nso that the desired marginal distribution is given as\\np(x | θ)\\n(11.65b)\\n=\\nKX\\nk=1\\np(x | θ, zk = 1)p(zk = 1|θ) (11.66a)\\n=\\nKX\\nk=1\\nπkN\\n\\x00\\nx | µk, Σk\\n\\x01\\n, (11.66b)\\nwhich we identify as the GMM model from (11.3). Given a dataset X , we\\nimmediately obtain the likelihood\\np(X | θ) =\\nNY\\nn=1\\np(xn | θ)\\n(11.66b)\\n=\\nNY\\nn=1\\nKX\\nk=1\\nπkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\n, (11.67)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='94dc58d3-a55a-4a7c-8208-4a88056ddf38', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 371, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='366 Density Estimation with Gaussian Mixture Models\\nFigure 11.12\\nGraphical model for\\na GMM with N data\\npoints.\\nπ\\nzn\\nxnΣk\\nµk\\nn = 1, . . . , Nk = 1, . . . , K\\nwhich is exactly the GMM likelihood from (11.9). Therefore, the latent-\\nvariable model with latent indicators zk is an equivalent way of thinking\\nabout a Gaussian mixture model.\\n11.4.3 Posterior Distribution\\nLet us have a brief look at the posterior distribution on the latent variable\\nz. According to Bayes’ theorem, the posterior of thekth component having\\ngenerated data point x\\np(zk = 1 | x) = p(zk = 1)p(x | zk = 1)\\np(x) , (11.68)\\nwhere the marginal p(x) is given in (11.66b). This yields the posterior\\ndistribution for the kth indicator variable zk\\np(zk = 1 | x) = p(zk = 1)p(x | zk = 1)\\nPK\\nj=1 p(zj = 1)p(x | zj = 1)\\n= πkN\\n\\x00\\nx | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nx | µj, Σj\\n\\x01 ,\\n(11.69)\\nwhich we identify as the responsibility of the kth mixture component for\\ndata point x. Note that we omitted the explicit conditioning on the GMM\\nparameters πk, µk, Σk where k = 1, . . . , K.\\n11.4.4 Extension to a Full Dataset\\nThus far, we have only discussed the case where the dataset consists only\\nof a single data point x. However, the concepts of the prior and posterior\\ncan be directly extended to the case of N data points X := {x1, . . . ,xN }.\\nIn the probabilistic interpretation of the GMM, every data pointxn pos-\\nsesses its own latent variable\\nzn = [zn1, . . . , znK]⊤ ∈ RK . (11.70)\\nPreviously (when we only considered a single data point x), we omitted\\nthe index n, but now this becomes important.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5ca30642-88b4-40c7-8ecd-3413761cd5e9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 372, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.4 Latent-Variable Perspective 367\\nWe share the same prior distribution π across all latent variables zn.\\nThe corresponding graphical model is shown in Figure 11.12, where we\\nuse the plate notation.\\nThe conditional distribution p(x1, . . . ,xN | z1, . . . ,zN) factorizes over\\nthe data points and is given as\\np(x1, . . . ,xN | z1, . . . ,zN) =\\nNY\\nn=1\\np(xn | zn) . (11.71)\\nTo obtain the posterior distribution p(znk = 1 | xn), we follow the same\\nreasoning as in Section 11.4.3 and apply Bayes’ theorem to obtain\\np(znk = 1 | xn) = p(xn | znk = 1)p(znk = 1)\\nPK\\nj=1 p(xn | znj = 1)p(znj = 1)\\n(11.72a)\\n= πkN\\n\\x00\\nxn | µk, Σk\\n\\x01\\nPK\\nj=1 πjN\\n\\x00\\nxn | µj, Σj\\n\\x01 = rnk . (11.72b)\\nThis means that p(zk = 1 | xn) is the (posterior) probability that the kth\\nmixture component generated data point xn and corresponds to the re-\\nsponsibility rnk we introduced in (11.17). Now the responsibilities also\\nhave not only an intuitive but also a mathematically justified interpreta-\\ntion as posterior probabilities.\\n11.4.5 EM Algorithm Revisited\\nThe EM algorithm that we introduced as an iterative scheme for maximum\\nlikelihood estimation can be derived in a principled way from the latent-\\nvariable perspective. Given a current setting θ(t) of model parameters, the\\nE-step calculates the expected log-likelihood\\nQ(θ | θ(t)) = Ez | x,θ(t)[log p(x, z | θ)] (11.73a)\\n=\\nZ\\nlog p(x, z | θ)p(z | x, θ(t))dz , (11.73b)\\nwhere the expectation of log p(x, z | θ) is taken with respect to the poste-\\nrior p(z | x, θ(t)) of the latent variables. The M-step selects an updated set\\nof model parameters θ(t+1) by maximizing (11.73b).\\nAlthough an EM iteration does increase the log-likelihood, there are\\nno guarantees that EM converges to the maximum likelihood solution.\\nIt is possible that the EM algorithm converges to a local maximum of\\nthe log-likelihood. Different initializations of the parameters θ could be\\nused in multiple EM runs to reduce the risk of ending up in a bad local\\noptimum. We do not go into further details here, but refer to the excellent\\nexpositions by Rogers and Girolami (2016) and Bishop (2006).\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b2b9666-6e3c-45e6-8229-1af5b7f7074b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 373, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='368 Density Estimation with Gaussian Mixture Models\\n11.5 Further Reading\\nThe GMM can be considered a generative model in the sense that it is\\nstraightforward to generate new data using ancestral sampling (Bishop,\\n2006). For given GMM parameters πk, µk, Σk, k = 1, . . . , K, we sample\\nan index k from the probability vector [π1, . . . , πK]⊤ and then sample a\\ndata point x ∼ N\\n\\x00\\nµk, Σk\\n\\x01\\n. If we repeat thisN times, we obtain a dataset\\nthat has been generated by a GMM. Figure 11.1 was generated using this\\nprocedure.\\nThroughout this chapter, we assumed that the number of components\\nK is known. In practice, this is often not the case. However, we could use\\nnested cross-validation, as discussed in Section 8.6.1, to find good models.\\nGaussian mixture models are closely related to the K-means clustering\\nalgorithm. K-means also uses the EM algorithm to assign data points to\\nclusters. If we treat the means in the GMM as cluster centers and ignore\\nthe covariances (or set them to I), we arrive at K-means. As also nicely\\ndescribed by MacKay (2003),K-means makes a “hard” assignment of data\\npoints to cluster centers µk, whereas a GMM makes a “soft” assignment\\nvia the responsibilities.\\nWe only touched upon the latent-variable perspective of GMMs and the\\nEM algorithm. Note that EM can be used for parameter learning in general\\nlatent-variable models, e.g., nonlinear state-space models (Ghahramani\\nand Roweis, 1999; Roweis and Ghahramani, 1999) and for reinforcement\\nlearning as discussed by Barber (2012). Therefore, the latent-variable per-\\nspective of a GMM is useful to derive the corresponding EM algorithm in\\na principled way (Bishop, 2006; Barber, 2012; Murphy, 2012).\\nWe only discussed maximum likelihood estimation (via the EM algo-\\nrithm) for finding GMM parameters. The standard criticisms of maximum\\nlikelihood also apply here:\\nAs in linear regression, maximum likelihood can suffer from severe\\noverfitting. In the GMM case, this happens when the mean of a mix-\\nture component is identical to a data point and the covariance tends to\\n0. Then, the likelihood approaches infinity . Bishop (2006) and Barber\\n(2012) discuss this issue in detail.\\nWe only obtain a point estimate of the parameters πk, µk, Σk for k =\\n1, . . . , K, which does not give any indication of uncertainty in the pa-\\nrameter values. A Bayesian approach would place a prior on the param-\\neters, which can be used to obtain a posterior distribution on the param-\\neters. This posterior allows us to compute the model evidence (marginal\\nlikelihood), which can be used for model comparison, which gives us a\\nprincipled way to determine the number of mixture components. Un-\\nfortunately , closed-form inference is not possible in this setting because\\nthere is no conjugate prior for this model. However, approximations,\\nsuch as variational inference, can be used to obtain an approximate\\nposterior (Bishop, 2006).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0c9c8b8d-a542-4a94-a623-45db64355643', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 374, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.5 Further Reading 369\\nFigure 11.13\\nHistogram (orange\\nbars) and kernel\\ndensity estimation\\n(blue line). The\\nkernel density\\nestimator produces\\na smooth estimate\\nof the underlying\\ndensity , whereas the\\nhistogram is an\\nunsmoothed count\\nmeasure of how\\nmany data points\\n(black) fall into a\\nsingle bin.\\n−4 −2 0 2 4 6 8\\nx\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(x)\\nData\\nKDE\\nHistogramIn this chapter, we discussed mixture models for density estimation.\\nThere is a plethora of density estimation techniques available. In practice,\\nwe often use histograms and kernel density estimation. histogram\\nHistograms provide a nonparametric way to represent continuous den-\\nsities and have been proposed by Pearson (1895). A histogram is con-\\nstructed by “binning” the data space and count, how many data points fall\\ninto each bin. Then a bar is drawn at the center of each bin, and the height\\nof the bar is proportional to the number of data points within that bin. The\\nbin size is a critical hyperparameter, and a bad choice can lead to overfit-\\nting and underfitting. Cross-validation, as discussed in Section 8.2.4, can\\nbe used to determine a good bin size. kernel density\\nestimationKernel density estimation, independently proposed by Rosenblatt (1956)\\nand Parzen (1962), is a nonparametric way for density estimation. Given\\nN i.i.d. samples, the kernel density estimator represents the underlying\\ndistribution as\\np(x) = 1\\nN h\\nNX\\nn=1\\nk\\n\\x12 x − xn\\nh\\n\\x13\\n, (11.74)\\nwhere k is a kernel function, i.e., a nonnegative function that integrates to\\n1 and h > 0 is a smoothing/bandwidth parameter, which plays a similar\\nrole as the bin size in histograms. Note that we place a kernel on every\\nsingle data point xn in the dataset. Commonly used kernel functions are\\nthe uniform distribution and the Gaussian distribution. Kernel density esti-\\nmates are closely related to histograms, but by choosing a suitable kernel,\\nwe can guarantee smoothness of the density estimate. Figure 11.13 illus-\\ntrates the difference between a histogram and a kernel density estimator\\n(with a Gaussian-shaped kernel) for a given dataset of 250 data points.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d957b41-ff2d-4934-a4dc-4a10b6089495', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 375, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12\\nClassification with Support Vector Machines\\nIn many situations, we want our machine learning algorithm to predict\\none of a number of (discrete) outcomes. For example, an email client sorts\\nmail into personal mail and junk mail, which has two outcomes. Another\\nexample is a telescope that identifies whether an object in the night sky\\nis a galaxy , star, or planet. There are usually a small number of outcomes,\\nand more importantly there is usually no additional structure on these\\noutcomes. In this chapter, we consider predictors that output binary val-An example of\\nstructure is if the\\noutcomes were\\nordered, like in the\\ncase of small,\\nmedium, and large\\nt-shirts.\\nues, i.e., there are only two possible outcomes. This machine learning task\\nis called binary classification. This is in contrast to Chapter 9, where we\\nbinary classification\\nconsidered a prediction problem with continuous-valued outputs.\\nFor binary classification, the set of possible values that the label/output\\ncan attain is binary , and for this chapter we denote them by{+1, −1}. In\\nother words, we consider predictors of the form\\nf : RD → {+1, −1} . (12.1)\\nRecall from Chapter 8 that we represent each example (data point) xn\\nas a feature vector of D real numbers. The labels are often referred to asInput example xn\\nmay also be referred\\nto as inputs, data\\npoints, features, or\\ninstances.\\nthe positive and negative classes, respectively . One should be careful not\\nclass\\nto infer intuitive attributes of positiveness of the +1 class. For example,\\nin a cancer detection task, a patient with cancer is often labeled +1. In\\nprinciple, any two distinct values can be used, e.g., {True, False}, {0, 1}\\nor {red, blue}. The problem of binary classification is well studied, andFor probabilistic\\nmodels, it is\\nmathematically\\nconvenient to use\\n{0, 1} as a binary\\nrepresentation; see\\nthe remark after\\nExample 6.12.\\nwe defer a survey of other approaches to Section 12.6.\\nWe present an approach known as the support vector machine (SVM),\\nwhich solves the binary classification task. As in regression, we have a su-\\npervised learning task, where we have a set of examples xn ∈ RD along\\nwith their corresponding (binary) labels yn ∈ { +1, −1}. Given a train-\\ning data set consisting of example–label pairs{(x1, y1), . . . ,(xN , yN)}, we\\nwould like to estimate parameters of the model that will give the smallest\\nclassification error. Similar to Chapter 9, we consider a linear model, and\\nhide away the nonlinearity in a transformation ϕ of the examples (9.13).\\nWe will revisit ϕ in Section 12.4.\\nThe SVM provides state-of-the-art results in many applications, with\\nsound theoretical guarantees (Steinwart and Christmann, 2008). There\\nare two main reasons why we chose to illustrate binary classification using\\n370\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2b47381f-e79b-4142-be57-0e47f23e8beb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 376, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Classification with Support Vector Machines 371\\nFigure 12.1\\nExample 2D data,\\nillustrating the\\nintuition of data\\nwhere we can find a\\nlinear classifier that\\nseparates orange\\ncrosses from blue\\ndiscs.\\nx(1)\\nx(2)\\nSVMs. First, the SVM allows for a geometric way to think about supervised\\nmachine learning. While in Chapter 9 we considered the machine learning\\nproblem in terms of probabilistic models and attacked it using maximum\\nlikelihood estimation and Bayesian inference, here we will consider an\\nalternative approach where we reason geometrically about the machine\\nlearning task. It relies heavily on concepts, such as inner products and\\nprojections, which we discussed in Chapter 3. The second reason why we\\nfind SVMs instructive is that in contrast to Chapter 9, the optimization\\nproblem for SVM does not admit an analytic solution so that we need to\\nresort to a variety of optimization tools introduced in Chapter 7.\\nThe SVM view of machine learning is subtly different from the max-\\nimum likelihood view of Chapter 9. The maximum likelihood view pro-\\nposes a model based on a probabilistic view of the data distribution, from\\nwhich an optimization problem is derived. In contrast, the SVM view starts\\nby designing a particular function that is to be optimized during training,\\nbased on geometric intuitions. We have seen something similar already\\nin Chapter 10, where we derived PCA from geometric principles. In the\\nSVM case, we start by designing a loss function that is to be minimized\\non training data, following the principles of empirical risk minimization\\n(Section 8.2).\\nLet us derive the optimization problem corresponding to training an\\nSVM on example–label pairs. Intuitively , we imagine binary classification\\ndata, which can be separated by a hyperplane as illustrated in Figure 12.1.\\nHere, every example xn (a vector of dimension 2) is a two-dimensional\\nlocation (x(1)\\nn and x(2)\\nn ), and the corresponding binary label yn is one of\\ntwo different symbols (orange cross or blue disc). “Hyperplane” is a word\\nthat is commonly used in machine learning, and we encountered hyper-\\nplanes already in Section 2.8. A hyperplane is an affine subspace of di-\\nmension D − 1 (if the corresponding vector space is of dimension D).\\nThe examples consist of two classes (there are two possible labels) that\\nhave features (the components of the vector representing the example)\\narranged in such a way as to allow us to separate/classify them by draw-\\ning a straight line.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ce40a1a-d58c-4482-a701-4df4b1f24aeb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 377, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='372 Classification with Support Vector Machines\\nIn the following, we formalize the idea of finding a linear separator\\nof the two classes. We introduce the idea of the margin and then extend\\nlinear separators to allow for examples to fall on the “wrong” side, incur-\\nring a classification error. We present two equivalent ways of formalizing\\nthe SVM: the geometric view (Section 12.2.4) and the loss function view\\n(Section 12.2.5). We derive the dual version of the SVM using Lagrange\\nmultipliers (Section 7.2). The dual SVM allows us to observe a third way\\nof formalizing the SVM: in terms of the convex hulls of the examples of\\neach class (Section 12.3.2). We conclude by briefly describing kernels and\\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\\n12.1 Separating Hyperplanes\\nGiven two examples represented as vectorsxi and xj, one way to compute\\nthe similarity between them is using an inner product⟨xi, xj⟩. Recall from\\nSection 3.2 that inner products are closely related to the angle between\\ntwo vectors. The value of the inner product between two vectors depends\\non the length (norm) of each vector. Furthermore, inner products allow\\nus to rigorously define geometric concepts such as orthogonality and pro-\\njections.\\nThe main idea behind many classification algorithms is to represent\\ndata in RD and then partition this space, ideally in a way that examples\\nwith the same label (and no other examples) are in the same partition.\\nIn the case of binary classification, the space would be divided into two\\nparts corresponding to the positive and negative classes, respectively . We\\nconsider a particularly convenient partition, which is to (linearly) split\\nthe space into two halves using a hyperplane. Let example x ∈ RD be an\\nelement of the data space. Consider a function\\nf : RD → R (12.2a)\\nx 7→ f(x) := ⟨w, x⟩ + b , (12.2b)\\nparametrized by w ∈ RD and b ∈ R. Recall from Section 2.8 that hy-\\nperplanes are affine subspaces. Therefore, we define the hyperplane that\\nseparates the two classes in our binary classification problem as\\n\\x08\\nx ∈ RD : f(x) = 0\\n\\t\\n. (12.3)\\nAn illustration of the hyperplane is shown in Figure 12.2, where the\\nvector w is a vector normal to the hyperplane and b the intercept. We can\\nderive that w is a normal vector to the hyperplane in (12.3) by choosing\\nany two examples xa and xb on the hyperplane and showing that the\\nvector between them is orthogonal to w. In the form of an equation,\\nf(xa) − f(xb) = ⟨w, xa⟩ + b − (⟨w, xb⟩ + b) (12.4a)\\n= ⟨w, xa − xb⟩ , (12.4b)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='04cb1448-6491-4f34-a2ce-dbd51916416b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 378, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.1 Separating Hyperplanes 373\\nFigure 12.2\\nEquation of a\\nseparating\\nhyperplane (12.3).\\n(a) The standard\\nway of representing\\nthe equation in 3D.\\n(b) For ease of\\ndrawing, we look at\\nthe hyperplane edge\\non.\\nw\\n(a) Separating hyperplane in 3D\\nw\\n.0\\n.Positive\\n.Negative\\nb\\n(b) Projection of the setting in (a) onto\\na plane\\nwhere the second line is obtained by the linearity of the inner product\\n(Section 3.2). Since we have chosen xa and xb to be on the hyperplane,\\nthis implies that f(xa) = 0 and f(xb) = 0 and hence ⟨w, xa − xb⟩ = 0.\\nRecall that two vectors are orthogonal when their inner product is zero. w is orthogonal to\\nany vector on the\\nhyperplane.\\nTherefore, we obtain thatw is orthogonal to any vector on the hyperplane.\\nRemark. Recall from Chapter 2 that we can think of vectors in different\\nways. In this chapter, we think of the parameter vector w as an arrow\\nindicating a direction, i.e., we consider w to be a geometric vector. In\\ncontrast, we think of the example vector x as a data point (as indicated\\nby its coordinates), i.e., we consider x to be the coordinates of a vector\\nwith respect to the standard basis. ♢\\nWhen presented with a test example, we classify the example as pos-\\nitive or negative depending on the side of the hyperplane on which it\\noccurs. Note that (12.3) not only defines a hyperplane; it additionally de-\\nfines a direction. In other words, it defines the positive and negative side\\nof the hyperplane. Therefore, to classify a test example xtest, we calcu-\\nlate the value of the function f(xtest) and classify the example as +1 if\\nf(xtest) ⩾ 0 and −1 otherwise. Thinking geometrically , the positive ex-\\namples lie “above” the hyperplane and the negative examples “below” the\\nhyperplane.\\nWhen training the classifier, we want to ensure that the examples with\\npositive labels are on the positive side of the hyperplane, i.e.,\\n⟨w, xn⟩ + b ⩾ 0 when yn = +1 (12.5)\\nand the examples with negative labels are on the negative side, i.e.,\\n⟨w, xn⟩ + b < 0 when yn = −1 . (12.6)\\nRefer to Figure 12.2 for a geometric intuition of positive and negative\\nexamples. These two conditions are often presented in a single equation\\nyn(⟨w, xn⟩ + b) ⩾ 0 . (12.7)\\nEquation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\\nsides of (12.5) and (12.6) with yn = 1 and yn = −1, respectively .\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c2cc436-fb28-42e0-becd-8c658037c2e9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 379, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='374 Classification with Support Vector Machines\\nFigure 12.3\\nPossible separating\\nhyperplanes. There\\nare many linear\\nclassifiers (green\\nlines) that separate\\norange crosses from\\nblue discs.\\nx(1)\\nx(2)\\n12.2 Primal Support Vector Machine\\nBased on the concept of distances from points to a hyperplane, we now\\nare in a position to discuss the support vector machine. For a dataset\\n{(x1, y1), . . . ,(xN , yN)} that is linearly separable, we have infinitely many\\ncandidate hyperplanes (refer to Figure 12.3), and therefore classifiers,\\nthat solve our classification problem without any (training) errors. To find\\na unique solution, one idea is to choose the separating hyperplane that\\nmaximizes the margin between the positive and negative examples. In\\nother words, we want the positive and negative examples to be separated\\nby a large margin (Section 12.2.1). In the following, we compute the dis-A classifier with\\nlarge margin turns\\nout to generalize\\nwell (Steinwart and\\nChristmann, 2008).\\ntance between an example and a hyperplane to derive the margin. Recall\\nthat the closest point on the hyperplane to a given point (example xn) is\\nobtained by the orthogonal projection (Section 3.8).\\n12.2.1 Concept of the Margin\\nThe concept of the margin is intuitively simple: It is the distance of themargin\\nseparating hyperplane to the closest examples in the dataset, assumingThere could be two\\nor more closest\\nexamples to a\\nhyperplane.\\nthat the dataset is linearly separable. However, when trying to formalize\\nthis distance, there is a technical wrinkle that may be confusing. The tech-\\nnical wrinkle is that we need to define a scale at which to measure the\\ndistance. A potential scale is to consider the scale of the data, i.e., the raw\\nvalues of xn. There are problems with this, as we could change the units\\nof measurement of xn and change the values in xn, and, hence, change\\nthe distance to the hyperplane. As we will see shortly , we define the scale\\nbased on the equation of the hyperplane (12.3) itself.\\nConsider a hyperplane ⟨w, x⟩ + b, and an example xa as illustrated in\\nFigure 12.4. Without loss of generality , we can consider the example xa\\nto be on the positive side of the hyperplane, i.e., ⟨w, xa⟩ + b > 0. We\\nwould like to compute the distance r > 0 of xa from the hyperplane. We\\ndo so by considering the orthogonal projection (Section 3.8) of xa onto\\nthe hyperplane, which we denote by x′\\na. Since w is orthogonal to the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6167ffec-6bcf-491a-908c-5ab02e675f46', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 380, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.2 Primal Support Vector Machine 375\\nFigure 12.4 Vector\\naddition to express\\ndistance to\\nhyperplane:\\nxa = x′\\na + r w\\n∥w∥ .\\n.0\\n.xa\\nw.x′a\\nr\\nhyperplane, we know that the distance r is just a scaling of this vector w.\\nIf the length of w is known, then we can use this scaling factor r factor\\nto work out the absolute distance between xa and x′\\na. For convenience,\\nwe choose to use a vector of unit length (its norm is 1) and obtain this\\nby dividing w by its norm, w\\n∥w∥. Using vector addition (Section 2.4), we\\nobtain\\nxa = x′\\na + r w\\n∥w∥ . (12.8)\\nAnother way of thinking about r is that it is the coordinate of xa in the\\nsubspace spanned by w/ ∥w∥. We have now expressed the distance of xa\\nfrom the hyperplane as r, and if we choose xa to be the point closest to\\nthe hyperplane, this distance r is the margin.\\nRecall that we would like the positive examples to be further than r\\nfrom the hyperplane, and the negative examples to be further than dis-\\ntance r (in the negative direction) from the hyperplane. Analogously to\\nthe combination of (12.5) and (12.6) into (12.7), we formulate this ob-\\njective as\\nyn(⟨w, xn⟩ + b) ⩾ r . (12.9)\\nIn other words, we combine the requirements that examples are at least\\nr away from the hyperplane (in the positive and negative direction) into\\none single inequality .\\nSince we are interested only in the direction, we add an assumption to\\nour model that the parameter vector w is of unit length, i.e., ∥w∥ = 1,\\nwhere we use the Euclidean norm ∥w∥ =\\n√\\nw⊤w (Section 3.1). This We will see other\\nchoices of inner\\nproducts\\n(Section 3.2) in\\nSection 12.4.\\nassumption also allows a more intuitive interpretation of the distance r\\n(12.8) since it is the scaling factor of a vector of length 1.\\nRemark. A reader familiar with other presentations of the margin would\\nnotice that our definition of ∥w∥ = 1 is different from the standard\\npresentation if the SVM was the one provided by Sch ¨olkopf and Smola\\n(2002), for example. In Section 12.2.3, we will show the equivalence of\\nboth approaches. ♢\\nCollecting the three requirements into a single constrained optimization\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ceb58cf0-5fdf-468a-85b0-ca526e78ac00', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 381, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='376 Classification with Support Vector Machines\\nFigure 12.5\\nDerivation of the\\nmargin: r = 1\\n∥w∥ .\\n.xa\\nw\\n⟨w,x⟩+b= 0\\n⟨w,x⟩+b= 1\\n.x′a\\nr\\nproblem, we obtain the objective\\nmax\\nw,b,r\\nr|{z}\\nmargin\\nsubject to yn(⟨w, xn⟩ + b) ⩾ r| {z }\\ndata fitting\\n, ∥w∥ = 1| {z }\\nnormalization\\n, r > 0 , (12.10)\\nwhich says that we want to maximize the margin r while ensuring that\\nthe data lies on the correct side of the hyperplane.\\nRemark. The concept of the margin turns out to be highly pervasive in ma-\\nchine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\\nto show that when the margin is large, the “complexity” of the function\\nclass is low, and hence learning is possible (Vapnik, 2000). It turns out\\nthat the concept is useful for various different approaches for theoret-\\nically analyzing generalization error (Steinwart and Christmann, 2008;\\nShalev-Shwartz and Ben-David, 2014). ♢\\n12.2.2 Traditional Derivation of the Margin\\nIn the previous section, we derived (12.10) by making the observation that\\nwe are only interested in the direction of w and not its length, leading to\\nthe assumption that ∥w∥ = 1. In this section, we derive the margin max-\\nimization problem by making a different assumption. Instead of choosing\\nthat the parameter vector is normalized, we choose a scale for the data.\\nWe choose this scale such that the value of the predictor⟨w, x⟩ + b is 1 at\\nthe closest example. Let us also denote the example in the dataset that isRecall that we\\ncurrently consider\\nlinearly separable\\ndata.\\nclosest to the hyperplane by xa.\\nFigure 12.5 is identical to Figure 12.4, except that now we rescaled the\\naxes, such that the example xa lies exactly on the margin, i.e., ⟨w, xa⟩ +\\nb = 1. Since x′\\na is the orthogonal projection of xa onto the hyperplane, it\\nmust by definition lie on the hyperplane, i.e.,\\n⟨w, x′\\na⟩ + b = 0 . (12.11)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a35e12e-b364-45e0-8a57-ed5c8806125d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 382, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.2 Primal Support Vector Machine 377\\nBy substituting (12.8) into (12.11), we obtain\\n\\x1c\\nw, xa − r w\\n∥w∥\\n\\x1d\\n+ b = 0 . (12.12)\\nExploiting the bilinearity of the inner product (see Section 3.2), we get\\n⟨w, xa⟩ + b − r ⟨w, w⟩\\n∥w∥ = 0 . (12.13)\\nObserve that the first term is 1 by our assumption of scale, i.e., ⟨w, xa⟩ +\\nb = 1. From (3.16) in Section 3.1, we know that ⟨w, w⟩ = ∥w∥2. Hence,\\nthe second term reduces to r∥w∥. Using these simplifications, we obtain\\nr = 1\\n∥w∥ . (12.14)\\nThis means we derived the distance r in terms of the normal vector w\\nof the hyperplane. At first glance, this equation is counterintuitive as we We can also think of\\nthe distance as the\\nprojection error that\\nincurs when\\nprojecting xa onto\\nthe hyperplane.\\nseem to have derived the distance from the hyperplane in terms of the\\nlength of the vector w, but we do not yet know this vector. One way to\\nthink about it is to consider the distance r to be a temporary variable\\nthat we only use for this derivation. Therefore, for the rest of this section\\nwe will denote the distance to the hyperplane by 1\\n∥w∥. In Section 12.2.3,\\nwe will see that the choice that the margin equals 1 is equivalent to our\\nprevious assumption of ∥w∥ = 1 in Section 12.2.1.\\nSimilar to the argument to obtain (12.9), we want the positive and\\nnegative examples to be at least1 away from the hyperplane, which yields\\nthe condition\\nyn(⟨w, xn⟩ + b) ⩾ 1 . (12.15)\\nCombining the margin maximization with the fact that examples need to\\nbe on the correct side of the hyperplane (based on their labels) gives us\\nmax\\nw,b\\n1\\n∥w∥ (12.16)\\nsubject to yn(⟨w, xn⟩ + b) ⩾ 1 for all n = 1, . . . , N. (12.17)\\nInstead of maximizing the reciprocal of the norm as in (12.16), we often\\nminimize the squared norm. We also often include a constant 1\\n2 that does The squared norm\\nresults in a convex\\nquadratic\\nprogramming\\nproblem for the\\nSVM (Section 12.5).\\nnot affect the optimal w, b but yields a tidier form when we compute the\\ngradient. Then, our objective becomes\\nmin\\nw,b\\n1\\n2 ∥w∥2 (12.18)\\nsubject to yn(⟨w, xn⟩ + b) ⩾ 1 for all n = 1, . . . , N . (12.19)\\nEquation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\\nexpression “hard” is because the formulation does not allow for any vi-\\nolations of the margin condition. We will see in Section 12.2.4 that this\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dacc9cd1-d338-4380-83eb-4399770f66a3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 383, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='378 Classification with Support Vector Machines\\n“hard” condition can be relaxed to accommodate violations if the data is\\nnot linearly separable.\\n12.2.3 Why We Can Set the Margin to 1\\nIn Section 12.2.1, we argued that we would like to maximize some value\\nr, which represents the distance of the closest example to the hyperplane.\\nIn Section 12.2.2, we scaled the data such that the closest example is of\\ndistance 1 to the hyperplane. In this section, we relate the two derivations,\\nand show that they are equivalent.\\nTheorem 12.1. Maximizing the margin r, where we consider normalized\\nweights as in (12.10),\\nmax\\nw,b,r\\nr|{z}\\nmargin\\nsubject to yn(⟨w, xn⟩ + b) ⩾ r| {z }\\ndata fitting\\n, ∥w∥ = 1| {z }\\nnormalization\\n, r > 0 , (12.20)\\nis equivalent to scaling the data, such that the margin is unity:\\nmin\\nw,b\\n1\\n2 ∥w∥\\n2\\n| {z }\\nmargin\\nsubject to yn(⟨w, xn⟩ + b) ⩾ 1| {z }\\ndata fitting\\n.\\n(12.21)\\nProof Consider (12.20). Since the square is a strictly monotonic trans-\\nformation for non-negative arguments, the maximum stays the same if we\\nconsider r2 in the objective. Since ∥w∥ = 1 we can reparametrize the\\nequation with a new weight vector w′ that is not normalized by explicitly\\nusing w′\\n∥w′∥. We obtain\\nmax\\nw′,b,r\\nr2\\nsubject to yn\\n\\x12\\x1c w′\\n∥w′∥ , xn\\n\\x1d\\n+ b\\n\\x13\\n⩾ r, r > 0 .\\n(12.22)\\nEquation (12.22) explicitly states that the distancer is positive. Therefore,\\nwe can divide the first constraint by r, which yieldsNote that r > 0\\nbecause we\\nassumed linear\\nseparability , and\\nhence there is no\\nissue to divide by r.\\nmax\\nw′,b,r\\nr2\\nsubject to yn\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n*\\nw′\\n∥w′∥ r| {z }\\nw′′\\n, xn\\n+\\n+ b\\nr|{z}\\nb′′\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 ⩾ 1, r > 0\\n(12.23)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac90d274-82e5-44c9-b7b3-169f622ed5c9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 384, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.2 Primal Support Vector Machine 379\\nFigure 12.6\\n(a) Linearly\\nseparable and\\n(b) non-linearly\\nseparable data.\\nx(1)\\nx(2)\\n(a) Linearly separable data, with a large\\nmargin\\nx(1)\\nx(2)\\n(b) Non-linearly separable data\\nrenaming the parameters to w′′ and b′′. Since w′′ = w′\\n∥w′∥r, rearranging for\\nr gives\\n∥w′′∥ =\\n\\r\\r\\r\\r\\nw′\\n∥w′∥ r\\n\\r\\r\\r\\r = 1\\nr ·\\n\\r\\r\\r\\r\\nw′\\n∥w′∥\\n\\r\\r\\r\\r = 1\\nr . (12.24)\\nBy substituting this result into (12.23), we obtain\\nmax\\nw′′,b′′\\n1\\n∥w′′∥\\n2\\nsubject to yn (⟨w′′, xn⟩ + b′′) ⩾ 1 .\\n(12.25)\\nThe final step is to observe that maximizing 1\\n∥w′′∥2 yields the same solution\\nas minimizing 1\\n2 ∥w′′∥\\n2\\n, which concludes the proof of Theorem 12.1.\\n12.2.4 Soft Margin SVM: Geometric View\\nIn the case where data is not linearly separable, we may wish to allow\\nsome examples to fall within the margin region, or even to be on the\\nwrong side of the hyperplane as illustrated in Figure 12.6.\\nThe model that allows for some classification errors is called the soft soft margin SVM\\nmargin SVM. In this section, we derive the resulting optimization problem\\nusing geometric arguments. In Section 12.2.5, we will derive an equiv-\\nalent optimization problem using the idea of a loss function. Using La-\\ngrange multipliers (Section 7.2), we will derive the dual optimization\\nproblem of the SVM in Section 12.3. This dual optimization problem al-\\nlows us to observe a third interpretation of the SVM: as a hyperplane that\\nbisects the line between convex hulls corresponding to the positive and\\nnegative data examples (Section 12.3.2).\\nThe key geometric idea is to introduce aslack variable ξn corresponding slack variable\\nto each example–label pair(xn, yn) that allows a particular example to be\\nwithin the margin or even on the wrong side of the hyperplane (refer to\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='34b2644a-becd-4742-b292-c4726eb0b32e', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 385, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='380 Classification with Support Vector Machines\\nFigure 12.7 Soft\\nmargin SVM allows\\nexamples to be\\nwithin the margin or\\non the wrong side of\\nthe hyperplane. The\\nslack variable ξ\\nmeasures the\\ndistance of a\\npositive example\\nx+ to the positive\\nmargin hyperplane\\n⟨w, x⟩ + b = 1\\nwhen x+ is on the\\nwrong side.\\n.x+\\nw\\n⟨w,x⟩+b= 0\\n⟨w,x⟩+b= 1\\n.\\nξ\\nFigure 12.7). We subtract the value of ξn from the margin, constraining\\nξn to be non-negative. To encourage correct classification of the samples,\\nwe add ξn to the objective\\nmin\\nw,b,ξ\\n1\\n2 ∥w∥2 + C\\nNX\\nn=1\\nξn (12.26a)\\nsubject to yn(⟨w, xn⟩ + b) ⩾ 1 − ξn (12.26b)\\nξn ⩾ 0 (12.26c)\\nfor n = 1, . . . , N. In contrast to the optimization problem (12.18) for the\\nhard margin SVM, this one is called the soft margin SVM. The parametersoft margin SVM\\nC > 0 trades off the size of the margin and the total amount of slack that\\nwe have. This parameter is called the regularization parameter since, asregularization\\nparameter we will see in the following section, the margin term in the objective func-\\ntion (12.26a) is a regularization term. The margin term ∥w∥2 is called\\nthe regularizer, and in many books on numerical optimization, the reg-regularizer\\nularization parameter is multiplied with this term (Section 8.2.3). This\\nis in contrast to our formulation in this section. Here a large value of C\\nimplies low regularization, as we give the slack variables larger weight,\\nhence giving more priority to examples that do not lie on the correct side\\nof the margin.There are\\nalternative\\nparametrizations of\\nthis regularization,\\nwhich is\\nwhy (12.26a) is also\\noften referred to as\\nthe C-SVM.\\nRemark. In the formulation of the soft margin SVM (12.26a) w is reg-\\nularized, but b is not regularized. We can see this by observing that the\\nregularization term does not contain b. The unregularized term b com-\\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\\nand decreases computational efficiency (Fan et al., 2008). ♢\\n12.2.5 Soft Margin SVM: Loss Function View\\nLet us consider a different approach for deriving the SVM, following the\\nprinciple of empirical risk minimization (Section 8.2). For the SVM, we\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ada21ef-7c13-40d2-b1f7-fefe56557bad', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 386, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.2 Primal Support Vector Machine 381\\nchoose hyperplanes as the hypothesis class, that is\\nf(x) = ⟨w, x⟩ + b. (12.27)\\nWe will see in this section that the margin corresponds to the regulariza-\\ntion term. The remaining question is, what is the loss function? In con- loss function\\ntrast to Chapter 9, where we consider regression problems (the output\\nof the predictor is a real number), in this chapter, we consider binary\\nclassification problems (the output of the predictor is one of two labels\\n{+1, −1}). Therefore, the error/loss function for each single example–\\nlabel pair needs to be appropriate for binary classification. For example,\\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\\nnary classification.\\nRemark. The ideal loss function between binary labels is to count the num-\\nber of mismatches between the prediction and the label. This means that\\nfor a predictor f applied to an example xn, we compare the output f(xn)\\nwith the label yn. We define the loss to be zero if they match, and one if\\nthey do not match. This is denoted by 1(f(xn) ̸= yn) and is called the\\nzero-one loss. Unfortunately , the zero-one loss results in a combinatorial zero-one loss\\noptimization problem for finding the best parameters w, b. Combinatorial\\noptimization problems (in contrast to continuous optimization problems\\ndiscussed in Chapter 7) are in general more challenging to solve. ♢\\nWhat is the loss function corresponding to the SVM? Consider the error\\nbetween the output of a predictor f(xn) and the label yn. The loss de-\\nscribes the error that is made on the training data. An equivalent way to\\nderive (12.26a) is to use the hinge loss hinge loss\\nℓ(t) = max{0, 1 − t} where t = yf (x) = y(⟨w, x⟩ + b) . (12.28)\\nIf f(x) is on the correct side (based on the corresponding label y) of the\\nhyperplane, and further than distance 1, this means that t ⩾ 1 and the\\nhinge loss returns a value of zero. If f(x) is on the correct side but too\\nclose to the hyperplane (0 < t < 1), the example x is within the margin,\\nand the hinge loss returns a positive value. When the example is on the\\nwrong side of the hyperplane (t < 0), the hinge loss returns an even larger\\nvalue, which increases linearly . In other words, we pay a penalty once we\\nare closer than the margin to the hyperplane, even if the prediction is\\ncorrect, and the penalty increases linearly . An alternative way to express\\nthe hinge loss is by considering it as two linear pieces\\nℓ(t) =\\n(\\n0 if t ⩾ 1\\n1 − t if t < 1 , (12.29)\\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\\nSVM 12.18 is defined as\\nℓ(t) =\\n(\\n0 if t ⩾ 1\\n∞ if t < 1 . (12.30)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d3e3647-2577-4966-82e6-809406d703b4', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 387, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='382 Classification with Support Vector Machines\\nFigure 12.8 The\\nhinge loss is a\\nconvex upper bound\\nof zero-one loss.\\n−2 0 2\\nt\\n0\\n2\\n4max{0,1−t} Zero-one loss\\nHinge loss\\nThis loss can be interpreted as never allowing any examples inside the\\nmargin.\\nFor a given training set {(x1, y1), . . . ,(xN , yN)}, we seek to minimize\\nthe total loss, while regularizing the objective with ℓ2-regularization (see\\nSection 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\\noptimization problem\\nmin\\nw,b\\n1\\n2 ∥w∥2\\n| {z }\\nregularizer\\n+ C\\nNX\\nn=1\\nmax{0, 1 − yn(⟨w, xn⟩ + b)}\\n| {z }\\nerror term\\n. (12.31)\\nThe first term in (12.31) is called the regularization term or theregularizerregularizer\\n(see Section 8.2.3), and the second term is called theloss term or the errorloss term\\nerror term term. Recall from Section 12.2.4 that the term 1\\n2 ∥w∥\\n2\\narises directly from\\nthe margin. In other words, margin maximization can be interpreted as\\nregularization.regularization\\nIn principle, the unconstrained optimization problem in (12.31) can\\nbe directly solved with (sub-)gradient descent methods as described in\\nSection 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\\nthe hinge loss (12.28) essentially consists of two linear parts, as expressed\\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\\nWe can equivalently replace minimization of the hinge loss over t with a\\nminimization of a slack variable ξ with two constraints. In equation form,\\nmin\\nt\\nmax{0, 1 − t} (12.32)\\nis equivalent to\\nmin\\nξ,t\\nξ\\nsubject to ξ ⩾ 0 , ξ ⩾ 1 − t .\\n(12.33)\\nBy substituting this expression into (12.31) and rearranging one of the\\nconstraints, we obtain exactly the soft margin SVM (12.26a).\\nRemark. Let us contrast our choice of the loss function in this section to the\\nloss function for linear regression in Chapter 9. Recall from Section 9.2.1\\nthat for finding maximum likelihood estimators, we usually minimize the\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6bfe3747-84c8-4ca9-bf90-8d7c30f4537d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 388, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.3 Dual Support Vector Machine 383\\nnegative log-likelihood. Furthermore, since the likelihood term for linear\\nregression with Gaussian noise is Gaussian, the negative log-likelihood for\\neach example is a squared error function. The squared error function is the\\nloss function that is minimized when looking for the maximum likelihood\\nsolution. ♢\\n12.3 Dual Support Vector Machine\\nThe description of the SVM in the previous sections, in terms of the vari-\\nables w and b, is known as the primal SVM. Recall that we consider inputs\\nx ∈ RD with D features. Since w is of the same dimension as x, this\\nmeans that the number of parameters (the dimension of w) of the opti-\\nmization problem grows linearly with the number of features.\\nIn the following, we consider an equivalent optimization problem (the\\nso-called dual view), which is independent of the number of features. In-\\nstead, the number of parameters increases with the number of examples\\nin the training set. We saw a similar idea appear in Chapter 10, where we\\nexpressed the learning problem in a way that does not scale with the num-\\nber of features. This is useful for problems where we have more features\\nthan the number of examples in the training dataset. The dual SVM also\\nhas the additional advantage that it easily allows kernels to be applied,\\nas we shall see at the end of this chapter. The word “dual” appears often\\nin mathematical literature, and in this particular case it refers to convex\\nduality . The following subsections are essentially an application of convex\\nduality , which we discussed in Section 7.2.\\n12.3.1 Convex Duality via Lagrange Multipliers\\nRecall the primal soft margin SVM (12.26a). We call the variables w, b,\\nand ξ corresponding to the primal SVM the primal variables. We useαn ⩾ In Chapter 7, we\\nused λ as Lagrange\\nmultipliers. In this\\nsection, we follow\\nthe notation\\ncommonly chosen in\\nSVM literature, and\\nuse α and γ.\\n0 as the Lagrange multiplier corresponding to the constraint (12.26b) that\\nthe examples are classified correctly and γn ⩾ 0 as the Lagrange multi-\\nplier corresponding to the non-negativity constraint of the slack variable;\\nsee (12.26c). The Lagrangian is then given by\\nL(w, b, ξ, α, γ) = 1\\n2 ∥w∥2 + C\\nNX\\nn=1\\nξn (12.34)\\n−\\nNX\\nn=1\\nαn(yn(⟨w, xn⟩ + b) − 1 + ξn)\\n| {z }\\nconstraint (12.26b)\\n−\\nNX\\nn=1\\nγnξn\\n| {z }\\nconstraint (12.26c)\\n.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ad81a306-e22c-490f-8a7c-92cc2b0011a3', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 389, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='384 Classification with Support Vector Machines\\nBy differentiating the Lagrangian (12.34) with respect to the three primal\\nvariables w, b, and ξ respectively , we obtain\\n∂L\\n∂w = w⊤ −\\nNX\\nn=1\\nαnynxn\\n⊤ , (12.35)\\n∂L\\n∂b = −\\nNX\\nn=1\\nαnyn , (12.36)\\n∂L\\n∂ξn\\n= C − αn − γn . (12.37)\\nWe now find the maximum of the Lagrangian by setting each of these\\npartial derivatives to zero. By setting (12.35) to zero, we find\\nw =\\nNX\\nn=1\\nαnynxn , (12.38)\\nwhich is a particular instance of the representer theorem (Kimeldorf andrepresenter theorem\\nWahba, 1970). Equation (12.38) states that the optimal weight vector inThe representer\\ntheorem is actually\\na collection of\\ntheorems saying\\nthat the solution of\\nminimizing\\nempirical risk lies in\\nthe subspace\\n(Section 2.4.3)\\ndefined by the\\nexamples.\\nthe primal is a linear combination of the examples xn. Recall from Sec-\\ntion 2.6.1 that this means that the solution of the optimization problem\\nlies in the span of training data. Additionally , the constraint obtained by\\nsetting (12.36) to zero implies that the optimal weight vector is an affine\\ncombination of the examples. The representer theorem turns out to hold\\nfor very general settings of regularized empirical risk minimization (Hof-\\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\\ngeneral versions (Sch ¨olkopf et al., 2001), and necessary and sufficient\\nconditions on its existence can be found in Yu et al. (2013).\\nRemark. The representer theorem (12.38) also provides an explanation\\nof the name “support vector machine.” The examples xn, for which the\\ncorresponding parameters αn = 0, do not contribute to the solution w at\\nall. The other examples, where αn > 0, are called support vectors sincesupport vector\\nthey “support” the hyperplane. ♢\\nBy substituting the expression for w into the Lagrangian (12.34), we\\nobtain the dual\\nD(ξ, α, γ) = 1\\n2\\nNX\\ni=1\\nNX\\nj=1\\nyiyjαiαj ⟨xi, xj⟩ −\\nNX\\ni=1\\nyiαi\\n* NX\\nj=1\\nyjαjxj, xi\\n+\\n+ C\\nNX\\ni=1\\nξi − b\\nNX\\ni=1\\nyiαi +\\nNX\\ni=1\\nαi −\\nNX\\ni=1\\nαiξi −\\nNX\\ni=1\\nγiξi .\\n(12.39)\\nNote that there are no longer any terms involving the primal variable w.\\nBy setting (12.36) to zero, we obtainPN\\nn=1 ynαn = 0. Therefore, the term\\ninvolving b also vanishes. Recall that inner products are symmetric and\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='992ffa8f-cd6d-4e96-b617-7c5d7f9b0dc6', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 390, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.3 Dual Support Vector Machine 385\\nbilinear (see Section 3.2). Therefore, the first two terms in (12.39) are\\nover the same objects. These terms (colored blue) can be simplified, and\\nwe obtain the Lagrangian\\nD(ξ, α, γ) = −1\\n2\\nNX\\ni=1\\nNX\\nj=1\\nyiyjαiαj ⟨xi, xj⟩ +\\nNX\\ni=1\\nαi +\\nNX\\ni=1\\n(C − αi − γi)ξi .\\n(12.40)\\nThe last term in this equation is a collection of all terms that contain slack\\nvariables ξi. By setting (12.37) to zero, we see that the last term in (12.40)\\nis also zero. Furthermore, by using the same equation and recalling that\\nthe Lagrange multiplers γi are non-negative, we conclude that αi ⩽ C.\\nWe now obtain the dual optimization problem of the SVM, which is ex-\\npressed exclusively in terms of the Lagrange multipliers αi. Recall from\\nLagrangian duality (Definition 7.1) that we maximize the dual problem.\\nThis is equivalent to minimizing the negative dual problem, such that we\\nend up with the dual SVM dual SVM\\nmin\\nα\\n1\\n2\\nNX\\ni=1\\nNX\\nj=1\\nyiyjαiαj ⟨xi, xj⟩ −\\nNX\\ni=1\\nαi\\nsubject to\\nNX\\ni=1\\nyiαi = 0\\n0 ⩽ αi ⩽ C for all i = 1, . . . , N .\\n(12.41)\\nThe equality constraint in (12.41) is obtained from setting (12.36) to\\nzero. The inequality constraint αi ⩾ 0 is the condition imposed on La-\\ngrange multipliers of inequality constraints (Section 7.2). The inequality\\nconstraint αi ⩽ C is discussed in the previous paragraph.\\nThe set of inequality constraints in the SVM are called “box constraints”\\nbecause they limit the vector α = [α1, · · · , αN]⊤ ∈ RN of Lagrange mul-\\ntipliers to be inside the box defined by 0 and C on each axis. These\\naxis-aligned boxes are particularly efficient to implement in numerical\\nsolvers (Dost´al, 2009, chapter 5). It turns out that\\nexamples that lie\\nexactly on the\\nmargin are\\nexamples whose\\ndual parameters lie\\nstrictly inside the\\nbox constraints,\\n0 < α i < C . This is\\nderived using the\\nKarush Kuhn Tucker\\nconditions, for\\nexample in\\nSch¨olkopf and\\nSmola (2002).\\nOnce we obtain the dual parameters α, we can recover the primal pa-\\nrameters w by using the representer theorem (12.38). Let us call the op-\\ntimal primal parameter w∗. However, there remains the question on how\\nto obtain the parameter b∗. Consider an example xn that lies exactly on\\nthe margin’s boundary , i.e.,⟨w∗, xn⟩ + b = yn. Recall that yn is either +1\\nor −1. Therefore, the only unknown is b, which can be computed by\\nb∗ = yn − ⟨w∗, xn⟩ . (12.42)\\nRemark. In principle, there may be no examples that lie exactly on the\\nmargin. In this case, we should compute |yn − ⟨w∗, xn⟩ | for all support\\nvectors and take the median value of this absolute value difference to be\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e58b330e-9e38-4355-81be-c2c3e3e002eb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 391, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='386 Classification with Support Vector Machines\\nFigure 12.9 Convex\\nhulls. (a) Convex\\nhull of points, some\\nof which lie within\\nthe boundary;\\n(b) convex hulls\\naround positive and\\nnegative examples.\\n(a) Convex hull.\\nc\\nd (b) Convex hulls around positive (blue) and\\nnegative (orange) examples. The distance be-\\ntween the two convex sets is the length of the\\ndifference vector c − d.\\nthe value of b∗. A derivation of this can be found in http://fouryears.\\neu/2012/06/07/the-svm-bias-term-conspiracy/ . ♢\\n12.3.2 Dual SVM: Convex Hull View\\nAnother approach to obtain the dual SVM is to consider an alternative\\ngeometric argument. Consider the set of examplesxn with the same label.\\nWe would like to build a convex set that contains all the examples such\\nthat it is the smallest possible set. This is called the convex hull and is\\nillustrated in Figure 12.9.\\nLet us first build some intuition about a convex combination of points.\\nConsider two points x1 and x2 and corresponding non-negative weights\\nα1, α2 ⩾ 0 such that α1+α2 = 1. The equationα1x1+α2x2 describes each\\npoint on a line between x1 and x2. Consider what happens when we add\\na third point x3 along with a weight α3 ⩾ 0 such that P3\\nn=1 αn = 1 .\\nThe convex combination of these three points x1, x2, x3 spans a two-\\ndimensional area. The convex hull of this area is the triangle formed byconvex hull\\nthe edges corresponding to each pair of of points. As we add more points,\\nand the number of points becomes greater than the number of dimen-\\nsions, some of the points will be inside the convex hull, as we can see in\\nFigure 12.9(a).\\nIn general, building a convex convex hull can be done by introducing\\nnon-negative weights αn ⩾ 0 corresponding to each example xn. Then\\nthe convex hull can be described as the set\\nconv (X) =\\n( NX\\nn=1\\nαnxn\\n)\\nwith\\nNX\\nn=1\\nαn = 1 and αn ⩾ 0, (12.43)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f58114b6-86c5-4379-890a-c9c9107c1ade', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 392, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.3 Dual Support Vector Machine 387\\nfor all n = 1 , . . . , N. If the two clouds of points corresponding to the\\npositive and negative classes are separated, then the convex hulls do not\\noverlap. Given the training data (x1, y1), . . . ,(xN , yN), we form two con-\\nvex hulls, corresponding to the positive and negative classes respectively .\\nWe pick a point c, which is in the convex hull of the set of positive exam-\\nples, and is closest to the negative class distribution. Similarly , we pick a\\npoint d in the convex hull of the set of negative examples and is closest to\\nthe positive class distribution; see Figure 12.9(b). We define a difference\\nvector between d and c as\\nw := c − d . (12.44)\\nPicking the points c and d as in the preceding cases, and requiring them\\nto be closest to each other is equivalent to minimizing the length/norm of\\nw, so that we end up with the corresponding optimization problem\\narg min\\nw\\n∥w∥ = arg min\\nw\\n1\\n2 ∥w∥\\n2\\n. (12.45)\\nSince c must be in the positive convex hull, it can be expressed as a convex\\ncombination of the positive examples, i.e., for non-negative coefficients\\nα+\\nn\\nc =\\nX\\nn:yn=+1\\nα+\\nn xn . (12.46)\\nIn (12.46), we use the notation n : yn = +1 to indicate the set of indices\\nn for which yn = +1. Similarly , for the examples with negative labels, we\\nobtain\\nd =\\nX\\nn:yn=−1\\nα−\\nn xn . (12.47)\\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\\nobjective\\nmin\\nα\\n1\\n2\\n\\r\\r\\r\\r\\r\\nX\\nn:yn=+1\\nα+\\nn xn −\\nX\\nn:yn=−1\\nα−\\nn xn\\n\\r\\r\\r\\r\\r\\n2\\n. (12.48)\\nLet α be the set of all coefficients, i.e., the concatenation ofα+ and α−.\\nRecall that we require that for each convex hull that their coefficients sum\\nto one,\\nX\\nn:yn=+1\\nα+\\nn = 1 and\\nX\\nn:yn=−1\\nα−\\nn = 1 . (12.49)\\nThis implies the constraint\\nNX\\nn=1\\nynαn = 0 . (12.50)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='49d3f2d0-37fe-4be2-acce-e21728faf673', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 393, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='388 Classification with Support Vector Machines\\nThis result can be seen by multiplying out the individual classes\\nNX\\nn=1\\nynαn =\\nX\\nn:yn=+1\\n(+1)α+\\nn +\\nX\\nn:yn=−1\\n(−1)α−\\nn (12.51a)\\n=\\nX\\nn:yn=+1\\nα+\\nn −\\nX\\nn:yn=−1\\nα−\\nn = 1 − 1 = 0 . (12.51b)\\nThe objective function (12.48) and the constraint (12.50), along with the\\nassumption that α ⩾ 0, give us a constrained (convex) optimization prob-\\nlem. This optimization problem can be shown to be the same as that of\\nthe dual hard margin SVM (Bennett and Bredensteiner, 2000a).\\nRemark. To obtain the soft margin dual, we consider the reduced hull. The\\nreduced hull is similar to the convex hull but has an upper bound to thereduced hull\\nsize of the coefficients α. The maximum possible value of the elements\\nof α restricts the size that the convex hull can take. In other words, the\\nbound on α shrinks the convex hull to a smaller volume (Bennett and\\nBredensteiner, 2000b). ♢\\n12.4 Kernels\\nConsider the formulation of the dual SVM (12.41). Notice that the in-\\nner product in the objective occurs only between examples xi and xj.\\nThere are no inner products between the examples and the parameters.\\nTherefore, if we consider a set of features ϕ(xi) to represent xi, the only\\nchange in the dual SVM will be to replace the inner product. This mod-\\nularity , where the choice of the classification method (the SVM) and the\\nchoice of the feature representation ϕ(x) can be considered separately ,\\nprovides flexibility for us to explore the two problems independently . In\\nthis section, we discuss the representation ϕ(x) and briefly introduce the\\nidea of kernels, but do not go into the technical details.\\nSince ϕ(x) could be a non-linear function, we can use the SVM (which\\nassumes a linear classifier) to construct classifiers that are nonlinear in\\nthe examples xn. This provides a second avenue, in addition to the soft\\nmargin, for users to deal with a dataset that is not linearly separable. It\\nturns out that there are many algorithms and statistical methods that have\\nthis property that we observed in the dual SVM: the only inner products\\nare those that occur between examples. Instead of explicitly defining a\\nnon-linear feature map ϕ(·) and computing the resulting inner product\\nbetween examples xi and xj, we define a similarity functionk(xi, xj) be-\\ntween xi and xj. For a certain class of similarity functions, called kernels,kernel\\nthe similarity function implicitly defines a non-linear feature map ϕ(·).\\nKernels are by definition functions k : X × X → R for which there existsThe inputs X of the\\nkernel function can\\nbe very general and\\nare not necessarily\\nrestricted to RD.\\na Hilbert space H and ϕ : X → H a feature map such that\\nk(xi, xj) = ⟨ϕ(xi), ϕ(xj)⟩H . (12.52)\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d0e4af21-d1aa-4abd-bb45-0f3b93d562e1', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 394, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.4 Kernels 389\\nFigure 12.10 SVM\\nwith different\\nkernels. Note that\\nwhile the decision\\nboundary is\\nnonlinear, the\\nunderlying problem\\nbeing solved is for a\\nlinear separating\\nhyperplane (albeit\\nwith a nonlinear\\nkernel).\\nFirst feature\\nSecond feature\\n(a) SVM with linear kernel\\nFirst feature\\nSecond feature (b) SVM with RBF kernel\\nFirst feature\\nSecond feature\\n(c) SVM with polynomial (degree 2) kernel\\nFirst feature\\nSecond feature (d) SVM with polynomial (degree 3) kernel\\nThere is a unique reproducing kernel Hilbert space associated with every\\nkernel k (Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\\nunique association, ϕ(x) = k(·, x) is called the canonical feature map . canonical feature\\nmapThe generalization from an inner product to a kernel function (12.52) is\\nknown as the kernel trick (Sch¨olkopf and Smola, 2002; Shawe-Taylor and kernel trick\\nCristianini, 2004), as it hides away the explicit non-linear feature map.\\nThe matrix K ∈ RN ×N, resulting from the inner products or the appli-\\ncation of k(·, ·) to a dataset, is called the Gram matrix, and is often just Gram matrix\\nreferred to as the kernel matrix. Kernels must be symmetric and positive kernel matrix\\nsemidefinite functions so that every kernel matrix K is symmetric and\\npositive semidefinite (Section 3.2.3):\\n∀z ∈ RN : z⊤Kz ⩾ 0 . (12.53)\\nSome popular examples of kernels for multivariate real-valued data xi ∈\\nRD are the polynomial kernel, the Gaussian radial basis function kernel,\\nand the rational quadratic kernel (Sch¨olkopf and Smola, 2002; Rasmussen\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='38e0a806-c33a-4765-8698-4ddf9f4cce45', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 395, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='390 Classification with Support Vector Machines\\nand Williams, 2006). Figure 12.10 illustrates the effect of different kernels\\non separating hyperplanes on an example dataset. Note that we are still\\nsolving for hyperplanes, that is, the hypothesis class of functions are still\\nlinear. The non-linear surfaces are due to the kernel function.\\nRemark. Unfortunately for the fledgling machine learner, there are mul-\\ntiple meanings of the word “kernel.” In this chapter, the word “kernel”\\ncomes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\\nszajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\\near algebra (Section 2.7.3), where the kernel is another word for the null\\nspace. The third common use of the word “kernel” in machine learning is\\nthe smoothing kernel in kernel density estimation (Section 11.5). ♢\\nSince the explicit representation ϕ(x) is mathematically equivalent to\\nthe kernel representation k(xi, xj), a practitioner will often design the\\nkernel function such that it can be computed more efficiently than the\\ninner product between explicit feature maps. For example, consider the\\npolynomial kernel (Sch ¨olkopf and Smola, 2002), where the number of\\nterms in the explicit expansion grows very quickly (even for polynomials\\nof low degree) when the input dimension is large. The kernel function\\nonly requires one multiplication per input dimension, which can provide\\nsignificant computational savings. Another example is the Gaussian ra-\\ndial basis function kernel (Sch ¨olkopf and Smola, 2002; Rasmussen and\\nWilliams, 2006), where the corresponding feature space is infinite dimen-\\nsional. In this case, we cannot explicitly represent the feature space but\\ncan still compute similarities between a pair of examples using the kernel.The choice of\\nkernel, as well as\\nthe parameters of\\nthe kernel, is often\\nchosen using nested\\ncross-validation\\n(Section 8.6.1).\\nAnother useful aspect of the kernel trick is that there is no need for\\nthe original data to be already represented as multivariate real-valued\\ndata. Note that the inner product is defined on the output of the function\\nϕ(·), but does not restrict the input to real numbers. Hence, the function\\nϕ(·) and the kernel function k(·, ·) can be defined on any object, e.g.,\\nsets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\\nG¨artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\\net al., 2010).\\n12.5 Numerical Solution\\nWe conclude our discussion of SVMs by looking at how to express the\\nproblems derived in this chapter in terms of the concepts presented in\\nChapter 7. We consider two different approaches for finding the optimal\\nsolution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\\npress this as an unconstrained optimization problem. Then we express the\\nconstrained versions of the primal and dual SVMs as quadratic programs\\nin standard form 7.3.2.\\nConsider the loss function view of the SVM (12.31). This is a convex\\nunconstrained optimization problem, but the hinge loss (12.28) is not dif-\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6444aade-2e51-490a-88f6-36ce86106550', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 396, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.5 Numerical Solution 391\\nferentiable. Therefore, we apply a subgradient approach for solving it.\\nHowever, the hinge loss is differentiable almost everywhere, except for\\none single point at the hinge t = 1. At this point, the gradient is a set of\\npossible values that lie between 0 and −1. Therefore, the subgradient g of\\nthe hinge loss is given by\\ng(t) =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\n−1 t < 1\\n[−1, 0] t = 1\\n0 t > 1\\n. (12.54)\\nUsing this subgradient, we can apply the optimization methods presented\\nin Section 7.1.\\nBoth the primal and the dual SVM result in a convex quadratic pro-\\ngramming problem (constrained optimization). Note that the primal SVM\\nin (12.26a) has optimization variables that have the size of the dimen-\\nsion D of the input examples. The dual SVM in (12.41) has optimization\\nvariables that have the size of the number N of examples.\\nTo express the primal SVM in the standard form (7.45) for quadratic\\nprogramming, let us assume that we use the dot product (3.5) as the\\ninner product. We rearrange the equation for the primal SVM (12.26a), Recall from\\nSection 3.2 that we\\nuse the phrase dot\\nproduct to mean the\\ninner product on\\nEuclidean vector\\nspace.\\nsuch that the optimization variables are all on the right and the inequality\\nof the constraint matches the standard form. This yields the optimization\\nmin\\nw,b,ξ\\n1\\n2 ∥w∥2 + C\\nNX\\nn=1\\nξn\\nsubject to −ynx⊤\\nn w − ynb − ξn ⩽ −1\\n−ξn ⩽ 0\\n(12.55)\\nn = 1, . . . , N. By concatenating the variables w, b, xn into a single vector,\\nand carefully collecting the terms, we obtain the following matrix form of\\nthe soft margin SVM:\\nmin\\nw,b,ξ\\n1\\n2\\n\\uf8ee\\n\\uf8f0\\nw\\nb\\nξ\\n\\uf8f9\\n\\uf8fb\\n⊤\\x14 I D 0D,N+1\\n0N+1,D 0N+1,N+1\\n\\x15\\uf8ee\\n\\uf8f0\\nw\\nb\\nξ\\n\\uf8f9\\n\\uf8fb +\\n\\x02\\n0D+1,1 C1N,1\\n\\x03⊤\\n\\uf8ee\\n\\uf8f0\\nw\\nb\\nξ\\n\\uf8f9\\n\\uf8fb\\nsubject to\\n\\x14 −Y X −y −I N\\n0N,D+1 −I N\\n\\x15\\uf8ee\\n\\uf8f0\\nw\\nb\\nξ\\n\\uf8f9\\n\\uf8fb ⩽\\n\\x14−1N,1\\n0N,1\\n\\x15\\n.\\n(12.56)\\nIn the preceding optimization problem, the minimization is over the pa-\\nrameters [w⊤, b, ξ⊤]⊤ ∈ RD+1+N, and we use the notation: I m to rep-\\nresent the identity matrix of size m × m, 0m,n to represent the matrix\\nof zeros of size m × n, and 1m,n to represent the matrix of ones of size\\nm × n. In addition, y is the vector of labels [y1, · · · , yN]⊤, Y = diag(y)\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ade78460-9466-4c69-b674-af75a2c9b8f5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 397, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='392 Classification with Support Vector Machines\\nis an N by N matrix where the elements of the diagonal are from y, and\\nX ∈ RN ×D is the matrix obtained by concatenating all the examples.\\nWe can similarly perform a collection of terms for the dual version of the\\nSVM (12.41). To express the dual SVM in standard form, we first have to\\nexpress the kernel matrix K such that each entry is Kij = k(xi, xj). If we\\nhave an explicit feature representation xi then we define Kij = ⟨xi, xj⟩.\\nFor convenience of notation we introduce a matrix with zeros everywhere\\nexcept on the diagonal, where we store the labels, that is, Y = diag(y).\\nThe dual SVM can be written as\\nmin\\nα\\n1\\n2 α⊤Y KY α − 1⊤\\nN,1α\\nsubject to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny⊤\\n−y⊤\\n−I N\\nI N\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb α ⩽\\n\\x140N+2,1\\nC1N,1\\n\\x15\\n.\\n(12.57)\\nRemark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\\nof the constraints to be inequality constraints. We will express the dual\\nSVM’s equality constraint as two inequality constraints, i.e.,\\nAx = b is replaced by Ax ⩽ b and Ax ⩾ b . (12.58)\\nParticular software implementations of convex optimization methods may\\nprovide the ability to express equality constraints. ♢\\nSince there are many different possible views of the SVM, there are\\nmany approaches for solving the resulting optimization problem. The ap-\\nproach presented here, expressing the SVM problem in standard convex\\noptimization form, is not often used in practice. The two main implemen-\\ntations of SVM solvers are Chang and Lin (2011) (which is open source)\\nand Joachims (1999). Since SVMs have a clear and well-defined optimiza-\\ntion problem, many approaches based on numerical optimization tech-\\nniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\\nSun, 2011).\\n12.6 Further Reading\\nThe SVM is one of many approaches for studying binary classification.\\nOther approaches include the perceptron, logistic regression, Fisher dis-\\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\\nMurphy, 2012). A short tutorial on SVMs and kernels on discrete se-\\nquences can be found in Ben-Hur et al. (2008). The development of SVMs\\nis closely linked to empirical risk minimization, discussed in Section 8.2.\\nHence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\\nwart and Christmann, 2008). The book about kernel methods (Sch¨olkopf\\nand Smola, 2002) includes many details of support vector machines and\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c66d03d-9398-4d51-8b77-e17068b2b69b', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 398, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12.6 Further Reading 393\\nhow to optimize them. A broader book about kernel methods (Shawe-\\nTaylor and Cristianini, 2004) also includes many linear algebra approaches\\nfor different machine learning problems.\\nAn alternative derivation of the dual SVM can be obtained using the\\nidea of the Legendre–Fenchel transform (Section 7.3.3). The derivation\\nconsiders each term of the unconstrained formulation of the SVM (12.31)\\nseparately and calculates their convex conjugates (Rifkin and Lippert,\\n2007). Readers interested in the functional analysis view (also the reg-\\nularization methods view) of SVMs are referred to the work by Wahba\\n(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\\n1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\\ning in linear operators (Akhiezer and Glazman, 1993). The idea of kernels\\nhave been generalized to Banach spaces (Zhang et al., 2009) and Kre ˘ın\\nspaces (Ong et al., 2004; Loosli et al., 2016).\\nObserve that the hinge loss has three equivalent representations, as\\nshown in (12.28) and (12.29), as well as the constrained optimization\\nproblem in (12.33). The formulation (12.28) is often used when compar-\\ning the SVM loss function with other loss functions (Steinwart, 2007).\\nThe two-piece formulation (12.29) is convenient for computing subgra-\\ndients, as each piece is linear. The third formulation (12.33), as seen\\nin Section 12.5, enables the use of convex quadratic programming (Sec-\\ntion 7.3.2) tools.\\nSince binary classification is a well-studied task in machine learning,\\nother words are also sometimes used, such as discrimination, separation,\\nand decision. Furthermore, there are three quantities that can be the out-\\nput of a binary classifier. First is the output of the linear function itself\\n(often called the score), which can take any real value. This output can be\\nused for ranking the examples, and binary classification can be thought\\nof as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\\ntianini, 2004). The second quantity that is often considered the output\\nof a binary classifier is the output determined after it is passed through\\na non-linear function to constrain its value to a bounded range, for ex-\\nample in the interval [0, 1]. A common non-linear function is the sigmoid\\nfunction (Bishop, 2006). When the non-linearity results in well-calibrated\\nprobabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),\\nthis is called class probability estimation. The third output of a binary\\nclassifier is the final binary decision{+1, −1}, which is the one most com-\\nmonly assumed to be the output of the classifier.\\nThe SVM is a binary classifier that does not naturally lend itself to a\\nprobabilistic interpretation. There are several approaches for converting\\nthe raw output of the linear function (the score) into a calibrated class\\nprobability estimate ( P (Y = 1 |X = x)) that involve an additional cal-\\nibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).\\nFrom the training perspective, there are many related probabilistic ap-\\nproaches. We mentioned at the end of Section 12.2.5 that there is a re-\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1568f35a-f3f0-4585-ac80-b62ec7809725', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 399, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='394 Classification with Support Vector Machines\\nlationship between loss function and the likelihood (also compare Sec-\\ntions 8.2 and 8.3). The maximum likelihood approach corresponding to\\na well-calibrated transformation during training is called logistic regres-\\nsion, which comes from a class of methods called generalized linear mod-\\nels. Details of logistic regression from this point of view can be found in\\nAgresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).\\nNaturally , one could take a more Bayesian view of the classifier output by\\nestimating a posterior distribution using Bayesian logistic regression. The\\nBayesian view also includes the specification of the prior, which includes\\ndesign choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\\nditionally , one could consider latent functions as priors, which results in\\nGaussian process classification (Rasmussen and Williams, 2006, chapter\\n3).\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='416aba8b-f6e7-4cdd-8e1e-295845d2d2eb', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 400, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References\\nAbel, Niels H. 1826. D´emonstration de l’Impossibilit´e de la R ´esolution Alg´ebrique des\\n´Equations G´en´erales qui Passent le Quatri`eme Degr´e. Grøndahl and Søn.\\nAdhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The\\nFoundations of Data Science. Gitbooks.\\nAgarwal, Arvind, and Daum ´e III, Hal. 2010. A Geometric View of Conjugate Priors.\\nMachine Learning, 81(1), 99–113.\\nAgresti, A. 2002. Categorical Data Analysis. Wiley .\\nAkaike, Hirotugu. 1974. A New Look at the Statistical Model Identification. IEEE\\nTransactions on Automatic Control, 19(6), 716–723.\\nAkhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\\nSpace. Dover Publications.\\nAlpaydin, Ethem. 2010. Introduction to Machine Learning. MIT Press.\\nAmari, Shun-ichi. 2016. Information Geometry and Its Applications. Springer.\\nArgyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer\\nTheorems. In: Proceedings of the International Conference on Machine Learning.\\nAronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the Amer-\\nican Mathematical Society, 68, 337–404.\\nAxler, Sheldon. 2015. Linear Algebra Done Right. Springer.\\nBakir, G¨okhan, Hofmann, Thomas, Sch¨olkopf, Bernhard, Smola, Alexander J., Taskar,\\nBen, and Vishwanathan, S. V. N. (eds). 2007.Predicting Structured Data. MIT Press.\\nBarber, David. 2012. Bayesian Reasoning and Machine Learning. Cambridge University\\nPress.\\nBarndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical The-\\nory. Wiley .\\nBartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models\\nand Factor Analysis: A Unified Approach. Wiley .\\nBaydin, Atılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.\\n2018. Automatic Differentiation in Machine Learning: A Survey.Journal of Machine\\nLearning Research, 18, 1–43.\\nBeck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\\ndient Methods for Convex Optimization. Operations Research Letters, 31(3), 167–\\n175.\\nBelabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine\\nLearning and New Strategies for Very Large Datasets. Proceedings of the National\\nAcademy of Sciences, 0810600105.\\nBelkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality\\nReduction and Data Representation. Neural Computation, 15(6), 1373–1396.\\nBen-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S¨oren, Sch¨olkopf, Bernhard, and R¨atsch,\\nGunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\\nPLoS Computational Biology, 4(10), e1000173.\\n395\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='65a7adc1-ec6b-4890-b6c2-bc0b311f0f24', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 401, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='396 References\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM\\nClassifiers. In: Proceedings of the International Conference on Machine Learning.\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages\\n132–145 of: Geometry at Work. Mathematical Association of America.\\nBerlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces\\nin Probability and Statistics. Springer.\\nBertsekas, Dimitri P. 1999. Nonlinear Programming. Athena Scientific.\\nBertsekas, Dimitri P. 2009. Convex Optimization Theory. Athena Scientific.\\nBickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\\nSelected Topics. Vol. 1. Prentice Hall.\\nBickson, Danny , Dolev, Danny , Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.\\nLinear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Con-\\nference on Communication, Control, and Computing.\\nBillingsley , Patrick. 1995.Probability and Measure. Wiley .\\nBishop, Christopher M. 1995. Neural Networks for Pattern Recognition . Clarendon\\nPress.\\nBishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Pro-\\ncessing Systems.\\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\\nBlei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\\nReview for Statisticians. Journal of the American Statistical Association , 112(518),\\n859–877.\\nBlum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Ma-\\nchine Learning Competitions. In: International Conference on Machine Learning.\\nBonnans, J. Fr´ed´eric, Gilbert, J. Charles, Lemar´echal, Claude, and Sagastiz´abal, Clau-\\ndia A. 2006. Numerical Optimization: Theoretical and Practical Aspects. Springer.\\nBorwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\\nOptimization. 2nd edn. Canadian Mathematical Society .\\nBottou, L´eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42\\nof: Online Learning and Neural Networks. Cambridge University Press.\\nBottou, L´eon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for\\nLarge-Scale Machine Learning. SIAM Review, 60(2), 223–311.\\nBoucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-\\nequalities: A Nonasymptotic Theory of Independence. Oxford University Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization. Cambridge\\nUniversity Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\\nbra. Cambridge University Press.\\nBrochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian\\nOptimization of Expensive Cost Functions, with Application to Active User Modeling\\nand Hierarchical Reinforcement Learning . Tech. rept. TR-2009-023. Department of\\nComputer Science, University of British Columbia.\\nBrooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011.Hand-\\nbook of Markov Chain Monte Carlo. Chapman and Hall/CRC.\\nBrown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\\nplications in Statistical Decision Theory. Institute of Mathematical Statistics.\\nBryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation\\nProcesses. In: Proceedings of the Harvard University Symposium on Digital Computers\\nand Their Applications.\\nBubeck, S´ebastien. 2015. Convex Optimization: Algorithms and Complexity . Founda-\\ntions and Trends in Machine Learning, 8(3-4), 231–357.\\nB¨uhlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data .\\nSpringer.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b9c09263-1747-423f-8dab-e555e5472e5d', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 402, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 397\\nBurges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and\\nTrends in Machine Learning, 2(4), 275–365.\\nCarroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in\\nMultidimensional Scaling via an N-Way Generalization of “Eckart-Young” Decom-\\nposition. Psychometrika, 35(3), 283–319.\\nCasella, George, and Berger, Roger L. 2002. Statistical Inference. Duxbury .\\nC ¸inlar, Erhan. 2011.Probability and Stochastics. Springer.\\nChang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector\\nMachines. ACM Transactions on Intelligent Systems and Technology, 2, 27:1–27:27.\\nCheeseman, Peter. 1985. In Defense of Probability . In:Proceedings of the International\\nJoint Conference on Artificial Intelligence.\\nChollet, Francois, and Allaire, J. J. 2018. Deep Learning with R. Manning Publications.\\nCodd, Edgar F. 1990. The Relational Model for Database Management. Addison-Wesley\\nLongman Publishing.\\nCunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Reduc-\\ntion: Survey , Insights, and Generalizations. Journal of Machine Learning Research ,\\n16, 2859–2900.\\nDatta, Biswa N. 2010. Numerical Linear Algebra and Applications. SIAM.\\nDavidson, Anthony C., and Hinkley , David V. 1997.Bootstrap Methods and Their Appli-\\ncation. Cambridge University Press.\\nDean, Jeffrey , Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale\\nDistributed Deep Networks. In: Advances in Neural Information Processing Systems.\\nDeisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus-\\nsian Process Dynamical Systems. Pages 2618–2626 of: Advances in Neural Informa-\\ntion Processing Systems.\\nDeisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian\\nFiltering and Smoothing: Explaining Current and Deriving New Algorithms. In:\\nProceedings of the American Control Conference.\\nDeisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\\nfor Data-Efficient Learning in Robotics and Control. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 37(2), 408–423.\\nDempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood\\nfrom Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society,\\n39(1), 1–38.\\nDeng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and\\nHinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep\\nAuto-Encoder. In: Proceedings of Interspeech.\\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer.\\nDonoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear\\nEmbedding Techniques for High-Dimensional Data. Proceedings of the National\\nAcademy of Sciences, 100(10), 5591–5596.\\nDost´al, Zden˘ek. 2009. Optimal Quadratic Programming Algorithms: With Applications\\nto Variational Inequalities. Springer.\\nDouven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy . Meta-\\nphysics Research Lab, Stanford University .\\nDowney , Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O’Reilly\\nMedia.\\nDreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\\nMathematical Analysis and Applications, 5(1), 30–45.\\nDrumm, Volker, and Weil, Wolfgang. 2001.Lineare Algebra und Analytische Geometrie.\\nLecture Notes, Universit¨at Karlsruhe (TH).\\nDudley , Richard M. 2002.Real Analysis and Probability. Cambridge University Press.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fc24e0ec-1455-4c07-961c-36a061659583', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 403, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='398 References\\nEaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach . Institute of\\nMathematical Statistics Lecture Notes.\\nEckart, Carl, and Young, Gale. 1936. The Approximation of One Matrix by Another of\\nLower Rank. Psychometrika, 1(3), 211–218.\\nEfron, Bradley , and Hastie, Trevor. 2016.Computer Age Statistical Inference: Algorithms,\\nEvidence and Data Science. Cambridge University Press.\\nEfron, Bradley , and Tibshirani, Robert J. 1993.An Introduction to the Bootstrap. Chap-\\nman and Hall/CRC.\\nElliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-\\ntional Programming.\\nEvgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\\nLearning Theory: A Primer. International Journal of Computer Vision, 38(1), 9–13.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.\\n2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine\\nLearning Research, 9, 1871–1874.\\nGal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variational\\nInference in Sparse Gaussian Process Regression and Latent Variable Models. In:\\nAdvances in Neural Information Processing Systems.\\nG¨artner, Thomas. 2008. Kernels for Structured Data. World Scientific.\\nGavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\\nValues is 4\\n√\\n3. IEEE Transactions on Information Theory, 60(8), 5040–5053.\\nGelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Bayesian\\nData Analysis. Chapman and Hall/CRC.\\nGentle, James E. 2004. Random Number Generation and Monte Carlo Methods .\\nSpringer.\\nGhahramani, Zoubin. 2015. Probabilistic Machine Learning and Artificial Intelligence.\\nNature, 521, 452–459.\\nGhahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Sys-\\ntems Using an EM Algorithm. In:Advances in Neural Information Processing Systems.\\nMIT Press.\\nGilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\\nMonte Carlo in Practice. Chapman and Hall/CRC.\\nGneiting, Tilmann, and Raftery , Adrian E. 2007. Strictly Proper Scoring Rules, Pre-\\ndiction, and Estimation. Journal of the American Statistical Association , 102(477),\\n359–378.\\nGoh, Gabriel. 2017. Why Momentum Really Works. Distill.\\nGohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Determi-\\nnants of Linear Operators. Birkh¨auser.\\nGolan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\\nKnow. Springer.\\nGolub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations. JHU Press.\\nGoodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning. MIT\\nPress.\\nGraepel, Thore, Candela, Joaquin Qui ˜nonero-Candela, Borchert, Thomas, and Her-\\nbrich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Sponsored\\nSearch Advertising in Microsoft’s Bing Search Engine. In:Proceedings of the Interna-\\ntional Conference on Machine Learning.\\nGriewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Differenti-\\nation. In: Proceedings in Applied Mathematics and Mechanics.\\nGriewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\\nTechniques of Algorithmic Differentiation. SIAM.\\nGrimmett, Geoffrey R., and Welsh, Dominic. 2014.Probability: An Introduction. Oxford\\nUniversity Press.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='daa9b963-9a5f-41c8-8044-2cc67626a52a', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 404, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 399\\nGrinstead, Charles M., and Snell, J. Laurie. 1997.Introduction to Probability. American\\nMathematical Society .\\nHacking, Ian. 2001. Probability and Inductive Logic. Cambridge University Press.\\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer.\\nHallin, Marc, Paindaveine, Davy , and ˇSiman, Miroslav. 2010. Multivariate Quan-\\ntiles and Multiple-Output Regression Quantiles: From ℓ1 Optimization to Halfspace\\nDepth. Annals of Statistics, 38, 635–669.\\nHasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a\\nPanorama of Recent Developments. Cambridge University Press.\\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\\ntistical Learning – Data Mining, Inference, and Prediction. Springer.\\nHausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,\\nMartin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:\\nProceedings of the International Conference on Learning Representations.\\nHazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and\\nTrends in Optimization, 2(3–4), 157–325.\\nHensman, James, Fusi, Nicol `o, and Lawrence, Neil D. 2013. Gaussian Processes for\\nBig Data. In: Proceedings of the Conference on Uncertainty in Artificial Intelligence.\\nHerbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\\nSkill Rating System. In: Advances in Neural Information Processing Systems.\\nHiriart-Urruty , Jean-Baptiste, and Lemar´echal, Claude. 2001. Fundamentals of Convex\\nAnalysis. Springer.\\nHoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for\\nLatent Dirichlet Allocation. Advances in Neural Information Processing Systems.\\nHoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley , John. 2013. Stochas-\\ntic Variational Inference. Journal of Machine Learning Research, 14(1), 1303–1347.\\nHofmann, Thomas, Sch¨olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\\nods in Machine Learning. Annals of Statistics, 36(3), 1171–1220.\\nHogben, Leslie. 2013. Handbook of Linear Algebra. Chapman and Hall/CRC.\\nHorn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis. Cambridge University\\nPress.\\nHotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal\\nComponents. Journal of Educational Psychology, 24, 417–441.\\nHyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001.Independent Component Anal-\\nysis. Wiley .\\nImbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\\nand Biomedical Sciences. Cambridge University Press.\\nJacod, Jean, and Protter, Philip. 2004. Probability Essentials. Springer.\\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge University\\nPress.\\nJefferys, William H., and Berger, James O. 1992. Ockham’s Razor and Bayesian Anal-\\nysis. American Scientist, 80, 64–72.\\nJeffreys, Harold. 1961. Theory of Probability. Oxford University Press.\\nJimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Nor-\\nmalizing Flows. In: Proceedings of the International Conference on Machine Learning.\\nJimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\\nBackpropagation and Approximate Inference in Deep Generative Models. In: Pro-\\nceedings of the International Conference on Machine Learning.\\nJoachims, Thorsten. 1999. Advances in Kernel Methods – Support Vector Learning. MIT\\nPress. Chap. Making Large-Scale SVM Learning Practical, pages 169–184.\\nJordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K.\\n1999. An Introduction to Variational Methods for Graphical Models.Machine Learn-\\ning, 37, 183–233.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='84bce92a-3190-471c-a196-fd55808f9880', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 405, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='400 References\\nJulier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter\\nto Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense\\nSensing, Simulation and Controls.\\nKaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but\\nShort Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS\\nComputational Biology, 2(7), e95.\\nKalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. Col-\\nlege Mathematics Journal, 27(1), 2–23.\\nKalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Problems.\\nTransactions of the ASME – Journal of Basic Engineering, 82(Series D), 35–45.\\nKamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efficient Reinforcement Learning\\nwith Probabilistic Model Predictive Control. In: Proceedings of the International\\nConference on Artificial Intelligence and Statistics.\\nKatz, Victor J. 2004. A History of Mathematics. Pearson/Addison-Wesley .\\nKelley , Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal, 30(10),\\n947–954.\\nKimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayesian\\nEstimation on Stochastic Processes and Smoothing by Splines. Annals of Mathemat-\\nical Statistics, 41(2), 495–502.\\nKingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\\nProceedings of the International Conference on Learning Representations.\\nKittler, Josef, and F¨oglein, Janos. 1984. Contextual Classification of Multispectral Pixel\\nData. Image and Vision Computing, 2(1), 13–29.\\nKolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications.\\nSIAM Review, 51(3), 455–500.\\nKoller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models. MIT Press.\\nKong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with\\nMultivariate Data. Statistica Sinica, 22, 1598–1610.\\nLang, Serge. 1987. Linear Algebra. Springer.\\nLawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with\\nGaussian Process Latent Variable Models. Journal of Machine Learning Research ,\\n6(Nov.), 1783–1816.\\nLeemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution\\nRelationships. American Statistician, 62(1), 45–53.\\nLehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses .\\nSpringer.\\nLehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation. Springer.\\nLiesen, J¨org, and Mehrmann, Volker. 2015. Linear Algebra. Springer.\\nLin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt’s Probabilistic\\nOutputs for Support Vector Machines. Machine Learning, 68, 267–276.\\nLjung, Lennart. 1999. System Identification: Theory for the User. Prentice Hall.\\nLoosli, Ga¨elle, Canu, St´ephane, and Ong, Cheng Soon. 2016. Learning SVM in Kre ˘ın\\nSpaces. IEEE Transactions of Pattern Analysis and Machine Intelligence, 38(6), 1204–\\n1216.\\nLuenberger, David G. 1969. Optimization by Vector Space Methods. Wiley .\\nMacKay , David J. C. 1992. Bayesian Interpolation.Neural Computation, 4, 415–447.\\nMacKay , David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of:\\nBishop, C. M. (ed), Neural Networks and Machine Learning. Springer.\\nMacKay , David J. C. 2003. Information Theory, Inference, and Learning Algorithms .\\nCambridge University Press.\\nMagnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Appli-\\ncations in Statistics and Econometrics. Wiley .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7137cc06-77ff-477a-90bb-d3beaf2057ee', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 406, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 401\\nManton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing\\nKernel Hilbert Spaces. Foundations and Trends in Signal Processing, 8(1–2), 1–126.\\nMarkovsky , Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-\\ncations. Springer.\\nMaybeck, Peter S. 1979. Stochastic Models, Estimation, and Control. Academic Press.\\nMcCullagh, Peter, and Nelder, John A. 1989.Generalized Linear Models. CRC Press.\\nMcEliece, Robert J., MacKay , David J. C., and Cheng, Jung-Fu. 1998. Turbo Decoding\\nas an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected\\nAreas in Communications, 16(2), 140–152.\\nMika, Sebastian, R ¨atsch, Gunnar, Weston, Jason, Sch ¨olkopf, Bernhard, and M ¨uller,\\nKlaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of:\\nProceedings of the Workshop on Neural Networks for Signal Processing.\\nMinka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference .\\nPh.D. thesis, Massachusetts Institute of Technology .\\nMinka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in\\nNeural Information Processing Systems.\\nMitchell, Tom. 1997. Machine Learning. McGraw-Hill.\\nMnih, Volodymyr, Kavukcuoglu, Koray , and Silver, David, et al. 2015. Human-Level\\nControl through Deep Reinforcement Learning. Nature, 518, 529–533.\\nMoonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,\\nArchitectures and Applications. Elsevier.\\nMoustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Mod-\\neling. American Cancer Society . Pages 1–10.\\nM¨uller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with\\nPython: A Guide for Data Scientists. O’Reilly Publishing.\\nMurphy , Kevin P. 2012.Machine Learning: A Probabilistic Perspective. MIT Press.\\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks. Ph.D. thesis, Depart-\\nment of Computer Science, University of Toronto.\\nNeal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that\\nJustifies Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in\\nGraphical Models. MIT Press.\\nNelsen, Roger. 2006. An Introduction to Copulas. Springer.\\nNesterov, Yuri. 2018. Lectures on Convex Optimization. Springer.\\nNeumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tu-\\ntorial on Regularization. SIAM Review, 40, 636–666.\\nNocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization. Springer.\\nNowozin, Sebastian, Gehler, Peter V., Jancsary , Jeremy , and Lampert, Christoph H.\\n(eds). 2014. Advanced Structured Prediction. MIT Press.\\nO’Hagan, Anthony . 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning\\nand Inference, 29, 245–260.\\nOng, Cheng Soon, Mary , Xavier, Canu, St´ephane, and Smola, Alexander J. 2004. Learn-\\ning with Non-Positive Kernels. In: Proceedings of the International Conference on\\nMachine Learning.\\nOrmoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.\\nLearning and Tracking Cyclic Human Motion. In: Advances in Neural Information\\nProcessing Systems.\\nPage, Lawrence, Brin, Sergey , Motwani, Rajeev, and Winograd, Terry . 1999. The\\nPageRank Citation Ranking: Bringing Order to the Web . Tech. rept. Stanford Info-\\nLab.\\nPaquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models. Ph.D. thesis, Uni-\\nversity of Cambridge.\\nParzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.\\nAnnals of Mathematical Statistics, 33(3), 1065–1076.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d8a836c6-1286-4ade-94cb-f6eac76635e5', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 407, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='402 References\\nPearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\\nInference. Morgan Kaufmann.\\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference . 2nd edn. Cambridge\\nUniversity Press.\\nPearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew\\nVariation in Homogeneous Material. Philosophical Transactions of the Royal Society\\nA: Mathematical, Physical and Engineering Sciences, 186, 343–414.\\nPearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space.\\nPhilosophical Magazine, 2(11), 559–572.\\nPeters, Jonas, Janzing, Dominik, and Sch ¨olkopf, Bernhard. 2017. Elements of Causal\\nInference: Foundations and Learning Algorithms. MIT Press.\\nPetersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook. Tech. rept.\\nTechnical University of Denmark.\\nPlatt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Compar-\\nisons to Regularized Likelihood Methods. In: Advances in Large Margin Classifiers.\\nPollard, David. 2002. A User’s Guide to Measure Theoretic Probability . Cambridge\\nUniversity Press.\\nPolyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages\\n437–507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\\nData Sciences. Springer.\\nPress, William H., Teukolsky , Saul A., Vetterling, William T., and Flannery , Brian P.\\n2007. Numerical Recipes: The Art of Scientific Computing . Cambridge University\\nPress.\\nProschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Condi-\\ntional Expectation. American Statistician, 52(3), 248–252.\\nRaschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine\\nLearning and Deep Learning with Python, scikit-learn, and TensorFlow. Packt Publish-\\ning.\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam’s Razor. In: Advances in\\nNeural Information Processing Systems.\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-\\nvances in Neural Information Processing Systems.\\nRasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Ma-\\nchine Learning. MIT Press.\\nReid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for\\nBinary Experiments. Journal of Machine Learning Research, 12, 731–817.\\nRifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Duality .\\nJournal of Machine Learning Research, 8, 441–479.\\nRockafellar, Ralph T. 1970. Convex Analysis. Princeton University Press.\\nRogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning. Chap-\\nman and Hall/CRC.\\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal\\nInference. Harvard University Press.\\nRosenblatt, Murray . 1956. Remarks on Some Nonparametric Estimates of a Density\\nFunction. Annals of Mathematical Statistics, 27(3), 832–837.\\nRoweis, Sam T. 1998. EM Algorithms for PCA and SPCA. Pages 626–632 of: Advances\\nin Neural Information Processing Systems.\\nRoweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\\nModels. Neural Computation, 11(2), 305–345.\\nRoy , Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for\\nStatistics. Chapman and Hall/CRC.\\nRubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo\\nMethod. Wiley .\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='945a3173-06fc-4a1f-b4b7-b4bf8e0888ec', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 408, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 403\\nRuffini, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la\\nSoluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto. Stampe-\\nria di S. Tommaso d’Aquino.\\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning\\nRepresentations by Back-Propagating Errors. Nature, 323(6088), 533–536.\\nSæmundsson, Steind´or, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-\\nforcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the\\nConference on Uncertainty in Artificial Intelligence.\\nSaitoh, Saburou. 1988. Theory of Reproducing Kernels and its Applications . Longman\\nScientific and Technical.\\nS¨arkk¨a, Simo. 2013. Bayesian Filtering and Smoothing. Cambridge University Press.\\nSch¨olkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels – Support\\nVector Machines, Regularization, Optimization, and Beyond. MIT Press.\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1997. Kernel\\nPrincipal Component Analysis. In: Proceedings of the International Conference on\\nArtificial Neural Networks.\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M¨uller, Klaus-Robert. 1998. Nonlinear\\nComponent Analysis as a Kernel Eigenvalue Problem. Neural Computation, 10(5),\\n1299–1319.\\nSch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\\nRepresenter Theorem. In: Proceedings of the International Conference on Computa-\\ntional Learning Theory.\\nSchwartz, Laurent. 1964. Sous Espaces Hilbertiens d’Espaces Vectoriels Topologiques\\net Noyaux Associ´es. Journal d’Analyse Math´ematique, 13, 115–256.\\nSchwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics,\\n6(2), 461–464.\\nShahriari, Bobak, Swersky , Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando.\\n2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\\nProceedings of the IEEE, 104(1), 148–175.\\nShalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:\\nFrom Theory to Algorithms. Cambridge University Press.\\nShawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Analysis.\\nCambridge University Press.\\nShawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodologies\\nin Support Vector Machines. Neurocomputing, 74(17), 3609–3618.\\nShental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny , and Dolev, Danny . 2008.\\nGaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863–\\n1867 of: Proceedings of the International Symposium on Information Theory.\\nShewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method with-\\nout the Agonizing Pain.\\nShi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888–905.\\nShi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,\\nand Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of\\nMachine Learning Research, 2615–2637.\\nShiryayev, Albert N. 1984. Probability. Springer.\\nShor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions. Springer.\\nShotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-\\nBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recog-\\nnition and Segmentation. In: Proceedings of the European Conference on Computer\\nVision.\\nSmith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Criteria\\nfor Linear Models. Journal of the Royal Statistical Society B, 42(2), 213–220.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c2bf8631-2f3a-459a-a7b7-cc140389ab01', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 409, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='404 References\\nSnoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-\\ntimization of Machine Learning Algorithms. In: Advances in Neural Information\\nProcessing Systems.\\nSpearman, Charles. 1904. “General Intelligence,” Objectively Determined and Mea-\\nsured. American Journal of Psychology, 15(2), 201–292.\\nSriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch ¨olkopf, Bernhard,\\nand Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Proba-\\nbility Measures. Journal of Machine Learning Research, 11, 1517–1561.\\nSteinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.\\nConstructive Approximation, 26, 225–287.\\nSteinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines. Springer.\\nStoer, Josef, and Burlirsch, Roland. 2002.Introduction to Numerical Analysis. Springer.\\nStrang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American\\nMathematical Monthly, 100(9), 848–855.\\nStrang, Gilbert. 2003. Introduction to Linear Algebra. Wellesley-Cambridge Press.\\nStray , Jonathan. 2016.The Curious Journalist’s Guide to Data. Tow Center for Digital\\nJournalism at Columbia’s Graduate School of Journalism.\\nStrogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\\nNotices of the American Mathematical Society, 61(3), 286–291.\\nSucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level\\nVision. Image and Vision Computing, 12(1), 42–60.\\nSzeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-\\native Study of Energy Minimization Methods for Markov Random Fields with\\nSmoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-\\ntelligence, 30(6), 1068–1080.\\nTandra, Haryono. 2014. The Relationship between the Change of Variable Theorem\\nand the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\\nMathematics, 17(2), 76–83.\\nTenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geometric\\nFramework for Nonlinear Dimensionality Reduction. Science, 290(5500), 2319–\\n2323.\\nTibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal\\nof the Royal Statistical Society B, 58(1), 267–288.\\nTipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Compo-\\nnent Analysis. Journal of the Royal Statistical Society: Series B, 61(3), 611–622.\\nTitsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\\nVariable Model. In: Proceedings of the International Conference on Artificial Intelli-\\ngence and Statistics.\\nToussaint, Marc. 2012. Some Notes on Gradient Descent . https://ipvs.informatik.uni-\\nstuttgart.de/mlr/marc/notes/gradientDescent.pdf.\\nTrefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra. SIAM.\\nTucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analysis.\\nPsychometrika, 31(3), 279–311.\\nVapnik, Vladimir N. 1998. Statistical Learning Theory. Wiley .\\nVapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory . IEEE Transac-\\ntions on Neural Networks, 10(5), 988–999.\\nVapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory. Springer.\\nVishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,\\nKarsten M. 2010. Graph Kernels. Journal of Machine Learning Research, 11, 1201–\\n1242.\\nvon Luxburg, Ulrike, and Sch ¨olkopf, Bernhard. 2011. Statistical Learning Theory:\\nModels, Concepts, and Results. Pages 651–706 of: D. M. Gabbay , S. Hartmann,\\nJ. Woods (ed), Handbook of the History of Logic, vol. 10. Elsevier.\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cbac9a11-27cd-40d9-aa53-52555512d617', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 410, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 405\\nWahba, Grace. 1990. Spline Models for Observational Data. Society for Industrial and\\nApplied Mathematics.\\nWalpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.\\nProbability and Statistics for Engineers and Scientists. Prentice Hall.\\nWasserman, Larry . 2004.All of Statistics. Springer.\\nWasserman, Larry . 2007.All of Nonparametric Statistics. Springer.\\nWhittle, Peter. 2000. Probability via Expectation. Springer.\\nWickham, Hadley . 2014. Tidy Data.Journal of Statistical Software, 59, 1–23.\\nWilliams, Christopher K. I. 1997. Computing with Infinite Networks. In: Advances in\\nNeural Information Processing Systems.\\nYu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv ´ari, Csaba. 2013. Charac-\\nterizing the Representer Theorem. In: Proceedings of the International Conference on\\nMachine Learning.\\nZadrozny , Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-\\nmates from Decision Trees and Naive Bayesian Classifiers. In: Proceedings of the\\nInternational Conference on Machine Learning.\\nZhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach\\nSpaces for Machine Learning.Journal of Machine Learning Research, 10, 2741–2775.\\nZia, Royce K. P., Redish, Edward F., and McKay , Susan R. 2009. Making Sense of the\\nLegendre Transform. American Journal of Physics, 77(614), 614–622.\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1a62684-d183-4d8a-96a2-02af4f3548bc', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 411, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21873759-58d8-4fd7-8c5d-e235815cbab7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 412, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Index\\n1-of-K representation, 364\\nℓ1 norm, 71\\nℓ2 norm, 72\\nabduction, 258\\nAbel-Ruffini theorem, 334\\nAbelian group, 36\\nabsolutely homogeneous, 71\\nactivation function, 315\\naffine mapping, 63\\naffine subspace, 61\\nAkaike information criterion, 288\\nalgebra, 17\\nalgebraic multiplicity , 106\\nanalytic, 143\\nancestral sampling, 340, 364\\nangle, 76\\nassociativity , 24, 26, 36\\nattribute, 253\\naugmented matrix, 29\\nauto-encoder, 343\\nautomatic differentiation, 161\\nautomorphism, 49\\nbackpropagation, 159\\nbasic variable, 30\\nbasis, 44\\nbasis vector, 45\\nBayes factor, 287\\nBayes’ law, 185\\nBayes’ rule, 185\\nBayes’ theorem, 185\\nBayesian GP-LVM, 347\\nBayesian inference, 274\\nBayesian information criterion, 288\\nBayesian linear regression, 303\\nBayesian model selection, 286\\nBayesian network, 278, 283\\nBayesian PCA, 346\\nBernoulli distribution, 205\\nBeta distribution, 206\\nbilinear mapping, 72\\nbijective, 48\\nbinary classification, 370\\nBinomial distribution, 206\\nblind-source separation, 346\\nBorel σ-algebra, 180\\ncanonical basis, 45\\ncanonical feature map, 389\\ncanonical link function, 315\\ncategorical variable, 180\\nCauchy-Schwarz inequality , 75\\nchange-of-variable technique, 219\\ncharacteristic polynomial, 104\\nCholesky decomposition, 114\\nCholesky factor, 114\\nCholesky factorization, 114\\nclass, 370\\nclassification, 315\\nclosure, 36\\ncode, 343\\ncodirected, 105\\ncodomain, 58, 139\\ncollinear, 105\\ncolumn, 22\\ncolumn space, 59\\ncolumn vector, 22, 38\\ncompleting the squares, 307\\nconcave function, 236\\ncondition number, 230\\nconditional probability , 179\\nconditionally independent, 195\\nconjugate, 208\\nconjugate prior, 208\\nconvex conjugate, 242\\nconvex function, 236\\nconvex hull, 386\\nconvex optimization problem, 236, 239\\nconvex set, 236\\ncoordinate, 50\\ncoordinate representation, 50\\ncoordinate vector, 50\\ncorrelation, 191\\ncovariance, 190\\ncovariance matrix, 190, 198\\ncovariate, 253\\nCP decomposition, 136\\ncross-covariance, 191\\ncross-validation, 258, 263\\ncumulative distribution function, 178,\\n181\\nd-separation, 281\\n407\\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1471b4a4-31fb-42b5-8250-ebc20bbe77c7', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 413, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='408 Index\\ndata covariance matrix, 318\\ndata point, 253\\ndata-fit term, 302\\ndecoder, 343\\ndeep auto-encoder, 347\\ndefective, 111\\ndenominator layout, 151\\nderivative, 141\\ndesign matrix, 294, 296\\ndeterminant, 99\\ndiagonal matrix, 115\\ndiagonalizable, 116\\ndiagonalization, 116\\ndifference quotient, 141\\ndimension, 45\\ndimensionality reduction, 317\\ndirected graphical model, 278, 283\\ndirection, 61\\ndirection space, 61\\ndistance, 75\\ndistribution, 177\\ndistributivity , 24, 26\\ndomain, 58, 139\\ndot product, 72\\ndual SVM, 385\\nEckart-Young theorem, 131, 334\\neigendecomposition, 116\\neigenspace, 106\\neigenspectrum, 106\\neigenvalue, 105\\neigenvalue equation, 105\\neigenvector, 105\\nelementary transformations, 28\\nEM algorithm, 360\\nembarrassingly parallel, 264\\nempirical covariance, 192\\nempirical mean, 192\\nempirical risk, 260\\nempirical risk minimization, 257, 260\\nencoder, 343\\nendomorphism, 49\\nepigraph, 236\\nequivalent, 56\\nerror function, 294\\nerror term, 382\\nEuclidean distance, 72, 75\\nEuclidean norm, 72\\nEuclidean vector space, 73\\nevent space, 175\\nevidence, 186, 285, 306\\nexample, 253\\nexpected risk, 261\\nexpected value, 187\\nexponential family , 205, 211\\nextended Kalman filter, 170\\nfactor analysis, 346\\nfactor graph, 283\\nfeature, 253\\nfeature map, 254\\nfeature matrix, 296\\nfeature vector, 295\\nFisher discriminant analysis, 136\\nFisher-Neyman theorem, 210\\nforward mode, 161\\nfree variable, 30\\nfull rank, 47\\nfull SVD, 128\\nfundamental theorem of linear\\nmappings, 60\\nGaussian elimination, 31\\nGaussian mixture model, 349\\nGaussian process, 316\\nGaussian process latent-variable model,\\n347\\ngeneral linear group, 37\\ngeneral solution, 28, 30\\ngeneralized linear model, 272, 315\\ngenerating set, 44\\ngenerative process, 272, 286\\ngenerator, 344\\ngeometric multiplicity , 108\\nGivens rotation, 94\\nglobal minimum, 225\\nGP-LVM, 347\\ngradient, 146\\nGram matrix, 389\\nGram-Schmidt orthogonalization, 89\\ngraphical model, 278\\ngroup, 36\\nHadamard product, 23\\nhard margin SVM, 377\\nHessian, 164\\nHessian eigenmaps, 136\\nHessian matrix, 165\\nhinge loss, 381\\nhistogram, 369\\nhyperparameter, 258\\nhyperplane, 61, 62\\nhyperprior, 281\\ni.i.d., 195\\nICA, 346\\nidentity automorphism, 49\\nidentity mapping, 49\\nidentity matrix, 23\\nimage, 58, 139\\nindependent and identically distributed,\\n195, 260, 266\\nindependent component analysis, 346\\ninference network, 344\\ninjective, 48\\ninner product, 73\\ninner product space, 73\\nintermediate variables, 162\\ninverse, 24\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ff20162a-fb90-4bee-87df-29fb04f10e91', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 414, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Index 409\\ninverse element, 36\\ninvertible, 24\\nIsomap, 136\\nisomorphism, 49\\nJacobian, 146, 150\\nJacobian determinant, 152\\nJeffreys-Lindley paradox, 287\\nJensen’s inequality , 239\\njoint probability , 178\\nKarhunen-Lo`eve transform, 318\\nkernel, 33, 47, 58, 254, 388\\nkernel density estimation, 369\\nkernel matrix, 389\\nkernel PCA, 347\\nkernel trick, 316, 347, 389\\nlabel, 253\\nLagrange multiplier, 234\\nLagrangian, 234\\nLagrangian dual problem, 234\\nLaplace approximation, 170\\nLaplace expansion, 102\\nLaplacian eigenmaps, 136\\nLASSO, 303, 316\\nlatent variable, 275\\nlaw, 177, 181\\nlaw of total variance, 203\\nleading coefficient, 30\\nleast-squares loss, 154\\nleast-squares problem, 261\\nleast-squares solution, 88\\nleft-singular vectors, 119\\nLegendre transform, 242\\nLegendre-Fenchel transform, 242\\nlength, 71\\nlikelihood, 185, 265, 269, 291\\nline, 61, 82\\nlinear combination, 40\\nlinear manifold, 61\\nlinear mapping, 48\\nlinear program, 239\\nlinear subspace, 39\\nlinear transformation, 48\\nlinearly dependent, 40\\nlinearly independent, 40\\nlink function, 272\\nloading, 322\\nlocal minimum, 225\\nlog-partition function, 211\\nlogistic regression, 315\\nlogistic sigmoid, 315\\nloss function, 260, 381\\nloss term, 382\\nlower-triangular matrix, 101\\nMaclaurin series, 143\\nManhattan norm, 71\\nMAP, 300\\nMAP estimation, 269\\nmargin, 374\\nmarginal, 190\\nmarginal likelihood, 186, 286, 306\\nmarginal probability , 179\\nmarginalization property , 184\\nMarkov random field, 283\\nmatrix, 22\\nmatrix factorization, 98\\nmaximum a posteriori, 300\\nmaximum a posteriori estimation, 269\\nmaximum likelihood, 257\\nmaximum likelihood estimate, 296\\nmaximum likelihood estimation, 265,\\n293\\nmean, 187\\nmean function, 309\\nmean vector, 198\\nmeasure, 180\\nmedian, 188\\nmetric, 76\\nminimal, 44\\nminimax inequality , 234\\nmisfit term, 302\\nmixture model, 349\\nmixture weight, 349\\nmode, 188\\nmodel, 251\\nmodel evidence, 286\\nmodel selection, 258\\nMoore-Penrose pseudo-inverse, 35\\nmultidimensional scaling, 136\\nmultiplication by scalars, 37\\nmultivariate, 178\\nmultivariate Gaussian distribution, 198\\nmultivariate Taylor series, 166\\nnatural parameters, 212\\nnegative log-likelihood, 265\\nnested cross-validation, 258, 284\\nneutral element, 36\\nnoninvertible, 24\\nnonsingular, 24\\nnorm, 71\\nnormal distribution, 197\\nnormal equation, 86\\nnormal vector, 80\\nnull space, 33, 47, 58\\nnumerator layout, 150\\nOccam’s razor, 285\\nONB, 79\\none-hot encoding, 364\\nordered basis, 50\\northogonal, 77\\northogonal basis, 79\\northogonal complement, 79\\northogonal matrix, 78\\northonormal, 77\\northonormal basis, 79\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ff5c449-cbff-4f44-b47f-89fa997796a2', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 415, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='410 Index\\nouter product, 38\\noverfitting, 262, 271, 299\\nPageRank, 114\\nparameters, 61\\nparametric equation, 61\\npartial derivative, 146\\nparticular solution, 27, 30\\nPCA, 317\\npdf, 181\\npenalty term, 263\\npivot, 30\\nplane, 62\\nplate, 281\\npopulation mean and covariance, 191\\npositive definite, 71, 73, 74, 76\\nposterior, 185, 269\\nposterior odds, 287\\npower iteration, 334\\npower series representation, 145\\nPPCA, 340\\npreconditioner, 230\\npredictor, 12, 255\\nprimal problem, 234\\nprincipal component, 322\\nprincipal component analysis, 136, 317\\nprincipal subspace, 327\\nprior, 185, 269\\nprior odds, 287\\nprobabilistic inverse, 186\\nprobabilistic PCA, 340\\nprobabilistic programming, 278\\nprobability , 175\\nprobability density function, 181\\nprobability distribution, 172\\nprobability integral transform, 217\\nprobability mass function, 178\\nproduct rule, 184\\nprojection, 82\\nprojection error, 88\\nprojection matrix, 82\\npseudo-inverse, 86\\nrandom variable, 172, 175\\nrange, 58\\nrank, 47\\nrank deficient, 47\\nrank-k approximation, 130\\nrank-nullity theorem, 60\\nraw-score formula for variance, 193\\nrecognition network, 344\\nreconstruction error, 88, 327\\nreduced hull, 388\\nreduced row-echelon form, 31\\nreduced SVD, 129\\nREF, 30\\nregression, 289\\nregular, 24\\nregularization, 262, 302, 382\\nregularization parameter, 263, 302, 380\\nregularized least squares, 302\\nregularizer, 263, 302, 380, 382\\nrepresenter theorem, 384\\nresponsibility , 352\\nreverse mode, 161\\nright-singular vectors, 119\\nRMSE, 298\\nroot mean square error, 298\\nrotation, 91\\nrotation matrix, 92\\nrow, 22\\nrow vector, 22, 38\\nrow-echelon form, 30\\nsample mean, 192\\nsample space, 175\\nscalar, 37\\nscalar product, 72\\nsigmoid, 213\\nsimilar, 56\\nsingular, 24\\nsingular value decomposition, 119\\nsingular value equation, 124\\nsingular value matrix, 119\\nsingular values, 119\\nslack variable, 379\\nsoft margin SVM, 379, 380\\nsolution, 20\\nspan, 44\\nspecial solution, 27\\nspectral clustering, 136\\nspectral norm, 131\\nspectral theorem, 111\\nspectrum, 106\\nsquare matrix, 25\\nstandard basis, 45\\nstandard deviation, 190\\nstandard normal distribution, 198\\nstandardization, 336\\nstatistical independence, 194\\nstatistical learning theory , 265\\nstochastic gradient descent, 231\\nstrong duality , 236\\nsufficient statistics, 210\\nsum rule, 184\\nsupport point, 61\\nsupport vector, 384\\nsupporting hyperplane, 242\\nsurjective, 48\\nSVD, 119\\nSVD theorem, 119\\nsymmetric, 73, 76\\nsymmetric matrix, 25\\nsymmetric, positive definite, 74\\nsymmetric, positive semidefinite, 74\\nsystem of linear equations, 20\\ntarget space, 175\\nDraft (2024-01-15) of “Mathematics for Machine Learning”. Feedback:https://mml-book.com.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='966f4747-20a9-4c49-b647-bcb7c56fe1e9', embedding=None, metadata={'file_name': 'mml-book.pdf', 'page_num': 416, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Index 411\\nTaylor polynomial, 142, 166\\nTaylor series, 142\\ntest error, 300\\ntest set, 262, 284\\nTikhonov regularization, 265\\ntrace, 103\\ntraining, 12\\ntraining error, 300\\ntraining set, 260, 292\\ntransfer function, 315\\ntransformation matrix, 51\\ntranslation vector, 63\\ntranspose, 25, 38\\ntriangle inequality , 71, 76\\ntruncated SVD, 129\\nTucker decomposition, 136\\nunderfitting, 271\\nundirected graphical model, 283\\nuniform distribution, 182\\nunivariate, 178\\nunscented transform, 170\\nupper-triangular matrix, 101\\nvalidation set, 263, 284\\nvariable selection, 316\\nvariance, 190\\nvector, 37\\nvector addition, 37\\nvector space, 37\\nvector space homomorphism, 48\\nvector space with inner product, 73\\nvector subspace, 39\\nweak duality , 235\\nzero-one loss, 381\\n©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='937b2e07-3fef-4508-817d-f112cf67d8f9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 0, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='efde02ff-1e86-4663-9cfe-15e1d7c061cb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 1, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Springer Series in Statistics\\nAdvisors:\\nP . Bickel, P . Diggle, S. Fienberg, U. Gather,\\nI. Olkin, S. Zeger', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f1a36c04-2889-4b9a-b4d9-060f88b22dc8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 2, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Springer Series in Statistics\\nFor other titles published in this series, go to\\nhttp://www.springer.com/series/692', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f41fa761-2b61-4bea-a8eb-f4e840e05bae', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 3, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Trevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical\\nSecond Edition\\nLearning', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1d325f8f-fcac-4bae-a898-e256e6308c82', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 4, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='c\\nAll rights reserved. This work may not be translated or copied in whole or in part without the written\\npermission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY\\n10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection\\nwith any form of information storage and retrieval, electronic adaptation, computer software, or by similar\\nor dissimilar methodology now known or hereafter developed is forbidden.\\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are\\nnot identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are subject\\nto proprietary rights.\\nspringer.com\\nTrevor Hastie\\nStanford University\\nDept. of Statistics\\nStanford CA 94305\\nUSA\\nRobert Tibshirani\\nStanford University\\nDept. of Statistics\\nStanford CA 94305\\nJerome Friedman\\nStanford University\\nDept. of Statistics\\nStanford CA 94305\\nUSA\\nLibrary of Congress Control Number: 2008941148\\nUSA\\nhastie@stanford.edu tibs@stanford.edu\\njhf@stanford.edu\\nISSN: 0172-7397\\ne-ISBN: 978-0-387-84858-7ISBN: 978-0-387-84857-0\\n⃝ Springer Science+Business Media, LLC 2009, Corrected at 12th printing 2017\\nDOI: 10.1007/b94608\\nPrinted on acid-free paper', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9801146d-3650-4db2-b629-08213acd9a56', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 5, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\na n dt oo u rf a m i l i e s :\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17a10ab4-876d-4147-86ae-ccb9532bbf2d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 6, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning.This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existing\\nchapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Here is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Deming and Robert W.\\nHayden; however Professor Hayden told us that he can claim no credit for this quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming actually said this.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f1e7c116-c17c-4a0a-bc01-e7c8f6e0718c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 7, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='viii Preface to the Second Edition\\nChapter What’s new\\n1. Introduction\\n2. Overview of Supervised Learning\\n3. Linear Methods for Regression LAR algorithm and generalizations\\nof the lasso\\n4. Linear Methods for Classiﬁcation Lasso path for logistic regression\\n5. Basis Expansions and Regulariza-\\ntion\\nAdditional illustrations of RKHS\\n6. Kernel Smoothing Methods\\n7. Model Assessment and Selection Strengths and pitfalls of cross-\\nvalidation\\n8. Model Inference and Averaging\\n9. Additive Models, Trees, and\\nRelated Methods\\n10. Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11. Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12. Support Vector Machines and\\nFlexible Discriminants\\nPath algorithm for SVM classiﬁer\\n13. Prototype Methods and\\nNearest-Neighbors\\n14. Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a\\ndirect approach to ICA\\n15. Random Forests New\\n16. Ensemble Learning New\\n17. Undirected Graphical Models New\\n18. High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in particular,\\nwe tended to favorred/green contrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with anorange/blue contrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to\\n“Kernel Smoothing Methods”, to avoid confusion with the machine-\\nlearning kernel method that is discussed in the context ofsupport vec-\\ntor machines (Chapter 12) and more generally in Chapters 5 and 14.\\n•In the ﬁrst edition, the discussion of error-rate estimation in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notions of\\nconditional error rates (conditional on the training set) and uncondi-\\ntional rates. We have ﬁxed this in the new edition.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8f2226e1-4b1d-4439-90ed-c2b23216eb2b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 8, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface to the Second Edition ix\\n•Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\\nters are probably best read in that order.\\n•In Chapter 17, we have not attempted a comprehensive treatment\\nof graphical models, and discuss only undirected models and some\\nnew methods for their estimation. Due to a lack of space, we have\\nspeciﬁcally omitted coverage of directed graphical models.\\n•Chapter 18 explores the “p≫ N” problem, which is learning in high-\\ndimensional feature spaces. These problems arise in many areas, in-\\ncluding genomic and proteomic studies, and document classiﬁcation.\\nWe thank the many readers who have found the (too numerous) errors in\\nthe ﬁrst edition. We apologize for those and have done our best to avoid er-\\nrors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\\nWasserman for comments on some of the new chapters, and many Stanford\\ngraduate and post-doctoral students who oﬀered comments, in particular\\nMohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Maleki, Donal\\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\\nHui Zou. We thank John Kimmel for his patience in guiding us through this\\nnew edition. RT dedicates this edition to the memory of Anna McPhee.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nAugust 2008', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8362fb3-c186-43a2-beb6-84456b0f3794', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 9, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface to the First Edition\\nWe are drowning in information and starving for knowledge.\\n–Rutherford D. Roger\\nThe ﬁeld of Statistics is constantly challenged by the problems that science\\nandindustrybringstoitsdoor.Intheearlydays,theseproblemsoftencame\\nfrom agricultural and industrial experiments and were relatively small in\\nscope. With the advent of computers and the information age, statistical\\nproblems have exploded both in size and complexity. Challenges in the\\nareas of data storage, organization and searching have led to the new ﬁeld\\nof “data mining”; statistical and computational problems in biology and\\nmedicine have created “bioinformatics.” Vast amounts of data are being\\ngenerated in many ﬁelds, and the statistician’s job is to make sense of it\\nall: to extract important patterns and trends, and understand “what the\\ndata says.” We call thislearning from data.\\nThe challenges in learning from data have led to a revolution in the sta-\\ntistical sciences. Since computation plays such a key role, it is not surprising\\nthat much of this new development has been done by researchers in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly categorized as\\neither supervisedor unsupervised. In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;\\nin unsupervised learning, there is no outcome measure, and the goal is to\\ndescribe the associations and patterns among a set of input measures.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac500e2e-8ecf-4cdb-bf71-7bfe6c349b1b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 10, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xii Preface to the First Edition\\nThis book is our attempt to bring together many of the important new\\nideas in learning, and explain them in a statistical framework. While some\\nmathematical details are needed, we emphasize the methods and their con-\\nceptual underpinnings rather than their theoretical properties. As a result,\\nwe hope that this book will appeal not just to statisticians but also to\\nresearchers and practitioners in a wide variety of ﬁelds.\\nJust as we have learned a great deal from researchers outside of the ﬁeld\\nof statistics, our statistical viewpoint may help others to better understand\\ndiﬀerent aspects of learning:\\nThere is no true interpretation of anything; interpretation is a\\nvehicle in the service of human comprehension. The value of\\ninterpretation is in enabling others to fruitfully think about an\\nidea.\\n–Andreas Buja\\nWe would like to acknowledge the contribution of many people to the\\nconception and completion of this book. David Andrews, Leo Breiman,\\nAndreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton, Werner\\nStuetzle, and John Tukey have greatly inﬂuenced our careers. Balasub-\\nramanian Narasimhan gave us advice and help on many computational\\nproblems, and maintained an excellent computing environment. Shin-Ho\\nBang helped in the production of a number of the ﬁgures. Lee Wilkinson\\ngave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Maya\\nGupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\\nZhu, two reviewers and many students read parts of the manuscript and\\noﬀered helpful suggestions. John Kimmel was supportive, patient and help-\\nful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\\nproduction team at Springer. Trevor Hastie would like to thank the statis-\\ntics department at the University of Cape Town for their hospitality during\\nthe ﬁnal stages of this book. We gratefully acknowledge NSF and NIH for\\ntheir support of this work. Finally, we would like to thank our families and\\nour parents for their love and support.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nMay 2001\\nThe quiet statisticians have changed our world; not by discov-\\nering new facts or technical developments, but by changing the\\nways that we reason, experiment and form our opinions ....\\n–Ian Hacking', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4828ea39-50af-4534-95ec-b6b0001d9ebd', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 11, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents\\nPreface to the Second Edition vii\\nPreface to the First Edition xi\\n1 Introduction 1\\n2 Overview of Supervised Learning 9\\n2 . 1 I n t r o d u c t i o n ......................... 9\\n2 . 2 V a r i a b l eT y p e sa n dT e r m i n o l o g y.............. 9\\n2.3 Two Simple Approaches to Prediction:\\nLeast Squares and Nearest Neighbors........... 1 1\\n2.3.1 Linear Models and Least Squares . ....... 1 1\\n2 . 3 . 2 N e a r e s t - N e i g h b o rM e t h o d s............ 1 4\\n2.3.3 From Least Squares to Nearest Neighbors .... 1 6\\n2 . 4 S t a t i s t i c a lD e c i s i o nT h e o r y................. 1 8\\n2 . 5 L o c a lM e t h o d si nH i g hD i m e n s i o n s............. 2 2\\n2.6 Statistical Models, Supervised Learning\\na n dF u n c t i o nA p p r o x i m a t i o n................ 2 8\\n2.6.1 A Statistical Model\\nfor the Joint Distribution Pr(X,Y ) ....... 2 8\\n2 . 6 . 2 S u p e r v i s e dL e a r n i n g................ 2 9\\n2 . 6 . 3 F u n c t i o nA p p r o x i m a t i o n ............. 2 9\\n2 . 7 S t r u c t u r e dR e g r e s s i o nM o d e l s ............... 3 2\\n2 . 7 . 1 D i ﬃ c u l t yo ft h eP r o b l e m ............. 3 2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b7441b02-c169-4680-a6ec-d7cac3805a52', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 12, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xiv Contents\\n2.8 Classes of Restricted Estimators .............. 3 3\\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\\n2 . 8 . 2 K e r n e lM e t h o d sa n dL o c a lR e g r e s s i o n...... 3 4\\n2 . 8 . 3 B a s i sF u n c t i o n sa n dD i c t i o n a r yM e t h o d s .... 3 5\\n2.9 Model Selection and the Bias–Variance Tradeoﬀ ..... 3 7\\nB i b l i o g r a p h i cN o t e s......................... 3 9\\nE x e r c i s e s............................... 3 9\\n3 Linear Methods for Regression 43\\n3 . 1 I n t r o d u c t i o n ......................... 4 3\\n3.2 Linear Regression Models and Least Squares ....... 4 4\\n3 . 2 . 1 E x a m p l e :P r o s t a t eC a n c e r ............ 4 9\\n3 . 2 . 2 T h eG a u s s – M a r k o vT h e o r e m........... 5 1\\n3.2.3 Multiple Regression\\nf r o mS i m p l eU n i v a r i a t eR e g r e s s i o n........ 5 2\\n3 . 2 . 4 M u l t i p l eO u t p u t s ................. 5 6\\n3.3 Subset Selection ....................... 5 7\\n3.3.1 Best-Subset Selection ............... 5 7\\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\\n3 . 3 . 3 F o r w a r d - S t a g e w i s eR e g r e s s i o n .......... 6 0\\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\\n3 . 4 S h r i n k a g eM e t h o d s...................... 6 1\\n3 . 4 . 1 R i d g eR e g r e s s i o n ................. 6 1\\n3 . 4 . 2 T h eL a s s o ..................... 6 8\\n3.4.3 Discussion: Subset Selection, Ridge Regression\\na n dt h eL a s s o ................... 6 9\\n3 . 4 . 4 L e a s tA n g l eR e g r e s s i o n.............. 7 3\\n3.5 Methods Using Derived Input Directions . . ....... 7 9\\n3 . 5 . 1 P r i n c i p a lC o m p o n e n t sR e g r e s s i o n ........ 7 9\\n3.5.2 Partial Least Squares ............... 8 0\\n3.6 Discussion: A Comparison of the Selection\\na n dS h r i n k a g eM e t h o d s................... 8 2\\n3.7 Multiple Outcome Shrinkage and Selection . ....... 8 4\\n3 . 8 M o r eo nt h eL a s s oa n dR e l a t e dP a t hA l g o r i t h m s..... 8 6\\n3.8.1 Incremental Forward Stagewise Regression . . . 86\\n3.8.2 Piecewise-Linear Path Algorithms . ....... 8 9\\n3.8.3 The Dantzig Selector ............... 8 9\\n3 . 8 . 4 T h eG r o u p e dL a s s o................ 9 0\\n3 . 8 . 5 F u r t h e rP r o p e r t i e so ft h eL a s s o.......... 9 1\\n3 . 8 . 6 P a t h w i s eC o o r d i n a t eO p t i m i z a t i o n........ 9 2\\n3 . 9 C o m p u t a t i o n a lC o n s i d e r a t i o n s ............... 9 3\\nB i b l i o g r a p h i cN o t e s......................... 9 4\\nE x e r c i s e s............................... 9 4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ebc2848-1816-44fa-91db-b03f12c08127', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 13, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents xv\\n4 Linear Methods for Classiﬁcation 101\\n4 . 1 I n t r o d u c t i o n ......................... 1 0 1\\n4 . 2 L i n e a rR e g r e s s i o no fa nI n d i c a t o rM a t r i x ......... 1 0 3\\n4 . 3 L i n e a rD i s c r i m i n a n tA n a l y s i s................ 1 0 6\\n4 . 3 . 1 R e g u l a r i z e dD i s c r i m i n a n tA n a l y s i s........ 1 1 2\\n4 . 3 . 2 C o m p u t a t i o n sf o rL D A .............. 1 1 3\\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\\n4 . 4 L o g i s t i cR e g r e s s i o n...................... 1 1 9\\n4 . 4 . 1 F i t t i n gL o g i s t i cR e g r e s s i o nM o d e l s........ 1 2 0\\n4 . 4 . 2 E x a m p l e :S o u t hA f r i c a nH e a r tD i s e a s e ..... 1 2 2\\n4 . 4 . 3 Q u a d r a t i cA p p r o x i m a t i o n sa n dI n f e r e n c e .... 1 2 4\\n4.4.4 L1 R e g u l a r i z e dL o g i s t i cR e g r e s s i o n ........ 1 2 5\\n4 . 4 . 5 L o g i s t i cR e g r e s s i o no rL D A ?........... 1 2 7\\n4 . 5 S e p a r a t i n gH y p e r p l a n e s................... 1 2 9\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\\n4 . 5 . 2 O p t i m a lS e p a r a t i n gH y p e r p l a n e s......... 1 3 2\\nB i b l i o g r a p h i cN o t e s......................... 1 3 5\\nE x e r c i s e s............................... 1 3 5\\n5 Basis Expansions and Regularization 139\\n5 . 1 I n t r o d u c t i o n ......................... 1 3 9\\n5.2 Piecewise Polynomials and Splines ............. 1 4 1\\n5 . 2 . 1 N a t u r a lC u b i cS p l i n e s............... 1 4 4\\n5.2.2 Example: South African Heart Disease (Continued)146\\n5 . 2 . 3 E x a m p l e :P h o n e m eR e c o g n i t i o n ......... 1 4 8\\n5 . 3 F i l t e r i n ga n dF e a t u r eE x t r a c t i o n.............. 1 5 0\\n5 . 4 S m o o t h i n gS p l i n e s...................... 1 5 1\\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\\n5.5 Automatic Selection of the Smoothing Parameters .... 1 5 6\\n5 . 5 . 1 F i x i n gt h eD e g r e e so fF r e e d o m.......... 1 5 8\\n5 . 5 . 2 T h eB i a s – V a r i a n c eT r a d e o ﬀ............ 1 5 8\\n5 . 6 N o n p a r a m e t r i cL o g i s t i cR e g r e s s i o n............. 1 6 1\\n5 . 7 M u l t i d i m e n s i o n a lS p l i n e s .................. 1 6 2\\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\\n5 . 8 . 2 E x a m p l e so fR K H S ................ 1 7 0\\n5 . 9 W a v e l e tS m o o t h i n g ..................... 1 7 4\\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\\n5 . 9 . 2 A d a p t i v eW a v e l e tF i l t e r i n g............ 1 7 9\\nB i b l i o g r a p h i cN o t e s......................... 1 8 1\\nE x e r c i s e s............................... 1 8 1\\nA p p e n d i x :C o m p u t a t i o n a lC o n s i d e r a t i o n sf o rS p l i n e s ...... 1 8 6\\nAppendix: B- s p l i n e s..................... 1 8 6\\nA p p e n d i x :C o m p u t a t i o n sf o rS m o o t h i n gS p l i n e s ..... 1 8 9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a58462e7-5dd2-49ce-b186-d3d48d8dc34a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 14, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xvi Contents\\n6 Kernel Smoothing Methods 191\\n6 . 1 O n e - D i m e n s i o n a lK e r n e lS m o o t h e r s ............ 1 9 2\\n6 . 1 . 1 L o c a lL i n e a rR e g r e s s i o n.............. 1 9 4\\n6 . 1 . 2 L o c a lP o l y n o m i a lR e g r e s s i o n........... 1 9 7\\n6.2 Selecting the Width of the Kernel ............. 1 9 8\\n6.3 Local Regression in IR p ................... 2 0 0\\n6.4 Structured Local Regression Models in IRp ........ 2 0 1\\n6 . 4 . 1 S t r u c t u r e dK e r n e l s................. 2 0 3\\n6 . 4 . 2 S t r u c t u r e dR e g r e s s i o nF u n c t i o n s......... 2 0 3\\n6 . 5 L o c a lL i k e l i h o o da n dO t h e rM o d e l s ............ 2 0 5\\n6 . 6 K e r n e lD e n s i t yE s t i m a t i o na n dC l a s s i ﬁ c a t i o n....... 2 0 8\\n6 . 6 . 1 K e r n e lD e n s i t yE s t i m a t i o n ............ 2 0 8\\n6 . 6 . 2 K e r n e lD e n s i t yC l a s s i ﬁ c a t i o n........... 2 1 0\\n6 . 6 . 3 T h eN a i v eB a y e sC l a s s i ﬁ e r ............ 2 1 0\\n6 . 7 R a d i a lB a s i sF u n c t i o n sa n dK e r n e l s ............ 2 1 2\\n6.8 Mixture Models for Density Estimation and Classiﬁcation 214\\n6 . 9 C o m p u t a t i o n a lC o n s i d e r a t i o n s ............... 2 1 6\\nB i b l i o g r a p h i cN o t e s......................... 2 1 6\\nE x e r c i s e s............................... 2 1 6\\n7 Model Assessment and Selection 219\\n7 . 1 I n t r o d u c t i o n ......................... 2 1 9\\n7 . 2 B i a s ,V a r i a n c ea n dM o d e lC o m p l e x i t y........... 2 1 9\\n7 . 3 T h eB i a s – V a r i a n c eD e c o m p o s i t i o n............. 2 2 3\\n7 . 3 . 1 E x a m p l e :B i a s – V a r i a n c eT r a d e o ﬀ ........ 2 2 6\\n7 . 4 O p t i m i s mo ft h eT r a i n i n gE r r o rR a t e ........... 2 2 8\\n7 . 5 E s t i m a t e so fI n - S a m p l eP r e d i c t i o nE r r o r.......... 2 3 0\\n7.6 The Eﬀective Number of Parameters ............ 2 3 2\\n7 . 7 T h eB a y e s i a nA p p r o a c ha n dB I C.............. 2 3 3\\n7 . 8 M i n i m u mD e s c r i p t i o nL e n g t h................ 2 3 5\\n7 . 9 V a p n i k – C h e r v o n e n k i sD i m e n s i o n.............. 2 3 7\\n7 . 9 . 1 E x a m p l e( C o n t i n u e d )............... 2 3 9\\n7 . 1 0 C r o s s - V a l i d a t i o n....................... 2 4 1\\n7.10.1 K- F o l dC r o s s - V a l i d a t i o n ............. 2 4 1\\n7.10.2 The Wrong and Right Way\\nt oD oC r o s s - v a l i d a t i o n ............... 2 4 5\\n7 . 1 0 . 3 D o e sC r o s s - V a l i d a t i o nR e a l l yW o r k ?....... 2 4 7\\n7 . 1 1 B o o t s t r a pM e t h o d s ..................... 2 4 9\\n7 . 1 1 . 1 E x a m p l e( C o n t i n u e d )............... 2 5 2\\n7.12 Conditional or Expected Test Error?............ 2 5 4\\nB i b l i o g r a p h i cN o t e s......................... 2 5 7\\nE x e r c i s e s............................... 2 5 7\\n8 Model Inference and Averaging 261\\n8 . 1 I n t r o d u c t i o n ......................... 2 6 1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='10c23604-65fb-4768-8643-dc58d1a7fb32', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 15, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents xvii\\n8 . 2 T h eB o o t s t r a pa n dM a x i m u mL i k e l i h o o dM e t h o d s.... 2 6 1\\n8 . 2 . 1 AS m o o t h i n gE x a m p l e .............. 2 6 1\\n8 . 2 . 2 M a x i m u mL i k e l i h o o dI n f e r e n c e.......... 2 6 5\\n8 . 2 . 3 B o o t s t r a pv e r s u sM a x i m u mL i k e l i h o o d ..... 2 6 7\\n8 . 3 B a y e s i a nM e t h o d s...................... 2 6 7\\n8.4 Relationship Between the Bootstrap\\na n dB a y e s i a nI n f e r e n c e ................... 2 7 1\\n8 . 5 T h eE MA l g o r i t h m ..................... 2 7 2\\n8 . 5 . 1 T w o - C o m p o n e n tM i x t u r eM o d e l......... 2 7 2\\n8 . 5 . 2 T h eE MA l g o r i t h mi nG e n e r a l.......... 2 7 6\\n8.5.3 EM as a Maximization–Maximization Procedure 277\\n8 . 6 M C M Cf o rS a m p l i n gf r o mt h eP o s t e r i o r.......... 2 7 9\\n8.7 Bagging ............................ 2 8 2\\n8 . 7 . 1 E x a m p l e :T r e e sw i t hS i m u l a t e dD a t a ...... 2 8 3\\n8 . 8 M o d e lA v e r a g i n ga n dS t a c k i n g............... 2 8 8\\n8 . 9 S t o c h a s t i cS e a r c h :B u m p i n g................. 2 9 0\\nB i b l i o g r a p h i cN o t e s......................... 2 9 2\\nE x e r c i s e s............................... 2 9 3\\n9 Additive Models, Trees, and Related Methods 295\\n9 . 1 G e n e r a l i z e dA d d i t i v eM o d e l s................ 2 9 5\\n9 . 1 . 1 F i t t i n gA d d i t i v eM o d e l s.............. 2 9 7\\n9 . 1 . 2 E x a m p l e :A d d i t i v eL o g i s t i cR e g r e s s i o n ..... 2 9 9\\n9 . 1 . 3 S u m m a r y...................... 3 0 4\\n9 . 2 T r e e - B a s e dM e t h o d s..................... 3 0 5\\n9 . 2 . 1 B a c k g r o u n d .................... 3 0 5\\n9 . 2 . 2 R e g r e s s i o nT r e e s.................. 3 0 7\\n9 . 2 . 3 C l a s s i ﬁ c a t i o nT r e e s ................ 3 0 8\\n9 . 2 . 4 O t h e rI s s u e s .................... 3 1 0\\n9 . 2 . 5 S p a mE x a m p l e( C o n t i n u e d ) ........... 3 1 3\\n9 . 3 P R I M :B u m pH u n t i n g.................... 3 1 7\\n9 . 3 . 1 S p a mE x a m p l e( C o n t i n u e d ) ........... 3 2 0\\n9 . 4 M A R S :M u l t i v a r i a t eA d a p t i v eR e g r e s s i o nS p l i n e s ..... 3 2 1\\n9 . 4 . 1 S p a mE x a m p l e( C o n t i n u e d ) ........... 3 2 6\\n9 . 4 . 2 E x a m p l e( S i m u l a t e dD a t a )............ 3 2 7\\n9 . 4 . 3 O t h e rI s s u e s .................... 3 2 8\\n9 . 5 H i e r a r c h i c a lM i x t u r e so fE x p e r t s.............. 3 2 9\\n9 . 6 M i s s i n gD a t a......................... 3 3 2\\n9 . 7 C o m p u t a t i o n a lC o n s i d e r a t i o n s ............... 3 3 4\\nB i b l i o g r a p h i cN o t e s......................... 3 3 4\\nE x e r c i s e s............................... 3 3 5\\n10 Boosting and Additive Trees 337\\n1 0 . 1 B o o s t i n gM e t h o d s...................... 3 3 7\\n1 0 . 1 . 1 O u t l i n eo fT h i sC h a p t e r.............. 3 4 0', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d77ece1-338b-45c2-9eb3-2c6bfaf94404', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 16, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xviii Contents\\n1 0 . 2 B o o s t i n gF i t sa nA d d i t i v eM o d e l.............. 3 4 1\\n1 0 . 3 F o r w a r dS t a g e w i s eA d d i t i v eM o d e l i n g........... 3 4 2\\n1 0 . 4 E x p o n e n t i a lL o s sa n dA d a B o o s t .............. 3 4 3\\n1 0 . 5 W h yE x p o n e n t i a lL o s s ?................... 3 4 5\\n1 0 . 6 L o s sF u n c t i o n sa n dR o b u s t n e s s............... 3 4 6\\n1 0 . 7 “ O ﬀ - t h e - S h e l f ”P r o c e d u r e sf o rD a t aM i n i n g........ 3 5 0\\n1 0 . 8 E x a m p l e :S p a mD a t a .................... 3 5 2\\n1 0 . 9 B o o s t i n gT r e e s........................ 3 5 3\\n1 0 . 1 0N u m e r i c a lO p t i m i z a t i o nv i aG r a d i e n tB o o s t i n g...... 3 5 8\\n10.10.1 Steepest Descent .................. 3 5 8\\n1 0 . 1 0 . 2 G r a d i e n tB o o s t i n g................. 3 5 9\\n1 0 . 1 0 . 3 I m p l e m e n t a t i o n so fG r a d i e n tB o o s t i n g...... 3 6 0\\n1 0 . 1 1R i g h t - S i z e dT r e e sf o rB o o s t i n g............... 3 6 1\\n1 0 . 1 2R e g u l a r i z a t i o n ........................ 3 6 4\\n1 0 . 1 2 . 1 S h r i n k a g e...................... 3 6 4\\n10.12.2 Subsampling .................... 3 6 5\\n1 0 . 1 3I n t e r p r e t a t i o n ........................ 3 6 7\\n10.13.1 Relative Importance of Predictor Variables . . . 367\\n1 0 . 1 3 . 2 P a r t i a lD e p e n d e n c eP l o t s............. 3 6 9\\n1 0 . 1 4I l l u s t r a t i o n s.......................... 3 7 1\\n10.14.1 California Housing ................. 3 7 1\\n1 0 . 1 4 . 2 N e wZ e a l a n dF i s h................. 3 7 5\\n1 0 . 1 4 . 3 D e m o g r a p h i c sD a t a................ 3 7 9\\nB i b l i o g r a p h i cN o t e s......................... 3 8 0\\nE x e r c i s e s............................... 3 8 4\\n11 Neural Networks 389\\n1 1 . 1 I n t r o d u c t i o n ......................... 3 8 9\\n11.2 Projection Pursuit Regression ............... 3 8 9\\n1 1 . 3 N e u r a lN e t w o r k s....................... 3 9 2\\n1 1 . 4 F i t t i n gN e u r a lN e t w o r k s................... 3 9 5\\n1 1 . 5 S o m eI s s u e si nT r a i n i n gN e u r a lN e t w o r k s......... 3 9 7\\n1 1 . 5 . 1 S t a r t i n gV a l u e s................... 3 9 7\\n1 1 . 5 . 2 O v e r ﬁ t t i n g..................... 3 9 8\\n11.5.3 Scaling of the Inputs ............... 3 9 8\\n1 1 . 5 . 4 N u m b e ro fH i d d e nU n i t sa n dL a y e r s....... 4 0 0\\n1 1 . 5 . 5 M u l t i p l eM i n i m a.................. 4 0 0\\n1 1 . 6 E x a m p l e :S i m u l a t e dD a t a.................. 4 0 1\\n1 1 . 7 E x a m p l e :Z I PC o d eD a t a.................. 4 0 4\\n1 1 . 8 D i s c u s s i o n .......................... 4 0 8\\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\\n11.9.1 Bayes, Boosting and Bagging ........... 4 1 0\\n1 1 . 9 . 2 P e r f o r m a n c eC o m p a r i s o n s ............ 4 1 2\\n1 1 . 1 0C o m p u t a t i o n a lC o n s i d e r a t i o n s ............... 4 1 4\\nB i b l i o g r a p h i cN o t e s......................... 4 1 5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c1e53f7-ad97-43bf-8e41-91405695c092', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 17, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents xix\\nE x e r c i s e s............................... 4 1 5\\n12 Support Vector Machines and\\nFlexible Discriminants 417\\n1 2 . 1 I n t r o d u c t i o n ......................... 4 1 7\\n12.2 The Support Vector Classiﬁer ................ 4 1 7\\n12.2.1 Computing the Support Vector Classiﬁer .... 4 2 0\\n1 2 . 2 . 2 M i x t u r eE x a m p l e( C o n t i n u e d ) .......... 4 2 1\\n12.3 Support Vector Machines and Kernels ........... 4 2 3\\n1 2 . 3 . 1 C o m p u t i n gt h eS V Mf o rC l a s s i ﬁ c a t i o n...... 4 2 3\\n1 2 . 3 . 2 T h eS V Ma saP e n a l i z a t i o nM e t h o d....... 4 2 6\\n12.3.3 Function Estimation and Reproducing Kernels . 428\\n1 2 . 3 . 4 S V M sa n dt h eC u r s eo fD i m e n s i o n a l i t y ..... 4 3 1\\n1 2 . 3 . 5 AP a t hA l g o r i t h mf o rt h eS V MC l a s s i ﬁ e r.... 4 3 2\\n12.3.6 Support Vector Machines for Regression ..... 4 3 4\\n1 2 . 3 . 7 R e g r e s s i o na n dK e r n e l s.............. 4 3 6\\n1 2 . 3 . 8 D i s c u s s i o n ..................... 4 3 8\\n1 2 . 4 G e n e r a l i z i n gL i n e a rD i s c r i m i n a n tA n a l y s i s ........ 4 3 8\\n1 2 . 5 F l e x i b l eD i s c r i m i n a n tA n a l y s i s............... 4 4 0\\n1 2 . 5 . 1 C o m p u t i n gt h eF D AE s t i m a t e s.......... 4 4 4\\n1 2 . 6 P e n a l i z e dD i s c r i m i n a n tA n a l y s i s.............. 4 4 6\\n1 2 . 7 M i x t u r eD i s c r i m i n a n tA n a l y s i s............... 4 4 9\\n1 2 . 7 . 1 E x a m p l e :W a v e f o r mD a t a............. 4 5 1\\nB i b l i o g r a p h i cN o t e s......................... 4 5 5\\nE x e r c i s e s............................... 4 5 5\\n13 Prototype Methods and Nearest-Neighbors 459\\n1 3 . 1 I n t r o d u c t i o n ......................... 4 5 9\\n1 3 . 2 P r o t o t y p eM e t h o d s ..................... 4 5 9\\n13.2.1 K- m e a n sC l u s t e r i n g................ 4 6 0\\n13.2.2 Learning Vector Quantization . . . ....... 4 6 2\\n1 3 . 2 . 3 G a u s s i a nM i x t u r e s................. 4 6 3\\n13.3 k- N e a r e s t - N e i g h b o rC l a s s i ﬁ e r s ............... 4 6 3\\n1 3 . 3 . 1 E x a m p l e :AC o m p a r a t i v eS t u d y ......... 4 6 8\\n13.3.2 Example: k-Nearest-Neighbors\\na n dI m a g eS c e n eC l a s s i ﬁ c a t i o n.......... 4 7 0\\n1 3 . 3 . 3 I n v a r i a n tM e t r i c sa n dT a n g e n tD i s t a n c e..... 4 7 1\\n1 3 . 4 A d a p t i v eN e a r e s t - N e i g h b o rM e t h o d s............ 4 7 5\\n1 3 . 4 . 1 E x a m p l e ...................... 4 7 8\\n13.4.2 Global Dimension Reduction\\nf o rN e a r e s t - N e i g h b o r s............... 4 7 9\\n1 3 . 5 C o m p u t a t i o n a lC o n s i d e r a t i o n s............... 4 8 0\\nB i b l i o g r a p h i cN o t e s......................... 4 8 1\\nE x e r c i s e s............................... 4 8 1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='241d6f10-e718-4d46-8899-91697cecc81a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 18, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xx Contents\\n14 Unsupervised Learning 485\\n1 4 . 1 I n t r o d u c t i o n ......................... 4 8 5\\n1 4 . 2 A s s o c i a t i o nR u l e s ...................... 4 8 7\\n1 4 . 2 . 1 M a r k e tB a s k e tA n a l y s i s.............. 4 8 8\\n1 4 . 2 . 2 T h eA p r i o r iA l g o r i t h m .............. 4 8 9\\n1 4 . 2 . 3 E x a m p l e :M a r k e tB a s k e tA n a l y s i s........ 4 9 2\\n1 4 . 2 . 4 U n s u p e r v i s e da sS u p e r v i s e dL e a r n i n g ...... 4 9 5\\n1 4 . 2 . 5 G e n e r a l i z e dA s s o c i a t i o nR u l e s .......... 4 9 7\\n1 4 . 2 . 6 C h o i c eo fS u p e r v i s e dL e a r n i n gM e t h o d ..... 4 9 9\\n14.2.7 Example: Market Basket Analysis (Continued) . 499\\n1 4 . 3 C l u s t e rA n a l y s i s....................... 5 0 1\\n1 4 . 3 . 1 P r o x i m i t yM a t r i c e s ................ 5 0 3\\n1 4 . 3 . 2 D i s s i m i l a r i t i e sB a s e do nA t t r i b u t e s ....... 5 0 3\\n1 4 . 3 . 3 O b j e c tD i s s i m i l a r i t y................ 5 0 5\\n1 4 . 3 . 4 C l u s t e r i n gA l g o r i t h m s............... 5 0 7\\n1 4 . 3 . 5 C o m b i n a t o r i a lA l g o r i t h m s ............ 5 0 7\\n14.3.6 K- m e a n s...................... 5 0 9\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\\n14.3.8 Example: Human Tumor Microarray Data . . . 512\\n14.3.9 Vector Quantization ................ 5 1 4\\n14.3.10 K- m e d o i d s..................... 5 1 5\\n1 4 . 3 . 1 1 P r a c t i c a lI s s u e s .................. 5 1 8\\n1 4 . 3 . 1 2 H i e r a r c h i c a lC l u s t e r i n g .............. 5 2 0\\n1 4 . 4 S e l f - O r g a n i z i n gM a p s .................... 5 2 8\\n1 4 . 5 P r i n c i p a lC o m p o n e n t s ,C u r v e sa n dS u r f a c e s........ 5 3 4\\n1 4 . 5 . 1 P r i n c i p a lC o m p o n e n t s............... 5 3 4\\n1 4 . 5 . 2 P r i n c i p a lC u r v e sa n dS u r f a c e s .......... 5 4 1\\n14.5.3 Spectral Clustering ................ 5 4 4\\n1 4 . 5 . 4 K e r n e lP r i n c i p a lC o m p o n e n t s ........... 5 4 7\\n1 4 . 5 . 5 S p a r s eP r i n c i p a lC o m p o n e n t s........... 5 5 0\\n1 4 . 6 N o n - n e g a t i v eM a t r i xF a c t o r i z a t i o n............. 5 5 3\\n1 4 . 6 . 1 A r c h e t y p a lA n a l y s i s................ 5 5 4\\n14.7 Independent Component Analysis\\nand Exploratory Projection Pursuit ............ 5 5 7\\n1 4 . 7 . 1 L a t e n tV a r i a b l e sa n dF a c t o rA n a l y s i s ...... 5 5 8\\n1 4 . 7 . 2 I n d e p e n d e n tC o m p o n e n tA n a l y s i s ........ 5 6 0\\n14.7.3 Exploratory Projection Pursuit . . . ....... 5 6 5\\n1 4 . 7 . 4 AD i r e c tA p p r o a c ht oI C A............ 5 6 5\\n1 4 . 8 M u l t i d i m e n s i o n a lS c a l i n g .................. 5 7 0\\n14.9 Nonlinear Dimension Reduction\\na n dL o c a lM u l t i d i m e n s i o n a lS c a l i n g ............ 5 7 2\\n1 4 . 1 0T h eG o o g l eP a g e R a n kA l g o r i t h m ............. 5 7 6\\nB i b l i o g r a p h i cN o t e s......................... 5 7 8\\nE x e r c i s e s............................... 5 7 9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='84e24bbb-98c7-445c-8df1-44df9d49aae1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 19, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents xxi\\n15 Random Forests 587\\n1 5 . 1 I n t r o d u c t i o n ......................... 5 8 7\\n1 5 . 2 D e ﬁ n i t i o no fR a n d o mF o r e s t s................ 5 8 7\\n1 5 . 3 D e t a i l so fR a n d o mF o r e s t s ................. 5 9 2\\n1 5 . 3 . 1 O u to fB a gS a m p l e s................ 5 9 2\\n1 5 . 3 . 2 V a r i a b l eI m p o r t a n c e................ 5 9 3\\n1 5 . 3 . 3 P r o x i m i t yP l o t s .................. 5 9 5\\n1 5 . 3 . 4 R a n d o mF o r e s t sa n dO v e r ﬁ t t i n g......... 5 9 6\\n1 5 . 4 A n a l y s i so fR a n d o mF o r e s t s................. 5 9 7\\n1 5 . 4 . 1 V a r i a n c ea n dt h eD e - C o r r e l a t i o nE ﬀ e c t ..... 5 9 7\\n1 5 . 4 . 2 B i a s......................... 6 0 0\\n1 5 . 4 . 3 A d a p t i v eN e a r e s tN e i g h b o r s ........... 6 0 1\\nB i b l i o g r a p h i cN o t e s......................... 6 0 2\\nE x e r c i s e s............................... 6 0 3\\n16 Ensemble Learning 605\\n1 6 . 1 I n t r o d u c t i o n ......................... 6 0 5\\n1 6 . 2 B o o s t i n ga n dR e g u l a r i z a t i o nP a t h s............. 6 0 7\\n1 6 . 2 . 1 P e n a l i z e dR e g r e s s i o n ............... 6 0 7\\n1 6 . 2 . 2 T h e“ B e to nS p a r s i t y ”P r i n c i p l e......... 6 1 0\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\\n1 6 . 3 L e a r n i n gE n s e m b l e s..................... 6 1 6\\n1 6 . 3 . 1 L e a r n i n gaG o o dE n s e m b l e............ 6 1 7\\n1 6 . 3 . 2 R u l eE n s e m b l e s .................. 6 2 2\\nB i b l i o g r a p h i cN o t e s......................... 6 2 3\\nE x e r c i s e s............................... 6 2 4\\n17 Undirected Graphical Models 625\\n1 7 . 1 I n t r o d u c t i o n ......................... 6 2 5\\n1 7 . 2 M a r k o vG r a p h sa n dT h e i rP r o p e r t i e s ........... 6 2 7\\n17.3 Undirected Graphical Models for Continuous Variables . 630\\n17.3.1 Estimation of the Parameters\\nw h e nt h eG r a p hS t r u c t u r ei sK n o w n....... 6 3 1\\n1 7 . 3 . 2 E s t i m a t i o no ft h eG r a p hS t r u c t u r e........ 6 3 5\\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\\n17.4.1 Estimation of the Parameters\\nw h e nt h eG r a p hS t r u c t u r ei sK n o w n....... 6 3 9\\n1 7 . 4 . 2 H i d d e nN o d e s ................... 6 4 1\\n1 7 . 4 . 3 E s t i m a t i o no ft h eG r a p hS t r u c t u r e........ 6 4 2\\n17.4.4 Restricted Boltzmann Machines . . ....... 6 4 3\\nE x e r c i s e s............................... 6 4 5\\n18 High-Dimensional Problems:p≫ N 649\\n18.1 When p is Much Bigger thanN .............. 6 4 9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f4a0d7e8-dc6a-4be1-9607-d78b586407e1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 20, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='xxii Contents\\n18.2 Diagonal Linear Discriminant Analysis\\na n dN e a r e s tS h r u n k e nC e n t r o i d s.............. 6 5 1\\n1 8 . 3 L i n e a rC l a s s i ﬁ e r sw i t hQ u a d r a t i cR e g u l a r i z a t i o n..... 6 5 4\\n1 8 . 3 . 1 R e g u l a r i z e dD i s c r i m i n a n tA n a l y s i s........ 6 5 6\\n18.3.2 Logistic Regression\\nw i t hQ u a d r a t i cR e g u l a r i z a t i o n.......... 6 5 7\\n18.3.3 The Support Vector Classiﬁer . . . ....... 6 5 7\\n18.3.4 Feature Selection .................. 6 5 8\\n18.3.5 Computational Shortcuts When p≫ N ..... 6 5 9\\n18.4 Linear Classiﬁers with L1 R e g u l a r i z a t i o n ......... 6 6 1\\n18.4.1 Application of Lasso\\nto Protein Mass Spectroscopy . . . ....... 6 6 4\\n1 8 . 4 . 2 T h eF u s e dL a s s of o rF u n c t i o n a lD a t a ...... 6 6 6\\n1 8 . 5 C l a s s i ﬁ c a t i o nW h e nF e a t u r e sa r eU n a v a i l a b l e....... 6 6 8\\n18.5.1 Example: String Kernels\\na n dP r o t e i nC l a s s i ﬁ c a t i o n............. 6 6 8\\n18.5.2 Classiﬁcation and Other Models Using\\nInner-Product Kernels and Pairwise Distances . 670\\n1 8 . 5 . 3 E x a m p l e :A b s t r a c t sC l a s s i ﬁ c a t i o n ........ 6 7 2\\n18.6 High-Dimensional Regression:\\nS u p e r v i s e dP r i n c i p a lC o m p o n e n t s ............. 6 7 4\\n18.6.1 Connection to Latent-Variable Modeling .... 6 7 8\\n18.6.2 Relationship with Partial Least Squares ..... 6 8 0\\n18.6.3 Pre-Conditioning for Feature Selection ..... 6 8 1\\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683\\n1 8 . 7 . 1 T h eF a l s eD i s c o v e r yR a t e............. 6 8 7\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\\n1 8 . 7 . 3 AB a y e s i a nI n t e r p r e t a t i o no ft h eF D R...... 6 9 2\\n1 8 . 8 B i b l i o g r a p h i cN o t e s ..................... 6 9 3\\nE x e r c i s e s............................... 6 9 4\\nReferences 699\\nAuthor Index 729\\nIndex 737', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='39190a2a-9faa-40c8-8923-be1032b84abf', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 21, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1\\nIntroduction\\nStatistical learningplays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attack, will\\nhave a second heart attack. The prediction is to be based on demo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis of\\ncompany performance measures and economic data.\\n•Identify the numbers in a handwritten ZIP code, from a digitized\\nimage.\\n•Estimate the amount of glucose in the blood of a diabetic person,\\nfrom the infrared absorption spectrum of that person’s blood.\\n•Identify the risk factors for prostate cancer, based on clinical and\\ndemographic variables.\\nThe science of learning plays a key role in the ﬁelds of statistics, data\\nmining and artiﬁcial intelligence, intersecting with areas of engineering and\\nother disciplines.\\nThis book is about learning from data. In a typical scenario, we have\\nan outcome measurement, usually quantitative (such as a stock price) or\\ncategorical (such as heart attack/no heart attack), that we wish to predict\\nbased on a set offeatures (such as diet and clinical measurements). We\\nhave atraining setof data, in which we observe the outcome and feature\\n \\n1\\n© Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, \\nDOI: 10.1007/b94608_1,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4377cdfb-e24f-4a27-9605-2cd6e3f6fe9f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 22, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 1. Introduction\\nTABLE 1.1. Average percentage of words or characters in an email message\\nequal to the indicated word or character. We have chosen the words and characters\\nshowing the largest diﬀerence betweenspam and email.\\ngeorge you your hp free hpl ! our re edu remove\\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\\nmeasurements for a set of objects (such as people). Using this data we build\\na prediction model, orlearner, which will enable us to predict the outcome\\nfor new unseen objects. A good learner is one that accurately predicts such\\nan outcome.\\nThe examples above describe what is called thesupervised learningprob-\\nlem. It is called “supervised” because of the presence of the outcome vari-\\nable to guide the learning process. In theunsupervised learning problem,\\nwe observe only the features and have no measurements of the outcome.\\nOur task is rather to describe how the data are organized or clustered. We\\ndevote most of this book to supervised learning; the unsupervised problem\\nis less developed in the literature, and is the focus of Chapter 14.\\nHere are some examples of real learning problems that are discussed in\\nthis book.\\nExample 1: Email Spam\\nThe data for this example consists of information from 4601 email mes-\\nsages, in a study to try to predict whether the email was junk email, or\\n“spam.” The objective was to design an automatic spam detector that\\ncould ﬁlter out spam before clogging the users’ mailboxes. For all 4601\\nemail messages, the true outcome (email type)email or spam is available,\\nalong with the relative frequencies of 57 of the most commonly occurring\\nwords and punctuation marks in the email message. This is a supervised\\nlearning problem, with the outcome the class variableemail/spam.I ti sa l s o\\ncalled aclassiﬁcation problem.\\nTable 1.1 lists the words and characters showing the largest average\\ndiﬀerence betweenspam and email.\\nOur learning method has to decide which features to use and how: for\\nexample, we might use a rule such as\\nif (%george < 0.6) & (%you > 1.5) then spam\\nelse email.\\nAnother form of a rule might be:\\nif (0.2·%you −0.3·%george) > 0t h e nspam\\nelse email.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e5511580-5d3a-4885-9e81-d12d1751d190', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 23, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1. Introduction 3\\nlpsa\\n− 1 1234\\noooo ooo ooo ooo oooo oo o o oo oo ooo o oooooo ooo ooo oo ooo oo ooo oo o ooooo oo ooooo ooo o ooo o ooo oooo oooo ooo oo ooo ooo o\\no ooo oooooo oooo oo ooo ooooo oo ooo o ooo oo ooo ooo ooooo oooo o oo oo oo o oo ooo oo oooo ooo oo oo ooo oo o oooo oo oo oo ooo o o\\n40 50 60 70 80\\no ooo ooo ooo ooo oo o ooo oooo o oooooooooo oo oo oo o ooo oo oooo ooo oo ooo oo oooooo o oooo ooo oo o ooooo ooo oo oo ooo oo o oo\\nooooooo ooooooooo ooo oo oo o ooo ooo ooooo ooo oooo oo oo ooooo oooo oo oo o ooo oo o o oo oo oo oo o oooo ooo o oo ooo oo oooo oo\\n0.0 0.4 0.8\\noooooooooooooooooooooooooooooooooooooo oooooooo ooooooooooooooo oo ooooooo oo oooooo oooo ooo oo oooo oooooo\\noooooooooooooo oo o oooo oo ooo oo o ooooo oo oo oooo o ooo oooooo o ooo ooooo oo oo o ooo o oo o o o ooo ooo oo oo oo o oooo o ooo o\\n6.0 7.0 8.0 9.0\\nooooooooooooo ooo ooooo oo ooo ooooooooo o oooo ooo ooo oooo oo oooooo ooo oooo oooo oooo oooooooooo oooo oooo oooooo\\n012345oooooooooooooooo ooooo oo ooo oo oooooooo oo oo oooo oo oooo oo ooo ooo o oo oo oo o oooo ooo oo o oo ooo oo oo oo o o oo o ooo oo\\n− 1 1234\\nooo\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no\\nooo\\no\\no\\noo\\no\\no\\no\\noo\\no\\noo\\no\\nlcavol\\no oo\\no\\no\\no\\noo\\no\\no o\\no\\noo oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no o\\no\\noo\\no\\noo o\\no\\no oo\\no\\no\\noo\\no\\no\\no\\no o\\no\\no o\\no\\no o o\\no\\no\\no\\noo\\no\\noo\\no\\no oo o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no o\\no\\no\\no o\\no\\no\\no oo\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no\\no oo\\no\\no\\no o\\no\\no\\no\\no o\\no\\no o\\no\\nooo\\no\\no\\no\\no o\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\noo\\no\\no o\\no\\nooo\\no\\noo o\\no\\no\\noo\\no\\no\\no\\noo\\no\\no o\\no\\nooo\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no o\\no\\noo\\no\\no oo\\no\\no oo\\no\\no\\no o\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no\\no\\no\\noo\\no\\noo\\no\\noo oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no o\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no o\\no\\no o\\no\\no oo\\no\\noo o\\no\\no\\no o\\no\\no\\no\\no o\\no\\noo\\no\\noo o\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no o\\no\\no\\no o\\no\\no\\nooo\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no\\noo o\\no\\no\\no o\\no\\no\\no\\noo\\no\\noo\\no\\noo o\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no o\\no\\no\\no o\\no\\no\\no oo\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\no\\no o\\no\\no oo\\no\\noo o\\no\\no\\no o\\no\\no\\no\\no o\\no\\no o\\no\\no\\no\\no\\no oo\\nooo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\nooo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\nooooo\\no\\no\\no\\no\\nooo\\no\\noo\\no\\no\\no\\no oo\\nooo\\no\\noo\\noo\\no\\no\\no o\\no\\no\\no oo oo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo o\\noo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo oo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\noo o\\no\\no o\\nlweight\\no\\no\\no\\no oo\\nooo\\no\\noo\\no o\\no\\no\\noo\\no\\no\\nooo o o\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no o\\nooo\\noo\\noo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo o o\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no oo\\no\\noo\\no\\no\\no\\nooo\\no oo\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no oo o o\\no\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no oo\\noo\\noo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o o o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o oo o\\no\\no\\no\\no\\nooo\\no\\noo\\no\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\nooo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\nooo\\no\\noo\\no\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\no o\\no\\no\\no oo oo\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no o o\\noo\\noo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no oo oo\\no\\no\\no\\no\\no o o\\no\\no o\\no\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no oo oo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\noo o\\noo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no ooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no oooo\\no\\no\\no\\no\\nooo\\no\\noo\\n2.5 3.5 4.5\\no\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no oo oo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\nooo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no o oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no oo oo\\no\\no\\no\\no\\no oo\\no\\noo\\n40 50 60 70 80\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\noooooooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\nooo\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no oooo ooo\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noo o\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\noo ooo oo o\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo o\\no\\no o o\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\nage\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo oo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\nooo oooo o\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo o\\no\\noo o\\no\\no\\no\\no\\no o\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\noooooooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\nooo\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\noo ooo ooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo o\\no\\noo o\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\noo oooooo\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo o\\no\\noo o\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\noo oooooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo o\\no\\nooo\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\nooooo o\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo o\\no\\no\\noooooo\\no\\no\\noo o\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\noooo oo\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\noo oooo\\no\\no\\noo o\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no lbph\\noooooo\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\noooooo\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo o\\no\\no\\noooooo\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\n− 1 012oooooo\\no\\no\\nooo\\no\\noooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\n0.0 0.4 0.8ooooo ooooooooooooooooooooooooooooooooo\\no\\nooooooo\\no\\noooooooooooooo\\no\\no\\no\\noooooo\\no\\no\\noooo\\noo\\no\\nooo\\no\\noo\\no\\no\\nooo\\no\\nooo ooo\\nooooooo ooo ooo ooooooo o ooo oooooooooooo oo\\no\\nooooo oo\\no\\nooooooo ooooooo\\no\\no\\no\\noooooo\\no\\no\\noo o o\\noo\\no\\nooo\\no\\noo\\no\\no\\nooo\\no\\noo oooo\\noooo ooooooooooooo oooooooooo ooo o ooooo oo\\no\\noo o o ooo\\no\\noooo ooooooo o oo\\no\\no\\no\\noooo o o\\no\\no\\noooo\\noo\\no\\nooo\\no\\noo\\no\\no\\nooo\\no\\nooooo o\\noo ooooo ooo o ooooo oooo oooo ooooooooooo ooo\\no\\noo ooooo\\no\\nooo oooooooo o oo\\no\\no\\no\\nooo ooo\\no\\no\\nooo o\\noo\\no\\nooo\\no\\noo\\no\\no\\noo o\\no\\noooo o o\\nooooooo ooooooooooooooo oo o ooo ooo ooooo oo\\no\\noooooo o\\no\\nooooo oooooo oo o\\no\\no\\no\\noo o ooo\\no\\no\\noooo\\noo\\no\\noo o\\no\\noo\\no\\no\\nooo\\no\\nooooo o\\nsvi\\nooooooooooooooooo oooooooooooo ooooooooo\\no\\noooo ooo\\no\\noooooooooooooo\\no\\no\\no\\noo oooo\\no\\no\\noo o o\\noo\\no\\nooo\\no\\noo\\no\\no\\nooo\\no\\noo o o oo\\nooooooooooooo oooooooooooooo oooooooooo o\\no\\nooooo oo\\no\\noooooo oooooo oo\\no\\no\\no\\noo oooo\\no\\no\\noooo\\noo\\no\\nooo\\no\\noo\\no\\no\\nooo\\no\\noooooo\\nooooooooooooooooooooooooooo ooooooooo oo\\no\\nooooo oo\\no\\noooooo oooooooo\\no\\no\\no\\noo oooo\\no\\no\\noooo\\noo\\no\\nooo\\no\\noo\\no\\no\\noo o\\no\\noo oooo\\nooooo ooooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\nooooooo ooo oo\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\noooo oooooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\noo ooooo ooo o o\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo oo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\nooooooo ooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\nooo o\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\noooooooooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\nlcp\\noooooooooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo oo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\n− 1 0123oooooooooooo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\n6.0 7.0 8.0 9.0oo\\no\\noo ooooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noooo\\no\\nooooooooo\\no\\noo\\no\\nooo\\no\\nooo ooo\\noo\\no\\noooo ooo oo\\nooo\\no\\no\\nooo o\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\noo ooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noooo\\no\\noooooooo o\\no\\noo\\no\\nooo\\no\\noo oooo\\noo\\no\\nooooooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\nooo\\noo oooo\\no\\no\\noo o\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noooo\\no\\nooo oooooo\\no\\noo\\no\\nooo\\no\\nooooo o\\noo\\no\\noooo ooo o o\\nooo\\no\\no\\nooo o\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\noo o\\no\\nooo o\\no\\nooo oooooo\\no\\noo\\no\\noo o\\no\\noooo o o\\noo\\no\\noooo ooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\noo o\\noo oooo\\no\\no\\noo o\\no\\no\\no\\noo o\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\noo o\\no\\noooo\\no\\noo o oooo oo\\no\\noo\\no\\nooo\\no\\nooooo o\\noo\\no\\nooooooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noooo\\no\\nooooooooo\\no\\noo\\no\\nooo\\no\\noooooo\\noo\\no\\nooooooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\noo o\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noooo\\no\\nooooo oooo\\no\\noo\\no\\nooo\\no\\noo o o oo\\ngleason\\noo\\no\\nooooooooo\\nooo\\no\\no\\noooo\\no\\no\\no\\noo\\nooo\\noooooo\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no\\noo oo\\no\\noo ooo oooo\\no\\noo\\no\\noo o\\no\\noo oooo\\n012345\\noo\\no\\noo ooooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\nooooooo\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noooo ooo oo\\no\\nooo\\no\\nooo o\\no\\no\\no\\noo\\no\\no\\no\\noooooo o\\noo\\no\\no\\no\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\n2.5 3.5 4.5\\noo\\no\\nooooooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\noo oooo o\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noooo ooo o o\\no\\noo o\\no\\nooo o\\no\\no\\no\\noo\\no\\no\\no\\noooooo o\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\n− 1 012\\noo\\no\\noooo ooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\noo oooo o\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\nooooooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\nooooooo\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\n− 1 0123\\noo\\no\\nooooooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\nooooooo\\noo\\no\\no\\no\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\nooooooooo\\no\\nooo\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\noooooo o\\noo\\no\\no\\no\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\n0 20 60 100\\n0 20 60 100\\npgg45\\nFIGURE 1.1.Scatterplot matrix of the prostate cancer data. The ﬁrst row shows\\nthe response against each of the predictors in turn. Two of the predictors,svi and\\ngleason, are categorical.\\nFor this problem not all errors are equal; we want to avoid ﬁltering out\\ngood email, while letting spam get through is not desirable but less serious\\nin its consequences. We discuss a number of diﬀerent methods for tackling\\nthis learning problem in the book.\\nExample 2: Prostate Cancer\\nThe data for this example, displayed in Figure 1.11, come from a study\\nby Stamey et al. (1989) that examined the correlation between the level of\\n1There was an error in these data in the ﬁrst edition of this book. Subject 32 had\\na value of 6.1 forlweight, which translates to a 449 gm prostate! The correct value is\\n44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ec17e0a-7b06-4094-a084-55b4e91ca458', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 24, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4 1. Introduction\\nFIGURE 1.2.Examples of handwritten digits from U.S. postal envelopes.\\nprostate speciﬁc antigen (PSA) and a number of clinical measures, in 97\\nmen who were about to receive a radical prostatectomy.\\nThe goal is to predict the log of PSA (lpsa) from a number of measure-\\nments including log cancer volume (lcavol) ,l o gp r o s t a t ew e i g h tlweight,\\nage, log of benign prostatic hyperplasia amountlbph, seminal vesicle in-\\nvasion svi, log of capsular penetration lcp, Gleason score gleason,a n d\\npercent of Gleason scores 4 or 5pgg45. Figure 1.1 is a scatterplot matrix\\nof the variables. Some correlations withlpsa are evident, but a good pre-\\ndictive model is diﬃcult to construct by eye.\\nThis is a supervised learning problem, known as aregression problem,\\nbecause the outcome measurement is quantitative.\\nExample 3: Handwritten Digit Recognition\\nThe data from this example come from the handwritten ZIP codes on\\nenvelopes from U.S. postal mail. Each image is a segment from a ﬁve digit\\nZIP code, isolating a single digit. The images are 16×16 eight-bit grayscale\\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\\nimages are shown in Figure 1.2.\\nThe images have been normalized to have approximately the same size\\nand orientation. The task is to predict, from the 16× 16 matrix of pixel\\nintensities, the identity of each image (0,1,..., 9) quickly and accurately. If\\nit is accurate enough, the resulting algorithm would be used as part of an\\nautomatic sorting procedure for envelopes. This is a classiﬁcation problem\\nfor which the error rate needs to be kept very low to avoid misdirection of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2f2caf3d-8aac-4c92-824d-95cf1bbd25a6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 25, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1. Introduction 5\\nmail. In order to achieve this low error rate, some objects can be assigned\\nto a “don’t know” category, and sorted instead by hand.\\nExample 4: DNA Expression Microarrays\\nDNA stands for deoxyribonucleic acid, and is the basic material that makes\\nup human chromosomes. DNA microarrays measure the expression of a\\ngene in a cell by measuring the amount of mRNA (messenger ribonucleic\\nacid) present for that gene. Microarrays are considered a breakthrough\\ntechnology in biology, facilitating the quantitative study of thousands of\\ngenes simultaneously from a single sample of cells.\\nHere is how a DNA microarray works. The nucleotide sequences for a few\\nthousand genes are printed on a glass slide. A target sample and a reference\\nsample are labeled with red and green dyes, and each are hybridized with\\nthe DNA on the slide. Through ﬂuoroscopy, the log (red/green) intensities\\nof RNA hybridizing at each site is measured. The result is a few thousand\\nnumbers, typically ranging from say−6 to 6, measuring the expression level\\nof each gene in the target relative to the reference sample. Positive values\\nindicate higher expression in the target versus the reference, and vice versa\\nfor negative values.\\nA gene expression dataset collects together the expression values from a\\nseries of DNA microarray experiments, with each column representing an\\nexperiment.Therearethereforeseveralthousandrowsrepresentingindivid-\\nual genes, and tens of columns representing samples: in the particular ex-\\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\\nalthough for clarity only a random sample of 100 rows are shown. The ﬁg-\\nure displays the data set as a heat map, ranging from green (negative) to\\nred (positive). The samples are 64 cancer tumors from diﬀerent patients.\\nThe challenge here is to understand how the genes and samples are or-\\nganized. Typical questions include the following:\\n(a) which samples are most similar to each other, in terms of their expres-\\nsion proﬁles across genes?\\n(b) which genes are most similar to each other, in terms of their expression\\nproﬁles across samples?\\n(c) do certain genes show very high (or low) expression for certain cancer\\nsamples?\\nWe could view this task as a regression problem, with two categorical\\npredictor variables—genes and samples—with the response variable being\\nthe level of expression. However, it is probably more useful to view it as\\nunsupervised learning problem. For example, for question (a) above, we\\nthink of the samples as points in 6830–dimensional space, which we want\\nto cluster together in some way.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30f05795-53bf-46cc-bc09-ac0811f3c63c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 26, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6 1. Introduction\\nSID42354\\nSID31984\\nSID301902\\nSIDW128368\\nSID375990\\nSID360097\\nSIDW325120\\nESTsChr.10\\nSIDW365099\\nSID377133\\nSID381508\\nSIDW308182\\nSID380265\\nSIDW321925\\nESTsChr.15\\nSIDW362471\\nSIDW417270\\nSIDW298052\\nSID381079\\nSIDW428642\\nTUPLE1TUP1\\nERLUMEN\\nSIDW416621\\nSID43609\\nESTs\\nSID52979\\nSIDW357197\\nSIDW366311\\nESTs\\nSMALLNUC\\nSIDW486740\\nESTs\\nSID297905\\nSID485148\\nSID284853\\nESTsChr.15\\nSID200394\\nSIDW322806\\nESTsChr.2\\nSIDW257915\\nSID46536\\nSIDW488221\\nESTsChr.5\\nSID280066\\nSIDW376394\\nESTsChr.15\\nSIDW321854\\nWASWiskott\\nHYPOTHETIC\\nSIDW376776\\nSIDW205716\\nSID239012\\nSIDW203464\\nHLACLASSI\\nSIDW510534\\nSIDW279664\\nSIDW201620\\nSID297117\\nSID377419\\nSID114241\\nESTsCh31\\nSIDW376928\\nSIDW310141\\nSIDW298203\\nPTPRC\\nSID289414\\nSID127504\\nESTsChr.3\\nSID305167\\nSID488017\\nSIDW296310\\nESTsChr.6\\nSID47116\\nMITOCHOND\\nChr\\nSIDW376586\\nHomosapiens\\nSIDW487261\\nSIDW470459\\nSID167117\\nSIDW31489\\nSID375812\\nDNAPOLYME\\nSID377451\\nESTsChr.1\\nMYBPROTO\\nSID471915\\nESTs\\nSIDW469884\\nHumanmRNA\\nSIDW377402\\nESTs\\nSID207172\\nRASGTPASE\\nSID325394\\nH.sapiensmR\\nGNAL\\nSID73161\\nSIDW380102\\nSIDW299104\\nBREAST\\nRENAL\\nMELANOMA\\nMELANOMA\\nMCF7D-repro\\nCOLON\\nCOLON\\nK562B-repro\\nCOLON\\nNSCLC\\nLEUKEMIA\\nRENAL\\nMELANOMA\\nBREAST\\nCNS\\nCNS\\nRENAL\\nMCF7A-repro\\nNSCLC\\nK562A-repro\\nCOLON\\nCNS\\nNSCLC\\nNSCLC\\nLEUKEMIA\\nCNS\\nOVARIAN\\nBREAST\\nLEUKEMIA\\nMELANOMA\\nMELANOMA\\nOVARIAN\\nOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nOVARIAN\\nOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nLEUKEMIA\\nCOLON\\nBREAST\\nLEUKEMIA\\nCOLON\\nCNS\\nMELANOMA\\nNSCLC\\nPROSTATE\\nNSCLC\\nRENAL\\nRENAL\\nNSCLC\\nRENAL\\nLEUKEMIA\\nOVARIAN\\nPROSTATE\\nCOLON\\nBREAST\\nRENAL\\nUNKNOWN\\nFIGURE 1.3. DNA microarray data: expression matrix of6830 genes (rows)\\nand 64 samples (columns), for the human tumor data. Only a random sample\\nof 100 rows are shown. The display is a heat map, ranging from bright green\\n(negative, under expressed) to bright red (positive, over expressed). Missing values\\nare gray. The rows and columns are displayed in a randomly chosen order.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='86914b79-2437-4f65-8b88-f682a9754d93', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 27, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1. Introduction 7\\nWho Should Read this Book\\nThis book is designed for researchers and students in a broad variety of\\nﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\\nexpect that the reader will have had at least one elementary course in\\nstatistics, covering basic topics including linear regression.\\nWe have not attempted to write a comprehensive catalog of learning\\nmethods, but rather to describe some of the most important techniques.\\nEqually notable, we describe the underlying concepts and considerations\\nby which a researcher can judge a learning method. We have tried to write\\nthis book in an intuitive fashion, emphasizing concepts rather than math-\\nematical details.\\nAs statisticians, our exposition will naturally reﬂect our backgrounds and\\nareas of expertise. However in the past eight years we have been attending\\nconferences in neural networks, data mining and machine learning, and our\\nthinking has been heavily inﬂuenced by these exciting ﬁelds. This inﬂuence\\nis evident in our current research, and in this book.\\nH o wT h i sB o o ki sO r g a n i z e d\\nOur view is that one must understand simple methods before trying to\\ngrasp more complex ones. Hence, after giving an overview of the supervis-\\ning learning problem inChapter 2, we discuss linear methods for regression\\nand classiﬁcation in Chapters 3 and 4.I n Chapter 5 we describe splines,\\nwavelets and regularization/penalization methods for a single predictor,\\nwhile Chapter 6covers kernel methods and local regression. Both of these\\nsets of methods are important building blocks for high-dimensional learn-\\ning techniques. Model assessment and selection is the topic ofChapter 7,\\ncovering the concepts of bias and variance, overﬁtting and methods such as\\ncross-validation for choosing models.Chapter 8discusses model inference\\nand averaging, including an overview of maximum likelihood, Bayesian in-\\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\\nA related procedure called boosting is the focus ofChapter 10.\\nIn Chapters 9–13 we describe a series of structured methods for su-\\npervised learning, withChapters 9 and 11covering regression andChap-\\nters 12 and 13focusing on classiﬁcation.Chapter 14describes methods for\\nunsupervised learning. Two recently proposed techniques, random forests\\nand ensemble learning, are discussed inChapters 15 and 16. We describe\\nundirected graphical models in Chapter 17 and ﬁnally we study high-\\ndimensional problems inChapter 18.\\nAt the end of each chapter we discusscomputational considerationsim-\\nportant for data mining applications, including how the computations scale\\nwith the number of observations and predictors. Each chapter ends with\\nBibliographic Notesgiving background references for the material.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6340e9c3-ad4a-4183-b295-d937a9111f7b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 28, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8 1. Introduction\\nWe recommend that Chapters 1–4 be ﬁrst read in sequence. Chapter 7\\nshould also be considered mandatory, as it covers central concepts that\\npertain to all learning methods. With this in mind, the rest of the book\\ncan be read sequentially, or sampled, depending on the reader’s interest.\\nThe symbol\\n indicates a technically diﬃcult section, one that can\\nbe skipped without interrupting the ﬂow of the discussion.\\nBook Website\\nThe website for this book is located at\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nIt contains a number of resources, including many of the datasets used in\\nthis book.\\nNote for Instructors\\nWe have successively used the ﬁrst edition of this book as the basis for a\\ntwo-quartercourse,andwiththeadditional materialsinthissecondedition,\\nit could even be used for a three-quarter sequence. Exercises are provided at\\nthe end of each chapter. It is important for students to have access to good\\nsoftware tools for these topics. We used the R and S-PLUS programming\\nlanguages in our courses.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02f53fa3-7b03-47e9-b3cb-eaf98cf8930e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 29, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2\\nOverview of Supervised Learning\\n2.1 Introduction\\nThe ﬁrst three examples described in Chapter 1 have several components\\nin common. For each there is a set of variables that might be denoted as\\ninputs, which are measured or preset. These have some inﬂuence on one or\\nmore outputs. For each example the goal is to use the inputs to predict the\\nvalues of the outputs. This exercise is calledsupervised learning.\\nWe have used the more modern language of machine learning. In the\\nstatistical literature the inputs are often called thepredictors, a term we\\nwill use interchangeably with inputs, and more classically theindependent\\nvariables.Inthepatternrecognitionliteraturetheterm featuresispreferred,\\nwhich we use as well. The outputs are called theresponses, or classically\\nthe dependent variables.\\n2.2 Variable Types and Terminology\\nThe outputs vary in nature among the examples. In the glucose prediction\\nexample, the output is aquantitative measurement, where some measure-\\nments are bigger than others, and measurements close in value are close\\nin nature. In the famous Iris discrimination example due to R. A. Fisher,\\nthe output isqualitative (species of Iris) and assumes values in a ﬁnite set\\nG = {Virginica, Setosa and Versicolor}. In the handwritten digit example\\nthe output is one of 10 diﬀerent digitclasses: G ={0,1,..., 9}.I nb o t ho f\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 9\\nDOI: 10.1007/b94608_2,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='232454b4-98a2-4bad-b35a-081ddb8d4e31', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 30, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10 2. Overview of Supervised Learning\\nthese there is no explicit ordering in the classes, and in fact often descrip-\\ntive labels rather than numbers are used to denote the classes. Qualitative\\nvariables are also referred to ascategoricalor discrete variables as well as\\nfactors.\\nFor both types of outputs it makes sense to think of using the inputs to\\npredict the output. Given some speciﬁc atmospheric measurements today\\nand yesterday, we want to predict the ozone level tomorrow. Given the\\ngrayscale values for the pixels of the digitized image of the handwritten\\ndigit, we want to predict its class label.\\nThis distinction in output type has led to a naming convention for the\\nprediction tasks:regressionwhen we predict quantitative outputs, andclas-\\nsiﬁcation when we predict qualitative outputs. We will see that these two\\ntasks have a lot in common, and in particular both can be viewed as a task\\nin function approximation.\\nInputs also vary in measurement type; we can have some of each of qual-\\nitative and quantitative input variables. These have also led to distinctions\\nin the types of methods that are used for prediction: some methods are\\ndeﬁned most naturally for quantitative inputs, some most naturally for\\nqualitative and some for both.\\nA third variable type isordered categorical,s u c ha ssmall, medium and\\nlarge, where there is an ordering between the values, but no metric notion\\nis appropriate (the diﬀerence between medium and small need not be the\\nsame as that between large and medium). These are discussed further in\\nChapter 4.\\nQualitative variables are typically represented numerically by codes. The\\neasiest case is when there are only two classes or categories, such as “suc-\\ncess” or “failure,” “survived” or “died.” These are often represented by a\\nsingle binary digit or bit as 0 or 1, or else by−1 and 1. For reasons that will\\nbecome apparent, such numeric codes are sometimes referred to astargets.\\nWhen there are more than two categories, several alternatives are available.\\nThe most useful and commonly used coding is viadummy variables.H e r ea\\nK-level qualitative variable is represented by a vector ofK binary variables\\nor bits, only one of which is “on” at a time. Although more compact coding\\nschemes are possible, dummy variables are symmetric in the levels of the\\nfactor.\\nWe will typically denote an input variable by the symbolX.I f X is\\na vector, its components can be accessed by subscriptsXj. Quantitative\\noutputs will be denoted byY, and qualitative outputs byG (for group).\\nWe use uppercase letters such asX, Y or G when referring to the generic\\naspects of a variable. Observed values are written in lowercase; hence the\\nith observed value of X is written as xi (where xi is again a scalar or\\nvector). Matrices are represented by bold uppercase letters; for example, a\\nset ofN input p-vectorsxi,i =1 ,...,N would be represented by theN×p\\nmatrix X. In general, vectors will not be bold, except when they haveN\\ncomponents; this convention distinguishes ap-vector of inputsxi for the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a675c0d8-abe1-41c3-a2a2-1ef720a8a05d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 31, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Least Squares and Nearest Neighbors 11\\nith observation from theN-vector xj consisting of all the observations on\\nvariable Xj. Since all vectors are assumed to be column vectors, theith\\nrow ofX is xT\\ni , the vector transpose ofxi.\\nFor the moment we can loosely state the learning task as follows: given\\nthe value of an input vectorX, make a good prediction of the outputY,\\ndenoted byˆY (pronounced “y-hat”). IfY takes values in IR then so should\\nˆY; likewise for categorical outputs,ˆG should take values in the same setG\\nassociated withG.\\nFor a two-classG, one approach is to denote the binary coded target\\nas Y, and then treat it as a quantitative output. The predictionsˆY will\\ntypically lie in [0,1], and we can assign toˆG the class label according to\\nwhether ˆy> 0.5. This approach generalizes toK-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus\\nsuppose we have available a set of measurements (xi,yi)o r( xi,gi),i =\\n1,...,N ,knownasthe training data,withwhichtoconstructourprediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful prediction methods: the\\nlinear model ﬁt by least squares and thek-nearest-neighbor prediction rule.\\nThe linear model makes huge assumptions about structure and yields stable\\nbut possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions are often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the past 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT =( X1,X2,...,X p), we predict the outputY via the model\\nˆY = ˆβ0 +\\np∑\\nj=1\\nXj ˆβj. (2.1)\\nThe term ˆβ0 is the intercept, also known as thebias in machine learning.\\nOften it is convenient to include the constant variable 1 inX, includeˆβ0 in\\nthe vector of coeﬃcientsˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY = XT ˆβ, (2.2)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='40958dab-358c-411e-85c5-a789ddf964c9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 32, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12 2. Overview of Supervised Learning\\nwhere XT denotes vector or matrix transpose (X being a column vector).\\nHere we are modeling a single output, soˆY is a scalar; in generalˆY can be\\na K–vector, in which caseβwould be ap×K matrix of coeﬃcients. In the\\n(p + 1)-dimensional input–output space, (X, ˆY) represents a hyperplane.\\nIf the constant is included inX, then the hyperplane includes the origin\\nand is a subspace; if not, it is an aﬃne set cutting theY-axis at the point\\n(0, ˆβ0). From now on we assume that the intercept is included inˆβ.\\nViewed as a function over thep-dimensional input space,f(X)= XTβ\\nis linear, and the gradientf′(X)= βis a vector in input space that points\\nin the steepest uphill direction.\\nHow do we ﬁt the linear model to a set of training data? There are\\nmany diﬀerent methods, but by far the most popular is the method of\\nleast squares. In this approach, we pick the coeﬃcientsβto minimize the\\nresidual sum of squares\\nRSS(β)=\\nN∑\\ni=1\\n(yi −xT\\ni β)2. (2.3)\\nRSS(β) is a quadratic function of the parameters, and hence its minimum\\nalways exists, but may not be unique. The solution is easiest to characterize\\nin matrix notation. We can write\\nRSS(β)=( y−Xβ)T(y−Xβ), (2.4)\\nwhere X is an N × p matrix with each row an input vector, andy is an\\nN-vector of the outputs in the training set. Diﬀerentiating w.r.t.βwe get\\nthe normal equations\\nXT(y−Xβ)=0 . (2.5)\\nIf XTX is nonsingular, then the unique solution is given by\\nˆβ=( XTX)−1XTy, (2.6)\\nand the ﬁtted value at theith input xi is ˆyi =ˆy(xi)= xT\\ni ˆβ.A ta na r b i -\\ntrary input x0 the prediction is ˆy(x0)= xT\\n0 ˆβ. The entire ﬁtted surface is\\ncharacterized by thep parameters ˆβ. Intuitively, it seems that we do not\\nneed a very large data set to ﬁt such a model.\\nLet’s look at an example of the linear model in a classiﬁcation context.\\nFigure 2.1 shows a scatterplot of training data on a pair of inputsX1 and\\nX2. The data are simulated, and for the present the simulation model is\\nnot important. The output class variableG has the valuesBLUE or ORANGE,\\nand is represented as such in the scatterplot. There are 100 points in each\\nof the two classes. The linear regression model was ﬁt to these data, with\\nthe response Y coded as 0 forBLUE and 1 forORANGE. The ﬁtted valuesˆY\\nare converted to a ﬁtted class variableˆG according to the rule\\nˆG =\\n{\\nORANGE if ˆY> 0.5,\\nBLUE if ˆY ≤0.5. (2.7)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b259811-878e-46fe-8f87-64cd323e14f0', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 33, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Least Squares and Nearest Neighbors 13\\nLinear Regression of 0/1 Response\\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. ..\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nFIGURE 2.1.A classiﬁcation example in two dimensions. The classes are coded\\nas a binary variable (BLUE =0 , ORANGE =1 ), and then ﬁt by linear regression.\\nThe line is the decision boundary deﬁned byxT ˆβ=0 .5. The orange shaded region\\ndenotes that part of input space classiﬁed as ORANGE, while the blue region is\\nclassiﬁed as BLUE.\\nThe set of points in IR2 classiﬁed asORANGE corresponds to{x : xT ˆβ>0.5},\\nindicated in Figure 2.1, and the two predicted classes are separated by the\\ndecision boundary{x : xT ˆβ=0 .5}, which is linear in this case. We see\\nthat for these data there are several misclassiﬁcations on both sides of the\\ndecisionboundary.Perhapsourlinearmodelistoorigid—oraresucherrors\\nunavoidable? Remember that these are errors on the training data itself,\\nand we have not said where the constructed data came from. Consider the\\ntwo possible scenarios:\\nScenario 1: The training data in each class were generated from bivariate\\nGaussian distributions with uncorrelated components and diﬀerent\\nmeans.\\nScenario 2: Thetrainingdataineachclasscamefromamixtureof10low-\\nvariance Gaussian distributions, with individual means themselves\\ndistributed as Gaussian.\\nA mixture of Gaussians is best described in terms of the generative\\nmodel. One ﬁrst generates a discrete variable that determines which of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02c3b774-c7e6-4a5e-8b3c-ed45b46a62f9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 34, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='14 2. Overview of Supervised Learning\\nthe component Gaussians to use, and then generates an observation from\\nthe chosen density. In the case of one Gaussian per class, we will see in\\nChapter 4 that a linear decision boundary is the best one can do, and that\\nour estimate is almost optimal. The region of overlap is inevitable, and\\nfuture data to be predicted will be plagued by this overlap as well.\\nIn the case of mixtures of tightly clustered Gaussians the story is dif-\\nferent. A linear decision boundary is unlikely to be optimal, and in fact is\\nnot. The optimal decision boundary is nonlinear and disjoint, and as such\\nwill be much more diﬃcult to obtain.\\nWe now look at another classiﬁcation and regression procedure that is\\nin some sense at the opposite end of the spectrum to the linear model, and\\nfar better suited to the second scenario.\\n2.3.2 Nearest-Neighbor Methods\\nNearest-neighbor methods use those observations in the training setT clos-\\nest in input space tox to form ˆY. Speciﬁcally, thek-nearest neighbor ﬁt\\nfor ˆY is deﬁned as follows:\\nˆY(x)= 1\\nk\\n∑\\nxi∈Nk(x)\\nyi, (2.8)\\nwhere Nk(x) is the neighborhood ofx deﬁned by thek closest pointsxi in\\nthe training sample. Closeness implies a metric, which for the moment we\\nassume is Euclidean distance. So, in words, we ﬁnd thek observations with\\nxi closest tox in input space, and average their responses.\\nI nF i g u r e2 . 2w eu s et h es a m et r a i n i n gd a t aa si nF i g u r e2 . 1 ,a n du s e\\n15-nearest-neighbor averaging of the binary coded response as the method\\nof ﬁtting. Thus ˆY is the proportion ofORANGE’s in the neighborhood, and\\nso assigning classORANGE to ˆG if ˆY> 0.5 amounts to a majority vote in\\nthe neighborhood. The colored regions indicate all those points in input\\nspace classiﬁed as BLUE or ORANGE by such a rule, in this case found by\\nevaluating the procedure on a ﬁne grid in input space. We see that the\\ndecision boundaries that separate theBLUE from theORANGE regions are far\\nmore irregular, and respond to local clusters where one class dominates.\\nFigure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆY is\\nassigned the valueyℓ of the closest pointxℓ to x in the training data. In\\nthis case the regions of classiﬁcation can be computed relatively easily, and\\ncorrespond to a Voronoi tessellation of the training data. Each pointxi\\nhas an associated tile bounding the region for which it is the closest input\\npoint. For all pointsx in the tile,ˆG(x)= gi. The decision boundary is even\\nmore irregular than before.\\nThe method of k-nearest-neighbor averaging is deﬁned in exactly the\\nsame way for regression of a quantitative outputY, althoughk =1w o u l d\\nbe an unlikely choice.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d341bd6-74c2-451c-b0a8-5b6803b4489a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 35, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Least Squares and Nearest Neighbors 15\\n15-Nearest Neighbor Classifier\\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ....................................................................................... ........................ ............................ ................................................... .................................................. ................................................ ............................................. ........................................... ........................................... ......................................... ........................................ ....................................... .................................. .................................... .................................... ................................... .................................. .................................. ................................. ................................. ................................ .............................. ............................ ........................... ......................... ....................... ...................... ...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n............................................................................................. ............ ................ . .................. .................. ..................... ....................... .......................... .......................... ........................... ........................... .............................. .................................. .................................. ................................ ................................. ................................... ................................... .................................... .................................... .................................... ...................................... ........................................ .......................................... ............................................ ............................................. ..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nFIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable(BLUE =0 ,ORANGE =1 ) and\\nthen ﬁt by15-nearest-neighbor averaging as in (2.8). The predicted class is hence\\nchosen by majority vote amongst the15-nearest neighbors.\\nIn Figure 2.2 we see that far fewer training observations are misclassiﬁed\\nthan in Figure 2.1. This should not give us too much comfort, though, since\\nin Figure 2.3none of the training data are misclassiﬁed. A little thought\\nsuggests that for k-nearest-neighbor ﬁts, the error on the training data\\nshould be approximately an increasing function ofk, and will always be 0\\nfor k = 1. An independent test set would give us a more satisfactory means\\nfor comparing the diﬀerent methods.\\nIt appears thatk-nearest-neighbor ﬁts have a single parameter, the num-\\nber of neighborsk, compared to thep parameters in least-squares ﬁts. Al-\\nthough this is the case, we will see that theeﬀectivenumber of parameters\\nof k-nearest neighbors isN/k and is generally bigger thanp, and decreases\\nwith increasing k. To get an idea of why, note that if the neighborhoods\\nwere nonoverlapping, there would beN/k neighborhoods and we would ﬁt\\none parameter (a mean) in each neighborhood.\\nIt is also clear that we cannot use sum-of-squared errors on the training\\nset as a criterion for pickingk, since we would always pickk =1 !I tw o u l d\\nseem thatk-nearest-neighbor methods would be more appropriate for the\\nmixture Scenario 2 described above, while for Gaussian data the decision\\nboundaries ofk-nearest neighbors would be unnecessarily noisy.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4649424-36a0-4dd3-b329-c51ca70bcc67', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 36, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16 2. Overview of Supervised Learning\\n1−Nearest Neighbor Classifier\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no o\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nFIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable(BLUE =0 ,ORANGE =1 ),a n d\\nthen predicted by1-nearest-neighbor classiﬁcation.\\n2.3.3 From Least Squares to Nearest Neighbors\\nThe linear decision boundary from least squares is very smooth, and ap-\\nparently stable to ﬁt. It does appear to rely heavily on the assumption\\nthat a linear decision boundary is appropriate. In language we will develop\\nshortly, it has low variance and potentially high bias.\\nOn the other hand, thek-nearest-neighbor procedures do not appear to\\nrelyonanystringentassumptionsabouttheunderlyingdata,andcanadapt\\nto any situation. However, any particular subregion of the decision bound-\\nary depends on a handful of input points and their particular positions,\\nand is thus wiggly and unstable—high variance and low bias.\\nEach method has its own situations for which it works best; in particular\\nlinear regression is more appropriate for Scenario 1 above, while nearest\\nneighbors are more suitable for Scenario 2. The time has come to expose\\nthe oracle! The data in fact were simulated from a model somewhere be-\\ntween the two, but closer to Scenario 2. First we generated 10 meansmk\\nfrom a bivariate Gaussian distributionN((1,0)T,I) and labeled this class\\nBLUE. Similarly, 10 more were drawn fromN((0,1)T,I) and labeled class\\nORANGE. Then for each class we generated 100 observations as follows: for\\neach observation, we picked anmk at random with probability 1/10, and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cb6755c6-9fa6-4692-a734-4f0a8a2a15b2', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 37, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Least Squares and Nearest Neighbors 17\\nDegrees of Freedom − N/k\\nTest Error\\n0.10 0.15 0.20 0.25 0.30\\n  2   3   5   8  12  18  29  67 200\\n151 101  69  45  31  21  11   7   5   3   1\\nTrain\\nTest\\nBayes\\nk −  Number of Nearest Neighbors\\nLinear\\nFIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fig-\\nures 2.1, 2.2 and 2.3. A single training sample of size200 was used, and a test\\nsample of size 10,000. The orange curves are test and the blue are training er-\\nror fork-nearest-neighbor classiﬁcation. The results for linear regression are the\\nbigger orange and blue squares at three degrees of freedom. The purple line is the\\noptimal Bayes error rate.\\nthen generated aN(mk,I/5), thus leading to a mixture of Gaussian clus-\\nters for each class. Figure 2.4 shows the results of classifying 10,000 new\\nobservations generated from the model. We compare the results for least\\nsquares and those fork-nearest neighbors for a range of values ofk.\\nA large subset of the most popular techniques in use today are variants of\\nthese two simple procedures. In fact 1-nearest-neighbor, the simplest of all,\\ncaptures a large percentage of the market for low-dimensional problems.\\nThe following list describes some ways in which these simple procedures\\nhave been enhanced:\\n•Kernel methods use weights that decrease smoothly to zero with dis-\\ntance from the target point, rather than the eﬀective 0/1 weights used\\nby k-nearest neighbors.\\n•In high-dimensional spaces the distance kernels are modiﬁed to em-\\nphasize some variable more than others.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='298fa73a-1c62-4d69-b1b3-b25aba094db8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 38, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='18 2. Overview of Supervised Learning\\n•Local regression ﬁts linear models by locally weighted least squares,\\nrather than ﬁtting constants locally.\\n•Linear models ﬁt to a basis expansion of the original inputs allow\\narbitrarily complex models.\\n•Projection pursuit and neural network models consist of sums of non-\\nlinearly transformed linear models.\\n2.4 Statistical Decision Theory\\nIn this section we develop a small amount of theory that provides a frame-\\nwork for developing models such as those discussed informally so far. We\\nﬁrst consider the case of a quantitative output, and place ourselves in the\\nworld of random variables and probability spaces. LetX ∈IRp denote a\\nreal valued random input vector, andY ∈IR a real valued random out-\\nput variable, with joint distribution Pr(X,Y ). We seek a functionf(X)\\nfor predicting Y given values of the inputX. This theory requires aloss\\nfunction L(Y,f (X)) for penalizing errors in prediction, and by far the most\\ncommon and convenient issquared error loss: L(Y,f (X)) = (Y −f(X))2.\\nThis leads us to a criterion for choosingf,\\nEPE(f)=E ( Y −f(X))2 (2.9)\\n=\\n∫\\n[y−f(x)]2 Pr(dx,dy), (2.10)\\nthe expected (squared) prediction error . By conditioning1 on X,w ec a n\\nwrite EPE as\\nEPE(f)=E XEY|X\\n⎤\\n[Y −f(X)]2|X\\n⎦\\n(2.11)\\nand we see that it suﬃces to minimize EPE pointwise:\\nf(x) = argmincEY|X\\n⎤\\n[Y −c]2|X = x\\n⎦\\n. (2.12)\\nThe solution is\\nf(x)=E (Y|X = x), (2.13)\\nthe conditional expectation, also known as theregression function. Thus\\nthe best prediction ofY at any pointX = x is the conditional mean, when\\nbest is measured by average squared error.\\nThe nearest-neighbor methods attempt to directly implement this recipe\\nusing the training data. At each pointx, we might ask for the average of all\\n1Conditioning here amounts to factoring the joint density Pr(X,Y )=P r (Y|X)Pr(X)\\nwhere Pr(Y|X)=P r (Y,X )/Pr(X), and splitting up the bivariate integral accordingly.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='43648938-2639-4bb4-a069-a1feccbf38e2', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 39, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Statistical Decision Theory 19\\nthose yis with inputxi = x. Since there is typically at most one observation\\nat any pointx, we settle for\\nˆf(x)=A v e (yi|xi ∈Nk(x)), (2.14)\\nwhere “Ave” denotes average, andNk(x) is the neighborhood containing\\nthe k points inT closest tox. Two approximations are happening here:\\n•expectation is approximated by averaging over sample data;\\n•conditioning at a point is relaxed to conditioning on some region\\n“close” to the target point.\\nFor large training sample sizeN, the points in the neighborhood are likely\\nto be close to x,a n da sk gets large the average will get more stable.\\nIn fact, under mild regularity conditions on the joint probability distri-\\nbution Pr(X,Y ), one can show that asN,k →∞ such that k/N →0,\\nˆf(x) → E(Y|X = x). In light of this, why look further, since it seems\\nwe have a universal approximator? We often do not have very large sam-\\nples. If the linear or some more structured model is appropriate, then we\\ncan usually get a more stable estimate thank-nearest neighbors, although\\nsuch knowledge has to be learned from the data as well. There are other\\nproblems though, sometimes disastrous. In Section 2.5 we see that as the\\ndimension p gets large, so does the metric size of thek-nearest neighbor-\\nhood. So settling for nearest neighborhood as a surrogate for conditioning\\nwill fail us miserably. The convergence above still holds, but therate of\\nconvergence decreases as the dimension increases.\\nHow does linear regression ﬁt into this framework? The simplest explana-\\ntion is that one assumes that the regression functionf(x) is approximately\\nlinear in its arguments:\\nf(x)≈xTβ. (2.15)\\nThisisamodel-basedapproach—wespecifyamodelfortheregressionfunc-\\ntion. Plugging this linear model forf(x) into EPE (2.9) and diﬀerentiating\\nwe can solve forβtheoretically:\\nβ=[ E (XXT)]−1E(XY ). (2.16)\\nNote we havenot conditioned on X; rather we have used our knowledge\\nof the functional relationship topool over values ofX. The least squares\\nsolution (2.6) amounts to replacing the expectation in (2.16) by averages\\nover the training data.\\nSo both k-nearest neighbors and least squares end up approximating\\nconditional expectations by averages. But they diﬀer dramatically in terms\\nof model assumptions:\\n•Least squares assumesf(x) is well approximated by a globally linear\\nfunction.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eea315c6-dcec-482a-957d-970321867bc5', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 40, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='20 2. Overview of Supervised Learning\\n•k-nearest neighbors assumesf(x) is well approximated by a locally\\nconstant function.\\nAlthough the latter seems more palatable, we have already seen that we\\nmay pay a price for this ﬂexibility.\\nMany of the more modern techniques described in this book are model\\nbased, although far more ﬂexible than the rigid linear model. For example,\\nadditive models assume that\\nf(X)=\\np∑\\nj=1\\nfj(Xj). (2.17)\\nThis retains the additivity of the linear model, but each coordinate function\\nfj is arbitrary. It turns out that the optimal estimate for the additive model\\nuses techniques such ask-nearest neighbors to approximateunivariate con-\\nditional expectations simultaneously for each of the coordinate functions.\\nThus the problems of estimating a conditional expectation in high dimen-\\nsionsaresweptawayinthiscasebyimposingsome(oftenunrealistic)model\\nassumptions, in this case additivity.\\nAre we happy with the criterion (2.11)? What happens if we replace the\\nL2 loss function with theL1:E |Y −f(X)|? The solution in this case is the\\nconditional median,\\nˆf(x)=m e d i a n (Y|X = x), (2.18)\\nwhich is a diﬀerent measure of location, and its estimates are more robust\\nthan those for the conditional mean. L1 criteria have discontinuities in\\ntheir derivatives, which have hindered their widespread use. Other more\\nresistant loss functions will be mentioned in later chapters, but squared\\nerror is analytically convenient and the most popular.\\nWhat do we do when the output is a categorical variableG?T h es a m e\\nparadigm works here, except we need a diﬀerent loss function for penalizing\\nprediction errors. An estimateˆG will assume values inG, the set of possible\\nclasses. Our loss function can be represented by aK×K matrix L,w h e r e\\nK =c a r d (G). L will be zero on the diagonal and nonnegative elsewhere,\\nwhere L(k,ℓ) is the price paid for classifying an observation belonging to\\nclass Gk as Gℓ. Most often we use thezero–one loss function, where all\\nmisclassiﬁcations are charged a single unit. The expected prediction error\\nis\\nEPE = E[L(G, ˆG(X))], (2.19)\\nwhere again the expectation is taken with respect to the joint distribution\\nPr(G,X). Again we condition, and can write EPE as\\nEPE = EX\\nK∑\\nk=1\\nL[Gk, ˆG(X)]Pr(Gk|X) (2.20)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f652d8c6-2208-42a3-9a72-14a2c3ec0119', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 41, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Statistical Decision Theory 21\\nBayes Optimal Classifier\\n..... ..... ..... ....... ........ .......... ........... ............. ............... ................. ................... .. ..................... ...... ....................... ....... ........................ ......... ........................... .......... ............................ .......... ............................. ........... .............................. ........... ................................ ........... ................................ ............ ................................. ............ ................................... ............. ................................... ............ .................................... ............ .................................... ............ ...................................... ............. ....................................... ............. ....................................... ............. ........................................ .............. ......................................... ............. .......................................................... .......................................................... .......................................................... ......................................................... .......................................................... .......................................................... .......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ......................................................... ........................................................ ........................................................ ......................................................... ......................................................... ............\\n............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. ............................................ ........................................ ..................................... ................................. ............................... ............................. ............................ .......................... .......................... ........................ ..................... ..................... ..................... ..................... .................. ................. ................. ............... .............. ........... ...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nFIGURE 2.5.The optimal Bayes decision boundary for the simulation example\\nof Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class,\\nthis boundary can be calculated exactly (Exercise 2.2).\\nand again it suﬃces to minimize EPE pointwise:\\nˆG(x) = argming∈G\\nK∑\\nk=1\\nL(Gk,g)Pr(Gk|X = x). (2.21)\\nWith the 0–1 loss function this simpliﬁes to\\nˆG(x) = argming∈G[1−Pr(g|X = x)] (2.22)\\nor simply\\nˆG(x)= Gk if Pr(Gk|X = x)=m a x\\ng∈G\\nPr(g|X = x). (2.23)\\nThis reasonable solution is known as theBayes classiﬁer, and says that\\nwe classify to the most probable class, using the conditional (discrete) dis-\\ntribution Pr(G|X). Figure 2.5 shows the Bayes-optimal decision boundary\\nfor our simulation example. The error rate of the Bayes classiﬁer is called\\nthe Bayes rate.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0665f3f6-ff44-4897-aeda-6a4c82b50e32', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 42, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='22 2. Overview of Supervised Learning\\nAgain we see that thek-nearest neighbor classiﬁer directly approximates\\nthis solution—a majority vote in a nearest neighborhood amounts to ex-\\nactly this, except that conditional probability at a point is relaxed to con-\\nditional probability within a neighborhood of a point, and probabilities are\\nestimated by training-sample proportions.\\nSuppose for a two-class problem we had taken the dummy-variable ap-\\nproach and codedG v i aab i n a r yY, followed by squared error loss estima-\\ntion. Then ˆf(X)=E (Y|X)=P r (G =G1|X)i fG1 corresponded toY =1 .\\nLikewise for a K-class problem, E(Yk|X)=P r (G = Gk|X). This shows\\nthat our dummy-variable regression procedure, followed by classiﬁcation to\\nthe largest ﬁtted value, is another way of representing the Bayes classiﬁer.\\nAlthough this theory is exact, in practice problems can occur, depending\\non the regression model used. For example, when linear regression is used,\\nˆf(X) need not be positive, and we might be suspicious about using it as\\nan estimate of a probability. We will discuss a variety of approaches to\\nmodeling Pr(G|X) in Chapter 4.\\n2.5 Local Methods in High Dimensions\\nWe have examined two learning techniques for prediction so far: the stable\\nbut biased linear model and the less stable but apparently less biased class\\nof k-nearest-neighbor estimates. It would seem that with a reasonably large\\nset of training data, we could always approximate the theoretically optimal\\nconditional expectation byk-nearest-neighbor averaging, since we should\\nbe able to ﬁnd a fairly large neighborhood of observations close to anyx\\nand average them. This approach and our intuition breaks down in high\\ndimensions, and the phenomenon is commonly referred to as the curse\\nof dimensionality (Bellman, 1961). There are many manifestations of this\\nproblem, and we will examine a few here.\\nConsider the nearest-neighbor procedure for inputs uniformly distributed\\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a\\nhypercubical neighborhood about a target point to capture a fractionr of\\nthe observations. Since this corresponds to a fractionr of the unit volume,\\ntheexpectededgelength will be ep(r)= r1/p. Intendimensions e10(0.01) =\\n0.63 ande10(0.1) = 0.80, while the entire range for each input is only 1.0.\\nSo to capture 1% or 10% of the data to form a local average, we must cover\\n63% or 80% of the range of each input variable. Such neighborhoods are no\\nlonger “local.” Reducingr dramatically does not help much either, since\\nthe fewer observations we average, the higher is the variance of our ﬁt.\\nAnother consequence of the sparse sampling in high dimensions is that\\nall sample points are close to an edge of the sample. ConsiderN data points\\nuniformly distributed in ap-dimensional unit ball centered at the origin.\\nSuppose we consider a nearest-neighbor estimate at the origin. The median', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='530bf8eb-503c-4e12-a460-2f2d804b2f4d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 43, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Local Methods in High Dimensions 23\\n1\\n1\\n0\\nUnit Cube\\nFraction of Volume\\nDistance\\n0.0 0.2 0.4 0.6\\n0.0 0.2 0.4 0.6 0.8 1.0\\np=1\\np=2\\np=3\\np=10\\nNeighborhood\\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubical\\nneighborhood for uniform data in a unit cube. The ﬁgure on the right shows the\\nside-length of the subcube needed to capture a fractionr of the volume of the data,\\nfor diﬀerent dimensionsp. In ten dimensions we need to cover80% of the range\\nof each coordinate to capture10% of the data.\\ndistance from the origin to the closest data point is given by the expression\\nd(p,N )=\\n⎤\\n1−1\\n2\\n1/N⎦1/p\\n(2.24)\\n(Exercise 2.3). A more complicated expression exists for the mean distance\\nto the closest point. ForN = 500, p =1 0, d(p,N ) ≈0.52, more than\\nhalfway totheboundary.Hencemostdatapointsareclosertotheboundary\\nof the sample space than to any other data point. The reason that this\\npresents a problem is that prediction is much more diﬃcult near the edges\\nof the training sample. One must extrapolate from neighboring sample\\npoints rather than interpolate between them.\\nAnother manifestation of the curse is that the sampling density is pro-\\nportional toN1/p,w h e r ep is the dimension of the input space andN is the\\nsample size. Thus, ifN1 = 100 represents a dense sample for a single input\\nproblem, then N10 = 10010 is the sample size required for the same sam-\\npling density with 10 inputs. Thus in high dimensions all feasible training\\nsamples sparsely populate the input space.\\nLet us construct another uniform example. Suppose we have 1000 train-\\ning examples xi generated uniformly on [−1,1]p. Assume that the true\\nrelationship betweenX and Y is\\nY = f(X)= e−8||X||2\\n,\\nwithout any measurement error. We use the 1-nearest-neighbor rule to\\npredict y0 at the test-pointx0 = 0. Denote the training set byT .W ec a n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cdd34dd9-e257-4260-bb2f-17700423b738', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 44, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='24 2. Overview of Supervised Learning\\ncompute the expected prediction error atx0 for our procedure, averaging\\nover all such samples of size 1000. Since the problem is deterministic, this\\nis the mean squared error (MSE) for estimatingf(0):\\nMSE(x0)=E T [f(x0)−ˆy0]2\\n=E T [ˆy0 −ET (ˆy0)]2 +[ET (ˆy0)−f(x0)]2\\n=V a rT (ˆy0)+Bias 2(ˆy0). (2.25)\\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\\ncomponents that will become familiar as we proceed: variance and squared\\nbias. Such a decomposition is always possible and often useful, and is known\\nas the bias–variance decomposition. Unless the nearest neighbor is at 0,\\nˆy0 will be smaller thanf(0) in this example, and so the average estimate\\nwill be biased downward. The variance is due to the sampling variance of\\nthe 1-nearest neighbor. In low dimensions and withN = 1000, the nearest\\nneighbor is very close to 0, and so both the bias and variance are small. As\\nthe dimension increases, the nearest neighbor tends to stray further from\\nthe target point, and both bias and variance are incurred. Byp = 10, for\\nmore than 99% of the samples the nearest neighbor is a distance greater\\nthan 0.5 from the origin. Thus asp increases, the estimate tends to be 0\\nmore often than not, and hence the MSE levels oﬀ at 1.0, as does the bias,\\nand the variance starts dropping (an artifact of this example).\\nAlthough this is a highly contrived example, similar phenomena occur\\nmore generally. The complexity of functions of many variables can grow\\nexponentially with the dimension, and if we wish to be able to estimate\\nsuch functions with the same accuracy as function in low dimensions, then\\nwe need the size of our training set to grow exponentially as well. In this\\nexample, the function is a complex interaction of allp variables involved.\\nThe dependence of the bias term on distance depends on the truth, and\\nit need not always dominate with 1-nearest neighbor. For example, if the\\nfunction always involves only a few dimensions as in Figure 2.8, then the\\nvariance can dominate instead.\\nSuppose, on the other hand, that we know that the relationship between\\nY and X is linear,\\nY = XTβ+ε, (2.26)\\nwhere ε∼N(0,σ2) and we ﬁt the model by least squares to the train-\\ning data. For an arbitrary test pointx0,w eh a v eˆy0 = xT\\n0 ˆβ,w h i c hc a n\\nbe written as ˆy0 = xT\\n0 β+∑N\\ni=1 ℓi(x0)εi,w h e r eℓi(x0)i st h eith element\\nof X(XTX)−1x0. Since under this model the least squares estimates are', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8387e1da-81b8-4b01-b5a6-4c352a7a1aca', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 45, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Local Methods in High Dimensions 25\\nX\\nf(X)\\n-1.0 -0.5 0.0 0.5 1.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\n\\x81\\n1-NN in One Dimension\\nX1\\nX2\\n-1.0 -0.5 0.0 0.5 1.0\\n-1.0 -0.5 0.0 0.5 1.0\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n1-NN in One vs. Two Dimensions\\nDimension\\nAverage Distance to Nearest Neighbor\\n2468 1 0\\n0.0 0.2 0.4 0.6 0.8\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nDistance to 1-NN vs. Dimension\\nDimension\\nMse\\n2468 1 0\\n0.0 0.2 0.4 0.6 0.8 1.0\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\nMSE vs. Dimension\\n\\x81 MSE\\n\\x81 Variance\\n\\x81 Sq. Bias\\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\\nity and its eﬀect on MSE, bias and variance. The input features are uniformly\\ndistributed in[−1,1]p for p =1 ,..., 10 The top left panel shows the target func-\\ntion (no noise) inIR: f(X)= e−8||X||2\\n, and demonstrates the error that1-nearest\\nneighbor makes in estimatingf(0). The training point is indicated by the blue tick\\nmark. The top right panel illustrates why the radius of the1-nearest neighborhood\\nincreases with dimensionp. The lower left panel shows the average radius of the\\n1-nearest neighborhoods. The lower-right panel shows the MSE, squared bias and\\nvariance curves as a function of dimensionp.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0023d88b-0f5f-44b4-ac99-774bc8279aaf', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 46, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='26 2. Overview of Supervised Learning\\nX\\nf(X)\\n-1.0 -0.5 0.0 0.5 1.0\\n01234\\n\\x81\\n1-NN in One Dimension\\nDimension\\nMSE\\n2468 1 0\\n0.0 0.05 0.10 0.15 0.20 0.25\\x81 \\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nMSE  vs. Dimension\\n\\x81 MSE\\n\\x81 Variance\\n\\x81 Sq. Bias\\nFIGURE 2.8.A simulation example with the same setup as in Figure 2.7. Here\\nthe function is constant in all but one dimension: f(X)= 1\\n2(X1 +1 )3.T h e\\nvariance dominates.\\nunbiased, we ﬁnd that\\nEPE(x0)=E y0|x0ET (y0 −ˆy0)2\\n=V a r (y0|x0)+E T [ˆy0 −ET ˆy0]2 +[ET ˆy0 −xT\\n0 β]2\\n=V a r (y0|x0)+V arT (ˆy0)+Bias 2(ˆy0)\\n= σ2 +ET xT\\n0 (XTX)−1x0σ2 +02. (2.27)\\nHere we have incurred an additional varianceσ2 in the prediction error,\\nsince our target is not deterministic. There is no bias, and the variance\\ndepends onx0.I fN is large andT were selected at random, and assuming\\nE(X) = 0, thenXTX→NCov(X)a n d\\nEx0EPE(x0) ∼ Ex0xT\\n0 Cov(X)−1x0σ2/N +σ2\\n= trace[Cov( X)−1Cov(x0)]σ2/N +σ2\\n= σ2(p/N)+ σ2. (2.28)\\nHere we see that the expected EPE increases linearly as a function ofp,\\nwith slope σ2/N.I f N is large and/or σ2 is small, this growth in vari-\\nance is negligible (0 in the deterministic case). By imposing some heavy\\nrestrictions on the class of models being ﬁtted, we have avoided the curse\\nof dimensionality. Some of the technical details in (2.27) and (2.28) are\\nderived in Exercise 2.5.\\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\\ntions, both of which have the formY = f(X)+ ε, X uniform as before,\\nand ε∼N(0,1). The sample size isN = 500. For the orange curve,f(x)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12718c00-f904-47e9-8c0a-9a88c451cb53', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 47, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Local Methods in High Dimensions 27\\nDimension\\nEPE Ratio\\n2468 1 0\\n1.6 1.7 1.8 1.9 2.0 2.1\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nExpected Prediction Error of 1NN vs. OLS\\n\\x81 Linear\\n\\x81 Cubic\\nFIGURE 2.9. The curves show the expected prediction error (atx0 =0 )f o r\\n1-nearest neighbor relative to least squares for the modelY = f(X)+ ε.F o rt h e\\norange curve,f(x)= x1, while for the blue curvef(x)= 1\\n2(x1 +1)3.\\nis linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\\nShown is the relative EPE of 1-nearest neighbor to least squares, which\\nappears to start at around 2 for the linear case. Least squares is unbiased\\nin this case, and as discussed above the EPE is slightly aboveσ2 =1 .\\nThe EPE for 1-nearest neighbor is always above 2, since the variance of\\nˆf(x0)i nt h i sc a s ei sa tl e a s tσ2, and the ratio increases with dimension as\\nthe nearest neighbor strays from the target point. For the cubic case, least\\nsquares is biased, which moderates the ratio. Clearly we could manufacture\\nexamples where the bias of least squares would dominate the variance, and\\nthe 1-nearest neighbor would come out the winner.\\nBy relying on rigid assumptions, the linear model has no bias at all and\\nnegligible variance, while the error in 1-nearest neighbor is substantially\\nlarger. However, if the assumptions are wrong, all bets are oﬀ and the\\n1-nearest neighbor may dominate. We will see that there is a whole spec-\\ntrum of models between the rigid linear models and the extremely ﬂexible\\n1-nearest-neighbor models, each with their own assumptions and biases,\\nwhich have been proposed speciﬁcally to avoid the exponential growth in\\ncomplexity of functions in high dimensions by drawing heavily on these\\nassumptions.\\nBefore we delve more deeply, let us elaborate a bit on the concept of\\nstatistical modelsand see how they ﬁt into the prediction framework.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e87018a-f8ed-4550-84ec-66b96aa86764', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 48, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='28 2. Overview of Supervised Learning\\n2.6 Statistical Models, Supervised Learning and\\nFunction Approximation\\nOur goal is to ﬁnd a useful approximationˆf(x)t ot h ef u n c t i o nf(x)t h a t\\nunderlies the predictive relationship between the inputs and outputs. In the\\ntheoretical setting of Section 2.4, we saw that squared error loss lead us\\nto the regression functionf(x)=E (Y|X = x) for a quantitative response.\\nThe class of nearest-neighbor methods can be viewed as direct estimates\\nof this conditional expectation, but we have seen that they can fail in at\\nleast two ways:\\n•if the dimension of the input space is high, the nearest neighbors need\\nnot be close to the target point, and can result in large errors;\\n•if special structure is known to exist, this can be used to reduce both\\nthe bias and the variance of the estimates.\\nWe anticipate using other classes of models forf(x), in many cases specif-\\nically designed to overcome the dimensionality problems, and here we dis-\\ncuss a framework for incorporating them into the prediction problem.\\n2.6.1 A Statistical Model for the Joint DistributionPr(X,Y )\\nSuppose in fact that our data arose from a statistical model\\nY = f(X)+ ε, (2.29)\\nwhere the random errorεhas E(ε) = 0 and is independent ofX.N o t et h a t\\nfor this model,f(x)=E (Y|X = x), and in fact the conditional distribution\\nPr(Y|X) depends onX only through the conditional meanf(x).\\nThe additive error model is a useful approximation to the truth. For\\nmost systems the input–output pairs (X,Y ) will not have a deterministic\\nrelationship Y = f(X). Generally there will be other unmeasured variables\\nthat also contribute toY, including measurement error. The additive model\\nassumes that we can capture all these departures from a deterministic re-\\nlationship via the errorε.\\nFor some problems a deterministic relationship does hold. Many of the\\nclassiﬁcation problems studied in machine learning are of this form, where\\nthe response surface can be thought of as a colored map deﬁned in IRp.\\nThe training data consist of colored examples from the map{xi,gi},a n d\\nthe goal is to be able to color any point. Here the function is deterministic,\\nand the randomness enters through thex location of the training points.\\nFor the moment we will not pursue such problems, but will see that they\\ncan be handled by techniques appropriate for the error-based models.\\nThe assumption in (2.29) that the errors are independent and identically\\ndistributed is not strictly necessary, but seems to be at the back of our mind', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9e2dad1f-998c-46b2-ada4-255aaffbddc3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 49, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.6 Statistical Models, Supervised Learning and Function Approximation 29\\nwhen we average squared errors uniformly in our EPE criterion. With such\\na model it becomes natural to use least squares as a data criterion for\\nmodel estimation as in (2.1). Simple modiﬁcations can be made to avoid\\nthe independence assumption; for example, we can have Var(Y|X = x)=\\nσ(x), and now both the mean and variance depend onX. In general the\\nconditional distribution Pr(Y|X) can depend onX in complicated ways,\\nbut the additive error model precludes these.\\nSo far we have concentrated on the quantitative response. Additive error\\nmodels are typically not used for qualitative outputsG; in this case the tar-\\nget functionp(X) is the conditional density Pr(G|X), and this is modeled\\ndirectly. For example, for two-class data, it is often reasonable to assume\\nthat the data arise from independent binary trials, with the probability of\\none particular outcome beingp(X), and the other 1−p(X). Thus ifY is\\nthe 0–1 coded version ofG,t h e nE (Y|X = x)= p(x), but the variance\\ndepends onx as well: Var(Y|X = x)= p(x)[1−p(x)].\\n2.6.2 Supervised Learning\\nBefore we launch into more statistically oriented jargon, we present the\\nfunction-ﬁtting paradigm from a machine learning point of view. Suppose\\nfor simplicity that the errors are additive and that the modelY = f(X)+ε\\nis a reasonable assumption. Supervised learning attempts to learnf by\\nexample through a teacher. One observes the system under study, both\\nthe inputs and outputs, and assembles atraining set of observationsT =\\n(xi,yi),i =1 ,...,N . The observed input values to the systemxi are also\\nfed into an artiﬁcial system, known as a learning algorithm (usually a com-\\nputer program), which also produces outputsˆf(xi)i nr e s p o n s et ot h ei n -\\nputs. The learning algorithm has the property that it can modify its in-\\nput/output relationship ˆf in response to diﬀerencesyi−ˆf(xi) between the\\noriginal and generated outputs. This process is known aslearning by exam-\\nple. Upon completion of the learning process the hope is that the artiﬁcial\\nand real outputs will be close enough to be useful for all sets of inputs likely\\nto be encountered in practice.\\n2.6.3 Function Approximation\\nThe learning paradigm of the previous section has been the motivation\\nfor research into the supervised learning problem in the ﬁelds of machine\\nlearning (with analogies to human reasoning) and neural networks (with\\nbiological analogies to the brain). The approach taken in applied mathe-\\nmatics and statistics has been from the perspective of function approxima-\\ntion and estimation. Here the data pairs{xi,yi} are viewed as points in a\\n(p+1)-dimensional Euclidean space. The functionf(x) has domain equal\\nto thep-dimensional input subspace, and is related to the data via a model', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e6a4a3a9-ab83-4606-909f-22b1a89b4602', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 50, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='30 2. Overview of Supervised Learning\\nsuch asyi = f(xi)+ εi. For convenience in this chapter we will assume the\\ndomain is IRp,a p-dimensional Euclidean space, although in general the\\ninputs can be of mixed type. The goal is to obtain a useful approximation\\nto f(x) for all x in some region of IRp, given the representations inT .\\nAlthough somewhat less glamorous than the learning paradigm, treating\\nsupervised learning as a problem in function approximation encourages the\\ngeometrical concepts of Euclidean spaces and mathematical concepts of\\nprobabilistic inference to be applied to the problem. This is the approach\\ntaken in this book.\\nMany of the approximations we will encounter have associated a set of\\nparameters θthat can be modiﬁed to suit the data at hand. For example,\\nthe linear modelf(x)= xTβhas θ= β. Another class of useful approxi-\\nmators can be expressed aslinear basis expansions\\nfθ(x)=\\nK∑\\nk=1\\nhk(x)θk, (2.30)\\nwhere thehk are a suitable set of functions or transformations of the input\\nvector x. Traditional examples are polynomial and trigonometric expan-\\nsions, where for example hk might be x2\\n1, x1x2\\n2,c o s (x1)a n ds oo n .W e\\nalso encounter nonlinear expansions, such as the sigmoid transformation\\ncommon to neural network models,\\nhk(x)= 1\\n1+exp( −xTβk). (2.31)\\nWe can use least squares to estimate the parametersθin fθas we did\\nfor the linear model, by minimizing the residual sum-of-squares\\nRSS(θ)=\\nN∑\\ni=1\\n(yi −fθ(xi))2 (2.32)\\nas a function ofθ. This seems a reasonable criterion for an additive error\\nmodel. In terms of function approximation, we imagine our parameterized\\nfunction as a surface inp + 1 space, and what we observe are noisy re-\\nalizations from it. This is easy to visualize whenp = 2 and the vertical\\ncoordinate is the outputy, as in Figure 2.10. The noise is in the output\\ncoordinate, so we ﬁnd the set of parameters such that the ﬁtted surface\\ngets as close to the observed points as possible, where close is measured by\\nthe sum of squared vertical errors in RSS(θ).\\nFor the linear model we get a simple closed form solution to the mini-\\nmization problem. This is also true for the basis function methods, if the\\nbasis functions themselves do not have any hidden parameters. Otherwise\\nthe solution requires either iterative methods or numerical optimization.\\nWhile least squares is generally very convenient, it is not the only crite-\\nrion used and in some cases would not make much sense. A more general', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e52a57b-4670-4649-9198-904bd419520d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 51, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.6 Statistical Models, Supervised Learning and Function Approximation 31\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81 \\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\nFIGURE 2.10.Least squares ﬁtting of a function of two inputs. The parameters\\nof fθ(x) are chosen so as to minimize the sum-of-squared vertical errors.\\nprinciple for estimation ismaximum likelihood estimation. Suppose we have\\na random sampleyi,i =1 ,...,N from a density Prθ(y) indexed by some\\nparameters θ. The log-probability of the observed sample is\\nL(θ)=\\nN∑\\ni=1\\nlogPrθ(yi). (2.33)\\nThe principle of maximum likelihood assumes that the most reasonable\\nvalues forθare those for which the probability of the observed sample is\\nlargest. Least squares for the additive error modelY = fθ(X)+ ε,w i t h\\nε∼N(0,σ2), is equivalent to maximum likelihood using the conditional\\nlikelihood\\nPr(Y|X,θ)= N(fθ(X),σ2). (2.34)\\nSo although the additional assumption of normality seems more restrictive,\\nthe results are the same. The log-likelihood of the data is\\nL(θ)= −N\\n2 log(2π)−N logσ− 1\\n2σ2\\nN∑\\ni=1\\n(yi −fθ(xi))2, (2.35)\\nand the only term involvingθis the last, which is RSS(θ)u pt oas c a l a r\\nnegative multiplier.\\nA more interesting example is the multinomial likelihood for the regres-\\nsion function Pr(G|X) for a qualitative outputG. Suppose we have a model\\nPr(G = Gk|X = x)= pk,θ(x),k =1 ,...,K for the conditional probabil-\\ni t yo fe a c hc l a s sg i v e nX, indexed by the parameter vectorθ. Then the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66958373-c1e1-43c1-965f-22d7c3401ad1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 52, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='32 2. Overview of Supervised Learning\\nlog-likelihood (also referred to as the cross-entropy) is\\nL(θ)=\\nN∑\\ni=1\\nlogpgi,θ(xi), (2.36)\\nand when maximized it delivers values ofθthat best conform with the data\\nin this likelihood sense.\\n2.7 Structured Regression Models\\nWe have seen that although nearest-neighbor and other local methods focus\\ndirectly on estimating the function at a point, they face problems in high\\ndimensions. They may also be inappropriate even in low dimensions in\\ncases where more structured approaches can make more eﬃcient use of the\\ndata. This section introduces classes of such structured approaches. Before\\nwe proceed, though, we discuss further the need for such classes.\\n2.7.1 Diﬃculty of the Problem\\nConsider the RSS criterion for an arbitrary functionf,\\nRSS(f)=\\nN∑\\ni=1\\n(yi −f(xi))2. (2.37)\\nMinimizing (2.37) leads to inﬁnitely many solutions: any functionˆf passing\\nthrough the training points (xi,yi) is a solution. Any particular solution\\nchosen might be a poor predictor at test points diﬀerent from the training\\npoints. If there are multiple observation pairsxi,yiℓ,ℓ =1 ,...,N i at each\\nvalue of xi, the risk is limited. In this case, the solutions pass through\\nthe average values of theyiℓ at eachxi; see Exercise 2.6. The situation is\\nsimilar to the one we have already visited in Section 2.4; indeed, (2.37) is\\nthe ﬁnite sample version of (2.11) on page 18. If the sample sizeN were\\nsuﬃciently large such that repeats were guaranteed and densely arranged,\\nit would seem that these solutions might all tend to the limiting conditional\\nexpectation.\\nIn order to obtain useful results for ﬁniteN, we must restrict the eligible\\nsolutions to (2.37) to a smaller set of functions. How to decide on the\\nnature of the restrictions is based on considerations outside of the data.\\nThese restrictions are sometimes encoded via the parametric representation\\nof fθ, or may be built into the learning method itself, either implicitly or\\nexplicitly. These restricted classes of solutions are the major topic of this\\nbook. One thing should be clear, though. Any restrictions imposed onf\\nthat lead to a unique solution to (2.37) do not really remove the ambiguity', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1cfb9267-7ee4-4285-958f-c1ba2edeb28f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 53, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.8 Classes of Restricted Estimators 33\\ncaused by the multiplicity of solutions. There are inﬁnitely many possible\\nrestrictions, each leading to a unique solution, so the ambiguity has simply\\nbeen transferred to the choice of constraint.\\nIn general the constraints imposed by most learning methods can be\\ndescribed as complexity restrictions of one kind or another. This usually\\nmeans some kind of regular behavior in small neighborhoods of the input\\nspace. That is, for all input points x suﬃciently close to each other in\\nsome metric, ˆf exhibits some special structure such as nearly constant,\\nlinear or low-order polynomial behavior. The estimator is then obtained by\\naveraging or polynomial ﬁtting in that neighborhood.\\nThe strength of the constraint is dictated by the neighborhood size. The\\nlarger the size of the neighborhood, the stronger the constraint, and the\\nmore sensitive the solution is to the particular choice of constraint. For\\nexample, local constant ﬁts in inﬁnitesimally small neighborhoods is no\\nconstraint at all; local linear ﬁts in very large neighborhoods is almost a\\nglobally linear model, and is very restrictive.\\nThe nature of the constraint depends on the metric used. Some methods,\\nsuch as kernel and local regression and tree-based methods, directly specify\\nthe metric and size of the neighborhood. The nearest-neighbor methods\\ndiscussed so far are based on the assumption that locally the function is\\nconstant; close to a target inputx0, the function does not change much, and\\nso close outputs can be averaged to produceˆf(x0). Other methods such\\nas splines, neural networks and basis-function methods implicitly deﬁne\\nneighborhoods of local behavior. In Section 5.4.1 we discuss the concept\\nof an equivalent kernel(see Figure 5.8 on page 157), which describes this\\nlocal dependence for any method linear in the outputs. These equivalent\\nkernels in many cases look just like the explicitly deﬁned weighting kernels\\ndiscussed above—peaked at the target point and falling away smoothly\\naway from it.\\nOne fact should be clear by now. Any method that attempts to pro-\\nduce locally varying functions in small isotropic neighborhoods will run\\ninto problems in high dimensions—again the curse of dimensionality. And\\nconversely, all methods that overcome the dimensionality problems have an\\nassociated—and often implicit or adaptive—metric for measuring neighbor-\\nhoods, which basically does not allow the neighborhood to be simultane-\\nously small in all directions.\\n2.8 Classes of Restricted Estimators\\nThe variety of nonparametric regression techniques or learning methods fall\\nintoanumberofdiﬀerentclassesdependingonthenatureoftherestrictions\\nimposed. These classes are not distinct, and indeed some methods fall in\\nseveral classes. Here we give a brief summary, since detailed descriptions', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92738e60-ccff-47ba-b927-be1b9902db0d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 54, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='34 2. Overview of Supervised Learning\\nare given in later chapters. Each of the classes has associated with it one\\nor more parameters, sometimes appropriately calledsmoothingparameters,\\nthat control the eﬀective size of the local neighborhood. Here we describe\\nthree broad classes.\\n2.8.1 Roughness Penalty and Bayesian Methods\\nHere the class of functions is controlled by explicitly penalizing RSS(f)\\nwith a roughness penalty\\nPRSS(f;λ) = RSS(f)+ λJ(f). (2.38)\\nThe user-selected functionalJ(f) will be large for functionsf that vary too\\nrapidly over small regions of input space. For example, the popularcubic\\nsmoothing splinefor one-dimensional inputs is the solution to the penalized\\nleast-squares criterion\\nPRSS(f;λ)=\\nN∑\\ni=1\\n(yi −f(xi))2 +λ\\n∫\\n[f′′(x)]2dx. (2.39)\\nThe roughness penalty here controls large values of the second derivative\\nof f, and the amount of penalty is dictated byλ≥0. Forλ= 0 no penalty\\nis imposed, and any interpolating function will do, while forλ= ∞only\\nfunctions linear inx are permitted.\\nPenalty functionalsJ can be constructed for functions in any dimension,\\nand special versions can be created to impose special structure. For ex-\\nample, additive penaltiesJ(f)= ∑p\\nj=1 J(fj) are used in conjunction with\\nadditive functions f(X)= ∑p\\nj=1 fj(Xj)t oc r e a t ea d d i t i v em o d e l sw i t h\\nsmooth coordinate functions. Similarly,projection pursuit regressionmod-\\nels havef(X)= ∑M\\nm=1 gm(αT\\nmX) for adaptively chosen directionsαm,a n d\\nthe functionsgm can each have an associated roughness penalty.\\nPenalty function, orregularizationmethods, express our prior belief that\\nthe type of functions we seek exhibit a certain type of smooth behavior, and\\nindeed can usually be cast in a Bayesian framework. The penaltyJ corre-\\nsponds to a log-prior, and PRSS(f;λ) the log-posterior distribution, and\\nminimizing PRSS(f;λ) amounts to ﬁnding the posterior mode. We discuss\\nroughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\\nChapter 8.\\n2.8.2 Kernel Methods and Local Regression\\nThese methods can be thought of as explicitly providing estimates of the re-\\ngression function or conditional expectation by specifying the nature of the\\nlocal neighborhood, and of the class of regular functions ﬁtted locally. The\\nlocal neighborhood is speciﬁed by akernel functionKλ(x0,x) which assigns', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0dabd151-9b74-479a-a701-6b8889bb18ca', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 55, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.8 Classes of Restricted Estimators 35\\nweights to pointsx in a region aroundx0 (see Figure 6.1 on page 192). For\\nexample, the Gaussian kernel has a weight function based on the Gaussian\\ndensity function\\nKλ(x0,x)= 1\\nλexp\\n[\\n−||x−x0||2\\n2λ\\n]\\n(2.40)\\nand assigns weights to points that die exponentially with their squared\\nEuclidean distance fromx0. The parameterλcorresponds to the variance\\nof the Gaussian density, and controls the width of the neighborhood. The\\nsimplest form of kernel estimate is the Nadaraya–Watson weighted average\\nˆf(x0)=\\n∑N\\ni=1 Kλ(x0,xi)yi\\n∑N\\ni=1 Kλ(x0,xi)\\n. (2.41)\\nIn general we can deﬁne a local regression estimate off(x0)a s fˆθ(x0),\\nwhere ˆθminimizes\\nRSS(fθ,x0)=\\nN∑\\ni=1\\nKλ(x0,xi)(yi −fθ(xi))2, (2.42)\\nand fθ is some parameterized function, such as a low-order polynomial.\\nSome examples are:\\n•fθ(x)= θ0, the constant function; this results in the Nadaraya–\\nWatson estimate in (2.41) above.\\n•fθ(x)= θ0 +θ1x gives the popular local linear regression model.\\nNearest-neighbor methods can be thought of as kernel methods having a\\nmore data-dependent metric. Indeed, the metric fork-nearest neighbors is\\nKk(x,x0)= I(||x−x0||≤||x(k) −x0||),\\nwhere x(k) is the training observation rankedkth in distance fromx0,a n d\\nI(S) is the indicator of the setS.\\nThese methods of course need to be modiﬁed in high dimensions, to avoid\\nthe curse of dimensionality. Various adaptations are discussed in Chapter 6.\\n2.8.3 Basis Functions and Dictionary Methods\\nThis class of methods includes the familiar linear and polynomial expan-\\nsions, but more importantly a wide variety of more ﬂexible models. The\\nmodel forf is a linear expansion of basis functions\\nfθ(x)=\\nM∑\\nm=1\\nθmhm(x), (2.43)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bae82310-b90b-4787-98fe-2d1bab088762', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 56, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='36 2. Overview of Supervised Learning\\nwhere each of the hm is a function of the input x, and the term linear here\\nrefers to the action of the parameters θ. This class covers a wide variety of\\nmethods. In some cases the sequence of basis functions is pre scribed, such\\nas a basis for polynomials in x of total degree M .\\nFor one-dimensional x, polynomial splines of degree K can be represented\\nby an appropriate sequence of M spline basis functions, determined in turn\\nby M −K −1 knots. These produce functions that are piecewise polynomials\\nof degree K between the knots, and joined up with continuity of degree\\nK − 1 at the knots. As an example consider linear splines, or piec ewise\\nlinear functions. One intuitively satisfying basis consis ts of the functions\\nb1(x) = 1, b2(x) = x, and bm+2(x) = ( x − tm)+, m = 1 , . . . , M − 2,\\nwhere tm is the mth knot, and z+ denotes positive part. Tensor products\\nof spline bases can be used for inputs with dimensions larger than one\\n(see Section 5.2, and the CART and MARS models in Chapter 9.) T he\\nparameter M controls the degree of the polynomial or the number of knots\\nin the case of splines.\\nRadial basis functions are symmetric p-dimensional kernels located at\\nparticular centroids,\\nfθ(x) =\\nM∑\\nm=1\\nKλm (µm, x)θm; (2.44)\\nfor example, the Gaussian kernel Kλ(µ, x) = e−||x− µ||2/2λ is popular.\\nRadial basis functions have centroids µm and scales λm that have to\\nbe determined. The spline basis functions have knots. In gen eral we would\\nlike the data to dictate them as well. Including these as para meters changes\\nthe regression problem from a straightforward linear probl em to a combi-\\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\\nalgorithms or two stage processes are used. Section 6.7 desc ribes some such\\napproaches.\\nA single-layer feed-forward neural network model with line ar output\\nweights can be thought of as an adaptive basis function metho d. The model\\nhas the form\\nfθ(x) =\\nM∑\\nm=1\\nβmσ(αT\\nmx + bm), (2.45)\\nwhere σ(x) = 1 /(1 + e− x) is known as the activation function. Here, as\\nin the projection pursuit model, the directions αm and the bias terms bm\\nhave to be determined, and their estimation is the meat of the computation.\\nDetails are given in Chapter 11.\\nThese adaptively chosen basis function methods are also kno wn as dictio-\\nnary methods, where one has available a possibly inﬁnite set or di ctionary\\nD of candidate basis functions from which to choose, and model s are built\\nup by employing some kind of search mechanism.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='033f606a-386c-4426-8185-608e9449f8cd', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 57, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\\n2.9 Model Selection and the Bias–Variance\\nTradeoﬀ\\nAll the models described above and many others discussed in later chapters\\nhave asmoothing or complexity parameter that has to be determined:\\n•the multiplier of the penalty term;\\n•the width of the kernel;\\n•or the number of basis functions.\\nIn the case of the smoothing spline, the parameterλindexes models ranging\\nfrom a straight line ﬁt to the interpolating model. Similarly a local degree-\\nm polynomial model ranges between a degree-m global polynomial when\\nthe window size is inﬁnitely large, to an interpolating ﬁt when the window\\nsize shrinks to zero. This means that we cannot use residual sum-of-squares\\non the training data to determine these parameters as well, since we would\\nalways pick those that gave interpolating ﬁts and hence zero residuals. Such\\na model is unlikely to predict future data well at all.\\nThe k-nearest-neighbor regression ﬁtˆfk(x0) usefully illustrates the com-\\npeting forces that aﬀect the predictive ability of such approximations. Sup-\\np o s et h ed a t aa r i s ef r o mam o d e lY = f(X)+ ε,w i t hE (ε)=0a n d\\nVar(ε)= σ2. For simplicity here we assume that the values ofxi in the\\nsample are ﬁxed in advance (nonrandom). The expected prediction error\\nat x0, also known astest or generalization error, can be decomposed:\\nEPEk(x0) = E[( Y −ˆfk(x0))2|X = x0]\\n= σ2 +[Bias2(ˆfk(x0))+Var T (ˆfk(x0))] (2.46)\\n= σ2 +\\n[\\nf(x0)−1\\nk\\nk∑\\nℓ=1\\nf(x(ℓ))\\n]2\\n+ σ2\\nk . (2.47)\\nThe subscripts in parentheses (ℓ) indicate the sequence of nearest neighbors\\nto x0.\\nThere are three terms in this expression. The ﬁrst termσ2 is the ir-\\nreducible error—the variance of the new test target—and is beyond our\\ncontrol, even if we know the truef(x0).\\nThe second and third terms are under our control, and make up the\\nmean squared errorof ˆfk(x0) in estimating f(x0), which is broken down\\ninto a bias component and a variance component. The bias term is the\\nsquared diﬀerence between the true meanf(x0) and the expected value of\\nthe estimate—[E T (ˆfk(x0))−f(x0)]2—where the expectation averages the\\nrandomness in the training data. This term will most likely increase with\\nk, if the true function is reasonably smooth. For smallk the few closest\\nneighbors will have valuesf(x(ℓ))c l o s et of(x0), so their average should', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='414a86a6-663d-4da3-a3e2-04e15260fa3b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 58, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='38 2. Overview of Supervised Learning\\nHigh Bias\\nLow Variance\\nLow Bias\\nHigh Variance\\nPrediction Error\\nModel Complexity\\nTraining Sample\\nTest Sample\\nLow High\\nFIGURE 2.11.Test and training error as a function of model complexity.\\nbe close to f(x0). As k grows, the neighbors are further away, and then\\nanything can happen.\\nThe variance term is simply the variance of an average here, and de-\\ncreases as the inverse ofk.S oa sk varies, there is abias–variance tradeoﬀ.\\nMore generally, as themodel complexityof our procedure is increased, the\\nvariance tends to increase and the squared bias tends to decrease. The op-\\nposite behavior occurs as the model complexity is decreased. Fork-nearest\\nneighbors, the model complexity is controlled byk.\\nTypically we would like to choose our model complexity to trade bias\\noﬀ with variance in such a way as to minimize the test error. An obvious\\nestimate of test error is thetraining error 1\\nN\\n∑\\ni(yi −ˆyi)2. Unfortunately\\ntraining error is not a good estimate of test error, as it does not properly\\naccount for model complexity.\\nFigure 2.11 shows the typical behavior of the test and training error, as\\nmodel complexity is varied. The training error tends to decrease whenever\\nwe increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too closely to the\\ntraining data, and will not generalize well (i.e., have large test error). In\\nthat case the predictionsˆf(x0) will have large variance, as reﬂected in the\\nlast term of expression (2.46). In contrast, if the model is not complex\\nenough, it willunderﬁt and may have large bias, again resulting in poor\\ngeneralization. In Chapter 7 we discuss methods for estimating the test\\nerror of a prediction method, and hence estimating the optimal amount of\\nmodel complexity for a given prediction method and training set.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='77ee3ca4-61a2-4f00-9652-9346a7f2ce46', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 59, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 39\\nBibliographic Notes\\nSome good general books on the learning problem are Duda et al. (2000),\\nBishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2007)\\nand Vapnik (1996). Parts of this chapter are based on Friedman (1994b).\\nExercises\\nEx. 2.1Suppose each ofK-classes has an associated targettk,w h i c hi sa\\nvector of all zeros, except a one in thekth position. Show that classifying to\\nthe largest element of ˆy amounts to choosing the closest target, mink ||tk −\\nˆy||, if the elements of ˆy sum to one.\\nEx. 2.2Show how to compute the Bayes decision boundary for the simula-\\ntion example in Figure 2.5.\\nEx. 2.3Derive equation (2.24).\\nEx. 2.4 The edge eﬀect problem discussed on page 23 is not peculiar to\\nuniform sampling from bounded domains. Consider inputs drawn from a\\nspherical multinormal distribution X ∼N(0,Ip). The squared distance\\nfrom any sample point to the origin has aχ2\\np distribution with mean p.\\nConsider a prediction pointx0 drawn from this distribution, and leta =\\nx0/||x0|| be an associated unit vector. Letzi = aTxi be the projection of\\neach of the training points on this direction.\\nShow that thezi are distributedN(0,1) with expected squared distance\\nfrom the origin 1, while the target point has expected squared distancep\\nfrom the origin.\\nHence for p = 10, a randomly drawn test point is about 3.1 standard\\ndeviations from the origin, while all the training points are on average\\none standard deviation along direction a. So most prediction points see\\nthemselves as lying on the edge of the training set.\\nEx. 2.5\\n(a) Derive equation (2.27). The last line makes use of (3.8) through a\\nconditioning argument.\\n(b) Derive equation (2.28), making use of thecyclic property of the trace\\noperator [trace(AB)=t r a c e (BA)], and its linearity (which allows us\\nto interchange the order of trace and expectation).\\nEx. 2.6Consider a regression problem with inputsxi and outputsyi,a n da\\nparameterized modelfθ(x) to be ﬁt by least squares. Show that if there are\\nobservations withtied or identical values ofx, then the ﬁt can be obtained\\nfrom a reduced weighted least squares problem.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='673d133f-0f09-4be8-b551-52aac3e89e38', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 60, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='40 2. Overview of Supervised Learning\\nEx. 2.7 Suppose we have a sample ofN pairs xi,yi drawn i.i.d. from the\\ndistribution characterized as follows:\\nxi ∼h(x), the design density\\nyi = f(xi)+ εi,f is the regression function\\nεi ∼(0,σ2) (mean zero, varianceσ2)\\nWe construct an estimator forf linear in theyi,\\nˆf(x0)=\\nN∑\\ni=1\\nℓi(x0;X)yi,\\nwhere the weightsℓi(x0;X) do not depend on theyi, but do depend on the\\nentire training sequence ofxi, denoted here byX.\\n(a)Showthatlinearregressionand k-nearest-neighborregressionaremem-\\nbersofthisclassofestimators.Describeexplicitlytheweights ℓi(x0;X)\\nin each of these cases.\\n(b) Decompose the conditional mean-squared error\\nEY|X (f(x0)−ˆf(x0))2\\ninto a conditional squared bias and a conditional variance component.\\nLike X, Y represents the entire training sequence ofyi.\\n(c) Decompose the (unconditional) mean-squared error\\nEY,X(f(x0)−ˆf(x0))2\\ninto a squared bias and a variance component.\\n(d) Establish a relationship between the squared biases and variances in\\nthe above two cases.\\nEx. 2.8Compare the classiﬁcation performance of linear regression andk–\\nnearest neighbor classiﬁcation on thezipcode data. In particular, consider\\nonly the2’s and3’s, andk =1 ,3,5,7 and 15. Show both the training and\\ntest error for each choice. Thezipcode data are available from the book\\nwebsite www-stat.stanford.edu/ElemStatLearn.\\nEx. 2.9Consider a linear regression model withp parameters, ﬁt by least\\ns q u a r e st oas e to ft r a i n i n gd a t a(x1,y1),..., (xN,yN) drawn at random\\nfrom a population. Letˆβbe the least squares estimate. Suppose we have\\ns o m et e s td a t a(˜x1,˜y1),..., (˜xM,˜yM) drawn at random from the same pop-\\nulation as the training data. IfRtr(β)= 1\\nN\\n∑N\\n1 (yi −βTxi)2 and Rte(β)=\\n1\\nM\\n∑M\\n1 (˜yi −βT ˜xi)2,p r o v et h a t\\nE[Rtr(ˆβ)]≤E[Rte(ˆβ)],', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e7d7b1a-ecb5-4b30-91c5-6a47ebe1e91a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 61, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 41\\nwhere the expectations are over all that is random in each expression. [This\\nexercisewas brought toourattention byRyan Tibshirani, fromahomework\\nassignment given by Andrew Ng.]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5600a89e-8a51-432c-b4e2-39d5b5cf2ee9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 62, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3\\nLinear Methods for Regression\\n3.1 Introduction\\nA linear regression model assumes that the regression function E(Y|X)i s\\nlinear in the inputsX1,...,X p. Linear models were largely developed in\\nthe precomputer age of statistics, but even in today’s computer era there\\nare still good reasons to study and use them. They are simple and often\\nprovide an adequate and interpretable description of how the inputs aﬀect\\nthe output. For prediction purposes they can sometimes outperform fancier\\nnonlinear models, especially in situations with small numbers of training\\ncases,lowsignal-to-noiseratioorsparsedata.Finally,linearmethodscanbe\\nappliedtotransformationsoftheinputsandthisconsiderablyexpandstheir\\nscope. These generalizations are sometimes called basis-function methods,\\nand are discussed in Chapter 5.\\nIn this chapter we describe linear methods for regression, while in the\\nnext chapter we discuss linear methods for classiﬁcation. On some topics we\\ngo into considerable detail, as it is our ﬁrm belief that an understanding\\nof linear methods is essential for understanding nonlinear ones. In fact,\\nmany nonlinear techniques are direct generalizations of the linear methods\\ndiscussed here.\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 43\\nDOI: 10.1007/b94608_3,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2dae0da3-206e-46e4-8163-b471065cf652', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 63, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAsintroducedinChapter2,wehaveaninputvector XT =(X1,X2,...,Xp),\\nand want to predict a real-valued outputY. The linear regression model\\nhas the form\\nf(X)= β0 +\\np∑\\nj=1\\nXjβj. (3.1)\\nThe linear model either assumes that the regression function E(Y|X)i s\\nlinear, or that the linear model is a reasonable approximation. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variablesXj can come\\nfrom diﬀerent sources:\\n•quantitative inputs;\\n•transformations of quantitative inputs, such as log, square-root or\\nsquare;\\n•basis expansions, such asX2 = X2\\n1,X3 = X3\\n1, leading to a polynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative inputs. For\\nexample, if G is a ﬁve-level factor input, we might createXj,j =\\n1,..., 5, such thatXj = I(G = j). Together this group ofXj repre-\\nsents the eﬀect ofG by a set of level-dependent constants, since in∑5\\nj=1 Xjβj,o n eo ft h eXjs is one, and the others are zero.\\n•interactions between variables, for example,X3 = X1 ·X2.\\nNo matter the source of theXj, the model is linear in the parameters.\\nTypically we have a set of training data (x1,y1)... (xN,yN)f r o mw h i c h\\nto estimate the parameters β.E a c hxi =( xi1,xi2,...,x ip)T is a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethodis least squares,inwhichwepickthecoeﬃcients β=(β0,β1,...,β p)T\\nto minimize the residual sum of squares\\nRSS(β)=\\nN∑\\ni=1\\n(yi −f(xi))2\\n=\\nN∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonable if the training\\nobservations (xi,yi) represent independent random draws from their popu-\\nlation. Even if thexi’s were not drawn randomly, the criterion is still valid\\nif the yi’s are conditionally independent given the inputsxi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b7d1a69a-05c1-4f03-8389-a69508fc9070', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 64, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 45\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nX1\\nX2\\nY\\nFIGURE 3.1. Linear least squares ﬁtting with X ∈IR2. We seek the linear\\nfunction ofX that minimizes the sum of squared residuals fromY.\\nspace occupied by the pairs (X,Y ). Note that (3.2) makes no assumptions\\nabout the validity of model (3.1); it simply ﬁnds the best linear ﬁt to the\\ndata. Least squares ﬁtting is intuitively satisfying no matter how the data\\narise; the criterion measures the average lack of ﬁt.\\nHow do we minimize (3.2)? Denote byX the N × (p +1 )m a t r i xw i t h\\neach row an input vector (with a 1 in the ﬁrst position), and similarly let\\ny be the N-vector of outputs in the training set. Then we can write the\\nresidual sum-of-squares as\\nRSS(β)=( y−Xβ)T(y−Xβ). (3.3)\\nThis is a quadratic function in thep+ 1 parameters. Diﬀerentiating with\\nrespect toβwe obtain\\n∂RSS\\n∂β=−2XT(y−Xβ)\\n∂2RSS\\n∂β∂βT =2 XTX.\\n(3.4)\\nAssuming (for the moment) thatX has full column rank, and henceXTX\\nis positive deﬁnite, we set the ﬁrst derivative to zero\\nXT(y−Xβ) = 0 (3.5)\\nto obtain the unique solution\\nˆβ=( XTX)−1XTy. (3.6)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf2794df-71fb-4077-affa-dd6ee50715cb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 65, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='46 3. Linear Methods for Regression\\nx1\\nx2\\ny\\nˆy\\nFIGURE 3.2.The N-dimensional geometry of least squares regression with two\\npredictors. The outcome vectory is orthogonally projected onto the hyperplane\\nspanned by the input vectorsx1 and x2. The projectionˆy represents the vector\\nof the least squares predictions\\nThe predicted values at an input vectorx0 are given byˆf(x0)=( 1: x0)T ˆβ;\\nthe ﬁtted values at the training inputs are\\nˆy = Xˆβ= X(XTX)−1XTy, (3.7)\\nwhere ˆyi = ˆf(xi). The matrixH = X(XTX)−1XT appearing in equation\\n(3.7) is sometimes called the “hat” matrix because it puts the hat ony.\\nFigure3.2showsadiﬀerentgeometricalrepresentationoftheleastsquares\\nestimate,thistimeinIR N.Wedenotethecolumnvectorsof Xbyx0,x1,..., xp,\\nwith x0 ≡1. For much of what follows, this ﬁrst column is treated like any\\nother. These vectors span a subspace of IRN, also referred to as the column\\nspace ofX. We minimize RSS(β)= ∥y−Xβ∥2 by choosing ˆβso that the\\nresidual vectory−ˆy is orthogonal to this subspace. This orthogonality is\\nexpressed in (3.5), and the resulting estimateˆy is hence theorthogonal pro-\\njectionof y onto this subspace. The hat matrixH computes the orthogonal\\nprojection, and hence it is also known as a projection matrix.\\nIt might happen that the columns ofX are not linearly independent, so\\nthat X is not of full rank. This would occur, for example, if two of the\\ninputs were perfectly correlated, (e.g.,x2 =3 x1). Then XTX is singular\\nand the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\\nthe ﬁtted values ˆy = Xˆβare still the projection of y onto the column\\nspace of X; there is just more than one way to express that projection\\nin terms of the column vectors ofX. The non-full-rank case occurs most\\noften when one or more qualitative inputs are coded in a redundant fashion.\\nThere is usually a natural way to resolve the non-unique representation,\\nby recoding and/or dropping redundant columns in X. Most regression\\nsoftware packages detect these redundancies and automatically implement', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='98a092d7-d689-4245-8df8-2eb66ac224c6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 66, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 47\\nsome strategy for removing them. Rank deﬁciencies can also occur in signal\\nand image analysis, where the number of inputsp can exceed the number\\nof training cases N. In this case, the features are typically reduced by\\nﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\\nChapter 18).\\nU pt on o ww eh a v em a d em i n i m a la s s u m p t i o n sa b o u tt h et r u ed i s t r i b u -\\ntion of the data. In order to pin down the sampling properties ofˆβ,w en o w\\nassume that the observationsyi are uncorrelated and have constant vari-\\nance σ2, and that thexi are ﬁxed (non random). The variance–covariance\\nmatrix of the least squares parameter estimates is easily derived from (3.6)\\nand is given by\\nVar(ˆβ)=( XTX)−1σ2. (3.8)\\nTypically one estimates the varianceσ2 by\\nˆσ2 = 1\\nN −p−1\\nN∑\\ni=1\\n(yi −ˆyi)2.\\nThe N −p−1 rather thanN in the denominator makes ˆσ2 an unbiased\\nestimate ofσ2:E ( ˆσ2)= σ2.\\nTo draw inferences about the parameters and the model, additional as-\\nsumptions are needed. We now assume that (3.1) is the correct model for\\nthe mean; that is, the conditional expectation ofY is linear inX1,...,X p.\\nWe also assume that the deviations ofY around its expectation are additive\\nand Gaussian. Hence\\nY =E ( Y|X1,...,X p)+ ε\\n= β0 +\\np∑\\nj=1\\nXjβj +ε, (3.9)\\nwhere the errorεis a Gaussian random variable with expectation zero and\\nvariance σ2, writtenε∼N(0,σ2).\\nUnder (3.9), it is easy to show that\\nˆβ∼N(β,(XTX)−1σ2). (3.10)\\nThis is a multivariate normal distribution with mean vector and variance–\\ncovariance matrix as shown. Also\\n(N −p−1)ˆσ2 ∼σ2χ2\\nN−p−1, (3.11)\\na chi-squared distribution withN−p−1 degrees of freedom. In additionˆβ\\nand ˆσ2 are statistically independent. We use these distributional properties\\nto form tests of hypothesis and conﬁdence intervals for the parametersβj.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='62835651-8e7b-4874-9f19-89a34b44230d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 67, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='48 3. Linear Methods for Regression\\nZ\\nTail Probabilities\\n2.0 2.2 2.4 2.6 2.8 3.0\\n0.01 0.02 0.03 0.04 0.05 0.06\\nt30\\nt100\\nnormal\\nFIGURE 3.3.The tail probabilitiesPr(|Z| >z ) for three distributions,t30, t100\\nand standard normal. Shown are the appropriate quantiles for testing signiﬁcance\\nat thep =0 .05 and 0.01 levels. The diﬀerence betweent and the standard normal\\nbecomes negligible forN bigger than about100.\\nTo test the hypothesis that a particular coeﬃcientβj =0 ,w ef o r mt h e\\nstandardized coeﬃcient orZ-score\\nzj =\\nˆβj\\nˆσ√vj\\n, (3.12)\\nwherevj isthe jthdiagonalelementof( XTX)−1.Underthenullhypothesis\\nthat βj =0 ,zj is distributed astN−p−1 (a t distribution with N −p−1\\ndegrees of freedom), and hence a large (absolute) value ofzj will lead to\\nrejection of this null hypothesis. If ˆσis replaced by a known valueσ,t h e n\\nzj would have a standard normal distribution. The diﬀerence between the\\ntail quantiles of at-distribution and a standard normal become negligible\\nas the sample size increases, and so we typically use the normal quantiles\\n(see Figure 3.3).\\nOften we need to test for the signiﬁcance of groups of coeﬃcients simul-\\ntaneously. For example, to test if a categorical variable withk levels can\\nbe excluded from a model, we need to test whether the coeﬃcients of the\\ndummy variables used to represent the levels can all be set to zero. Here\\nwe use theF statistic,\\nF = (RSS0 −RSS1)/(p1 −p0)\\nRSS1/(N −p1 −1) , (3.13)\\nwhereRSS1 istheresidualsum-of-squaresfortheleastsquaresﬁtofthebig-\\nger model withp1+1 parameters, and RSS0 the same for the nested smaller\\nmodel withp0 +1 parameters, havingp1−p0 parameters constrained to be', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f9a04ba-ebc7-44c4-a499-7f91bd89ae35', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 68, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 49\\nzero. The F statistic measures the change in residual sum-of-squares per\\nadditional parameter in the bigger model, and it is normalized by an esti-\\nmate ofσ2. Under the Gaussian assumptions, and the null hypothesis that\\nthe smaller model is correct, theF statistic will have aFp1−p0,N−p1−1 dis-\\ntribution. It can be shown (Exercise 3.1) that thezj in (3.12) are equivalent\\nto theF statistic for dropping the single coeﬃcientβj from the model. For\\nlarge N, the quantiles ofFp1−p0,N−p1−1 approach those ofχ2\\np1−p0/(p1−p0).\\nSimilarly, we can isolateβj in (3.10) to obtain a 1−2αconﬁdence interval\\nfor βj:\\n(ˆβj −z(1−α)v\\n1\\n2\\nj ˆσ, ˆβj +z(1−α)v\\n1\\n2\\nj ˆσ). (3.14)\\nHere z(1−α) is the 1−αpercentile of the normal distribution:\\nz(1−0.025) =1 .96,\\nz(1−.05) =1 .645, etc.\\nHence the standard practice of reportingˆβ± 2· se(ˆβ) amounts to an ap-\\nproximate 95% conﬁdence interval. Even if the Gaussian error assumption\\ndoes not hold, this interval will be approximately correct, with its coverage\\napproaching 1−2αas the sample sizeN →∞.\\nIn a similar fashion we can obtain an approximate conﬁdence set for the\\nentire parameter vectorβ,n a m e l y\\nCβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\\np+1\\n(1−α)\\n}, (3.15)\\nwhere χ2\\nℓ\\n(1−α)\\nis the 1−αpercentile of the chi-squared distribution onℓ\\ndegrees of freedom: for example,χ2\\n5\\n(1−0.05)\\n=1 1.1, χ2\\n5\\n(1−0.1)\\n=9 .2. This\\nconﬁdence set forβgenerates a corresponding conﬁdence set for the true\\nfunction f(x)= xTβ,n a m e l y{xTβ|β∈Cβ} (Exercise 3.2; see also Fig-\\nure 5.4 in Section 5.2.2 for examples of conﬁdence bands for functions).\\n3.2.1 Example: Prostate Cancer\\nThe data for this example come from a study by Stamey et al. (1989). They\\nexamined the correlation between the level of prostate-speciﬁc antigen and\\na number of clinical measures in men who were about to receive a radical\\nprostatectomy. The variables are log cancer volume (lcavol), log prostate\\nweight (lweight), age, log of the amount of benign prostatic hyperplasia\\n(lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp),\\nGleason score (gleason), and percent of Gleason scores 4 or 5 (pgg45).\\nThe correlation matrix of the predictors given in Table 3.1 shows many\\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matrix\\nshowing every pairwise plot between the variables. We see thatsvi is a\\nbinary variable, andgleason is an ordered categorical variable. We see, for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ac7a716-f6f0-4f51-8b2d-cd6b0b6b6d01', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 69, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='50 3. Linear Methods for Regression\\nTABLE 3.1.Correlations of predictors in the prostate cancer data.\\nlcavol lweight age lbph svi lcp gleason\\nlweight 0.300\\nage 0.286 0.317\\nlbph 0.063 0.437 0.287\\nsvi 0.593 0.181 0.129 −0.139\\nlcp 0.692 0.157 0.173 −0.089 0.671\\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\\npgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\\nTABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\\ncoeﬃcient divided by its standard error (3.12). Roughly aZ score larger than two\\nin absolute value is signiﬁcantly nonzero at thep =0 .05 level.\\nTerm Coeﬃcient Std. Error Z Score\\nIntercept 2.46 0.09 27.60\\nlcavol 0.68 0.13 5.37\\nlweight 0.26 0.10 2.75\\nage −0.14 0.10 −1.40\\nlbph 0.21 0.10 2.06\\nsvi 0.31 0.12 2.47\\nlcp −0.29 0.15 −1.87\\ngleason −0.02 0.15 −0.15\\npgg45 0.27 0.15 1.74\\nexample, that both lcavol and lcp show a strong relationship with the\\nresponse lpsa, and with each other. We need to ﬁt the eﬀects jointly to\\nuntangle the relationships between the predictors and the response.\\nWe ﬁt a linear model to the log of prostate-speciﬁc antigen,lpsa, after\\nﬁrst standardizing the predictors to have unit variance. We randomly split\\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\\nplied least squares estimation to the training set, producing the estimates,\\nstandard errors andZ-scores shown in Table 3.2. TheZ-scores are deﬁned\\nin (3.12), and measure the eﬀect of dropping that variable from the model.\\nA Z-score greater than 2 in absolute value is approximately signiﬁcant at\\nthe 5% level. (For our example, we have nine parameters, and the 0.025 tail\\nquantiles of thet67−9 distribution are±2.002!) The predictorlcavol shows\\nthe strongest eﬀect, withlweight and svi also strong. Notice thatlcp is\\nnot signiﬁcant, oncelcavol is in the model (when used in a model without\\nlcavol, lcp is strongly signiﬁcant). We can also test for the exclusion of\\na number of terms at once, using theF-statistic (3.13). For example, we\\nconsider dropping all the non-signiﬁcant terms in Table 3.2, namelyage,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4a444e5-5e4a-4718-bb9b-c555b02ab312', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 70, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 51\\nlcp, gleason,a n dpgg45.W eg e t\\nF = (32.81−29.43)/(9−5)\\n29.43/(67−9) =1 .67, (3.16)\\nwhich has ap-value of 0.17 (Pr(F4,58 > 1.67) = 0.17), and hence is not\\nsigniﬁcant.\\nThe mean prediction error on the test data is 0.521. In contrast, predic-\\ntion using the mean training value oflpsa has a test error of 1.057, which\\nis called the “base error rate.” Hence the linear model reduces the base\\nerror rate by about 50%. We will return to this example later to compare\\nvarious selection and shrinkage methods.\\n3.2.2 The Gauss–Markov Theorem\\nOne of the most famous results in statistics asserts that the least squares\\nestimates of the parametersβhave the smallest variance among all linear\\nunbiased estimates. We will make this precise here, and also make clear\\nthat the restriction to unbiased estimates is not necessarily a wise one. This\\nobservation will lead ustoconsider biased estimates suchas ridge regression\\nlater in the chapter. We focus on estimation of any linear combination of\\nthe parametersθ= aTβ; for example, predictionsf(x0)= xT\\n0 βare of this\\nform. The least squares estimate ofaTβis\\nˆθ= aT ˆβ= aT(XTX)−1XTy. (3.17)\\nConsidering X to be ﬁxed, this is a linear functioncT\\n0 y of the response\\nvector y. If we assume that the linear model is correct,aT ˆβis unbiased\\nsince\\nE(aT ˆβ)=E ( aT(XTX)−1XTy)\\n= aT(XTX)−1XTXβ\\n= aTβ. (3.18)\\nThe Gauss–Markov theorem states that if we have any other linear estima-\\ntor ˜θ= cTy that is unbiased foraTβ,t h a ti s ,E (cTy)= aTβ,t h e n\\nVar(aT ˆβ)≤Var(cTy). (3.19)\\nThe proof (Exercise 3.3) uses the triangle inequality. For simplicity we have\\nstated the result in terms of estimation of a single parameteraTβ, but with\\na few more deﬁnitions one can state it in terms of the entire parameter\\nvector β(Exercise 3.3).\\nConsider the mean squared error of an estimator˜θin estimatingθ:\\nMSE(˜θ)=E ( ˜θ−θ)2\\n=V a r (˜θ)+[E( ˜θ)−θ]2. (3.20)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c7b1894f-3023-48c5-9bc8-95b3c89b609d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 71, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='52 3. Linear Methods for Regression\\nThe ﬁrst term is the variance, while the second term is the squared bias.\\nThe Gauss-Markov theorem implies that the least squares estimator has the\\nsmallest mean squared error of all linear estimators with no bias. However,\\nthere may well exist a biased estimator with smaller mean squared error.\\nSuchanestimatorwouldtradealittlebiasforalargerreductioninvariance.\\nBiased estimates are commonly used. Any method that shrinks or sets to\\nzero some of the least squares coeﬃcients may result in a biased estimate.\\nWe discuss many examples, including variable subset selection and ridge\\nregression, later in this chapter. From a more pragmatic point of view, most\\nmodels are distortions of the truth, and hence are biased; picking the right\\nmodel amounts to creating the right balance between bias and variance.\\nWe go into these issues in more detail in Chapter 7.\\nMean squared error is intimately related to prediction accuracy, as dis-\\ncussed in Chapter 2. Consider the prediction of the new response at input\\nx0,\\nY0 = f(x0)+ ε0. (3.21)\\nThen the expected prediction error of an estimate˜f(x0)= xT\\n0 ˜βis\\nE(Y0 −˜f(x0))2 = σ2 +E(xT\\n0 ˜β−f(x0))2\\n= σ2 +MSE( ˜f(x0)). (3.22)\\nTherefore, expected prediction error and mean squared error diﬀer only by\\nthe constantσ2, representing the variance of the new observationy0.\\n3.2.3 Multiple Regression from Simple Univariate Regression\\nThe linear model (3.1) with p> 1 inputs is called the multiple linear\\nregression model. The least squares estimates (3.6) for this model are best\\nunderstood in terms of the estimates for the univariate (p = 1) linear\\nmodel, as we indicate in this section.\\nSuppose ﬁrst that we have a univariate model with no intercept, that is,\\nY = Xβ+ε. (3.23)\\nThe least squares estimate and residuals are\\nˆβ=\\n∑N\\n1 xiyi\\n∑N\\n1 x2\\ni\\n,\\nri = yi −xi ˆβ.\\n(3.24)\\nIn convenient vector notation, we lety =( y1,...,y N)T, x =( x1,...,x N)T\\nand deﬁne\\n⟨x,y⟩ =\\nN∑\\ni=1\\nxiyi,\\n= xTy, (3.25)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1dcc4100-9762-4169-99b2-6b0e7e535098', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 72, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 53\\nthe inner productbetween x and y1.T h e nw ec a nw r i t e\\nˆβ= ⟨x,y⟩\\n⟨x,x⟩,\\nr = y−xˆβ.\\n(3.26)\\nAs we will see, this simple univariate regression provides the building block\\nfor multiple linear regression. Suppose next that the inputsx1,x2,..., xp\\n(the columns of the data matrixX) are orthogonal; that is⟨xj,xk⟩ =0\\nfor allj ̸= k. Then it is easy to check that the multiple least squares esti-\\nmates ˆβj are equal to⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on each other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with observational\\ndata. Hence we will have to orthogonalize them in order to carry this idea\\nfurther. Suppose next that we have an intercept and a single inputx.T h e n\\nthe least squares coeﬃcient ofx has the form\\nˆβ1 = ⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯x =∑\\ni xi/N,a n d1 = x0, the vector ofN ones. We can view the\\nestimate (3.27) as the result of two applications of the simple regression\\n(3.26). The steps are:\\n1. regress x on 1 to produce the residualz = x−¯x1;\\n2. regress y on the residualz to give the coeﬃcientˆβ1.\\nInthisprocedure,“regress bona”meansasimpleunivariateregressionof b\\non a with no intercept, producing coeﬃcient ˆγ=⟨a,b⟩/⟨a,a⟩ and residual\\nvector b−ˆγa.W es a yt h a tb is adjusted fora, or is “orthogonalized” with\\nrespect toa.\\nStep 1 orthogonalizesx with respect tox0 = 1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors1 and z. Figure 3.4\\nshows this process for two general inputsx1 and x2. The orthogonalization\\ndoes not change the subspace spanned byx1 and x2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case ofp inputs, as shown in Algorithm 3.1.\\nNote that the inputsz0,..., zj−1 in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the multiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizations of linear regression to\\ndiﬀerent metric spaces, as well as to probability spaces.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f646138-4791-45cd-912d-f8e98d1effec', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 73, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='54 3. Linear Methods for Regression\\nx1\\nx2\\ny\\nˆy\\nzzzzz\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\\nvector x2 is regressed on the vectorx1, leaving the residual vectorz. The regres-\\nsion ofy on z gives the multiple regression coeﬃcient ofx2. Adding together the\\nprojections ofy on each ofx1 and z gives the least squares ﬁtˆy.\\nAlgorithm 3.1Regression by Successive Orthogonalization.\\n1. Initialize z0 = x0 = 1.\\n2. For j =1 ,2,...,p\\nRegress xj on z0,z1,...,, zj−1 to produce coeﬃcients ˆγℓj =\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩, ℓ =0 ,...,j −1 and residual vector zj =\\nxj −∑j−1\\nk=0 ˆγkjzk.\\n3. Regress y on the residualzp to give the estimateˆβp.\\nThe result of this algorithm is\\nˆβp = ⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of thexj is a linear\\ncombination of thezk,k ≤j. Since thezj are all orthogonal, they form\\na basis for the column space ofX, and hence the least squares projection\\nonto this subspace isˆy.S i n c ezp alone involvesxp (with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of\\ny on xp. This key result exposes the eﬀect of correlated inputs in multiple\\nregression. Note also that by rearranging thexj, any one of them could\\nbe in the last position, and a similar results holds. Hence stated more\\ng e n e r a l l y ,w eh a v es h o w nt h a tt h ejth multiple regression coeﬃcient is the\\nunivariate regression coeﬃcient ofy on xj·012...(j−1)(j+1)...,p, the residual\\nafter regressingxj on x0,x1,..., xj−1,xj+1,..., xp:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21db4e8b-ef2a-4b81-bcd0-e29dc3433e95', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 74, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Linear Regression Models and Least Squares 55\\nThe multiple regression coeﬃcientˆβj represents the additional\\ncontribution ofxj on y,a f t e rxj has been adjusted forx0,x1,..., xj−1,\\nxj+1,..., xp.\\nIf xp is highly correlated with some of the otherxk’s, the residual vector\\nzp will be close to zero, and from (3.28) the coeﬃcientˆβp will be very\\nunstable. This will be true for all the variables in the correlated set. In\\nsuch situations, we might have all the Z-scores (as in Table 3.2) be small—\\nany one of the set can be deleted—yet we cannot delete them all. From\\n(3.28) we also obtain an alternate formula for the variance estimates (3.8),\\nVar(ˆβp)= σ2\\n⟨zp,zp⟩ = σ2\\n∥zp∥2. (3.29)\\nIn other words, the precision with which we can estimateˆβp depends on\\nthe length of the residual vector zp;t h i sr e p r e s e n t sh o wm u c ho fxp is\\nunexplained by the otherxk’s.\\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple\\nregression, and is also a useful numerical strategy for computing the esti-\\nmates. We can obtain from it not justˆβp, but also the entire multiple least\\nsquares ﬁt, as shown in Exercise 3.4.\\nWe can represent step 2 of Algorithm 3.1 in matrix form:\\nX = ZΓ, (3.30)\\nwhere Z has as columns thezj (in order), andΓis the upper triangular ma-\\ntrix with entries ˆγkj. Introducing the diagonal matrixD with jth diagonal\\nentry Djj =∥zj∥,w eg e t\\nX = ZD−1DΓ\\n= QR, (3.31)\\nthe so-calledQR decomposition ofX.H e r eQ is anN×(p+1) orthogonal\\nmatrix, QTQ = I,a n dR is a (p+1) ×(p+1) upper triangular matrix.\\nThe QR decomposition represents a convenient orthogonal basis for the\\ncolumn space ofX. It is easy to see, for example, that the least squares\\nsolution is given by\\nˆβ = R−1QTy, (3.32)\\nˆy = QQTy. (3.33)\\nEquation (3.32) is easy to solve becauseR is upper triangular\\n(Exercise 3.4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4016a3d5-54e4-4a55-8f11-36f49b9c506a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 75, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='56 3. Linear Methods for Regression\\n3.2.4 Multiple Outputs\\nSuppose we have multiple outputsY1,Y2,...,Y K that we wish to predict\\nfrom our inputs X0,X1,X2,...,X p. We assume a linear model for each\\noutput\\nYk = β0k +\\np∑\\nj=1\\nXjβjk +εk (3.34)\\n= fk(X)+ εk. (3.35)\\nWith N training cases we can write the model in matrix notation\\nY = XB+E. (3.36)\\nHere Y is theN×K response matrix, withik entryyik, X is theN×(p+1)\\ninput matrix, B is the (p +1 )× K matrix of parameters and E is the\\nN×K matrix of errors. A straightforward generalization of the univariate\\nloss function (3.2) is\\nRSS(B)=\\nK∑\\nk=1\\nN∑\\ni=1\\n(yik −fk(xi))2 (3.37)\\n=t r [ (Y−XB)T(Y−XB)]. (3.38)\\nThe least squares estimates have exactly the same form as before\\nˆB =( XTX)−1XTY. (3.39)\\nHence the coeﬃcients for thekth outcome are just the least squares es-\\ntimates in the regression ofyk on x0,x1,..., xp. Multiple outputs do not\\naﬀect one another’s least squares estimates.\\nIf the errorsε=( ε1,...,ε K) in (3.34) are correlated, then it might seem\\nappropriate to modify (3.37) in favor of a multivariate version. Speciﬁcally,\\nsuppose Cov(ε)= Σ, then the multivariate weighted criterion\\nRSS(B;Σ)=\\nN∑\\ni=1\\n(yi −f(xi))TΣ−1(yi −f(xi)) (3.40)\\narises naturally from multivariate Gaussian theory. Heref(x) is the vector\\nfunction (f1(x),...,f K(x))T,a n dyi the vector ofK responses for obser-\\nvation i. However, it can be shown that again the solution is given by\\n(3.39); K separate regressions that ignore the correlations (Exercise 3.11).\\nIf theΣi vary among observations, then this is no longer the case, and the\\nsolution forB no longer decouples.\\nIn Section 3.7 we pursue the multiple outcome problem, and consider\\nsituations where it does pay to combine the regressions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1f6f22d4-4791-4739-a7aa-09ac523b040f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 76, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Subset Selection 57\\n3.3 Subset Selection\\nThere are two reasons why we are often not satisﬁed with the least squares\\nestimates (3.6).\\n•The ﬁrst isprediction accuracy: the least squares estimates often have\\nlow bias but large variance. Prediction accuracy can sometimes be\\nimproved by shrinking or setting some coeﬃcients to zero. By doing\\nsowesacriﬁcealittlebitofbiastoreducethevarianceofthepredicted\\nvalues, and hence may improve the overall prediction accuracy.\\n•The second reason isinterpretation. With a large number of predic-\\ntors, we often would like to determine a smaller subset that exhibit\\nthe strongest eﬀects. In order to get the “big picture,” we are willing\\nto sacriﬁce some of the small details.\\nIn this section we describe a number of approaches to variable subset selec-\\ntion with linear regression. In later sections we discuss shrinkage and hybrid\\napproaches for controlling variance, as well as other dimension-reduction\\nstrategies. These all fall under the general headingmodel selection.M o d e l\\nselection is not restricted to linear models; Chapter 7 covers this topic in\\nsome detail.\\nWith subset selection we retain only a subset of the variables, and elim-\\ninate the rest from the model. Least squares regression is used to estimate\\nthe coeﬃcients of the inputs that are retained. There are a number of dif-\\nferent strategies for choosing the subset.\\n3.3.1 Best-Subset Selection\\nBest subset regression ﬁnds for eachk ∈{0,1,2,...,p } the subset of sizek\\nthat gives smallest residual sum of squares (3.2). An eﬃcient algorithm—\\nthe leaps and boundsprocedure (Furnival and Wilson, 1974)—makes this\\nfeasible forp as large as 30 or 40. Figure 3.5 shows all the subset models\\nfor the prostate cancer example. The lower boundary represents the models\\nthat are eligible for selection by the best-subsets approach. Note that the\\nbest subset of size 2, for example, need not include the variable that was\\nin the best subset of size 1 (for this example all the subsets are nested).\\nThe best-subset curve (red lower boundary in Figure 3.5) is necessarily\\ndecreasing, so cannot be used to select the subset sizek. The question of\\nhow to choosek involves the tradeoﬀ between bias and variance, along with\\nthe more subjective desire for parsimony. There are a number of criteria\\nthat one may use; typically we choose the smallest model that minimizes\\nan estimate of the expected prediction error.\\nMany of the other approaches that we discuss in this chapter are similar,\\nin that they use the training data to produce a sequence of models varying\\nin complexity and indexed by a single parameter. In the next section we use', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b300598-f7ae-423b-9665-1c8eee5c7382', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 77, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='58 3. Linear Methods for Regression\\nSubset Size k\\nResidual Sum−of−Squares\\n0 2 04 06 08 0 1 0 0\\n012345678\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81\\nFIGURE 3.5. All possible subset models for the prostate cancer example. At\\neach subset size is shown the residual sum-of-squares for each model of that size.\\ncross-validation to estimate prediction error and selectk; the AIC criterion\\nis a popular alternative. We defer more detailed discussion of these and\\nother approaches to Chapter 7.\\n3.3.2 Forward- and Backward-Stepwise Selection\\nRather than search through all possible subsets (which becomes infeasible\\nforpmuchlargerthan40),wecanseekagoodpaththroughthem. Forward-\\nstepwise selectionstarts with the intercept, and then sequentially adds into\\nthe model the predictor that most improves the ﬁt. With many candidate\\npredictors, this might seem like a lot of computation; however, clever up-\\ndating algorithms can exploit the QR decomposition for the current ﬁt to\\nrapidly establish the next candidate (Exercise 3.9). Like best-subset re-\\ngression, forward stepwise produces a sequence of models indexed byk,t h e\\nsubset size, which must be determined.\\nForward-stepwise selection is agreedy algorithm, producing a nested se-\\nquence of models. In this sense it might seem sub-optimal compared to\\nbest-subset selection. However, there are several reasons why it might be\\npreferred:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22bf6c50-8356-4ba7-8d8b-0cab6b123cfa', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 78, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Subset Selection 59\\n•Computational; for large p we cannot compute the best subset se-\\nquence, but we can always compute the forward stepwise sequence\\n(even whenp≫ N).\\n•Statistical; a price is paid in variance for selecting the best subset\\nof each size; forward stepwise is a more constrained search, and will\\nhave lower variance, but perhaps more bias.\\n0 5 10 15 20 25 30\\n0.65 0.70 0.75 0.80 0.85 0.90 0.95\\nBest Subset\\nForward Stepwise\\nBackward Stepwise\\nForward Stagewise\\nE||ˆβ(k)−β||2\\nSubset Sizek\\nFIGURE 3.6.Comparison of four subset-selection techniques on a simulated lin-\\near regression problemY = XTβ+ε. There areN = 300observations onp =3 1\\nstandard Gaussian variables, with pairwise correlations all equal to0.85.F o r10 of\\nthe variables, the coeﬃcients are drawn at random from aN(0,0.4) distribution;\\nthe rest are zero. The noiseε∼N(0,6.25), resulting in a signal-to-noise ratio of\\n0.64. Results are averaged over50 simulations. Shown is the mean-squared error\\nof the estimated coeﬃcientˆβ(k) at each step from the trueβ.\\nBackward-stepwise selection starts with the full model, and sequentially\\ndeletes the predictor that has the least impact on the ﬁt. The candidate for\\ndroppingisthevariablewiththesmallestZ-score(Exercise3.10).Backward\\nselection can only be used whenN>p , while forward stepwise can always\\nbe used.\\nFigure 3.6 shows the results of a small simulation study to compare\\nbest-subset regression with the simpler alternatives forward and backward\\nselection. Their performance is very similar, as is often the case. Included in\\nthe ﬁgure is forward stagewise regression (next section), which takes longer\\nto reach minimum error.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b997a065-f26f-42c2-bf8b-db462dbe226a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 79, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='60 3. Linear Methods for Regression\\nOn the prostate cancer example, best-subset, forward and backward se-\\nlection all gave exactly the same sequence of terms.\\nSome software packages implement hybrid stepwise-selection strategies\\nthat consider both forward and backward moves at each step, and select\\nt h e“ b e s t ”o ft h et w o .F o re x a m p l ei nt h eR package thestep function uses\\nthe AIC criterion for weighing the choices, which takes proper account of\\nthe number of parameters ﬁt; at each step an add or drop will be performed\\nthat minimizes the AIC score.\\nOthermoretraditionalpackagesbasetheselectionon F-statistics,adding\\n“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\\nof fashion, since they do not take proper account of the multiple testing\\nissues. It is also tempting after a model search to print out a summary of\\nthe chosen model, such as in Table 3.2; however, the standard errors are\\nnot valid, since they do not account for the search process. The bootstrap\\n(Section 8.2) can be useful in such settings.\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (such asstep in R) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n3.3.3 Forward-Stagewise Regression\\nForward-stagewise regression (FS) is even more constrained than forward-\\nstepwise regression. It starts like forward-stepwise regression, with an in-\\ntercept equal to ¯y, and centered predictors with coeﬃcients initially all 0.\\nAt each step the algorithm identiﬁes the variable most correlated with the\\ncurrent residual. It then computes the simple linear regression coeﬃcient\\nof the residual on this chosen variable, and then adds it to the current co-\\neﬃcient for that variable. This is continued till none of the variables have\\ncorrelation with the residuals—i.e. the least-squares ﬁt whenN>p .\\nUnlike forward-stepwise regression, none of the other variables are ad-\\njusted when a term is added to the model. As a consequence, forward\\nstagewise can take many more thanp steps to reach the least squares ﬁt,\\nand historically has been dismissed as being ineﬃcient. It turns out that\\nthis “slow ﬁtting” can pay dividends in high-dimensional problems. We\\nsee in Section 3.8.1 that both forward stagewise and a variant which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10−4. For subset size\\nk, we plotted the error for the last step for which there wherek nonzero\\ncoeﬃcients. Although it catches up with the best ﬁt, it takes longer to\\ndo so.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2874d90b-400b-4760-9389-ba2d27e9422e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 80, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 61\\n3.3.4 Prostate Cancer Data Example (Continued)\\nTable 3.3 shows the coeﬃcients from a number of diﬀerent selection and\\nshrinkagemethods.Theyare best-subset selectionusinganall-subsetssearch,\\nridge regression,t h elasso, principal components regressionand partial least\\nsquares. Each method has a complexity parameter, and this was chosen to\\nminimize an estimate of prediction error based on tenfold cross-validation;\\nfulldetailsaregiveninSection7.10.Brieﬂy,cross-validationworksbydivid-\\ning the training data randomly into ten equal parts. The learning method\\nis ﬁt—for a range of values of the complexity parameter—to nine-tenths of\\nthe data, and the prediction error is computed on the remaining one-tenth.\\nThis is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimated prediction\\nerror curve as a function of the complexity parameter.\\nNote that we have already divided these data into a training set of size\\n67 and a test set of size 30. Cross-validation is applied to the training set,\\nsince selecting the shrinkage parameter is part of the training process. The\\ntest set is there to judge the performance of the selected model.\\nThe estimated prediction error curves are shown in Figure 3.7. Many of\\nthe curves are very ﬂat over large ranges near their minimum. Included\\nare estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We have used the\\n“one-standard-error” rule—we pick the most parsimonious model within\\none standard error of the minimum (Section 7.10, page 244). Such a rule\\nacknowledges the fact that the tradeoﬀ curve is estimated with error, and\\nhence takes a conservative approach.\\nBest-subset selection chose to use the two predictorslcvol and lweight.\\nThe last two lines of the table give the average prediction error (and its\\nestimated standard error) over the test set.\\n3.4 Shrinkage Methods\\nBy retaining a subset of the predictors and discarding the rest, subset selec-\\ntion produces a model that is interpretable and has possibly lower predic-\\ntion error than the full model. However, because it is a discrete process—\\nvariables are either retained or discarded—it often exhibits high variance,\\nand so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suﬀer as much from high\\nvariability.\\n3.4.1 Ridge Regression\\nRidge regression shrinks the regression coeﬃcients by imposing a penalty\\non their size. The ridge coeﬃcients minimize a penalized residual sum of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='839080aa-5b59-4fe4-9055-468bc28aa3b6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 81, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='62 3. Linear Methods for Regression\\nSubset Size\\nCV Error\\n02468\\n0.6 0.8 1.0 1.2 1.4 1.6 1.8\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nAll Subsets\\nDegrees of Freedom\\nCV Error\\n02468\\n0.6 0.8 1.0 1.2 1.4 1.6 1.8\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nRidge Regression\\nShrinkage Factor s\\nCV Error\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.6 0.8 1.0 1.2 1.4 1.6 1.8\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nLasso\\nNumber of Directions\\nCV Error\\n02468\\n0.6 0.8 1.0 1.2 1.4 1.6 1.8\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\nPrincipal Components Regression\\nNumber of  Directions\\nCV Error\\n02468\\n0.6 0.8 1.0 1.2 1.4 1.6 1.8\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81\\nPartial Least Squares\\nFIGURE 3.7. Estimated prediction error curves and their standard errors for\\nthe various selection and shrinkage methods. Each curve is plotted as a function\\nof the corresponding complexity parameter for that method. The horizontal axis\\nhas been chosen so that the model complexity increases as we move from left to\\nright. The estimates of prediction error and their standard errors were obtained by\\ntenfold cross-validation; full details are given in Section 7.10. The least complex\\nmodel within one standard error of the best is chosen, indicated by the purple\\nvertical broken lines.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2748490c-ce93-4204-8d4a-b650f6153fb2', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 82, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 63\\nTABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent subset\\nand shrinkage methods applied to the prostate data. The blank entries correspond\\nto variables omitted.\\nTerm LS Best Subset Ridge Lasso PCR PLS\\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\\nage −0.141 −0.046 −0.152 −0.026\\nlbph 0.210 0.162 0.002 0.214 0.220\\nsvi 0.305 0.227 0.094 0.315 0.243\\nlcp −0.288 0.000 −0.051 0.079\\ngleason −0.021 0.040 0.232 0.011\\npgg45 0.267 0.133 −0.056 0.084\\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\\nsquares,\\nˆβridge =a r g m i n\\nβ\\n{ N∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\n+λ\\np∑\\nj=1\\nβ2\\nj\\n}\\n. (3.41)\\nHere λ≥0 is a complexity parameter that controls the amount of shrink-\\nage: the larger the value ofλ, the greater the amount of shrinkage. The\\ncoeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\\ning by the sum-of-squares of the parameters is also used in neural networks,\\nwhere it is known asweight decay(Chapter 11).\\nAn equivalent way to write the ridge problem is\\nˆβridge =a r g m i n\\nβ\\nN∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\n,\\nsubject to\\np∑\\nj=1\\nβ2\\nj ≤t,\\n(3.42)\\nwhich makes explicit the size constraint on the parameters. There is a one-\\nto-one correspondence between the parametersλin (3.41) andt in (3.42).\\nWhen there are many correlated variables in a linear regression model,\\ntheir coeﬃcients can become poorly determined and exhibit high variance.\\nA wildly large positive coeﬃcient on one variable can be canceled by a\\nsimilarly large negative coeﬃcient on its correlated cousin. By imposing a\\nsize constraint on the coeﬃcients, as in (3.42), this problem is alleviated.\\nThe ridge solutions are not equivariant under scaling of the inputs, and\\nso one normally standardizes the inputs before solving (3.41). In addition,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b005f52-b7f0-43af-8478-b1ed76a121bd', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 83, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='64 3. Linear Methods for Regression\\nnotice that the interceptβ0 has been left out of the penalty term. Penal-\\nization of the intercept would make the procedure depend on the origin\\nchosen forY; that is, adding a constantc to each of the targetsyi would\\nnot simply result in a shift of the predictions by the same amountc.I t\\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\\ninto two parts, after reparametrization usingcenteredinputs: eachxij gets\\nreplaced byxij −¯xj.W ee s t i m a t eβ0 by ¯y = 1\\nN\\n∑N\\n1 yi. The remaining co-\\neﬃcients get estimated by a ridge regression without intercept, using the\\ncentered xij. Henceforth we assume that this centering has been done, so\\nthat the input matrixX has p (rather thanp+1) columns.\\nWriting the criterion in (3.41) in matrix form,\\nRSS(λ)=( y−Xβ)T(y−Xβ)+ λβTβ, (3.43)\\nthe ridge regression solutions are easily seen to be\\nˆβridge =( XTX+λI)−1XTy, (3.44)\\nwhere I is thep×p identity matrix. Notice that with the choice of quadratic\\npenalty βTβ, the ridge regression solution is again a linear function of\\ny. The solution adds a positive constant to the diagonal ofXTX before\\ninversion. This makes the problem nonsingular, even ifXTX is not of full\\nrank, and was the main motivation for ridge regression when it was ﬁrst\\nintroducedinstatistics (HoerlandKennard,1970). Traditional descriptions\\nof ridge regression start with deﬁnition (3.44). We choose to motivate it via\\n(3.41) and (3.42), as these provide insight into how it works.\\nFigure 3.8 shows the ridge coeﬃcient estimates for the prostate can-\\ncer example, plotted as functions of df(λ), theeﬀective degrees of freedom\\nimplied by the penaltyλ(deﬁned in (3.50) on page 68). In the case of or-\\nthonormal inputs, the ridge estimates are just a scaled version of the least\\nsquares estimates, that is,ˆβridge = ˆβ/(1+ λ).\\nRidge regression can also be derived as the mean or mode of a poste-\\nrior distribution, with a suitably chosen prior distribution. In detail, sup-\\npose yi ∼N(β0 +xT\\ni β,σ2), and the parametersβj are each distributed as\\nN(0,τ2), independently of one another. Then the (negative) log-posterior\\ndensity of β,w i t hτ2 and σ2 assumed known, is equal to the expression\\nin curly braces in (3.41), withλ= σ2/τ2 (Exercise 3.6). Thus the ridge\\nestimate is the mode of the posterior distribution; since the distribution is\\nGaussian, it is also the posterior mean.\\nThe singular value decomposition(SVD) of the centered input matrixX\\ngives us some additional insight into the nature of ridge regression. This de-\\ncomposition is extremely useful in the analysis of many statistical methods.\\nThe SVD of theN ×p matrix X has the form\\nX = UDVT. (3.45)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='338637ab-2877-4c6d-8b88-443e9d5383f1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 84, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 65\\nCoefficients\\n02468\\n−0.2 0.0 0.2 0.4 0.6\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\nlcavol\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\nlweight\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\nage\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\nlbph\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\nsvi\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\nlcp\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81 gleason\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\npgg45\\ndf(λ)\\nFIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, as\\nthe tuning parameterλis varied. Coeﬃcients are plotted versusdf(λ), the eﬀective\\ndegrees of freedom. A vertical line is drawn atdf = 5.0, the value chosen by\\ncross-validation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f675a6b-b2e0-4271-ad1c-a9c0aa229fc8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 85, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='66 3. Linear Methods for Regression\\nHere U and V are N×p and p×p orthogonal matrices, with the columns\\nof U spanning the column space ofX, and the columns ofV spanning the\\nrow space.D is ap×p diagonal matrix, with diagonal entriesd1 ≥d2 ≥\\n···≥ dp ≥0 called the singular values ofX.I fo n eo rm o r ev a l u e sdj =0 ,\\nX is singular.\\nUsing the singular value decomposition we can write the least squares\\nﬁtted vector as\\nXˆβls = X(XTX)−1XTy\\n= UUTy, (3.46)\\nafter some simpliﬁcation. Note that UTy are the coordinates of y with\\nrespect to the orthonormal basisU. Note also the similarity with (3.33);\\nQ and U are generally diﬀerent orthogonal bases for the column space of\\nX (Exercise 3.8).\\nNow the ridge solutions are\\nXˆβridge = X(XTX+λI)−1XTy\\n= UD (D2 +λI)−1DU Ty\\n=\\np∑\\nj=1\\nuj\\nd2\\nj\\nd2\\nj +λuT\\nj y, (3.47)\\nwhere theuj are the columns ofU.N o t et h a ts i n c eλ≥0, we haved2\\nj/(d2\\nj +\\nλ)≤1. Like linear regression, ridge regression computes the coordinates of\\nywithrespecttotheorthonormalbasis U.Itthenshrinksthesecoordinates\\nby the factorsd2\\nj/(d2\\nj +λ). This means that a greater amount of shrinkage\\nis applied to the coordinates of basis vectors with smallerd2\\nj.\\nWhat does a small value ofd2\\nj mean? The SVD of the centered matrix\\nX is another way of expressing theprincipal componentsof the variables\\nin X. The sample covariance matrix is given byS = XTX/N,a n df r o m\\n(3.45) we have\\nXTX = VD2VT, (3.48)\\nwhich is the eigen decompositionof XTX (and of S, up to a factorN).\\nThe eigenvectors vj (columns of V) are also called theprincipal compo-\\nnents (or Karhunen–Loeve) directions ofX. The ﬁrst principal component\\ndirection v1 has the property thatz1 = Xv1 has the largest sample vari-\\nance amongst all normalized linear combinations of the columns ofX.T h i s\\nsample variance is easily seen to be\\nVar(z1)=V a r (Xv1)= d2\\n1\\nN , (3.49)\\nand in factz1 = Xv1 = u1d1. The derived variablez1 is called the ﬁrst\\nprincipal component ofX, and henceu1 is the normalized ﬁrst principal', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='abde5049-5d84-429a-8180-5a6c6311c012', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 86, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 67\\n-4 -2 0 2 4\\n-4 -2 0 2 4\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no o\\no\\no\\no o\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no oo o\\no\\no\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\nLargest Principal\\nComponent\\nSmallest Principal\\nComponent\\nX1\\nX2\\nFIGURE 3.9.Principal components of some input data points. The largest prin-\\ncipal component is the direction that maximizes the variance of the projected data,\\nand the smallest principal component minimizes that variance. Ridge regression\\nprojects y onto these components, and then shrinks the coeﬃcients of the low–\\nvariance components more than the high-variance components.\\ncomponent. Subsequent principal componentszj have maximum variance\\nd2\\nj/N, subject to being orthogonal to the earlier ones. Conversely the last\\nprincipal component hasminimum variance. Hence the small singular val-\\nues dj correspond to directions in the column space of X having small\\nvariance, and ridge regression shrinks these directions the most.\\nFigure 3.9 illustrates the principal components of some data points in\\ntwo dimensions. If we consider ﬁtting a linear surface over this domain\\n(the Y-axis is sticking out of the page), the conﬁguration of the data allow\\nus to determine its gradient more accurately in the long direction than\\nthe short. Ridge regression protects against the potentially high variance\\nof gradients estimated in the short directions. The implicit assumption is\\nthat the response will tend to vary most in the directions of high variance\\nof the inputs. This is often a reasonable assumption, since predictors are\\noften chosen for study because they vary with the response variable, but\\nneed not hold in general.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d4f4db99-6762-4ed0-a3c7-9bf7520d49f9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 87, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='68 3. Linear Methods for Regression\\nIn Figure 3.7 we have plotted the estimated prediction error versus the\\nquantity\\ndf(λ)=t r [ X(XTX+λI)−1XT],\\n=t r (Hλ)\\n=\\np∑\\nj=1\\nd2\\nj\\nd2\\nj +λ. (3.50)\\nThis monotone decreasing function ofλis theeﬀective degrees of freedom\\nof the ridge regression ﬁt. Usually in a linear-regression ﬁt withp variables,\\nthe degrees-of-freedom of the ﬁt isp, the number of free parameters. The\\nidea is that although allp coeﬃcients in a ridge ﬁt will be non-zero, they\\nare ﬁt in a restricted fashion controlled byλ.N o t et h a td f (λ)= p when\\nλ= 0 (no regularization) and df(λ) → 0a s λ→∞.O fc o u r s et h e r e\\nis always an additional one degree of freedom for the intercept, which was\\nremovedapriori. This deﬁnition is motivated in more detail in Section 3.4.4\\nand Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df(λ)=5 .0.\\nTable 3.3 shows that ridge regression reduces the test error of the full least\\nsquares estimates by a small amount.\\n3.4.2 The Lasso\\nThe lasso is a shrinkage method like ridge, with subtle but important dif-\\nferences. The lasso estimate is deﬁned by\\nˆβlasso =a r g m i n\\nβ\\nN∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\nsubject to\\np∑\\nj=1\\n|βj|≤t. (3.51)\\nJust as in ridge regression, we can re-parametrize the constantβ0 by stan-\\ndardizing the predictors; the solution forˆβ0 is ¯y, and thereafter we ﬁt a\\nmodel without an intercept (Exercise 3.5). In the signal processing litera-\\nture, the lasso is also known asbasis pursuit(Chen et al., 1998).\\nWe can also write the lasso problem in the equivalentLagrangian form\\nˆβlasso =a r g m i n\\nβ\\n{ 1\\n2\\nN∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\n+λ\\np∑\\nj=1\\n|βj|\\n}\\n. (3.52)\\nNotice the similarity to the ridge regression problem (3.42) or (3.41): the\\nL2 ridge penalty∑p\\n1 β2\\nj is replaced by theL1 lasso penalty∑p\\n1 |βj|.T h i s\\nlatter constraint makes the solutions nonlinear in theyi, and there is no\\nclosed form expression as in ridge regression. Computing the lasso solution', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1ed885c-2916-4329-b1fd-4689c11890d6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 88, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 69\\nis a quadratic programming problem, although we see in Section 3.4.4 that\\neﬃcient algorithms are available for computing the entire path of solutions\\nas λis varied, with the same computational cost as for ridge regression.\\nBecause of the nature of the constraint, makingt suﬃciently small will\\ncause some of the coeﬃcients to be exactly zero. Thus the lasso does a kind\\nof continuous subset selection. Ift is chosen larger thant0 =∑p\\n1 |ˆβj| (where\\nˆβj = ˆβls\\nj , the least squares estimates), then the lasso estimates are theˆβj’s.\\nOn the other hand, fort = t0/2 say, then the least squares coeﬃcients are\\nshrunk by about 50% on average. However, the nature of the shrinkage\\nis not obvious, and we investigate it further in Section 3.4.4 below. Like\\nthe subset size in variable subset selection, or the penalty parameter in\\nridge regression,t should be adaptively chosen to minimize an estimate of\\nexpected prediction error.\\nIn Figure 3.7, for ease of interpretation, we have plotted the lasso pre-\\ndiction error estimates versus the standardized parameters = t/∑p\\n1 |ˆβj|.\\nAv a l u eˆs ≈0.36 was chosen by 10-fold cross-validation; this caused four\\ncoeﬃcients to be set to zero (ﬁfth column of Table 3.3). The resulting\\nmodel has the second lowest test error, slightly lower than the full least\\nsquares model, but the standard errors of the test error estimates (last line\\nof Table 3.3) are fairly large.\\nFigure 3.10 shows the lasso coeﬃcients as the standardized tuning pa-\\nrameter s = t/∑p\\n1 |ˆβj| is varied. Ats =1 .0 these are the least squares\\nestimates; they decrease to 0 ass→0. This decrease is not always strictly\\nmonotonic, although it is in this example. A vertical line is drawn at\\ns =0 .36, the value chosen by cross-validation.\\n3.4.3 Discussion: Subset Selection, Ridge Regression and the\\nLasso\\nIn this section we discuss and compare the three approaches discussed so far\\nfor restricting the linear regression model: subset selection, ridge regression\\nand the lasso.\\nIn the case of an orthonormal input matrixX the three procedures have\\nexplicit solutions. Each method applies a simple transformation to the least\\nsquares estimate ˆβj, as detailed in Table 3.4.\\nRidge regression does a proportional shrinkage. Lasso translates each\\ncoeﬃcient by a constant factorλ, truncating at zero. This is called “soft\\nthresholding,”andisusedinthecontextofwavelet-based smoothinginSec-\\ntion 5.9. Best-subset selection drops all variables with coeﬃcients smaller\\nthan theMth largest; this is a form of “hard-thresholding.”\\nBack to the nonorthogonal case; some pictures help understand their re-\\nlationship. Figure 3.11 depicts the lasso (left) and ridge regression (right)\\nwhen there are only two parameters. The residual sum of squares has ellip-\\ntical contours, centered at the full least squares estimate. The constraint', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='be23ef91-05d2-4867-a8cc-27e257b44706', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 89, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='70 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0\\n−0.2 0.0 0.2 0.4 0.6\\nShrinkage Factor s\\nCoefficients\\nlcavol\\nlweight\\nage\\nlbph\\nsvi\\nlcp\\ngleason\\npgg45\\nFIGURE 3.10.Proﬁles of lasso coeﬃcients, as the tuning parametert is varied.\\nCoeﬃcients are plotted versuss = t/∑p\\n1 |ˆβj|. A vertical line is drawn ats =0 .36,\\nthe value chosen by cross-validation. Compare Figure 3.8 on page 65; the lasso\\nproﬁles hit zero, while those for ridge do not. The proﬁles are piece-wise linear,\\nand so are computed only at the points displayed; see Section 3.4.4 for details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='317bdeef-c7b1-48ee-873e-97da072d0e5b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 90, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 71\\nTABLE 3.4.Estimators ofβj in the case of orthonormal columns ofX. M and λ\\nare constants chosen by the corresponding techniques;sign denotes the sign of its\\nargument (±1), andx+ denotes “positive part” ofx. Below the table, estimators\\nare shown by broken red lines. The45◦line in gray shows the unrestricted estimate\\nfor reference.\\nEstimator Formula\\nBest subset (sizeM) ˆβj ·I(|ˆβj|≥| ˆβ(M)|)\\nRidge ˆβj/(1+ λ)\\nLasso sign( ˆβj)(|ˆβj|−λ)+\\n(0,0) (0,0) (0,0)\\n|ˆβ(M)|\\nλ\\nBest Subset Ridge Lasso\\nβ^ β^2 ..β\\n1\\nβ 2\\nβ1 β\\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\\n(right). Shown are contours of the error and constraint functions. The solid blue\\nareas are the constraint regions|β1| + |β2|≤ t and β2\\n1 + β2\\n2 ≤t2, respectively,\\nwhile the red ellipses are the contours of the least squares error function.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5935d966-85f1-4793-b662-907efab00007', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 91, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='72 3. Linear Methods for Regression\\nregion for ridge regression is the diskβ2\\n1 + β2\\n2 ≤t, while that for lasso is\\nthe diamond|β1| +|β2|≤ t. Both methods ﬁnd the ﬁrst point where the\\nelliptical contours hit the constraint region. Unlike the disk, the diamond\\nhas corners; if the solution occurs at a corner, then it has one parameter\\nβj equal to zero. Whenp> 2, the diamond becomes a rhomboid, and has\\nmany corners, ﬂat edges and faces; there are many more opportunities for\\nthe estimated parameters to be zero.\\nWe can generalize ridge regression and the lasso, and view them as Bayes\\nestimates. Consider the criterion\\n˜β=a r g m i n\\nβ\\n{ N∑\\ni=1\\n⎤\\nyi −β0 −\\np∑\\nj=1\\nxijβj\\n⎦2\\n+λ\\np∑\\nj=1\\n|βj|q\\n}\\n(3.53)\\nfor q ≥0. The contours of constant value of∑\\nj |βj|q are shown in Fig-\\nure 3.12, for the case of two inputs.\\nThinking of|βj|q as the log-prior density forβj, these are also the equi-\\ncontours of the prior distribution of the parameters. The valueq = 0 corre-\\nspondstovariablesubsetselection,asthepenaltysimplycountsthenumber\\nof nonzero parameters;q = 1 corresponds to the lasso, whileq =2t or i d g e\\nregression. Notice that forq ≤1, the prior is not uniform in direction, but\\nconcentrates more mass in the coordinate directions. The prior correspond-\\ning to theq = 1 case is an independent double exponential (or Laplace)\\ndistribution for each input, with density (1/2τ)exp(−|β|/τ)a n dτ=1 /λ.\\nThe case q = 1 (lasso) is the smallestq such that the constraint region\\nis convex; non-convex constraint regions make the optimization problem\\nmore diﬃcult.\\nIn this view, the lasso, ridge regression and best subset selection are\\nBayes estimates with diﬀerent priors. Note, however, that they are derived\\nas posterior modes, that is, maximizers of the posterior. It is more common\\nto use the mean of the posterior as the Bayes estimate. Ridge regression is\\nalso the posterior mean, but the lasso and best subset selection are not.\\nLooking again at the criterion (3.53), we might try using other values\\nof q besides 0, 1, or 2. Although one might consider estimatingq from\\nthe data, our experience is that it is not worth the eﬀort for the extra\\nvariance incurred. Values ofq ∈(1,2) suggest a compromise between the\\nlasso and ridge regression. Although this is the case, withq> 1, |βj|q is\\ndiﬀerentiable at 0, and so does not share the ability of lasso (q =1 )f o r\\nq =4 q =2 q =1 q =0 .5 q =0 .1\\nFIGURE 3.12.Contours of constant value of∑\\nj |βj|q for given values ofq.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5fa0f0bb-266c-45d7-8827-d22b186e38cf', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 92, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 73\\nq =1 .2 α=0 .2\\nLq Elastic Net\\nFIGURE 3.13. Contours of constant value of∑\\nj |βj|q for q =1 .2 (left plot),\\nand the elastic-net penalty∑\\nj(αβ2\\nj +(1−α)|βj|) for α=0 .2 (right plot). Although\\nvisually very similar, the elastic-net has sharp (non-diﬀerentiable) corners, while\\nthe q =1 .2 penalty does not.\\nsetting coeﬃcients exactly to zero. Partly for this reason as well as for\\ncomputational tractability, Zou and Hastie (2005) introduced theelastic-\\nnet penalty\\nλ\\np∑\\nj=1\\n⎤\\nαβ2\\nj +(1 −α)|βj|\\n⎦\\n, (3.54)\\na diﬀerent compromise between ridge and lasso. Figure 3.13 compares the\\nLq penalty with q =1 .2 and the elastic-net penalty withα=0 .2; it is\\nhard to detect the diﬀerence by eye. The elastic-net selects variables like\\nthe lasso, and shrinks together the coeﬃcients of correlated predictors like\\nridge. It also has considerable computational advantages over theLq penal-\\nties. We discuss the elastic-net further in Section 18.4.\\n3.4.4 Least Angle Regression\\nLeast angle regression (LAR) is a relative newcomer (Efron et al., 2004),\\nand can be viewed as a kind of “democratic” version of forward stepwise\\nregression (Section 3.3.2). As we will see, LAR is intimately connected\\nwith the lasso, and in fact provides an extremely eﬃcient algorithm for\\ncomputing the entire lasso path as in Figure 3.10.\\nForward stepwise regression builds a model sequentially, adding one vari-\\nable at a time. At each step, it identiﬁes the best variable to include in the\\nactive set, and then updates the least squares ﬁt to include all the active\\nvariables.\\nLeast angle regression uses a similar strategy, but only enters “as much”\\nof a predictor as it deserves. At the ﬁrst step it identiﬁes the variable\\nmost correlated with the response. Rather than ﬁt this variable completely,\\nLAR moves the coeﬃcient of this variable continuously toward its least-\\nsquares value (causing its correlation with the evolving residual to decrease\\nin absolute value). As soon as another variable “catches up” in terms of\\ncorrelation with the residual, the process is paused. The second variable\\nthen joins the active set, and their coeﬃcients are moved together in a way\\nthat keeps their correlations tied and decreasing. This process is continued', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='47ed3aae-bdc7-4cbd-8dae-8c2c7af42f41', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 93, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='74 3. Linear Methods for Regression\\nuntil all the variables are in the model, and ends at the full least-squares\\nﬁt. Algorithm 3.2 provides the details. The termination condition in step 5\\nrequires some explanation. Ifp>N −1, the LAR algorithm reaches a zero\\nresidual solution afterN−1 steps (the−1 is because we have centered the\\ndata).\\nAlgorithm 3.2Least Angle Regression.\\n1. Standardize the predictors to have mean zero and unit norm. Start\\nwith the residualr = y−¯y, β1,β2,...,β p =0 .\\n2. Find the predictorxj most correlated withr.\\n3. Moveβj from 0 towards its least-squares coeﬃcient⟨xj,r⟩,u n t i ls o m e\\nother competitorxk has as much correlation with the current residual\\nas doesxj.\\n4. Move βj and βk in the direction deﬁned by their joint least squares\\ncoeﬃcient of the current residual on (xj,xk), until some other com-\\npetitor xl has as much correlation with the current residual.\\n5. Continue in this way until allp predictors have been entered. After\\nmin(N −1,p) steps, we arrive at the full least-squares solution.\\nSuppose Ak is the active set of variables at the beginning of thekth\\nstep, and letβAk be the coeﬃcient vector for these variables at this step;\\nthere will bek−1 nonzero values, and the one just entered will be zero. If\\nrk = y−XAkβAk is the current residual, then the direction for this step is\\nδk =( XT\\nAkXAk)−1XT\\nAkrk. (3.55)\\nThe coeﬃcient proﬁle then evolves asβAk(α)= βAk +α·δk. Exercise 3.23\\nveriﬁes that the directions chosen in this fashion do what is claimed: keep\\nthe correlations tied and decreasing. If the ﬁt vector at the beginning of\\nthis step isˆfk,t h e ni te v o l v e sa sˆfk(α)= ˆfk + α· uk,w h e r euk = XAkδk\\nis the new ﬁt direction. The name “least angle” arises from a geometrical\\ninterpretation of this process; uk makes the smallest (and equal) angle\\nwith each of the predictors inAk (Exercise 3.24). Figure 3.14 shows the\\nabsolute correlations decreasing and joining ranks with each step of the\\nLAR algorithm, using simulated data.\\nBy construction the coeﬃcients in LAR change in a piecewise linear fash-\\nion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\\nfunction of theirL1 arc length2. Note that we do not need to take small\\n2The L1 arc-length of a diﬀerentiable curveβ(s)f o rs ∈[0,S] is given by TV(β,S)=∫S\\n0 || ˙β(s)||1ds, where ˙β(s)= ∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\\nthis amounts to summing theL1 norms of the changes in coeﬃcients from step to step.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42245339-2a0f-484c-9418-356f912c0b46', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 94, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 75\\n0 5 10 15\\n0.0 0.1 0.2 0.3 0.4\\nv2 v6 v4 v5 v3 v1\\nL1 Arc Length\\nAbsolute Correlations\\nFIGURE 3.14.Progression of the absolute correlations during each step of the\\nLAR procedure, using a simulated data set with six predictors. The labels at the\\ntop of the plot indicate which variables enter the active set at each step. The step\\nlength are measured in units ofL1 arc length.\\n0 5 10 15\\n−1.5 −1.0 −0.5 0.0 0.5\\nLeast Angle Regression\\n0 5 10 15\\n−1.5 −1.0 −0.5 0.0 0.5\\nLasso\\nL1 Arc LengthL1 Arc Length\\nCoeﬃcients\\nCoeﬃcients\\nFIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\\ndata, as a function of theL1 arc length. The right panel shows the Lasso proﬁle.\\nThey are identical until the dark-blue coeﬃcient crosses zero at an arc length of\\nabout 18.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5a128a3-ba3d-48e0-8033-46d01acd8bae', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 95, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='76 3. Linear Methods for Regression\\nsteps and recheck the correlations in step 3; using knowledge of the covari-\\nance of the predictors and the piecewise linearity of the algorithm, we can\\nwork out the exact step length at the beginning of each step (Exercise 3.25).\\nThe right panel of Figure 3.15 shows the lasso coeﬃcient proﬁles on the\\nsame data. They are almost identical to those in the left panel, and diﬀer\\nfor the ﬁrst time when the blue coeﬃcient passes back through zero. For the\\nprostate data, the LAR coeﬃcient proﬁle turns out to be identical to the\\nlasso proﬁle in Figure 3.10, which never crosses zero. These observations\\nlead to a simple modiﬁcation of the LAR algorithm that gives the entire\\nlasso path, which is also piecewise-linear.\\nAlgorithm 3.2aLeast Angle Regression: Lasso Modiﬁcation.\\n4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\\nof variables and recompute the current joint least squares direction.\\nThe LAR(lasso) algorithm is extremely eﬃcient, requiring the same order\\nof computation as that of a single least squares ﬁt using thep predictors.\\nLeast angle regression always takesp steps to get to the full least squares\\nestimates. The lasso path can have more thanp steps, although the two\\nare often quite similar. Algorithm 3.2 with the lasso modiﬁcation 3.2a is\\nan eﬃcient way of computing the solution to any lasso problem, especially\\nwhen p≫ N. Osborne et al. (2000a) also discovered a piecewise-linear path\\nfor computing the lasso, which they called ahomotopy algorithm.\\nWenowgiveaheuristicargumentforwhytheseproceduresaresosimilar.\\nAlthough the LAR algorithm is stated in terms of correlations, if the input\\nfeatures are standardized, it is equivalent and easier to work with inner-\\nproducts. Suppose A is the active set of variables at some stage in the\\nalgorithm, tied in their absolute inner-product with the current residuals\\ny−Xβ. We can express this as\\nxT\\nj (y−Xβ)= γ·sj, ∀j ∈A (3.56)\\nwhere sj ∈{ −1,1} indicates the sign of the inner-product, andγis the\\ncommon value. Also|xT\\nk (y−Xβ)|≤ γ∀k ̸∈A. Now consider the lasso\\ncriterion (3.52), which we write in vector form\\nR(β)= 1\\n2||y−Xβ||2\\n2 +λ||β||1. (3.57)\\nLet B be the active set of variables in the solution for a given value ofλ.\\nFor these variablesR(β) is diﬀerentiable, and the stationarity conditions\\ngive\\nxT\\nj (y−Xβ)= λ·sign(βj), ∀j ∈B (3.58)\\nComparing (3.58) with (3.56), we see that they are identical only if the\\nsign of βj matches the sign of the inner product. That is why the LAR', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89103b5a-8134-45c2-9b3c-4969019813c2', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 96, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Shrinkage Methods 77\\nalgorithm and lasso start to diﬀer when an active coeﬃcient passes through\\nzero; condition (3.58) is violated for that variable, and it is kicked out of the\\nactive setB. Exercise 3.23 shows that these equations imply a piecewise-\\nlinear coeﬃcient proﬁle asλdecreases. The stationarity conditions for the\\nnon-active variables require that\\n|xT\\nk (y−Xβ)|≤λ,∀k ̸∈B, (3.59)\\nwhich again agrees with the LAR algorithm.\\nFigure 3.16 compares LAR and lasso to forward stepwise and stagewise\\nregression. The setup is the same as in Figure 3.6 on page 59, except here\\nN = 100 here rather than 300, so the problem is more diﬃcult. We see\\nthat the more aggressive forward stepwise starts to overﬁt quite early (well\\nbefore the 10 true variables can enter the model), and ultimately performs\\nworse than the slower forward stagewise regression. The behavior of LAR\\nand lasso is similar to that of forward stagewise regression. Incremental\\nforward stagewise is similar to LAR and lasso, and is described in Sec-\\ntion 3.8.1.\\nDegrees-of-Freedom Formula for LAR and Lasso\\nSuppose that we ﬁt a linear model via the least angle regression procedure,\\nstopping at some number of stepsk<p , or equivalently using a lasso bound\\nt that produces a constrained version of the full least squares ﬁt. How many\\nparameters, or “degrees of freedom” have we used?\\nConsiderﬁrstalinearregressionusingasubsetof k features.Ifthissubset\\nis prespeciﬁed in advance without reference to the training data, then the\\ndegrees of freedom used in the ﬁtted model is deﬁned to bek. Indeed, in\\nclassical statistics, the number of linearly independent parameters is what\\nis meant by “degrees of freedom.” Alternatively, suppose that we carry out\\na best subset selection to determine the “optimal” set ofk predictors. Then\\nthe resulting model hask parameters, but in some sense we have used up\\nmore thank degrees of freedom.\\nWe need a more general deﬁnition for the eﬀective degrees of freedom of\\nan adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\\nvector ˆy =(ˆy1,ˆy2,..., ˆyN)a s\\ndf(ˆy)= 1\\nσ2\\nN∑\\ni=1\\nCov(ˆyi,yi). (3.60)\\nHere Cov(ˆyi,yi) refers to the sampling covariance between the predicted\\nvalue ˆyi and its corresponding outcome valueyi. This makes intuitive sense:\\nthe harder that we ﬁt to the data, the larger this covariance and hence\\ndf(ˆy). Expression (3.60) is a useful notion of degrees of freedom, one that\\ncan be applied to any model predictionˆy. This includes models that are', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='33f63a6b-4b08-4d9b-a34f-ab47f925107e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 97, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='78 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.55 0.60 0.65\\nForward Stepwise\\nLAR\\nLasso\\nForward Stagewise\\nIncremental Forward Stagewise\\nE||ˆβ(k)−β||2\\nFraction ofL1 arc-length\\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\\nstagewise (FS) and incremental forward stagewise (FS0) regression. The setup\\nis the same as in Figure 3.6, exceptN = 100 here rather than 300.H e r et h e\\nslower FS regression ultimately outperforms forward stepwise. LAR and lasso\\ns h o ws i m i l a rb e h a v i o rt oF Sa n dF S0. Since the procedures take diﬀerent numbers\\nof steps (across simulation replicates and methods), we plot the MSE as a function\\nof the fraction of totalL1 arc-length toward the least-squares ﬁt.\\nadaptively ﬁtted to the training data. This deﬁnition is motivated and\\ndiscussed further in Sections 7.4–7.6.\\nNow for a linear regression withk ﬁxed predictors, it is easy to show\\nthat df(ˆy)= k. Likewise for ridge regression, this deﬁnition leads to the\\nclosed-form expression (3.50) on page 68: df(ˆy)=t r (Sλ). In both these\\ncases, (3.60) is simple to evaluate because the ﬁtˆy = Hλy is linear iny.\\nIf we think about deﬁnition (3.60) in the context of a best subset selection\\nof size k, it seems clear that df(ˆy) will be larger thank, and this can be\\nveriﬁed by estimating Cov(ˆyi,yi)/σ2 directly by simulation. However there\\nis no closed form method for estimating df(ˆy) for best subset selection.\\nFor LAR and lasso, something magical happens. These techniques are\\nadaptiveinasmootherwaythanbestsubsetselection,andhenceestimation\\nof degrees of freedom is more tractable. Speciﬁcally it can be shown that\\nafter thekth step of the LAR procedure, the eﬀective degrees of freedom of\\nthe ﬁt vector is exactlyk. Now for the lasso, the (modiﬁed) LAR procedure', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9edd7195-6fcf-4b9f-8dd4-9cb99fcd8d8f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 98, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5 Methods Using Derived Input Directions 79\\noften takes more thanp steps, since predictors can drop out. Hence the\\ndeﬁnition is a little diﬀerent; for the lasso, at any stage df(ˆy) approximately\\nequals the number of predictors in the model. While this approximation\\nworks reasonably well anywhere in the lasso path, for eachk it works best\\nat the last model in the sequence that containsk predictors. A detailed\\nstudy of the degrees of freedom for the lasso may be found in Zou et al.\\n(2007).\\n3.5 Methods Using Derived Input Directions\\nIn many situations we have a large number of inputs, often very correlated.\\nThe methods in this section produce a small number of linear combinations\\nZm,m =1 ,...,M of the original inputsXj,a n dt h eZm are then used in\\nplace of theXj as inputs in the regression. The methods diﬀer in how the\\nlinear combinations are constructed.\\n3.5.1 Principal Components Regression\\nIn this approach the linear combinationsZm used are the principal com-\\nponents as deﬁned in Section 3.4.1 above.\\nPrincipal component regression forms the derived input columnszm =\\nXvm, and then regressesy on z1,z2,..., zM for someM ≤p. Since thezm\\nare orthogonal, this regression is just a sum of univariate regressions:\\nˆypcr\\n(M) =¯y1+\\nM∑\\nm=1\\nˆθmzm, (3.61)\\nwhere ˆθm = ⟨zm,y⟩/⟨zm,zm⟩. Since thezm are each linear combinations\\nof the originalxj, we can express the solution (3.61) in terms of coeﬃcients\\nof thexj (Exercise 3.13):\\nˆβpcr(M)=\\nM∑\\nm=1\\nˆθmvm. (3.62)\\nAs with ridge regression, principal components depend on the scaling of\\nthe inputs, so typically we ﬁrst standardize them. Note that ifM = p,w e\\nwould just get back the usual least squares estimates, since the columns of\\nZ = UD span the column space ofX.F o rM<p we get a reduced regres-\\nsion. We see that principal components regression is very similar to ridge\\nregression: both operate via the principal components of the input ma-\\ntrix. Ridge regression shrinks the coeﬃcients of the principal components\\n(Figure 3.17), shrinking more depending on the size of the corresponding\\neigenvalue; principal components regression discards thep−M smallest\\neigenvalue components. Figure 3.17 illustrates this.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d3b9317c-67dd-4647-b657-5b04b920172c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 99, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='80 3. Linear Methods for Regression\\nIndex\\nShrinkage Factor\\n2468\\n0.0 0.2 0.4 0.6 0.8 1.0\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81 \\x81 \\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\nridge\\npcr\\nFIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the prin-\\ncipal components, using shrinkage factors d2\\nj/(d2\\nj + λ) as in (3.47). Principal\\ncomponent regression truncates them. Shown are the shrinkage and truncation\\npatterns corresponding to Figure 3.7, as a function of the principal component\\nindex.\\nIn Figure 3.7 we see that cross-validation suggests seven terms; the re-\\nsulting model has the lowest test error in Table 3.3.\\n3.5.2 Partial Least Squares\\nThis technique also constructs a set of linear combinations of the inputs\\nfor regression, but unlike principal components regression it usesy (in ad-\\ndition to X) for this construction. Like principal component regression,\\npartial least squares (PLS) is not scale invariant, so we assume that each\\nxj is standardized to have mean 0 and variance 1. PLS begins by com-\\nputing ˆϕ1j = ⟨xj,y⟩ for eachj. From this we construct the derived input\\nz1 = ∑\\nj ˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\\nin the construction of eachzm, the inputs are weighted by the strength\\nof their univariate eﬀect ony3. The outcomey is regressed onz1 giving\\ncoeﬃcientˆθ1, and then we orthogonalizex1,..., xp with respect toz1.W e\\ncontinue this process, untilM ≤p directions have been obtained. In this\\nmanner, partial least squares produces a sequence of derived, orthogonal\\ninputs or directions z1,z2,..., zM. As with principal-component regres-\\nsion, if we were to construct allM = p directions, we would get back a\\nsolution equivalent to the usual least squares estimates; usingM<p di-\\nrections produces a reduced regression. The procedure is described fully in\\nAlgorithm 3.3.\\n3Since thexj are standardized, the ﬁrst directions ˆϕ1j are the univariate regression\\ncoeﬃcients (up to an irrelevant constant); this is not the case for subsequent directions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='88b0f1aa-972f-4254-bde7-7beb125a9c0f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 100, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5 Methods Using Derived Input Directions 81\\nAlgorithm 3.3Partial Least Squares.\\n1. Standardize eachxj to have mean zero and variance one. Setˆy(0) =\\n¯y1,a n dx(0)\\nj = xj,j =1 ,...,p .\\n2. For m =1 ,2,...,p\\n(a) zm =∑p\\nj=1 ˆϕmjx(m−1)\\nj ,w h e r eˆϕmj =⟨x(m−1)\\nj ,y⟩.\\n(b) ˆθm =⟨zm,y⟩/⟨zm,zm⟩.\\n(c) ˆy(m) = ˆy(m−1) + ˆθmzm.\\n(d) Orthogonalize eachx(m−1)\\nj with respect tozm: x(m)\\nj = x(m−1)\\nj −\\n[⟨zm,x(m−1)\\nj ⟩/⟨zm,zm⟩]zm, j =1 ,2,...,p .\\n3. Output the sequence of ﬁtted vectors{ˆy(m)}p\\n1. Since the{zℓ}m\\n1 are\\nlinear in the originalxj,s oi sˆy(m) = Xˆβpls(m). These linear coeﬃ-\\ncients can be recovered from the sequence of PLS transformations.\\nIn the prostate cancer example, cross-validation choseM = 2 PLS direc-\\ntions in Figure 3.7. This produced the model given in the rightmost column\\nof Table 3.3.\\nWhat optimization problem is partial least squares solving? Since it uses\\nthe response y to construct its directions, its solution path is a nonlinear\\nfunction of y. It can be shown (Exercise 3.15) that partial least squares\\nseeks directions that have high varianceand have high correlation with the\\nresponse, in contrast to principal components regression which keys only\\non high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In\\nparticular, themth principal component directionvm solves:\\nmaxαVar(Xα) (3.63)\\nsubject to||α|| =1 ,αTSvℓ =0 ,ℓ =1 ,...,m −1,\\nwhere S is the sample covariance matrix of thexj. The conditionsαTSvℓ =\\n0 ensures thatzm = Xαis uncorrelated with all the previous linear com-\\nbinations zℓ = Xvℓ.T h emth PLS direction ˆϕm solves:\\nmaxαCorr2(y,Xα)Var(Xα) (3.64)\\nsubject to||α|| =1 ,αTSˆϕℓ =0 ,ℓ =1 ,...,m −1.\\nFurther analysis reveals that the variance aspect tends to dominate, and\\nso partial least squares behaves much like ridge regression and principal\\ncomponents regression. We discuss this further in the next section.\\nIf the input matrixX is orthogonal, then partial least squares ﬁnds the\\nleast squares estimates afterm = 1 steps. Subsequent steps have no eﬀect', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ebe3e2e6-2722-4e8e-90a5-ab9bdeb0a67a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 101, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='82 3. Linear Methods for Regression\\nsince the ˆϕmj are zero form> 1 (Exercise 3.14). It can also be shown that\\nthe sequence of PLS coeﬃcients form =1 ,2,...,p represents the conjugate\\ngradient sequence for computing the least squares solutions (Exercise 3.18).\\n3.6 Discussion: A Comparison of the Selection and\\nShrinkage Methods\\nThere are some simple settings where we can understand better the rela-\\ntionship between the diﬀerent methods described above. Consider an exam-\\nple with two correlated inputsX1 and X2, with correlationρ. We assume\\nthat the true regression coeﬃcients areβ1 =4a n d β2 = 2. Figure 3.18\\nshows the coeﬃcient proﬁles for the diﬀerent methods, as their tuning pa-\\nrameters are varied. The top panel hasρ=0 .5, the bottom panelρ=−0.5.\\nThe tuning parameters for ridge and lasso vary over a continuous range,\\nwhile best subset, PLS and PCR take just two discrete steps to the least\\nsquares solution. In the top panel, starting at the origin, ridge regression\\nshrinks the coeﬃcients together until it ﬁnally converges to least squares.\\nPLS and PCR show similar behavior to ridge, although are discrete and\\nmore extreme. Best subset overshoots the solution and then backtracks.\\nThe behavior of the lasso is intermediate to the other methods. When the\\ncorrelation is negative (lower panel), again PLS and PCR roughly track\\nthe ridge path, while all of the methods are more similar to one another.\\nIt is interesting to compare the shrinkage behavior of these diﬀerent\\nmethods. Recall that ridge regression shrinks all directions, but shrinks\\nlow-variance directions more. Principal components regression leavesM\\nhigh-variance directions alone, and discards the rest. Interestingly, it can\\nbe shown that partial least squares also tends to shrink the low-variance\\ndirections, but can actually inﬂate some of the higher variance directions.\\nThis can make PLS a little unstable, and cause it to have slightly higher\\nprediction error compared to ridge regression. A full study is given in Frank\\nand Friedman (1993). These authors conclude that for minimizing predic-\\ntion error, ridge regression is generally preferable to variable subset selec-\\ntion, principal components regression and partial least squares. However\\nthe improvement over the latter two methods was only slight.\\nTo summarize, PLS, PCR and ridge regression tend to behave similarly.\\nRidge regression may be preferred because it shrinks smoothly, rather than\\nin discrete steps. Lasso falls somewhere between ridge regression and best\\nsubset regression, and enjoys some of the properties of each.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9d9c81e-d46a-4b82-923c-389cae653bdd', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 102, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\\n0123456\\n- 1 0123\\nLeast Squares\\n0\\nRidge\\nLasso\\nBest Subset\\nPLSPCR\\n\\x81\\n0123456\\n- 1 0123\\nLeast Squares\\nRidge\\nBest Subset\\nPLS\\nPCR\\nLasso\\n\\x81\\n0\\nρ=0 .5\\nρ=−0.5\\nβ1\\nβ1\\nβ2 β2\\nFIGURE 3.18.Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\\nt w oi n p u t sw i t hc o r r e l a t i o n±0.5, and the true regression coeﬃcientsβ=( 4,2).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='20e22b63-2614-4157-bfe0-b2253b070f90', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 103, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='84 3. Linear Methods for Regression\\n3.7 Multiple Outcome Shrinkage and Selection\\nAs noted in Section 3.2.4, the least squares estimates in a multiple-output\\nlinear model are simply the individual least squares estimates for each of\\nthe outputs.\\nTo apply selection and shrinkage methods in the multiple output case,\\none could apply a univariate technique individually to each outcome or si-\\nmultaneously to all outcomes. With ridge regression, for example, we could\\napply formula (3.44) to each of theK columns of the outcome matrixY,\\nusing possibly diﬀerent parametersλ, or apply it to all columns using the\\nsame value of λ. The former strategy would allow diﬀerent amounts of\\nregularization to be applied to diﬀerent outcomes but require estimation\\nof k separate regularization parametersλ1,...,λ k, while the latter would\\npermit all k outputs to be used in estimating the sole regularization pa-\\nrameter λ.\\nOther more sophisticated shrinkage and selection strategies that exploit\\ncorrelations in the diﬀerent responses can be helpful in the multiple output\\ncase. Suppose for example that among the outputs we have\\nYk = f(X)+ εk (3.65)\\nYℓ = f(X)+ εℓ; (3.66)\\ni.e., (3.65) and (3.66) share the same structural partf(X) in their models.\\nIt is clear in this case that we should pool our observations onYk and Yl\\nto estimate the commonf.\\nCombining responses is at the heart of canonical correlation analysis\\n(CCA), a data reduction technique developed for the multiple output case.\\nSimilar to PCA, CCA ﬁnds a sequence of uncorrelated linear combina-\\ntions Xvm,m =1 ,...,M of the xj, and a corresponding sequence of\\nuncorrelated linear combinationsYum of the responsesyk, such that the\\ncorrelations\\nCorr2(Yum,Xvm) (3.67)\\nare successively maximized. Note that at mostM =m i n (K,p) directions\\ncan be found. The leading canonical response variates are those linear com-\\nbinations (derived responses) best predicted by the xj; in contrast, the\\ntrailing canonical variates can be poorly predicted by thexj, and are can-\\ndidates for being dropped. The CCA solution is computed using a general-\\nized SVD of the sample cross-covariance matrixYTX/N (assuming Y and\\nX are centered; Exercise 3.20).\\nReduced-rank regression(Izenman, 1975; van der Merwe and Zidek, 1980)\\nformalizes this approach in terms of a regression model that explicitly pools\\ninformation. Given an error covariance Cov(ε)= Σ, we solve the following', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='423ed70d-d38f-4e74-9976-0e74088c592a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 104, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.7 Multiple Outcome Shrinkage and Selection 85\\nrestricted multivariate regression problem:\\nˆBrr(m) = argmin\\nrank(B)=m\\nN∑\\ni=1\\n(yi −BTxi)TΣ−1(yi −BTxi). (3.68)\\nWith Σreplaced by the estimateYTY/N, one can show (Exercise 3.21)\\nthat the solution is given by a CCA ofY and X:\\nˆBrr(m)= ˆBUmU−\\nm, (3.69)\\nwhere Um is theK×m sub-matrix ofU consisting of the ﬁrstm columns,\\nand U is theK ×M matrix ofleft canonical vectorsu1,u2,...,u M. U−\\nm\\nis its generalized inverse. Writing the solution as\\nˆBrr(M)=( XTX)−1XT(YUm)U−\\nm, (3.70)\\nwe see that reduced-rank regression performs a linear regression on the\\npooled response matrixYUm, and then maps the coeﬃcients (and hence\\nthe ﬁts as well) back to the original response space. The reduced-rank ﬁts\\nare given by\\nˆYrr(m)= X(XTX)−1XTYUmU−\\nm\\n= HYPm, (3.71)\\nwhere H is the usual linear regression projection operator, andPm is the\\nrank-m CCA response projection operator. Although a better estimate of\\nΣwould be (Y−XˆB)T(Y−XˆB)/(N−pK), one can show that the solution\\nremains the same (Exercise 3.22).\\nReduced-rank regression borrows strength among responses by truncat-\\ning the CCA. Breiman and Friedman (1997) explored with some success\\nshrinkage of the canonical variates betweenX and Y,as m o o t hv e r s i o no f\\nreduced rankregression. Their proposal has the form (compare (3.69))\\nˆBc+w = ˆBUΛU−1, (3.72)\\nwhere Λ is a diagonal shrinkage matrix (the “c+w” stands for “Curds\\nand Whey,” the name they gave to their procedure). Based on optimal\\nprediction in the population setting, they show thatΛhas diagonal entries\\nλm = c2\\nm\\nc2m + p\\nN (1−c2m),m =1 ,...,M, (3.73)\\nwhere cm is themth canonical correlation coeﬃcient. Note that as the ratio\\nof the number of input variables to sample sizep/N gets small, the shrink-\\nage factors approach 1. Breiman and Friedman (1997) proposed modiﬁed\\nversions ofΛbased on training data and cross-validation, but the general\\nform is the same. Here the ﬁtted response has the form\\nˆYc+w = HYSc+w, (3.74)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6dfdbda9-c992-4c1d-9478-246fdc9f8026', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 105, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='86 3. Linear Methods for Regression\\nwhere Sc+w = UΛU−1 is the response shrinkage operator.\\nBreiman and Friedman (1997) also suggested shrinking in both theY\\nspace andX space. This leads to hybrid shrinkage models of the form\\nˆYridge,c+w = AλYSc+w, (3.75)\\nwhere Aλ= X(XTX+λI)−1XT is the ridge regression shrinkage operator,\\nas in (3.46) on page 66. Their paper and the discussions thereof contain\\nmany more details.\\n3.8 More on the Lasso and Related Path\\nAlgorithms\\nSince the publication of the LAR algorithm (Efron et al., 2004) there has\\nbeen a lot of activity in developing algorithms for ﬁtting regularization\\npaths for a variety of diﬀerent problems. In addition,L1 regularization has\\ntaken on a life of its own, leading to the development of the ﬁeldcompressed\\nsensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).\\nIn this section we discuss some related proposals and other path algorithms,\\nstarting oﬀ with a precursor to the LAR algorithm.\\n3.8.1 Incremental Forward Stagewise Regression\\nHere we present another LAR-like algorithm, this time focused on forward\\nstagewise regression. Interestingly, eﬀorts to understand a ﬂexible nonlinear\\nregression procedure (boosting) led to a new algorithm for linear models\\n(LAR). In reading the ﬁrst edition of this book and the forward stagewise\\nAlgorithm 3.4Incremental Forward Stagewise Regression—FSϵ.\\n1. Start with the residualr equal to y and β1,β2,...,β p =0 .A l lt h e\\npredictors are standardized to have mean zero and unit norm.\\n2. Find the predictorxj most correlated withr\\n3. Update βj ←βj +δj,w h e r eδj = ϵ·sign[⟨xj,r⟩]a n dϵ> 0i sas m a l l\\nstep size, and setr←r−δjxj.\\n4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated\\nwith all the predictors.\\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\\n4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4756ae23-18bd-435b-904e-0f4126cc5aab', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 106, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 More on the Lasso and Related Path Algorithms 87\\n−0.2 0.0 0.2 0.4 0.6\\nlcavol\\nlweight\\nage\\nlbph\\nsvi\\nlcp\\ngleason\\npgg45\\n0 50 100 150 200\\n−0.2 0.0 0.2 0.4 0.6\\nlcavol\\nlweight\\nage\\nlbph\\nsvi\\nlcp\\ngleason\\npgg45\\n0.0 0.5 1.0 1.5 2.0\\nFSϵ FS0\\nIteration\\nCoeﬃcients\\nCoeﬃcients\\nL1 Arc-length of Coeﬃcients\\nFIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\\nincremental forward stagewise regression with step sizeϵ =0 .01. The right panel\\nshows the inﬁnitesimal version FS0 obtained lettingϵ → 0. This proﬁle was ﬁt by\\nthe modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example the FS0 proﬁles\\nare monotone, and hence identical to those of lasso and LAR.\\nlinearmodels,onecouldexplicitlyconstructthepiecewise-linearlassopaths\\nof Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,\\nas well as the incremental version of forward-stagewise regression presented\\nhere.\\nConsider the linear-regression version of the forward-stagewise boosting\\nalgorithm16.1proposedinSection16.1(page608).Itgeneratesacoeﬃcient\\nproﬁle by repeatedly updating (by a small amountϵ) the coeﬃcient of the\\nvariable most correlated with the current residuals. Algorithm 3.4 gives\\nthe details. Figure 3.19 (left panel) shows the progress of the algorithm on\\nthe prostate data with step sizeϵ =0 .01. Ifδj =⟨xj,r⟩ (the least-squares\\ncoeﬃcient of the residual onjth predictor), then this is exactly the usual\\nforward stagewise procedure (FS) outlined in Section 3.3.3.\\nHere we are mainly interested in small values ofϵ. Letting ϵ →0g i v e s\\nthe right panel of Figure 3.19, which in this case is identical to the lasso\\npath in Figure 3.10. We call this limiting procedureinﬁnitesimal forward\\nstagewise regression or FS0. This procedure plays an important role in\\nnon-linear, adaptive methods like boosting (Chapters 10 and 16) and is the\\nversion of incremental forward stagewise regression that is most amenable\\nto theoretical analysis. B¨uhlmann and Hothorn (2007) refer to the same\\nprocedure as “L2boost”, because of its connections to boosting.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='793236b7-4400-4231-ad63-d099bf71ef11', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 107, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='88 3. Linear Methods for Regression\\nEfron originally thought that the LAR Algorithm 3.2 was an implemen-\\ntation of FS0, allowing each tied predictor a chance to update their coeﬃ-\\ncients in a balanced way, while remaining tied in correlation. However, he\\nthen realized that the LAR least-squares ﬁt amongst the tied predictors\\ncan result in coeﬃcients moving in theopposite direction to their correla-\\ntion, which cannot happen in Algorithm 3.4. The following modiﬁcation of\\nthe LAR algorithm implements FS0:\\nAlgorithm 3.2bLeast Angle Regression: FS0 Modiﬁcation.\\n4. Find the new direction by solving the constrained least squares prob-\\nlem\\nmin\\nb\\n||r−XAb||2\\n2 subject tobjsj ≥0,j ∈A,\\nwhere sj is the sign of⟨xj,r⟩.\\nThe modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\\nsigns of the coeﬃcients the same as those of the correlations. One can show\\nthat this achieves the optimal balancing of inﬁnitesimal “update turns”\\nfor the variables tied for maximal correlation (Hastie et al., 2007). Like\\nlasso, the entire FS0 path can be computed very eﬃciently via the LAR\\nalgorithm.\\nAs a consequence of these results, if the LAR proﬁles are monotone non-\\nincreasing or non-decreasing, as they are in Figure 3.19, then all three\\nmethods—LAR, lasso, and FS0—give identical proﬁles. If the proﬁles are\\nnot monotone but do not cross the zero axis, then LAR and lasso are\\nidentical.\\nSince FS0 is diﬀerent from the lasso, it is natural to ask if it optimizes\\na criterion. The answer is more complex than for lasso; the FS0 coeﬃcient\\nproﬁle is the solution to a diﬀerential equation. While the lasso makes op-\\ntimal progress in terms of reducing the residual sum-of-squares per unit\\nincrease in L1-norm of the coeﬃcient vectorβ,F S0 is optimal per unit\\nincrease inL1 arc-length traveled along the coeﬃcient path. Hence its co-\\neﬃcient path is discouraged from changing directions too often.\\nFS0 is more constrained than lasso, and in fact can be viewed as a mono-\\ntone version of the lasso; see Figure 16.3 on page 614 for a dramatic exam-\\nple. FS0 may be useful inp ≫ N situations, where its coeﬃcient proﬁles\\nare much smoother and hence have less variance than those of lasso. More\\ndetails on FS0 are given in Section 16.2.3 and Hastie et al. (2007). Fig-\\nure 3.16 includes FS0 where its performance is very similar to that of the\\nlasso.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ae1fa7b3-1652-49a9-9926-887b1bc730ec', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 108, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 More on the Lasso and Related Path Algorithms 89\\n3.8.2 Piecewise-Linear Path Algorithms\\nThe least angle regression procedure exploits the piecewise linear nature of\\nthe lasso solution paths. It has led to similar “path algorithms” for other\\nregularized problems. Suppose we solve\\nˆβ(λ) = argminβ[R(β)+ λJ(β)], (3.76)\\nwith\\nR(β)=\\nN∑\\ni=1\\nL(yi,β0 +\\np∑\\nj=1\\nxijβj), (3.77)\\nwhere both the loss function L and the penalty function J are convex.\\nThen the following are suﬃcient conditions for the solution pathˆβ(λ)t o\\nbe piecewise linear (Rosset and Zhu, 2007):\\n1. R is quadratic or piecewise-quadratic as a function ofβ,a n d\\n2. J is piecewise linear inβ.\\nThis also implies (in principle) that the solution path can be eﬃciently\\ncomputed. Examples include squared- and absolute-error loss, “Huberized”\\nlosses, and theL1,L∞ penalties onβ. Another example is the “hinge loss”\\nfunction used in the support vector machine. There the loss is piecewise\\nlinear, and the penalty is quadratic. Interestingly, this leads to a piecewise-\\nlinear path algorithm in the dual space; more details are given in Sec-\\ntion 12.3.5.\\n3.8.3 The Dantzig Selector\\nCandes and Tao (2007) proposed the following criterion:\\nminβ||β||1 subject to||XT(y−Xβ)||∞ ≤s. (3.78)\\nThey call the solution theDantzig selector(DS). It can be written equiva-\\nlently as\\nminβ||XT(y−Xβ)||∞ subject to||β||1 ≤t. (3.79)\\nHere | |·| |∞ denotes the L∞ norm, the maximum absolute value of the\\ncomponents of the vector. In this form it resembles the lasso, replacing\\nsquared error loss by the maximum absolute value of its gradient. Note\\nthat as t gets large, both procedures yield the least squares solution if\\nN<p .I fp≥N, they both yield the least squares solution with minimum\\nL1 norm. However for smaller values oft, the DS procedure produces a\\ndiﬀerent path of solutions than the lasso.\\nCandes and Tao (2007) show that the solution to DS is a linear pro-\\ngramming problem; hence the name Dantzig selector, in honor of the late', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd88465d-4de9-4af4-94ca-6fabdd6be100', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 109, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='90 3. Linear Methods for Regression\\nGeorge Dantzig, the inventor of the simplex method for linear program-\\nming. They also prove a number of interesting mathematical properties for\\nthe method, related to its ability to recover an underlying sparse coeﬃ-\\ncient vector. These same properties also hold for the lasso, as shown later\\nby Bickel et al. (2008).\\nUnfortunately the operating properties of the DS method are somewhat\\nunsatisfactory. The method seems similar in spirit to the lasso, especially\\nwhen we look at the lasso’s stationary conditions (3.58). Like the LAR al-\\ngorithm, the lasso maintains the same inner product (and correlation) with\\nthe current residual for all variables in the active set, and moves their co-\\neﬃcients to optimally decrease the residual sum of squares. In the process,\\nthis common correlation is decreased monotonically (Exercise 3.23), and at\\nall times this correlation is larger than that for non-active variables. The\\nDantzig selector instead tries to minimize the maximum inner product of\\nthe current residual with all the predictors. Hence it can achieve a smaller\\nmaximum than the lasso, but in the process a curious phenomenon can\\noccur. If the size of the active set ism, there will bem variables tied with\\nmaximum correlation. However, these need not coincide with the active set!\\nHence it can include a variable in the model that has smaller correlation\\nwith the current residual than some of the excluded variables (Efron et\\nal., 2007). This seems unreasonable and may be responsible for its some-\\ntimes inferior prediction accuracy. Efron et al. (2007) also show that DS\\ncan yield extremely erratic coeﬃcient paths as the regularization parameter\\ns is varied.\\n3.8.4 The Grouped Lasso\\nIn some problems, the predictors belong to pre-deﬁned groups; for example\\ngenes that belong to the same biological pathway, or collections of indicator\\n(dummy) variables for representing the levels of a categorical predictor. In\\nthis situation it may be desirable to shrink and select the members of a\\ngroup together. Thegrouped lassois one way to achieve this. Suppose that\\nthe p predictors are divided intoL groups, with pℓ the number in group\\nℓ. For ease of notation, we use a matrixXℓ to represent the predictors\\ncorresponding to theℓth group, with corresponding coeﬃcient vectorβℓ.\\nThe grouped-lasso minimizes the convex criterion\\nmin\\nβ∈IRp\\n⎤\\n||y−β01−\\nL∑\\nℓ=1\\nXℓβℓ||2\\n2 +λ\\nL∑\\nℓ=1\\n√pℓ||βℓ||2\\n⎦\\n, (3.80)\\nwhere the √pℓ terms accounts for the varying group sizes, and| |·| |2 is\\nthe Euclidean norm (not squared). Since the Euclidean norm of a vector\\nβℓ is zero only if all of its components are zero, this procedure encourages\\nsparsity at both the group and individual levels. That is, for some values of\\nλ, an entire group of predictors may drop out of the model. This procedure', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8597d89-02f4-42bf-acd8-81d7a74b71d7', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 110, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.8 More on the Lasso and Related Path Algorithms 91\\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and studied and\\ngeneralized by Yuan and Lin (2007). Generalizations include more general\\nL2 norms ||η||K =( ηTKη)1/2, as well as allowing overlapping groups of\\npredictors (Zhao et al., 2008). There are also connections to methods for\\nﬁtting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,\\n2008).\\n3.8.5 Further Properties of the Lasso\\nA number of authors have studied the ability of the lasso and related pro-\\ncedures to recover the correct model, asN and p grow. Examples of this\\nwork include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp\\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and B¨uhlmann\\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea\\net al. (2007). For example Donoho (2006b) focuses on thep>N case and\\nconsiders the lasso solution as the boundt gets large. In the limit this gives\\nthe solution with minimumL1 norm among all models with zero training\\nerror. He shows that under certain assumptions on the model matrixX,i f\\nthe true model is sparse, this solution identiﬁes the correct predictors with\\nhigh probability.\\nMany of the results in this area assume a condition on the model matrix\\nof the form\\nmax\\nj∈Sc\\n||xT\\nj XS(XS\\nTXS)−1||1 ≤(1−ϵ)f o rs o m eϵ∈(0,1]. (3.81)\\nHere S indexes the subset of features with non-zero coeﬃcients in the true\\nunderlying model, and XS are the columns ofX corresponding to those\\nfeatures. SimilarlySc are the features with true coeﬃcients equal to zero,\\nand XSc the corresponding columns. This says that the least squares coef-\\nﬁcients for the columns ofXSc on XS are not too large, that is, the “good”\\nvariablesS are not too highly correlated with the nuisance variablesSc.\\nRegarding the coeﬃcients themselves, the lasso shrinkage causes the esti-\\nmates of the non-zero coeﬃcients to be biased towards zero, and in general\\nthey are not consistent5. One approach for reducing this bias is to run\\nthe lasso to identify the set of non-zero coeﬃcients, and then ﬁt an un-\\nrestricted linear model to the selected set of features. This is not always\\nfeasible, if the selected set is large. Alternatively, one can use the lasso to\\nselect the set of non-zero predictors, and then apply the lasso again, but\\nusing only the selected predictors from the ﬁrst step. This is known as the\\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\\nestimate the initial penalty parameter for the lasso, and then again for a\\nsecond penalty parameter applied to the selected set of predictors. Since\\n5Statistical consistency means as the sample size grows, the estimates converge to\\nthe true values.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66b1a669-4746-446a-81eb-2792489b2bcb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 111, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='92 3. Linear Methods for Regression\\nthe variables in the second step have less “competition” from noise vari-\\nables, cross-validation will tend to pick a smaller value forλ, and hence\\ntheir coeﬃcients will be shrunken less than those in the initial estimate.\\nAlternatively, one can modify the lasso penalty function so that larger co-\\neﬃcients are shrunken less severely; thesmoothly clipped absolute deviation\\n(SCAD) penalty of Fan and Li (2005) replacesλ|β| by Ja(β,λ), where\\ndJa(β,λ)\\ndβ = λ·sign(β)\\n[\\nI(|β|≤λ)+ (aλ−|β|)+\\n(a−1)λ I(|β| >λ)\\n]\\n(3.82)\\nfor some a ≥2. The second term in square-braces reduces the amount of\\nshrinkage in the lasso for larger values ofβ, with ultimately no shrinkage\\nas a→∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\\n−4 −2 0 2 4\\n012345\\n−4 −2 0 2 4\\n0.0 0.5 1.0 1.5 2.0 2.5\\n−4 −2 0 2 4\\n0.5 1.0 1.5 2.0\\n|β| SCAD |β|1−ν\\nβββ\\nFIGURE 3.20.The lasso and two alternative non-convex penalties designed to\\npenalize large coeﬃcients less. For SCAD we useλ=1 and a =4 ,a n dν= 1\\n2 in\\nthe last panel.\\n|β|1−ν. However this criterion is non-convex, which is a drawback since it\\nmakes the computation much more diﬃcult. Theadaptive lasso(Zou, 2006)\\nuses a weighted penalty of the form∑p\\nj=1 wj|βj| where wj =1 /|ˆβj|ν, ˆβj is\\nthe ordinary least squares estimate andν>0. This is a practical approxi-\\nmation to the|β|q penalties (q =1 −νhere) discussed in Section 3.4.3. The\\nadaptive lasso yields consistent estimates of the parameters while retaining\\nthe attractive convexity property of the lasso.\\n3.8.6 Pathwise Coordinate Optimization\\nAn alternate approach to the LARS algorithm for computing the lasso\\nsolution is simple coordinate descent. This idea was proposed by Fu (1998)\\nandDaubechiesetal.(2004),andlaterstudiedandgeneralizedbyFriedman\\net al. (2007), Wu and Lange (2008) and others. The idea is to ﬁx the penalty\\nparameter λin the Lagrangian form (3.52) and optimize successively over\\neach parameter, holding the other parameters ﬁxed at their current values.\\nSuppose the predictors are all standardized to have mean zero and unit\\nnorm. Denote by ˜βk(λ) the current estimate forβk at penalty parameter', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0bc3c0e3-d090-4751-838f-8b0e023fd0e6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 112, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.9 Computational Considerations 93\\nλ. We can rearrange (3.52) to isolateβj,\\nR(˜β(λ),βj)= 1\\n2\\nN∑\\ni=1\\n⎤\\nyi −\\n∑\\nk̸=j\\nxik ˜βk(λ)−xijβj\\n⎦2\\n+λ\\n∑\\nk̸=j\\n|˜βk(λ)|+λ|βj|,\\n(3.83)\\nwhere we have suppressed the intercept and introduced a factor1\\n2 for con-\\nvenience. This can be viewed as a univariate lasso problem with response\\nvariable the partial residualyi −˜y(j)\\ni = yi −∑\\nk̸=j xik ˜βk(λ). This has an\\nexplicit solution, resulting in the update\\n˜βj(λ)←S\\n⎤N∑\\ni=1\\nxij(yi −˜y(j)\\ni ),λ\\n⎦\\n. (3.84)\\nHereS(t,λ)=s i g n (t)(|t|−λ)+ is the soft-thresholding operator in Table 3.4\\non page 71. The ﬁrst argument toS(·) is the simple least-squares coeﬃcient\\nof the partial residual on the standardized variablexij. Repeated iteration\\nof (3.84)—cycling through each variable in turn until convergence—yields\\nthe lasso estimateˆβ(λ).\\nWe can also use this simple algorithm to eﬃciently compute the lasso\\nsolutions at a grid of values ofλ. We start with the smallest valueλmax\\nfor which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\\nuntil convergence. Thenλis decreased again and the process is repeated,\\nusing the previous solution as a “warm start” for the new value ofλ.T h i s\\ncan be faster than the LARS algorithm, especially in large problems. A\\nkey to its speed is the fact that the quantities in (3.84) can be updated\\nquickly asj varies, and often the update is to leave˜βj = 0. On the other\\nhand, it delivers solutions over a grid ofλvalues, rather than the entire\\nsolution path. The same kind of algorithm can be applied to the elastic\\nnet, the grouped lasso and many other models in which the penalty is a\\nsum of functions of the individual parameters (Friedman et al., 2010). It\\ncan also be applied, with some substantial modiﬁcations, to the fused lasso\\n(Section 18.4.2); details are in Friedman et al. (2007).\\n3.9 Computational Considerations\\nLeast squares ﬁtting is usually done via the Cholesky decomposition of\\nthe matrixXTX or a QR decomposition ofX.W i t hN observations andp\\nfeatures, the Cholesky decomposition requiresp3+Np2/2 operations, while\\nthe QR decomposition requiresNp2 operations. Depending on the relative\\nsize ofN and p, the Cholesky can sometimes be faster; on the other hand,\\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\\nof the lasso via the LAR algorithm has the same order of computation as\\na least squares ﬁt.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='baf28639-cb02-438b-8df4-11899da3d40e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 113, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='94 3. Linear Methods for Regression\\nBibliographic Notes\\nLinear regression is discussed in many statistics books, for example, Seber\\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression was\\nintroduced by Hoerl and Kennard (1970), while the lasso was proposed by\\nTibshirani (1996). Around the same time, lasso-type penalties were pro-\\nposed in thebasis pursuitmethod for signal processing (Chen et al., 1998).\\nThe least angle regression procedure was proposed in Efron et al. (2004);\\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\\nand Osborne et al. (2000b). Their algorithm also exploits the piecewise\\nlinearity used in the LAR/lasso algorithm, but lacks its transparency. The\\ncriterion for the forward stagewise criterion is discussed in Hastie et al.\\n(2007). Park and Hastie (2007) develop a path algorithm similar to least\\nangle regression for generalized regression models. Partial least squares\\nwas introduced by Wold (1975). Comparisons of shrinkage methods may\\nbe found in Copas (1983) and Frank and Friedman (1993).\\nExercises\\nEx. 3.1 Show that theF statistic (3.13) for dropping a single coeﬃcient\\nfrom a model is equal to the square of the correspondingz-score (3.12).\\nEx. 3.2 G i v e nd a t ao nt w ov a r i a b l e sX and Y, consider ﬁtting a cubic\\npolynomial regression modelf(X)= ∑3\\nj=0 βjXj. In addition to plotting\\nthe ﬁtted curve, you would like a 95% conﬁdence band about the curve.\\nConsider the following two approaches:\\n1. At each pointx0, form a 95% conﬁdence interval for the linear func-\\ntion aTβ=∑3\\nj=0 βjxj\\n0.\\n2. Form a 95% conﬁdence set forβas in (3.15), which in turn generates\\nconﬁdence intervals forf(x0).\\nHow do these approaches diﬀer? Which band is likely to be wider? Conduct\\na small simulation experiment to compare the two methods.\\nEx. 3.3Gauss–Markov theorem:\\n(a) Prove the Gauss–Markov theorem: the least squares estimate of a\\nparameter aTβhas variance no bigger than that of any other linear\\nunbiased estimate ofaTβ(Section 3.2.2).\\n(b) The matrix inequalityB⪯ A holds ifA−B is positive semideﬁnite.\\nShow that ifˆV is the variance-covariance matrix of the least squares\\nestimate of βand ˜V is the variance-covariance matrix of any other\\nlinear unbiased estimate, thenˆV⪯ ˜V.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='74e37daf-479d-4a03-a697-4ef5c67856b0', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 114, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 95\\nEx. 3.4Show how the vector of least squares coeﬃcients can be obtained\\nfrom a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\\nresent your solution in terms of the QR decomposition ofX.\\nEx. 3.5Consider the ridge regression problem (3.41). Show that this prob-\\nlem is equivalent to the problem\\nˆβc =a r g m i n\\nβc\\n{ N∑\\ni=1\\n[\\nyi −βc\\n0 −\\np∑\\nj=1\\n(xij −¯xj)βc\\nj\\n]2\\n+λ\\np∑\\nj=1\\nβc\\nj\\n2\\n}\\n. (3.85)\\nGive the correspondence betweenβc and the original βin (3.41). Char-\\nacterize the solution to this modiﬁed criterion. Show that a similar result\\nholds for the lasso.\\nEx. 3.6 Show that the ridge regression estimate is the mean (and mode)\\nof the posterior distribution, under a Gaussian priorβ∼N(0,τI), and\\nGaussian sampling modely ∼N(Xβ,σ2I). Find the relationship between\\nthe regularization parameterλin the ridge formula, and the variancesτ\\nand σ2.\\nEx. 3.7Assume yi ∼N(β0 +xT\\ni β,σ2),i =1 ,2,...,N , and the parameters\\nβj,j =1 ,...,p are each distributed as N(0,τ2), independently of one\\nanother. Assuming σ2 and τ2 are known, and β0 is not governed by a\\nprior (or has a ﬂat improper prior), show that the (minus) log-posterior\\ndensity of βis proportional to ∑N\\ni=1(yi −β0 −∑\\nj xijβj)2 + λ∑p\\nj=1 β2\\nj\\nwhere λ= σ2/τ2.\\nEx. 3.8 Consider the QR decomposition of the uncentered N × (p +1 )\\nmatrix X (whose ﬁrst column is all ones), and the SVD of the N × p\\ncentered matrix ˜X. Show thatQ2 and U span the same subspace, where\\nQ2 is the sub-matrix of Q with the ﬁrst column removed. Under what\\ncircumstances will they be the same, up to sign ﬂips?\\nEx. 3.9Forward stepwise regression.Suppose we have the QR decomposi-\\ntion for theN×q matrix X1 in a multiple regression problem with response\\ny, and we have an additionalp−q predictors in the matrixX2.D e n o t et h e\\ncurrent residual byr. We wish to establish which one of these additional\\nvariables will reduce the residual-sum-of squares the most when included\\nwith those inX1. Describe an eﬃcient procedure for doing this.\\nEx. 3.10 Backward stepwise regression.Suppose we have the multiple re-\\ngression ﬁt ofy on X, along with the standard errors and Z-scores as in\\nTable 3.2. We wish to establish which variable, when dropped, will increase\\nthe residual sum-of-squares the least. How would you do this?\\nEx. 3.11Show that the solution to the multivariate linear regression prob-\\nlem (3.40) is given by (3.39). What happens if the covariance matricesΣi\\nare diﬀerent for each observation?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85a26f02-f559-45ac-a6dd-7006b91f9cca', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 115, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='96 3. Linear Methods for Regression\\nEx. 3.12 Show that the ridge regression estimates can be obtained by\\nordinary least squares regression on an augmented data set. We augment\\nthe centered matrixX with p additional rows\\n√\\nλI, and augmenty with p\\nzeros. By introducing artiﬁcial data having response value zero, the ﬁtting\\nprocedure is forced to shrink the coeﬃcients toward zero. This is related to\\nthe idea ofhints due to Abu-Mostafa (1995), where model constraints are\\nimplemented by adding artiﬁcial data examples that satisfy them.\\nEx. 3.13Derive the expression (3.62), and show thatˆβpcr(p)= ˆβls.\\nEx. 3.14 Show that in the orthogonal case, PLS stops afterm = 1 steps,\\nbecause subsequent ˆϕmj in step 2 in Algorithm 3.3 are zero.\\nEx. 3.15 Verify expression (3.64), and hence show that the partial least\\nsquares directions are a compromise between the ordinary regression coef-\\nﬁcient and the principal component directions.\\nEx. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators\\nin the orthogonal case.\\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\\nChapter 1.\\nEx. 3.18Read about conjugate gradient algorithms (Murray et al., 1981, for\\nexample), and establish a connection between these algorithms and partial\\nleast squares.\\nEx. 3.19Show that∥ˆβridge∥ increases as its tuning parameterλ→0. Does\\nthe same property hold for the lasso and partial least squares estimates?\\nFor the latter, consider the “tuning parameter” to be the successive steps\\nin the algorithm.\\nEx. 3.20Consider the canonical-correlation problem (3.67). Show that the\\nleading pair of canonical variatesu1 and v1 solve the problem\\nmax\\nuT (YT Y)u=1\\nvT (XT X)v=1\\nuT(YTX)v, (3.86)\\na generalized SVD problem. Show that the solution is given by u1 =\\n(YTY)−1\\n2 u∗\\n1,a n dv1 =( XTX)−1\\n2 v∗\\n1,w h e r eu∗\\n1 and v∗\\n1 are the leading left\\nand right singular vectors in\\n(YTY)−1\\n2 (YTX)(XTX)−1\\n2 = U∗D∗V∗T. (3.87)\\nShow that the entire sequenceum,v m,m =1 ,..., min(K,p) is also given\\nby (3.87).\\nEx. 3.21 Show that the solution to the reduced-rank regression problem\\n(3.68), withΣestimated byYTY/N, is given by (3.69).Hint: Transform', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1f1d7a84-1aee-4163-81d2-de6729a398ed', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 116, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 97\\nY to Y∗= YΣ−1\\n2 , and solved in terms of the canonical vectorsu∗\\nm. Show\\nthat Um = Σ−1\\n2 U∗\\nm, and a generalized inverse isU−\\nm = U∗\\nm\\nTΣ\\n1\\n2 .\\nEx. 3.22 Show that the solution in Exercise 3.21 does not change ifΣis\\nestimated by the more natural quantity (Y−XˆB)T(Y−XˆB)/(N −pK).\\nEx. 3.23Consider a regression problem with all variables and response hav-\\ning mean zero and standard deviation one. Suppose also that each variable\\nhas identical absolute correlation with the response:\\n1\\nN|⟨xj,y⟩| = λ, j=1 ,...,p.\\nLet ˆβbe the least-squares coeﬃcient ofy on X,a n dl e tu(α)= αXˆβfor\\nα∈[0,1] be the vector that moves a fractionαtoward the least squares ﬁt\\nu.L e tRSS be the residual sum-of-squares from the full least squares ﬁt.\\n(a) Show that\\n1\\nN|⟨xj,y−u(α)⟩| =( 1−α)λ, j=1 ,...,p,\\nand hence the correlations of eachxj with the residuals remain equal\\nin magnitude as we progress towardu.\\n(b) Show that these correlations are all equal to\\nλ(α)= (1−α)√\\n(1−α)2 + α(2−α)\\nN ·RSS\\n·λ,\\nand hence they decrease monotonically to zero.\\n(c) Use these results to show that the LAR algorithm in Section 3.4.4\\nkeeps the correlations tied and monotonically decreasing, as claimed\\nin (3.55).\\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\\npage 74, show that the LAR direction makes an equal angle with each of\\nthe predictors inAk.\\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2).Starting at the be-\\nginning of thekth step of the LAR algorithm, derive expressions to identify\\nthe next variable to enter the active set at stepk+1, and the value ofαat\\nwhich this occurs (using the notation around equation (3.55) on page 74).\\nEx. 3.26Forward stepwise regression enters the variable at each step that\\nmost reduces the residual sum-of-squares. LAR adjusts variables that have\\nthe most (absolute) correlation with the current residuals. Show that these\\ntwo entry criteria are not necessarily the same. [Hint: letxj.A be the jth', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e3a633a-2f4c-4f93-8fb0-ec602b690d99', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 117, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='98 3. Linear Methods for Regression\\nvariable, linearly adjusted for all the variables currently in the model. Show\\nthat the ﬁrst criterion amounts to identifying thej for which Cor(xj.A,r)\\nis largest in magnitude.\\nEx. 3.27Lasso and LAR: Consider the lasso problem in Lagrange multiplier\\nform: withL(β)= 1\\n2\\n∑\\ni(yi −∑\\nj xijβj)2, we minimize\\nL(β)+ λ\\n∑\\nj\\n|βj| (3.88)\\nfor ﬁxedλ>0.\\n(a) Setting βj = β+\\nj −β−\\nj with β+\\nj ,β−\\nj ≥0, expression (3.88) becomes\\nL(β)+ λ∑\\nj(β+\\nj +β−\\nj ). Show that the Lagrange dual function is\\nL(β)+ λ\\n∑\\nj\\n(β+\\nj +β−\\nj )−\\n∑\\nj\\nλ+\\nj β+\\nj −\\n∑\\nj\\nλ−\\nj β−\\nj (3.89)\\nand the Karush–Kuhn–Tucker optimality conditions are\\n∇L(β)j +λ−λ+\\nj =0\\n−∇L(β)j +λ−λ−\\nj =0\\nλ+\\nj β+\\nj =0\\nλ−\\nj β−\\nj =0 ,\\nalong with the non-negativity constraints on the parameters and all\\nthe Lagrange multipliers.\\n(b) Show that|∇L(β)j|≤λ∀j, and that the KKT conditions imply one\\nof the following three scenarios:\\nλ=0 ⇒∇ L(β)j =0 ∀j\\nβ+\\nj > 0,λ> 0 ⇒ λ+\\nj =0 , ∇L(β)j =−λ<0,β−\\nj =0\\nβ−\\nj > 0,λ> 0 ⇒ λ−\\nj =0 , ∇L(β)j = λ>0,β+\\nj =0 .\\nHence show that for any “active” predictor havingβj ̸=0 ,w em u s t\\nhave ∇L(β)j = −λif βj > 0, and∇L(β)j = λif βj < 0. Assuming\\nthe predictors are standardized, relateλto the correlation between\\nthe jth predictor and the current residuals.\\n(c) Suppose that the set of active predictors is unchanged forλ0 ≥λ≥λ1.\\nShow that there is a vectorγ0 such that\\nˆβ(λ)= ˆβ(λ0)−(λ−λ0)γ0 (3.90)\\nThus the lasso solution path is linear asλranges fromλ0 to λ1(Efron\\net al., 2004; Rosset and Zhu, 2007).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0acfa003-61e8-4c95-8944-6053bbae7474', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 118, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 99\\nEx. 3.28 Suppose for a given t in (3.51), the ﬁtted lasso coeﬃcient for\\nvariable Xj is ˆβj = a. Suppose we augment our set of variables with an\\nidentical copyX∗\\nj = Xj. Characterize the eﬀect of this exact collinearity\\nby describing the set of solutions forˆβj and ˆβ∗\\nj , using the same value oft.\\nEx. 3.29 Suppose we run a ridge regression with parameterλon a single\\nvariable X,a n dg e tc o e ﬃ c i e n ta. We now include an exact copyX∗= X,\\nand reﬁt our ridge regression. Show that both coeﬃcients are identical, and\\nderive their value. Show in general that ifm copies of a variableXj are\\nincluded in a ridge regression, their coeﬃcients are all the same.\\nEx. 3.30Consider the elastic-net optimization problem:\\nmin\\nβ\\n||y−Xβ||2 +λ\\n[\\nα||β||2\\n2 +(1 −α)||β||1\\n]\\n. (3.91)\\nShow how one can turn this into a lasso problem, using an augmented\\nversion ofX and y.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='224c9b95-6f66-47ef-99b9-57b3704b2ff4', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 119, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4\\nLinear Methods for Classiﬁcation\\n4.1 Introduction\\nIn this chapter we revisit the classiﬁcation problem and focus on linear\\nmethods for classiﬁcation. Since our predictorG(x) takes values in a dis-\\ncrete setG, we can always divide the input space into a collection of regions\\nlabeled according to the classiﬁcation. We saw in Chapter 2 that the bound-\\naries of these regions can be rough or smooth, depending on the prediction\\nfunction. For an important class of procedures, thesedecision boundaries\\nare linear; this is what we will mean by linear methods for classiﬁcation.\\nThere are several diﬀerent ways in which linear decision boundaries can\\nbe found. In Chapter 2 we ﬁt linear regression models to the class indicator\\nvariables, and classify to the largest ﬁt. Suppose there areK classes, for\\nconvenience labeled 1,2,...,K , and the ﬁtted linear model for the kth\\nindicator response variable isˆfk(x)= ˆβk0 + ˆβT\\nk x. The decision boundary\\nbetween classk and ℓ is that set of points for whichˆfk(x)= ˆfℓ(x), that is,\\nthe set{x :( ˆβk0 −ˆβℓ0)+( ˆβk −ˆβℓ)Tx =0 },a na ﬃ n es e to rh y p e r p l a n e .1\\nSince the same is true for any pair of classes, the input space is divided\\ninto regions of constant classiﬁcation, with piecewise hyperplanar decision\\nboundaries. This regression approach is a member of a class of methods\\nthat modeldiscriminant functionsδk(x) for each class, and then classifyx\\nto the class with the largest value for its discriminant function. Methods\\n1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\\nnot. We sometimes ignore the distinction and refer in general to hyperplanes.\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 101\\nDOI: 10.1007/b94608_4,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4e07d255-b3c8-4d73-af83-0fb6d0e12e15', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 120, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='102 4. Linear Methods for Classiﬁcation\\nthat model the posterior probabilities Pr(G = k|X = x) are also in this\\nclass. Clearly, if either theδk(x) or Pr(G = k|X = x) are linear inx,t h e n\\nthe decision boundaries will be linear.\\nActually, all we require is that some monotone transformation ofδk or\\nPr(G = k|X = x) be linear for the decision boundaries to be linear. For\\nexample, if there are two classes, a popular model for the posterior proba-\\nbilities is\\nPr(G =1|X = x)= exp(β0 +βTx)\\n1+exp( β0 +βTx),\\nPr(G =2|X = x)= 1\\n1+exp( β0 +βTx).\\n(4.1)\\nHerethemonotonetransformationisthe logittransformation:log[ p/(1−p)],\\nand in fact we see that\\nlog Pr(G =1|X = x)\\nPr(G =2|X = x) = β0 +βTx. (4.2)\\nThe decision boundary is the set of points for which thelog-oddsare zero,\\nand this is a hyperplane deﬁned by\\n{\\nx|β0 +βTx =0\\n}\\n. We discuss two very\\npopular but diﬀerent methods that result in linear log-odds or logits: linear\\ndiscriminant analysis and linear logistic regression. Although they diﬀer in\\ntheir derivation, the essential diﬀerence between them is in the way the\\nlinear function is ﬁt to the training data.\\nA more direct approach is to explicitly model the boundaries between\\nthe classes as linear. For a two-class problem in a p-dimensional input\\nspace, this amounts to modeling the decision boundary as a hyperplane—in\\nother words, a normal vector and a cut-point. We will look at two methods\\nthat explicitly look for “separating hyperplanes.” The ﬁrst is the well-\\nknown perceptronmodel of Rosenblatt (1958), with an algorithm that ﬁnds\\na separating hyperplane in the training data, if one exists. The second\\nmethod, due to Vapnik (1996), ﬁnds anoptimally separating hyperplaneif\\none exists, else ﬁnds a hyperplane that minimizes some measure of overlap\\nin the training data. We treat the separable case here, and defer treatment\\nof the nonseparable case to Chapter 12.\\nWhile this entire chapter is devoted to linear decision boundaries, there is\\nconsiderable scope for generalization. For example, we can expand our vari-\\nableset X1,...,X p byincludingtheirsquaresandcross-products X2\\n1,X2\\n2,...,\\nX1X2,... , thereby addingp(p+1)/2 additional variables. Linear functions\\nin the augmented space map down to quadratic functions in the original\\nspace—hence linear decision boundaries to quadratic decision boundaries.\\nFigure 4.1 illustrates the idea. The data are the same: the left plot uses\\nlinear decision boundaries in the two-dimensional space shown, while the\\nright plot uses linear decision boundaries in the augmented ﬁve-dimensional\\nspace described above. This approach can be used with any basis transfor-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='662e4191-befa-48c9-a1ed-4a19ef5dd8f3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 121, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Linear Regression of an Indicator Matrix 103\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n222\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n22\\n2 2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33 3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3 3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n222\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n22\\n2 2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33 3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3 3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\nFIGURE 4.1. The left plot shows some data from three classes, with linear\\ndecision boundaries found by linear discriminant analysis. The right plot shows\\nquadratic decision boundaries. These were obtained by ﬁnding linear boundaries\\nin the ﬁve-dimensional space X1,X2,X1X2,X 2\\n1,X 2\\n2. Linear inequalities in this\\nspace are quadratic inequalities in the original space.\\nmation h(X)w h e r eh :I Rp ↦→IRq with q>p , and will be explored in later\\nchapters.\\n4.2 Linear Regression of an Indicator Matrix\\nHere each of the response categories are coded via an indicator variable.\\nThus ifG has K classes, there will beK such indicatorsYk,k =1 ,...,K ,\\nwith Yk =1i f G = k else 0. These are collected together in a vector\\nY =( Y1,...,Y K), and theN training instances of these form anN × K\\nindicator response matrixY. Y is a matrix of 0’s and 1’s, with each row\\nhaving a single 1. We ﬁt a linear regression model to each of the columns\\nof Y simultaneously, and the ﬁt is given by\\nˆY = X(XTX)−1XTY. (4.3)\\nChapter 3 has more details on linear regression. Note that we have a coeﬃ-\\ncient vector for each response columnyk, and hence a (p+1)×K coeﬃcient\\nmatrix ˆB =( XTX)−1XTY.H e r eX is the model matrix withp+1columns\\ncorresponding to thep inputs, and a leading column of 1’s for the intercept.\\nA new observation with inputx is classiﬁed as follows:\\n•compute the ﬁtted outputˆf(x)T =( 1,xT)ˆB,a K vector;\\n•identify the largest component and classify accordingly:\\nˆG(x) = argmaxk∈Gˆfk(x). (4.4)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6995590e-34c2-47f2-9134-270a9d8c2ace', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 122, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='104 4. Linear Methods for Classiﬁcation\\nWhat is the rationale for this approach? One rather formal justiﬁcation\\nis to view the regression as an estimate of conditional expectation. For the\\nrandom variable Yk, E(Yk|X = x)=P r (G = k|X = x), so conditional\\nexpectation of each of theYk seems a sensible goal. The real issue is: how\\ngood an approximation to conditional expectation is the rather rigid linear\\nregression model? Alternatively, are theˆfk(x) reasonable estimates of the\\nposterior probabilities Pr(G = k|X = x), and more importantly, does this\\nmatter?\\nIt is quite straightforward to verify that∑\\nk∈G\\nˆfk(x) = 1 for anyx,a s\\nlong as there is an intercept in the model (column of 1’s inX). However,\\nthe ˆfk(x) can be negative or greater than 1, and typically some are. This\\nis a consequence of the rigid nature of linear regression, especially if we\\nmake predictions outside the hull of the training data. These violations in\\nthemselves do not guarantee that this approach will not work, and in fact\\non many problems it gives similar results to more standard linear meth-\\nods for classiﬁcation. If we allow linear regression onto basis expansions\\nh(X) of the inputs, this approach can lead to consistent estimates of the\\nprobabilities. As the size of the training setN grows bigger, we adaptively\\ninclude more basis elements so that linear regression onto these basis func-\\ntions approaches conditional expectation. We discuss such approaches in\\nChapter 5.\\nA more simplistic viewpoint is to construct targets tk for each class,\\nwhere tk is thekth column of theK ×K identity matrix. Our prediction\\nproblem is to try and reproduce the appropriate target for an observation.\\nWith the same coding as before, the response vectoryi (ith row ofY)f o r\\nobservation i has the valueyi = tk if gi = k. We might then ﬁt the linear\\nmodel by least squares:\\nmin\\nB\\nN∑\\ni=1\\n||yi −[(1,xT\\ni )B]T||2. (4.5)\\nThe criterion is a sum-of-squared Euclidean distances of the ﬁtted vectors\\nfrom their targets. A new observation is classiﬁed by computing its ﬁtted\\nvector ˆf(x) and classifying to the closest target:\\nˆG(x) = argmin\\nk\\n||ˆf(x)−tk||2. (4.6)\\nThis is exactly the same as the previous approach:\\n•The sum-of-squared-norm criterion is exactly the criterion for multi-\\nple response linear regression, just viewed slightly diﬀerently. Since\\na squared norm is itself a sum of squares, the components decouple\\nand can be rearranged as a separate linear model for each element.\\nNote that this is only possible because there is nothing in the model\\nthat binds the diﬀerent responses together.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8db185a-71e9-4082-a56a-3fe0e1b81e4c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 123, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Linear Regression of an Indicator Matrix 105\\nLinear Regression\\n1\\n1\\n1\\n1\\n1\\n1111\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1 11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n111 1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 111\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 11\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11 1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2 2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2 2\\n2\\n2 22 2\\n2\\n2\\n2\\n2\\n2\\n2 2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2 2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n22\\n2\\n2 22\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2 2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3 3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 333\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33 3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3 33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\nLinear Discriminant Analysis\\n1\\n1\\n1\\n1\\n1\\n1111\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1 11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n111 1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 111\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11 1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2 2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2 2\\n2\\n2 22 2\\n2\\n2\\n2\\n2\\n2\\n2 2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2 2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n22\\n2\\n2 22\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2 2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3 3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 333\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33 3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3 33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\nX1X1\\nX2\\nX2\\nFIGURE 4.2.The data come from three classes inIR2 and are easily separated\\nby linear decision boundaries. The right plot shows the boundaries found by linear\\ndiscriminant analysis. The left plot shows the boundaries found by linear regres-\\nsion of the indicator response variables. The middle class is completely masked\\n(never dominates).\\n•The closest target classiﬁcation rule (4.6) is easily seen to be exactly\\nthe same as the maximum ﬁtted component criterion (4.4).\\nThereisaseriousproblemwiththeregressionapproachwhenthenumber\\nof classesK ≥3, especially prevalent whenK is large. Because of the rigid\\nnature of the regression model, classes can bemaskedby others. Figure 4.2\\nillustrates an extreme situation whenK = 3. The three classes are perfectly\\nseparated by linear decision boundaries, yet linear regression misses the\\nmiddle class completely.\\nIn Figure 4.3 we have projected the data onto the line joining the three\\ncentroids (there is no information in the orthogonal direction in this case),\\nand we have included and coded the three response variablesY1, Y2 and\\nY3. The three regression lines (left panel) are included, and we see that\\nthe line corresponding to the middle class is horizontal and its ﬁtted values\\nare never dominant! Thus, observations from class 2 are classiﬁed either\\nas class 1 or class 3. The right panel uses quadratic regression rather than\\nlinear regression. For this simple example a quadratic rather than linear\\nﬁt (for the middle class at least) would solve the problem. However, it\\ncan be seen that if there were four rather than three classes lined up like\\nthis, a quadratic would not come down fast enough, and a cubic would\\nbe needed as well. A loose but general rule is that ifK ≥3 classes are\\nlined up, polynomial terms up to degreeK−1 might be needed to resolve\\nthem. Note also that these are polynomials along the derived direction\\npassing through the centroids, which can have arbitrary orientation. So in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1756ee10-57dc-40b6-ab71-d90805c9306d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 124, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='106 4. Linear Methods for Classiﬁcation\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n11 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n111\\n11\\n11 1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n11\\n1 1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n111\\n1\\n1\\n1\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n1\\n111\\n1\\n222 222 2 2 2222 22 22 2 22 22222 2 22222 22 2 22 22 22 22222 22 2 222 2 2222 222 222 222 222222 22 2 22 222 2222 22 222 222 222 222 222 2 22 22 222 222 22 22 2222 22 2 222 2 22 2 22 2 222 2 222 2 222 222 222 2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n33 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n333\\n33\\n33 3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n33\\n3 3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n3\\n3\\n33\\n3\\n3\\n33\\n33\\n3\\n333\\n3\\n33\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3\\n333\\n3\\n0.0\\n0.5\\n1.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n111\\n11\\n11\\n1\\n1\\n1\\n1\\n1\\n111\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1111 1\\n1\\n11 1\\n1\\n1 1\\n1\\n1\\n1111 11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n1\\n11 11 111 11\\n1\\n11 11 1111 11 1\\n1\\n11 1 11 1 11 1 111 1 111 1 111 111 111\\n1\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n222\\n22\\n22\\n2\\n2\\n2\\n2\\n2 222\\n2\\n2\\n22\\n2\\n22 222 2222 22 22 2\\n2\\n2\\n2\\n22 2222 22 222 2\\n22 222 222\\n2\\n22 2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n222\\n2\\n333 333\\n3 3 333\\n3\\n3\\n3 3\\n3\\n3\\n33 333\\n33\\n3 33 33\\n3\\n3\\n3\\n3 33 3\\n3\\n33 33333 33 3 33\\n3 3\\n333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3333 3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3333 33\\n33\\n3\\n3\\n33\\n33\\n3\\n333\\n3\\n33\\n3\\n3\\n3\\n33\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n333\\n3\\n0.0\\n0.5\\n1.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\nDegree = 1; Error = 0.33 Degree = 2; Error = 0.04\\nFIGURE 4.3.The eﬀects of masking on linear regression inIRfor a three-class\\nproblem. Therug plotat the base indicates the positions and class membership of\\neach observation. The three curves in each panel are the ﬁtted regressions to the\\nthree-class indicator variables; for example, for the blue class,yblue is 1 for the\\nblue observations, and0 for the green and orange. The ﬁts are linear and quadratic\\npolynomials. Above each plot is the training error rate. The Bayes error rate is\\n0.025 for this problem, as is the LDA error rate.\\np-dimensional input space, one would need general polynomial terms and\\ncross-products of total degreeK−1, O(pK−1) terms in all, to resolve such\\nworst-case scenarios.\\nThe example is extreme, but for large K and small p such maskings\\nnaturally occur. As a more realistic illustration, Figure 4.4 is a projection\\nof the training data for a vowel recognition problem onto an informative\\ntwo-dimensional subspace. There areK = 11 classes inp = 10 dimensions.\\nThis is a diﬃcult classiﬁcation problem, and the best methods achieve\\naround 40% errors on the test data. The main point here is summarized in\\nTable 4.1; linear regression has an error rate of 67%, while a close relative,\\nlinear discriminant analysis, has an error rate of 56%. It seems that masking\\nhas hurt in this case. While all the other methods in this chapter are based\\non linear functions ofx as well, they use them in such a way that avoids\\nthis masking problem.\\n4.3 Linear Discriminant Analysis\\nDecision theory for classiﬁcation (Section 2.4) tells us that we need to know\\nthe class posteriors Pr(G|X) for optimal classiﬁcation. Suppose fk(x)i s\\nthe class-conditional density ofX in classG = k,a n dl e tπk be the prior\\nprobability of classk,w i t h∑K\\nk=1 πk = 1. A simple application of Bayes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6b6470e-b0f9-4b11-83d3-c407af9cb063', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 125, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 107\\nCoordinate 1 for Training Data\\nCoordinate 2 for Training Data\\n-4 -2 0 2 4\\n-6 -4 -2 0 2 4\\no ooooo\\no o\\no\\no\\noo\\no\\no\\no o o o\\noo\\no\\no\\no\\no\\no o o o oo\\nooo o oo\\no\\no\\no\\no\\no o\\no o\\no\\nooo\\noo oooo\\no\\no\\noo\\no\\no\\noooo\\no\\no\\no\\nooooo\\no o\\noo\\no\\no\\no\\no\\noooo\\no oo o\\noo\\no\\noo\\no\\no\\no\\noooo\\no\\nooo\\nooo\\no\\no\\no\\noo\\no\\no\\no ooo\\no o\\nooo o o\\no\\no\\noo ooo\\noo o oo\\no\\no o\\no\\no o\\no\\noooooo\\noooooo\\noo\\noooo\\noooooo\\nooooo o\\nooooo o\\noooooo\\no\\no\\noooo\\no oo\\no\\no\\no\\no ooooo\\nooooo\\no\\noooooo\\no\\no\\noooo\\noooooo\\noooooo\\no\\no\\no\\no\\noo\\nooooo o\\no ooo\\no o\\no ooooo\\no ooo\\no\\no\\noo\\no\\no\\no\\no\\nooo\\no\\no\\no\\noooo\\no\\no o\\no\\no\\no\\no\\no\\noooooo\\no\\nooooo\\no\\no\\noo o\\no oo\\no\\noo o\\no o o\\noo\\no\\noo\\no\\nooo\\nooooo\\no\\no\\no\\no\\noo\\no\\nooo\\no oo\\no o o\\noo o\\no oo\\noo\\no\\noooooo\\no o\\no\\no\\noo\\noo o oo oooooo o\\no\\no\\noo o o\\nooooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\nooo\\no\\noooo\\no\\no\\no ooo\\no\\no\\noooooo\\nooo o\\noo\\nooooo o\\no ooo o o\\noooooo\\no\\no\\nooo o\\no\\noo o o o\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\noooo\\noooo\\noo\\nooo ooo\\noo\\no\\no oo\\no oo\\no\\no\\noooooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\no\\no\\no o\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nLinear Discriminant Analysis\\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are\\neleven classes withX ∈IR10, and this is the best view in terms of a LDA model\\n(Section 4.3.3). The heavy circles are the projected mean vectors for each class.\\nThe class overlap is considerable.\\nTABLE 4.1.Training and test error rates using a variety of linear techniques\\non the vowel data. There are eleven classes in ten dimensions, of which three\\naccount for 90% of the variance (via a principal components analysis). We see\\nthat linear regression is hurt by masking, increasing the test and training error\\nby over10%.\\nTechnique Error Rates\\nTraining Test\\nLinear regression 0.48 0.67\\nLinear discriminant analysis 0.32 0.56\\nQuadratic discriminant analysis 0.01 0.53\\nLogistic regression 0.22 0.51', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ab551ae8-b965-4b2d-b49c-8ad10c1b3007', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 126, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='108 4. Linear Methods for Classiﬁcation\\ntheorem gives us\\nPr(G = k|X = x)= fk(x)πk\\n∑K\\nℓ=1 fℓ(x)πℓ\\n. (4.7)\\nWe see that in terms of ability to classify, having thefk(x) is almost equiv-\\nalent to having the quantity Pr(G = k|X = x).\\nMany techniques are based on models for the class densities:\\n•linear and quadratic discriminant analysis use Gaussian densities;\\n•moreﬂexiblemixturesofGaussiansallowfornonlineardecisionbound-\\naries (Section 6.8);\\n•general nonparametric density estimates for each class density allow\\nthe most ﬂexibility (Section 6.6.2);\\n•Naive Bayes models are a variant of the previous case, and assume\\nthat each of the class densities are products of marginal densities;\\nthat is, they assume that the inputs are conditionally independent in\\neach class (Section 6.6.3).\\nSuppose that we model each class density as multivariate Gaussian\\nfk(x)= 1\\n(2π)p/2|Σk|1/2e−1\\n2(x−μk)T Σ−1\\nk (x−μk). (4.8)\\nLinear discriminant analysis (LDA) arises in the special case when we\\nassume that the classes have a common covariance matrixΣk = Σ∀k.I n\\ncomparing two classesk and ℓ, it is suﬃcient to look at the log-ratio, and\\nwe see that\\nlog Pr(G = k|X = x)\\nPr(G = ℓ|X = x) =l o gfk(x)\\nfℓ(x) +log πk\\nπℓ\\n=l o gπk\\nπℓ\\n−1\\n2(μk +μℓ)TΣ−1(μk −μℓ)\\n+xTΣ−1(μk −μℓ),\\n(4.9)\\nan equation linear inx. The equal covariance matrices cause the normal-\\nization factors to cancel, as well as the quadratic part in the exponents.\\nThis linear log-odds function implies that the decision boundary between\\nclasses k and ℓ—the set where Pr(G = k|X = x)=P r (G = ℓ|X = x)—is\\nlinear inx;i np dimensions a hyperplane. This is of course true for any pair\\nof classes, so all the decision boundaries are linear. If we divide IRp into\\nregions that are classiﬁed as class 1, class 2, etc., these regions will be sep-\\narated by hyperplanes. Figure 4.5 (left panel) shows an idealized example\\nwith three classes and p = 2. Here the data do arise from three Gaus-\\nsian distributions with a common covariance matrix. We have included in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='272453f6-d287-45ad-8f92-9d9a23d3fa64', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 127, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 109\\n+ +\\n+\\n3\\n21\\n1\\n1\\n2\\n3\\n3\\n3\\n1\\n2\\n3\\n3\\n2\\n1 1 21\\n1\\n3\\n3\\n1 21\\n2\\n3\\n2\\n3\\n3\\n1\\n2\\n2\\n1\\n1\\n1\\n1\\n3\\n2\\n2\\n2\\n2\\n1 3\\n22\\n3\\n1\\n3\\n1\\n3\\n3 2\\n1\\n3\\n3\\n2\\n3\\n1\\n3\\n3\\n2\\n1\\n3\\n3\\n2\\n2\\n3\\n2\\n2\\n21\\n1\\n1\\n1\\n1\\n2\\n1\\n3\\n3\\n1\\n1\\n3\\n3\\n2\\n2\\n2\\n23\\n1\\n2\\nFIGURE 4.5.The left panel shows three Gaussian distributions, with the same\\ncovariance and diﬀerent means. Included are the contours of constant density\\nenclosing 95% of the probability in each case. The Bayes decision boundaries\\nbetween each pair of classes are shown (broken straight lines), and the Bayes\\ndecision boundaries separating all three classes are the thicker solid lines (a subset\\nof the former). On the right we see a sample of30 drawn from each Gaussian\\ndistribution, and the ﬁtted LDA decision boundaries.\\nthe ﬁgure the contours corresponding to 95% highest probability density,\\nas well as the class centroids. Notice that the decision boundaries are not\\nthe perpendicular bisectors of the line segments joining the centroids. This\\nwould be the case if the covarianceΣ were spherical σ2I,a n dt h ec l a s s\\npriors were equal. From (4.9) we see that thelinear discriminant functions\\nδk(x)= xTΣ−1μk −1\\n2μT\\nk Σ−1μk +log πk (4.10)\\nareanequivalentdescriptionofthedecisionrule,with G(x) = argmaxkδk(x).\\nIn practice we do not know the parameters of the Gaussian distributions,\\nand will need to estimate them using our training data:\\n•ˆπk = Nk/N,w h e r eNk is the number of class-k observations;\\n•ˆμk =∑\\ngi=k xi/Nk;\\n•ˆΣ=∑K\\nk=1\\n∑\\ngi=k(xi −ˆμk)(xi −ˆμk)T/(N −K).\\nFigure 4.5 (right panel) shows the estimated decision boundaries based on\\na sample of size 30 each from three Gaussian distributions. Figure 4.1 on\\npage 103 is another example, but here the classes are not Gaussian.\\nWith two classes there is a simple correspondence between linear dis-\\ncriminant analysis and classiﬁcation by linear regression, as in (4.5). The\\nLDA rule classiﬁes to class 2 if\\nxT ˆΣ\\n−1\\n(ˆμ2 −ˆμ1) > 1\\n2(ˆμ2 +ˆμ1)T ˆΣ\\n−1\\n(ˆμ2 −ˆμ1)−log(N2/N1), (4.11)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9046e632-767a-429e-a886-57adf8109af9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 128, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='110 4. Linear Methods for Classiﬁcation\\nand class 1 otherwise. Suppose we code the targets in the two classes as +1\\nand−1, respectively. It is easy to show that the coeﬃcient vector from least\\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4.2).\\n[In fact, this correspondence occurs for any (distinct) coding of the targets;\\nsee Exercise 4.2]. However unlessN1 = N2 the intercepts are diﬀerent and\\nhence the resulting decision rules are diﬀerent.\\nSince this derivation of the LDA direction via least squares does not use a\\nGaussian assumption for the features, its applicability extends beyond the\\nrealm of Gaussian data. However the derivation of the particular intercept\\nor cut-point given in (4.11) does require Gaussian data. Thus it makes\\nsense to instead choose the cut-point that empirically minimizes training\\nerror for a given dataset. This is something we have found to work well in\\npractice, but have not seen it mentioned in the literature.\\nWith more than two classes, LDA is not the same as linear regression of\\nthe class indicator matrix, and it avoids the masking problems associated\\nwith that approach (Hastie et al., 1994). A correspondence between regres-\\nsion and LDA can be established through the notion ofoptimal scoring,\\ndiscussed in Section 12.5.\\nGetting back to the general discriminant problem (4.8), if theΣk are\\nnot assumed to be equal, then the convenient cancellations in (4.9) do not\\noccur; in particular the pieces quadratic inx remain. We then getquadratic\\ndiscriminant functions(QDA),\\nδk(x)= −1\\n2 log|Σk|−1\\n2(x−μk)TΣ−1\\nk (x−μk)+log πk. (4.12)\\nThe decision boundary between each pair of classesk and ℓ is described by\\na quadratic equation{x : δk(x)= δℓ(x)}.\\nFigure 4.6 shows an example (from Figure 4.1 on page 103) where the\\nthree classes are Gaussian mixtures (Section 6.8) and the decision bound-\\naries are approximated by quadratic equations in x. Here we illustrate\\ntwo popular ways of ﬁtting these quadratic boundaries. The right plot\\nuses QDA as described here, while the left plot uses LDA in the enlarged\\nﬁve-dimensional quadratic polynomial space. The diﬀerences are generally\\nsmall; QDA is the preferred approach, with the LDA method a convenient\\nsubstitute 2.\\nThe estimates for QDAaresimilar to those for LDA, except that separate\\ncovariance matrices must be estimated for each class. Whenp is large this\\ncan mean a dramatic increase in parameters. Since the decision boundaries\\nare functions of the parameters of the densities, counting the number of\\nparameters must be done with care. For LDA, it seems there are (K −\\n1)×(p+ 1) parameters, since we only need the diﬀerencesδk(x)−δK(x)\\n2For this ﬁgure and many similar ﬁgures in the book we compute the decision bound-\\naries by an exhaustive contouring method. We compute the decision rule on a ﬁne lattice\\nof points, and then use contouring algorithms to compute the boundaries.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1dfbd83c-9913-47e4-9b63-6ca04a3c95b9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 129, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 111\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n222\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n22\\n2 2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33 3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3 3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n1\\n1\\n1\\n11\\n11\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n11\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n11\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n11\\n1 1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22 2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 22\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n22\\n2 2\\n22 2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n2\\n22\\n2\\n2\\n2\\n222\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2 2\\n2\\n2\\n2\\n22\\n22\\n2 2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n33\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n33 3\\n3\\n3\\n3 3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3 3\\n3\\n3\\n3\\n3 3\\n33\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n3 3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 3\\n3\\n3\\n33\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\nFIGURE 4.6.Two methods for ﬁtting quadratic boundaries. The left plot shows\\nthe quadratic decision boundaries for the data in Figure 4.1 (obtained using LDA\\nin the ﬁve-dimensional space X1,X2,X1X2,X 2\\n1,X 2\\n2). The right plot shows the\\nquadratic decision boundaries found by QDA. The diﬀerences are small, as is\\nusually the case.\\nbetween the discriminant functions whereK is some pre-chosen class (here\\nwe have chosen the last), and each diﬀerence requiresp + 1 parameters3.\\nLikewise for QDA there will be (K −1)×{ p(p +3 )/2+1 } parameters.\\nBoth LDA and QDA perform well on an amazingly large and diverse set\\nof classiﬁcation tasks. For example, in the STATLOG project (Michie et\\nal., 1994) LDA was among the top three classiﬁers for 7 of the 22 datasets,\\nQDA among the top three for four datasets, and one of the pair were in the\\ntop three for 10 datasets. Both techniques are widely used, and entire books\\nare devoted to LDA. It seems that whatever exotic tools are the rage of the\\nday, we should always have available these two simple tools. The question\\narises why LDA and QDA have such a good track record. The reason is not\\nlikely to be that the data are approximately Gaussian, and in addition for\\nLDA that the covariances are approximately equal. More likely a reason is\\nthat the data can only support simple decision boundaries such as linear or\\nquadratic, and the estimates provided via the Gaussian models are stable.\\nThis is a bias variance tradeoﬀ—we can put up with the bias of a linear\\ndecision boundary because it can be estimated with much lower variance\\nthan more exotic alternatives. This argument is less believable for QDA,\\nsince it can have many parameters itself, although perhaps fewer than the\\nnon-parametric alternatives.\\n3Although we ﬁt the covariance matrixˆΣto compute the LDA discriminant functions,\\na much reduced function of it is all that is required to estimate theO(p) parameters\\nneeded to compute the decision boundaries.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2526b111-90fc-4170-9691-f182f7c01642', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 130, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='112 4. Linear Methods for Classiﬁcation\\nMisclassification Rate\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.1 0.2 0.3 0.4 0.5\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81\\n\\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81 \\x81 \\x81 \\x81\\n\\x81\\nRegularized Discriminant Analysis on the Vowel Data\\nTest Data\\nTrain Data\\nα\\nFIGURE 4.7. Test and training errors for the vowel data, using regularized\\ndiscriminant analysis with a series of values ofα∈[0,1]. The optimum for the\\ntest data occurs aroundα=0 .9, close to quadratic discriminant analysis.\\n4.3.1 Regularized Discriminant Analysis\\nFriedman (1989) proposed a compromise between LDA and QDA, which\\nallows one to shrink the separate covariances of QDA toward a common\\ncovariance as in LDA. These methods are very similar in ﬂavor to ridge\\nregression. The regularized covariance matrices have the form\\nˆΣk(α)= αˆΣk +(1 −α)ˆΣ, (4.13)\\nwhere ˆΣis the pooled covariance matrix as used in LDA. Hereα∈[0,1]\\nallows a continuum of models between LDA and QDA, and needs to be\\nspeciﬁed. In practice αcan be chosen based on the performance of the\\nmodel on validation data, or by cross-validation.\\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\\nthe training and test error improve with increasingα, although the test\\nerror increases sharply afterα=0 .9. The large discrepancy between the\\ntraining and test error is partly due to the fact that there are many repeat\\nmeasurements on a small number of individuals, diﬀerent in the training\\nand test set.\\nSimilar modiﬁcations allowˆΣitself to be shrunk toward the scalar\\ncovariance,\\nˆΣ(γ)= γˆΣ+(1 −γ)ˆσ2I (4.14)\\nfor γ∈[0,1]. Replacing ˆΣin (4.13) byˆΣ(γ) leads to a more general family\\nof covariances ˆΣ(α,γ) indexed by a pair of parameters.\\nIn Chapter 12, we discuss other regularized versions of LDA, which are\\nmore suitable when the data arise from digitized analog signals and images.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='09ba465e-505c-4e99-96b8-12e3a853cb5b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 131, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 113\\nInthesesituationsthefeaturesarehigh-dimensionalandcorrelated,andthe\\nLDA coeﬃcients can be regularized to be smooth or sparse in the original\\ndomain of the signal. This leads to better generalization and allows for\\neasier interpretation of the coeﬃcients. In Chapter 18 we also deal with\\nvery high-dimensional problems, where for example the features are gene-\\nexpression measurements in microarray studies. There the methods focus\\non the caseγ= 0 in (4.14), and other severely regularized versions of LDA.\\n4.3.2 Computations for LDA\\nAs a lead-in to the next topic, we brieﬂy digress on the computations\\nrequired for LDA and especially QDA. Their computations are simpliﬁed\\nby diagonalizing ˆΣor ˆΣk. For the latter, suppose we compute the eigen-\\ndecomposition for each ˆΣk = UkDkUT\\nk ,w h e r eUk is p× p orthonormal,\\nand Dk a diagonal matrix of positive eigenvaluesdkℓ. Then the ingredients\\nfor δk(x) (4.12) are\\n•(x−ˆμk)T ˆΣ\\n−1\\nk (x−ˆμk)=[ UT\\nk (x−ˆμk)]TD−1\\nk [UT\\nk (x−ˆμk)];\\n•log|ˆΣk| =∑\\nℓ logdkℓ.\\nIn light of the computational steps outlined above, the LDA classiﬁer\\ncan be implemented by the following pair of steps:\\n•Sphere the data with respect to the common covariance estimateˆΣ:\\nX∗←D−1\\n2 UTX,w h e r eˆΣ= UDUT. The common covariance esti-\\nmate ofX∗will now be the identity.\\n•Classify to the closest class centroid in the transformed space, modulo\\nthe eﬀect of the class prior probabilitiesπk.\\n4.3.3 Reduced-Rank Linear Discriminant Analysis\\nSo far we have discussed LDA as a restricted Gaussian classiﬁer. Part of\\nits popularity is due to an additional restriction that allows us to view\\ninformative low-dimensional projections of the data.\\nThe K centroids in p-dimensional input space lie in an aﬃne subspace\\nof dimension≤K−1, and ifp is much larger thanK, this will be a con-\\nsiderable drop in dimension. Moreover, in locating the closest centroid, we\\ncan ignore distances orthogonal to this subspace, since they will contribute\\nequally to each class. Thus we might just as well project theX∗onto this\\ncentroid-spanning subspaceHK−1, and make distance comparisons there.\\nThus there is a fundamental dimension reduction in LDA, namely, that we\\nneed only consider the data in a subspace of dimension at mostK −1.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='572772a9-f123-4ac6-9ff7-d190011093ef', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 132, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='114 4. Linear Methods for Classiﬁcation\\nIf K = 3, for instance, this could allow us to view the data in a two-\\ndimensional plot, color-coding the classes. In doing so we would not have\\nrelinquished any of the information needed for LDA classiﬁcation.\\nWhat ifK> 3? We might then ask for aL<K −1 dimensional subspace\\nHL ⊆HK−1 optimal for LDA in some sense. Fisher deﬁned optimal to\\nmean that the projected centroids were spread out as much as possible in\\nterms of variance. This amounts to ﬁnding principal component subspaces\\nof the centroids themselves (principal components are described brieﬂy in\\nSection3.5.1,andinmoredetailinSection14.5.1).Figure4.4showssuchan\\noptimal two-dimensional subspace for the vowel data. Here there are eleven\\nclasses, each a diﬀerent vowel sound, in a ten-dimensional input space. The\\ncentroids require the full space in this case, sinceK −1= p, but we have\\nshown an optimal two-dimensional subspace. The dimensions are ordered,\\nso we can compute additional dimensions in sequence. Figure 4.8 shows four\\nadditional pairs of coordinates, also known ascanonical or discriminant\\nvariables. In summary then, ﬁnding the sequences of optimal subspaces\\nfor LDA involves the following steps:\\n•compute the K × p matrix of class centroids M and the common\\ncovariance matrixW (for within-class covariance);\\n•compute M∗= MW−1\\n2 using the eigen-decomposition ofW;\\n•computeB∗, the covariance matrix ofM∗(Bforbetween-classcovari-\\nance), and its eigen-decompositionB∗= V∗DBV∗T.T h ec o l u m n s\\nv∗\\nℓ of V∗in sequence from ﬁrst to last deﬁne the coordinates of the\\noptimal subspaces.\\nCombining all these operations theℓth discriminant variable is given by\\nZℓ = vT\\nℓ X with vℓ = W−1\\n2 v∗\\nℓ.\\nFisher arrived at this decomposition via a diﬀerent route, without refer-\\nring to Gaussian distributions at all. He posed the problem:\\nFind the linear combinationZ = aTX such that the between-\\nclass variance is maximized relative to the within-class variance.\\nAgain, the between class variance is the variance of the class means of\\nZ, and the within class variance is the pooled variance about the means.\\nFigure 4.9 shows why this criterion makes sense. Although the direction\\njoining the centroids separates the means as much as possible (i.e., max-\\nimizes the between-class variance), there is considerable overlap between\\nthe projected classes due to the nature of the covariances. By taking the\\ncovariance into account as well, a direction with minimum overlap can be\\nfound.\\nThe between-class variance ofZ is aTBa and the within-class variance\\naTWa,w h e r eW is deﬁned earlier, andB is the covariance matrix of the\\nclass centroid matrix M.N o t et h a tB + W = T,w h e r eT is the total\\ncovariance matrix ofX, ignoring class information.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c47c4fa-864a-4ca5-86c7-6cf2e567ebf5', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 133, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 115\\nCoordinate 1 \\nCoordinate 3 \\n-4 -2 0 2 4\\n-2 0 2\\no\\no\\noooo\\no\\no\\noo\\no\\no\\noo\\no o o\\no\\nooo\\no\\noo\\no\\no\\no\\no oo\\no\\no\\no\\nooo\\no o o o o\\no o o\\no\\no\\no\\no\\no\\noo\\noo\\no\\noooo\\noo\\noooo\\no o\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\noooo\\no o\\noo\\no o\\noo\\noooo\\noo\\no\\no\\no\\no o\\nooo oo\\no\\nooo\\noo\\noo\\no\\no ooo\\noooo\\no\\noo o\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\noo\\no\\nooo\\noooooooo\\no\\no\\no\\no\\nooo\\no\\no\\no o\\noo\\noo o\\no\\nooo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no o\\noo\\nooo\\no\\nooo\\noo\\nooo\\noo\\no\\noo\\noo\\noo\\no\\nooooo\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no o\\no\\no\\no\\no\\no\\no\\nooo\\no o\\noo\\nooo\\noooo\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\noo\\nooo\\no\\no\\no\\noooo\\no\\no\\noo oo\\noo\\no\\no\\no\\no\\no oo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\no\\no\\no o\\noo\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no o\\no\\nooo\\nooo\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\noooo\\no oooo\\no\\nooo\\no\\no\\no\\no\\noo o o\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\nooo\\no o oo\\no ooo\\no o o\\no\\noo o\\no\\no\\no\\no\\no oooo\\no\\no\\no\\no\\no\\no\\noooooo\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo oo\\noooooo oooooo\\no\\nooo\\no\\no\\noo\\no\\noo\\no\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81\\x81\\n\\x81\\x81\\nCoordinate 2 \\nCoordinate 3 \\n-6 -4 -2 0 2 4\\n-2 0 2\\no\\no\\noooo\\no\\no\\no o\\no\\no\\noo\\nooo\\no\\nooo\\no\\noo\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\nooooo\\no oo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noooo\\noo\\noooo\\noo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\noo oo\\no o\\noo\\no o\\noo\\noooo\\noo\\no\\no\\no\\noo\\no oooo\\no\\nooo\\noo\\noo\\no\\noooo\\noooo\\no\\nooo\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\noo\\no\\nooo\\noo oooo oo\\no\\no\\no\\no\\nooo\\no\\no\\noo\\noo\\nooo\\no\\nooo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo\\noo\\noo\\nooo\\no\\nooo\\noo\\nooo\\noo\\no\\noo\\noo\\noo\\no\\nooooo\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\nooo\\no o\\noo\\nooo\\noooo\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\noo\\nooo\\no\\no\\no\\noooo\\no\\no\\nooo o\\noo\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\no\\no\\noo\\noo\\no\\no\\no\\no\\nooo o\\no\\no\\no\\no\\no\\no o\\no\\noo o\\nooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\noo oo\\nooooo\\no\\nooo\\no\\no\\no\\no\\nooo o\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\nooo\\noooo\\nooo o\\nooo\\no\\noo o\\no\\no\\no\\no\\noo o oo\\no\\no\\no\\no\\no\\no\\noooo oo\\no\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\no\\noooo\\nooooo oooo ooo\\no\\no oo\\no\\no\\noo\\no\\noo\\no\\n\\x81\\x81\\n\\x81\\x81 \\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\nCoordinate 1 \\nCoordinate 7 \\n-4 -2 0 2 4\\n-3 -2 -1 0 1 2 3\\noooooo\\no\\no\\no\\no\\no\\no\\noo\\no oo o\\noooooo\\no o oo ooo\\noo\\nooo\\no o o\\no\\no\\no\\no o\\no\\noo\\no\\no\\noo\\no\\no\\no\\noooo\\no\\no\\noooo\\no\\nooooooo\\no\\no\\noo\\no\\no\\no\\no oooooooo\\noo\\no\\no\\no\\no\\no o\\noo\\noo\\noo\\noo\\no\\no\\noo\\noo\\no\\no oo\\no o\\no\\nooo\\noo\\no o\\noo\\no\\noo\\nooo\\noo o\\no\\noo\\no o oo\\noo\\noooooo\\noooo\\no\\no\\noooooo\\noooo\\no\\no\\noo\\no\\nooo\\nooooo o\\no\\no\\no\\no\\noo\\nooo\\no\\no\\no\\no\\nooo\\no\\no\\no ooo\\noo\\noo\\noooo\\noo\\noooo\\nooo\\noo\\no\\nooo\\noo\\no\\noo\\noooo\\no\\no o\\no\\no\\no\\noooooo\\no o\\noo\\no o o oooo\\no\\no\\noooo\\no\\no\\no\\noo\\no o o\\no\\noooo\\no\\no\\no\\no\\no\\no\\no o o\\noo\\no\\no\\nooo\\no\\no\\no\\noooo\\nooo\\noo oo o\\nooo\\noo\\no\\no o\\no\\noooo\\noo\\noo\\noo\\no\\no\\no\\no\\no o\\nooo\\no\\noo\\noo\\no\\no\\no\\no\\nooo o\\noo\\no\\no\\no\\no\\no\\nooooo\\no\\no\\nooo\\no\\no\\noo oo\\no\\no\\noo\\noo\\no o\\nooo o o\\noo\\noo\\noo\\noo\\no\\no\\noo\\no\\no o\\no\\noo\\noooo\\noo\\noooooo\\no\\no\\noo\\no\\no\\noo\\no\\nooo\\nooo o\\noo\\no\\no\\nooo\\no\\no oo\\no o\\no\\noo\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\no o\\no\\no\\no\\no\\noo oo\\noo\\noooo\\no\\no\\no\\no\\no\\no\\noooooo\\no\\noo\\noo\\no\\nooo o ooo o\\noo\\no\\no\\no\\noooo\\no\\no\\no\\nooooo\\no\\no\\noo\\no\\no\\noo\\no\\no o\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81 \\x81\\x81\\x81\\x81\\n\\x81\\x81 \\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\nCoordinate 9 \\nCoordinate 10 \\n- 2 - 1 0123\\n-2 -1 0 1 2\\noo\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\noo o o\\no\\noo\\no\\nooo\\no\\nooo\\noo\\no\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\noo\\no o\\no\\noo\\no\\no\\no\\no\\nooo\\noooo\\no\\no\\noooooo\\no\\no o o\\no oo\\no o\\no o o\\no\\no\\no\\no\\no o\\no\\no o o\\no o\\no\\no\\no ooo\\noooo ooo\\noo\\nooo\\no\\noo\\noo\\no\\noo oo o\\no\\no\\no\\no\\no\\no o\\noo\\no\\noo\\no\\noo o\\no\\no o\\no\\noo\\no\\no o oooooo\\noooooo\\no\\no oo\\no\\no\\no\\no\\no o\\no\\no\\nooo ooo\\no\\no\\no\\no\\noo\\nooooo\\no\\noo\\no\\noo\\no o\\noooo\\no\\noooo\\noo\\noooo o o ooo\\no\\no\\no\\no ooo\\noo\\noo o\\no\\no\\no\\no\\no\\no ooo\\noo o\\no\\no\\no\\no\\noo\\no\\no o\\noo\\noo\\no\\no\\no\\noo\\nooo\\no\\noo\\no\\no\\no\\noooo\\no o\\nooooo\\no\\no\\noo\\no\\no\\no\\no o o\\no\\noo\\no\\no\\nooo o\\nooo\\no o o\\no\\nooo o\\no\\no o\\no\\no\\no\\no\\no\\no o\\noo\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\no o\\nooo oo o\\no o o\\nooo\\no\\no\\no\\no\\no\\no\\noooo o o\\noo\\noo\\no o\\no\\nooo oo\\no\\no o oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\noo\\no o o\\no\\no\\no\\noo\\no o\\noo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no o\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\noo o o\\noooooo\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\nooo\\no\\no\\no\\no o\\noo\\noo\\no\\noo\\noo\\no\\no\\no\\noo\\no\\no\\noooooo\\no\\noo o\\noo\\no oo\\no\\noo\\no\\nooo\\no\\no\\noo\\no\\no\\no\\no\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\nLinear Discriminant Analysis\\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice that as\\nthe rank of the canonical variates increases, the centroids become less spread out.\\nIn the lower right panel they appear to be superimposed, and the classes most\\nconfused.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ddd5f43f-7572-4fa6-af34-519e6012ce9d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 134, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='116 4. Linear Methods for Classiﬁcation\\n+\\n+\\n+\\n+\\nFIGURE 4.9. Although the line joining the centroids deﬁnes the direction of\\ngreatest centroid spread, the projected data overlap because of the covariance\\n(left panel). The discriminant direction minimizes this overlap for Gaussian data\\n(right panel).\\nFisher’s problem therefore amounts to maximizing theRayleigh quotient,\\nmax\\na\\naTBa\\naTWa, (4.15)\\nor equivalently\\nmax\\na\\naTBa subject toaTWa =1 . (4.16)\\nThis is a generalized eigenvalue problem, witha given by the largest\\neigenvalue ofW−1B. It is not hard to show (Exercise 4.1) that the optimal\\na1 is identical tov1 deﬁned above. Similarly one can ﬁnd the next direction\\na2, orthogonal in W to a1, such that aT\\n2 Ba2/aT\\n2 Wa2 is maximized; the\\nsolution is a2 = v2,a n ds oo n .T h eaℓ a r er e f e r r e dt oa sdiscriminant\\ncoordinates, not to be confused with discriminant functions. They are also\\nreferred to as canonical variates, since an alternative derivation of these\\nresults is through a canonical correlation analysis of the indicator response\\nmatrix Y on the predictor matrixX. This line is pursued in Section 12.5.\\nTo summarize the developments so far:\\n•Gaussian classiﬁcation with common covariances leads to linear deci-\\nsion boundaries. Classiﬁcation can be achieved by sphering the data\\nwith respect to W, and classifying to the closest centroid (modulo\\nlogπk) in the sphered space.\\n•Since only the relative distances to the centroids count, one can con-\\nﬁne the data to the subspace spanned by the centroids in the sphered\\nspace.\\n•This subspace can be further decomposed into successively optimal\\nsubspaces in term of centroid separation. This decomposition is iden-\\ntical to the decomposition due to Fisher.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='18014e47-4e9c-41bb-85bb-2ba3992c1c9a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 135, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3 Linear Discriminant Analysis 117\\nDimension\\nMisclassification Rate\\n2468 1 0\\n0.3 0.4 0.5 0.6 0.7\\nLDA and Dimension Reduction on the Vowel Data\\n\\x81\\n\\x81 \\x81 \\x81 \\x81\\n\\x81\\x81 \\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81\\nTest Data\\nTrain Data\\nFIGURE 4.10. Training and test error rates for the vowel data, as a function\\nof the dimension of the discriminant subspace. In this case the best error rate is\\nfor dimension2. Figure 4.11 shows the decision boundaries in this space.\\nThe reduced subspaces have been motivated as a data reduction (for\\nviewing) tool. Can they also be used for classiﬁcation, and what is the\\nrationale? Clearly they can, as in our original derivation; we simply limit\\nthe distance-to-centroid calculations to the chosen subspace. One can show\\nthat this is a Gaussian classiﬁcation rule with the additional restriction\\nthat the centroids of the Gaussians lie in aL-dimensional subspace of IRp.\\nFitting such a model by maximum likelihood, and then constructing the\\nposterior probabilities using Bayes’ theorem amounts to the classiﬁcation\\nrule described above (Exercise 4.8).\\nGaussian classiﬁcation dictates the logπk correction factor in the dis-\\ntance calculation. The reason for this correction can be seen in Figure 4.9.\\nThe misclassiﬁcation rate is based on the area of overlap between the two\\ndensities. If the πk are equal (implicit in that ﬁgure), then the optimal\\ncut-point is midway between the projected means. If theπk are not equal,\\nmoving the cut-point toward thesmaller class will improve the error rate.\\nAs mentioned earlier for two classes, one can derive the linear rule using\\nLDA (or any other method), and then choose the cut-point to minimize\\nmisclassiﬁcation error over the training data.\\nAs an example of the beneﬁt of the reduced-rank restriction, we return\\nto the vowel data. There are 11 classes and 10 variables, and hence 10\\npossible dimensions for the classiﬁer. We can compute the training and\\ntest error in each of these hierarchical subspaces; Figure 4.10 shows the\\nresults. Figure 4.11 shows the decision boundaries for the classiﬁer based\\non the two-dimensional LDA solution.\\nThere is a close connection between Fisher’s reduced rank discriminant\\nanalysis and regression of an indicator response matrix. It turns out that', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cacc8877-4ebd-413f-8eea-75a275feb2ac', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 136, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='118 4. Linear Methods for Classiﬁcation\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nCanonical Coordinate 1\\nCanonical Coordinate 2\\nClassification in Reduced Subspace\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nFIGURE 4.11. Decision boundaries for the vowel training data, in the two-di-\\nmensional subspace spanned by the ﬁrst two canonical variates. Note that in\\nany higher-dimensional subspace, the decision boundaries are higher-dimensional\\naﬃne planes, and could not be represented as lines.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2c4a25f2-734a-492f-a18b-f5052eb46bb7', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 137, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Logistic Regression 119\\nLDA amounts to the regression followed by an eigen-decomposition of\\nˆYTY. In the case of two classes, there is a single discriminant variable\\nthat is identical up to a scalar multiplication to either of the columns ofˆY.\\nThese connections are developed in Chapter 12. A related fact is that if one\\ntransforms the original predictorsX to ˆY, then LDA usingˆY is identical\\nto LDA in the original space (Exercise 4.3).\\n4.4 Logistic Regression\\nThe logistic regression model arises from the desire to model the posterior\\nprobabilities of theK classes via linear functions inx, while at the same\\ntime ensuring that they sum to one and remain in [0,1]. The model has\\nthe form\\nlog Pr(G =1|X = x)\\nPr(G = K|X = x) = β10 +βT\\n1 x\\nlog Pr(G =2|X = x)\\nPr(G = K|X = x) = β20 +βT\\n2 x\\n...\\nlog Pr(G = K−1|X = x)\\nPr(G = K|X = x) = β(K−1)0 +βT\\nK−1x.\\n(4.17)\\nThe model is speciﬁed in terms ofK−1 log-odds or logit transformations\\n(reﬂecting the constraint that the probabilities sum to one). Although the\\nmodel uses the last class as the denominator in the odds-ratios, the choice\\nof denominator is arbitrary in that the estimates are equivariant under this\\nchoice. A simple calculation shows that\\nPr(G = k|X = x)= exp(βk0 +βT\\nk x)\\n1+ ∑K−1\\nℓ=1 exp(βℓ0 +βT\\nℓ x)\\n,k =1 ,...,K −1,\\nPr(G = K|X = x)= 1\\n1+ ∑K−1\\nℓ=1 exp(βℓ0 +βT\\nℓ x)\\n, (4.18)\\nand they clearly sum to one. To emphasize the dependence on the entire pa-\\nrameter setθ= {β10,βT\\n1 ,...,β (K−1)0,βT\\nK−1}, we denote the probabilities\\nPr(G = k|X = x)= pk(x;θ).\\nWhen K = 2, this model is especially simple, since there is only a single\\nlinear function. It is widely used in biostatistical applications where binary\\nresponses(twoclasses)occurquitefrequently.Forexample,patientssurvive\\nor die, have heart disease or not, or a condition is present or absent.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a734356e-3f21-4472-a08e-d28d4b52ef6b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 138, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='120 4. Linear Methods for Classiﬁcation\\n4.4.1 Fitting Logistic Regression Models\\nLogistic regression models are usually ﬁt by maximum likelihood, using the\\nconditional likelihood ofG givenX. Since Pr(G|X) completely speciﬁes the\\nconditional distribution, themultinomial distribution is appropriate. The\\nlog-likelihood forN observations is\\nℓ(θ)=\\nN∑\\ni=1\\nlogpgi(xi;θ), (4.19)\\nwhere pk(xi;θ)=P r (G = k|X = xi;θ).\\nWe discuss in detail the two-class case, since the algorithms simplify\\nconsiderably. It is convenient to code the two-classgi via a 0/1r e s p o n s eyi,\\nwhere yi =1w h e ngi =1 ,a n dyi =0w h e ngi =2 .L e tp1(x;θ)= p(x;θ),\\nand p2(x;θ)=1 −p(x;θ). The log-likelihood can be written\\nℓ(β)=\\nN∑\\ni=1\\n{\\nyi logp(xi;β)+(1 −yi)log(1 −p(xi;β))\\n}\\n=\\nN∑\\ni=1\\n{\\nyiβTxi −log(1+ eβT xi)\\n}\\n. (4.20)\\nHere β= {β10,β1}, and we assume that the vector of inputsxi includes\\nthe constant term 1 to accommodate the intercept.\\nTo maximize the log-likelihood, we set its derivatives to zero. Thesescore\\nequations are\\n∂ℓ(β)\\n∂β=\\nN∑\\ni=1\\nxi(yi −p(xi;β)) = 0, (4.21)\\nwhich arep+1 equationsnonlinearin β.N o t i c et h a ts i n c et h eﬁ r s tc o m p o -\\nnentof xi is1,theﬁrstscoreequationspeciﬁesthat ∑N\\ni=1 yi =∑N\\ni=1 p(xi;β);\\nthe expectednumber of class ones matches the observed number (and hence\\nalso class twos.)\\nTo solve the score equations (4.21), we use the Newton–Raphson algo-\\nrithm, which requires the second-derivative or Hessian matrix\\n∂2ℓ(β)\\n∂β∂βT =−\\nN∑\\ni=1\\nxixi\\nTp(xi;β)(1−p(xi;β)). (4.22)\\nStarting withβold, a single Newton update is\\nβnew = βold −\\n⎤∂2ℓ(β)\\n∂β∂βT\\n⎦−1\\n∂ℓ(β)\\n∂β, (4.23)\\nwhere the derivatives are evaluated atβold.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c88b5ea7-4f33-418a-8ec2-34afaea6ff94', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 139, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Logistic Regression 121\\nIt is convenient to write the score and Hessian in matrix notation. Let\\ny denote the vector ofyi values, X the N × (p +1 )m a t r i xo fxi values,\\np the vector of ﬁtted probabilities withith element p(xi;βold)a n dW a\\nN×N diagonal matrix of weights withith diagonal elementp(xi;βold)(1−\\np(xi;βold)). Then we have\\n∂ℓ(β)\\n∂β = XT(y−p) (4.24)\\n∂2ℓ(β)\\n∂β∂βT = −XTWX (4.25)\\nThe Newton step is thus\\nβnew = βold +(XTWX)−1XT(y−p)\\n=( XTWX)−1XTW\\n⎤\\nXβold +W−1(y−p)\\n⎦\\n=( XTWX)−1XTWz. (4.26)\\nIn the second and third line we have re-expressed the Newton step as a\\nweighted least squares step, with the response\\nz = Xβold +W−1(y−p), (4.27)\\nsometimes known as theadjusted response. These equations get solved re-\\npeatedly, since at each iterationp changes, and hence so doesW and z.\\nThis algorithm is referred to asiteratively reweighted least squaresor IRLS,\\nsince each iteration solves the weighted least squares problem:\\nβnew ←argmin\\nβ\\n(z−Xβ)TW(z−Xβ). (4.28)\\nIt seems thatβ= 0 is a good starting value for the iterative procedure,\\nalthough convergence is never guaranteed. Typically the algorithm does\\nconverge, since the log-likelihood is concave, but overshooting can occur.\\nIn the rare cases that the log-likelihood decreases, step size halving will\\nguarantee convergence.\\nFor the multiclass case (K ≥3) the Newton algorithm can also be ex-\\npressed as an iteratively reweighted least squares algorithm, but with a\\nvectorof K−1 responses and a nondiagonal weight matrix per observation.\\nThe latter precludes any simpliﬁed algorithms, and in this case it is numer-\\nically more convenient to work with the expanded vectorθdirectly (Ex-\\nercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can\\nbe used to maximize the log-likelihood eﬃciently. TheR package glmnet\\n(Friedman et al., 2010) can ﬁt very large logistic regression problems ef-\\nﬁciently, both in N and p. Although designed to ﬁt regularized models,\\noptions allow for unregularized ﬁts.\\nLogistic regression models are used mostly as a data analysis and infer-\\nence tool, where the goal is to understand the role of the input variables', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='18d8c275-c2ff-4ce1-81f1-4785db1b83b4', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 140, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='122 4. Linear Methods for Classiﬁcation\\nTABLE 4.2. Results from a logistic regression ﬁt to the South African heart\\ndisease data.\\nCoeﬃcient Std. Error Z Score\\n(Intercept) −4.130 0 .964 −4.285\\nsbp 0.006 0 .006 1 .023\\ntobacco 0.080 0 .026 3 .034\\nldl 0.185 0 .057 3 .219\\nfamhist 0.939 0 .225 4 .178\\nobesity -0.035 0 .029 −1.187\\nalcohol 0.001 0 .004 0 .136\\nage 0.043 0 .010 4 .184\\nin explaining the outcome. Typically many models are ﬁt in a search for a\\nparsimonious model involving a subset of the variables, possibly with some\\ninteractions terms. The following example illustrates some of the issues\\ninvolved.\\n4.4.2 Example: South African Heart Disease\\nHere we present an analysis of binary data to illustrate the traditional\\nstatistical use of the logistic regression model. The data in Figure 4.12 are a\\nsubset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried\\nout in three rural areas of the Western Cape, South Africa (Rousseauw et\\nal., 1983). The aim of the study was to establish the intensity of ischemic\\nheart disease risk factors in that high-incidence region. The data represent\\nwhite males between 15 and 64, and the response variable is the presence or\\nabsence of myocardial infarction (MI) at the time of the survey (the overall\\nprevalence of MI was 5.1% in this region). There are 160 cases in our data\\nset, and a sample of 302 controls. These data are described in more detail\\nin Hastie and Tibshirani (1987).\\nWe ﬁt a logistic-regression model by maximum likelihood, giving the\\nresults shown in Table 4.2. This summary includesZ scores for each of the\\ncoeﬃcients in the model (coeﬃcients divided by their standard errors); a\\nnonsigniﬁcant Z scoresuggestsacoeﬃcientcanbedroppedfromthemodel.\\nEach of these correspond formally to a test of the null hypothesis that the\\ncoeﬃcient in question is zero, while all the others are not (also known as\\nthe Wald test). AZ score greater than approximately 2 in absolute value\\nis signiﬁcant at the 5% level.\\nThere are some surprises in this table of coeﬃcients, which must be in-\\nterpreted with caution. Systolic blood pressure (sbp) is not signiﬁcant! Nor\\nis obesity, and its sign is negative. This confusion is a result of the corre-\\nlation between the set of predictors. On their own, bothsbp and obesity\\nare signiﬁcant, and with positive sign. However, in the presence of many', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='769bcc12-adde-418f-b48f-dee59204a0c9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 141, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Logistic Regression 123\\nsbp\\n0 1 02 03 0\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\no\\no o\\noo\\noooo\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\nooo\\no\\no\\noo\\no o\\no\\no\\noo ooo oo\\no oooo oooooo\\no\\noo\\noo o\\noo oooooo\\no\\no\\no\\noooo\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\noo\\no\\noo\\noooo\\no\\no\\no\\noo o\\noo\\noo oo\\nooooo\\no\\nooo\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no oooo\\no\\no\\no\\no\\no\\noo\\no\\nooo\\no\\no\\no\\noooo\\nooo\\no oooo\\noo\\noooooo\\noo\\no\\no\\noo\\no\\no\\no\\no ooo\\no\\noo\\no\\no\\nooo\\no oo\\no\\noo\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\noo o\\noo o\\no\\nooo o o\\noo\\no\\no\\noo oo\\no\\no\\no\\no\\nooo\\no\\no\\no\\no o\\nooo\\noooo\\no\\no\\noooo o\\no\\noo\\no\\nooo\\no\\nooo\\no\\no\\noo\\no\\no\\noo\\no\\noo o\\no\\noo\\no\\noo\\no\\noo\\noo\\noo\\no\\nooo\\noo o\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\noooo oo\\no\\noo\\no\\no o\\no\\no\\noo\\no o\\noo o\\noo ooo\\no\\no\\nooo o\\no\\no oo\\no\\no\\no\\no o\\noo\\no\\no\\noo oo\\no\\no\\no\\no\\no\\no\\noo\\noo o\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noooo\\noooo\\noooooooo\\noo\\no\\noooo\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no oo\\noo\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\nooo\\noo\\noooo\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\noo\\no\\no\\no\\noo o oo\\no\\no\\no o\\noo\\no\\no\\no oo\\nooo\\noo ooooo ooo oo\\no\\noo\\no oo\\noooooooo\\no\\no\\no\\nooo o\\no\\no\\no\\nooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\noo oo\\no ooo\\no\\no\\no\\noo o\\noo\\noo oo ooooo\\no\\nooo\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no ooo o\\no\\no\\no\\no\\no\\noo\\no\\nooo\\no o\\no\\noooo\\no oo\\no o ooo\\noo\\nooo ooo\\no o\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\noo o\\no o\\no\\no\\no\\no\\noo o\\noo o\\no\\nooo\\noooo\\no\\no\\noooo\\no\\no\\no\\no\\noooo\\no\\no\\no o\\nooo o\\noo o\\no\\no\\noooo o\\no\\no o\\no\\nooo\\no\\nooo\\no\\no\\noo\\no\\no\\no o\\no\\nooo\\no\\noo\\no\\noo\\no\\noo\\no o\\no o\\no\\no oo\\no oo\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\no oooo o\\no\\noo\\no\\no o\\no\\no\\no o\\no o\\nooo\\no\\noooo\\no\\no\\no ooo\\no\\no oo\\no\\no\\no\\noo\\noo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\noo\\noo o\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo oo\\noooo\\no\\noooo ooo\\noo\\no\\nooo oo o\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\n0.0 0.4 0.8\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\noo\\no ooo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\nooo oo\\no\\no\\noo\\noo\\no\\no\\no o\\no\\noo oo\\no ooooo oooo o\\no\\noo\\no oo\\nooo o\\noooo\\no\\no\\no\\no oo\\no\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\no oo\\no o\\noooo\\no\\no\\no\\no\\noo\\noo\\noooo oo\\noo o\\no\\nooo\\no\\no\\no\\nooo\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\nooooo\\no\\no\\no\\noo\\noo\\no\\no oo\\no o\\no\\noo o\\no\\no oo\\no\\nooo o\\no o\\noo oo oo\\noo\\no\\no\\noo\\no\\no\\no\\no ooo\\no\\no o\\no\\no\\nooo\\nooo\\no\\noo\\no o\\no\\no\\noo o\\noo\\no\\no\\no\\no\\nooo\\nooo\\no\\no oooooo\\no\\no\\nooo o\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\nooooooo\\no\\no\\nooooo\\no\\noo\\no\\noo o\\no\\noo\\no\\no\\no\\noo\\no\\no\\noo\\no\\no oo\\no\\noo\\no\\noo\\no\\noo\\noo\\noo\\no\\noo o\\noo o\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\nooooo o\\no\\noo o\\noo\\no\\no\\noo\\no o\\noo o\\no\\no\\no oo\\no\\no\\noo\\noo\\no\\nooo\\no\\no\\no\\noo\\noo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\noo\\nooo\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo oo\\noooo\\no\\no oo oooo\\no o\\no\\noo oo oo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\noo\\nooo\\noo\\no\\no\\no\\no oo\\no o\\no ooo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\noo ooo\\no\\no\\noo\\noo\\no\\no\\nooooo o oo oooo oo oo oo\\no\\noo\\nooo\\no oo\\noo oo o\\no\\no\\no\\no oo\\no\\no\\no\\no\\nooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\noo\\noo\\noooo\\noo\\no\\no\\noo\\noo\\noo oo\\noo\\noo o\\no\\no o o\\no\\no\\no\\nooo\\no\\nooo\\nooo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no ooo o\\no\\no\\no\\no\\no\\noo\\no\\nooo\\no\\no\\no\\nooo o\\nooo\\noo ooo\\noo\\noo oo oo\\noo\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\noo\\no\\no\\nooo\\nooo\\no\\noo\\noo\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\noo\\nooo\\no\\no oo oooo\\no\\no\\no o oo\\no\\no\\no\\no\\noo oo\\no\\no\\no\\no\\no oo\\noooo\\no\\no\\noooo o\\no\\no o\\no\\noo o\\no\\no o o\\no\\no\\no o\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\noo\\no\\noo\\noo\\noo\\no\\noo o\\nooo\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\noo\\no o oo\\no\\noo\\no\\noo\\no\\no\\noo\\noo\\no oo o\\nooo o\\no\\no\\nooo o\\no\\no oo\\no\\no\\no\\no o\\noo\\no\\no\\nooo o\\no\\no\\no\\no\\no\\no\\noo\\no oo\\no\\noo\\no\\no\\nooo\\no\\no\\no\\no\\no oo\\no ooo\\no\\noo o oooo\\noo\\no\\no oo ooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\n0 50 100\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no oo\\no o\\no\\no\\no\\nooo\\noo\\noooo\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no o\\no\\no\\no\\no ooo o\\no\\no\\no o\\noo\\no\\no\\noo ooo o\\nooooo\\no oooooo\\no\\noo\\nooo\\no oo\\no\\noooo\\no\\no\\no\\no ooo\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\no\\nooo o\\noooo\\no\\no\\no\\no\\noo\\no o\\noo o oooooo\\no\\noo o\\no\\no\\no\\noo\\no\\no\\nooo o\\noo\\no\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\nooooo\\no\\noo\\no\\no\\noo\\no\\no oo\\noo\\no\\noooo\\nooo\\no\\noooo\\noo\\noo oo oooo\\no\\no\\noo\\no\\no\\no\\noooo\\no\\noo\\no\\no\\nooo\\nooo\\no\\noo\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\noo\\noo o\\no\\nooo\\noooo\\no\\no\\noooo\\no\\no\\no\\no\\nooo o\\no\\no\\no\\no\\noooo\\nooo\\no\\no\\no ooo o\\no\\noo\\no\\nooo\\no\\no o\\no\\no\\no\\no o\\no\\no\\no o\\no\\no oo\\no\\noo\\no\\noo\\no\\noo\\noo\\noo\\no\\noo o\\nooo\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\noo oooo\\no\\noo\\no\\noo\\no\\no\\noo\\noo ooo o\\no oo o\\no\\no\\nooo o\\no\\noo o\\no\\no\\no\\noo\\noo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\noo\\no oo\\no\\no o\\no\\no\\no o o\\no\\no\\no\\no o oo\\noooo\\no\\no\\noooooo\\noo\\no\\no ooooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\n100 160 220\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\nooo\\no o\\no\\no\\no\\no oo\\no o\\noo oo\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no o\\no o\\no\\noo\\no o o\\no\\no\\noo\\no o\\no\\no\\no o\\nooo o\\noo ooo o oo o oo o\\no\\noo\\nooo\\no oo\\no\\no oo o\\no\\no\\no\\noo o\\no\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\nooo o o\\no oo o\\no\\no\\no\\no o o\\noo\\noo oo\\noooo o\\no\\no o o\\no\\no\\no\\noo o\\no\\nooo\\no\\noo\\no\\no\\no\\no\\no\\no o\\no o\\no\\no\\no\\no\\no\\no o\\no\\noooo o\\no\\no\\no\\no\\no\\noo\\no\\no oo\\no o\\no\\noooo\\no o\\no\\no oo oo\\no\\no\\no\\nooooo\\noo\\no\\no\\noo\\no\\no\\no\\noo o\\no\\no\\no o\\no\\no\\nooo\\no o o\\no\\noo\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no o\\no o o\\no\\nooo o o\\noo\\no\\no\\no ooo\\no\\no\\no\\no\\nooo o\\no\\no\\no\\no\\noooo oo\\no\\no\\no\\no ooo o\\no\\noo\\no\\noo o\\no\\noo\\no\\no\\no\\noo\\no\\no\\noo\\no\\no oo\\no\\noo\\no\\noo\\no\\noo\\no o\\noo\\no\\nooo\\no o o\\no\\no o\\no\\no\\noo\\no\\no\\no\\no\\noooo oo\\no\\noo\\no\\noo\\no\\no o o\\no o\\noo oo\\noo oo\\no\\no\\noo\\no o\\no\\nooo\\no\\no\\no\\noo\\noo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\nooo\\no\\no\\no\\nooo o\\noooo\\no\\nooooo o o\\noo\\no\\nooo ooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\n01 0 2 0 3 0\\no\\noo\\no\\no\\no ooo o\\no\\no\\no o\\no\\no\\no o\\no\\no\\no\\no\\noo\\no\\noo o\\no\\noo o\\noo\\noo oo\\no\\no\\no oooo\\no o\\nooo\\noo o\\no\\noo\\no\\no\\no o oo\\no\\noo\\noo\\no\\no\\nooo\\no\\nooooo\\no\\noo\\no\\no\\no\\noo\\no\\nooooo oo ooooo\\no\\no\\noo oo oo\\no\\nooo\\no\\no o\\no\\no\\no\\noooooo\\no oo\\noo\\no\\noo\\noo\\noooo ooo\\no\\noooo oo oo\\no\\noo\\noo o\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\noo\\no ooo oooooo\\no\\noo oo\\noo\\no\\noo\\noooo o ooo oooo oooo o\\no\\no oo\\nooo oo\\no\\nooo\\no\\no oo\\noooo\\no\\no oo\\no\\noo o\\no\\no o\\noo\\noo o\\no\\no\\noo\\noo\\no o\\no o\\no oo\\noo\\no\\noooo\\no\\no\\noo\\no\\no\\no\\nooo\\no\\no\\no\\noo\\nooo o o\\no\\no\\nooooooo o ooooo\\no\\no\\noo oooo\\no\\no\\noo\\no\\no\\noo\\no\\nooo\\no\\no\\no\\no oo\\no ooo\\no\\nooo\\no\\no\\no\\nooo\\no\\nooo o\\noo o\\noo\\no\\no\\no\\no o\\no\\no oo\\noo o\\no\\no\\no\\nooo oo\\no o\\noo\\no\\no\\noo\\nooo oo\\no\\noo\\no\\no\\no\\noo\\no\\no o oo\\no\\no\\no\\noo\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\noo oo\\no\\nooooooooooo\\noo\\no\\noooooo ooo\\noo oo o o\\nooo\\no\\ntobacco o\\noo\\no\\no\\nooo\\no o\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\nooo\\nooo o\\noo\\noo oo\\no\\no\\nooo oo\\no o\\noo\\no\\noo o\\no\\no o\\no\\no\\nooo o\\no\\noo\\noo\\no\\no\\nooo\\no\\nooo oo\\no\\noo\\noo\\no\\noo\\no\\noooooo o oooo\\no\\no\\no\\noooooo\\no\\noo o\\no\\no o\\no\\no\\no\\no\\no\\no ooo\\noo o\\noo\\nooo\\noo\\noo ooooo\\no\\nooo\\nooo oo\\no\\noo\\noo o\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\noo\\nooo ooo\\nooo o\\no\\noo\\noo\\noo\\no\\no o\\noooo ooooo ooo o oooo\\no\\nooo\\noooo o\\no\\no oo\\no\\no oo\\noo o o\\no\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\no\\nooo\\no\\no o\\noo\\noo\\no\\noo\\no\\noooo\\no\\no\\no o\\no\\no\\no\\nooo oo\\no\\no o\\noooo\\noo\\no\\nooo ooo oo ooooo\\no\\no\\no oo ooo\\no\\no\\noo\\no\\no\\noo\\no\\noo o\\no\\no\\no\\nooo\\noo oo\\no\\nooo o\\no\\no\\noo o\\no\\no ooo\\noo o\\noo\\no\\no\\no\\no o\\no\\nooo\\nooo\\no\\no\\no\\no oooo\\noo\\noo\\noo\\no o\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\no o\\no\\no\\no\\no o\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\no\\no\\noo oo\\no\\nooo ooooo ooo\\noo\\no\\nooo oo oooo\\no\\noooo ooo o\\no\\no\\no o\\no\\no\\noo o\\noo\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\noo\\no\\nooo\\no\\nooo\\noooooo\\no\\no\\nooo oo\\no o\\noo\\no\\nooo\\no\\noo\\no\\no\\no oo o\\no\\noo\\noo\\no\\no\\nooo\\no\\noooo oo\\noo\\no o\\no\\noo\\no\\noooooo\\nooo oo\\no\\no o\\no ooooo\\no\\nooo\\no\\no o\\no\\no\\noo o\\noooo\\noo o\\no o\\no oo\\noo\\noo oo oo o\\no\\nooooo ooo\\no\\noo\\noo o\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\noo\\nooooo o\\noooo\\no\\no o\\noo\\no\\no\\no\\no o\\noo oooo ooo ooo ooo oo o\\noo o\\no oooo\\no\\nooo\\no\\no oo\\nooo o\\no\\nooo\\no\\nooo\\no\\no o\\noo\\no ooo\\no\\no o\\noo\\noo\\noo\\noo\\no\\noo\\no\\noo oo\\no\\no\\noo\\no\\no\\no\\noo oo\\no\\no\\noo\\noooooo\\no\\noooooooo ooooo\\no\\no\\nooo oo o\\no\\no\\no oo\\no\\noo\\no\\no oo\\no\\no\\no\\noo o\\noo oo\\no\\noo o\\no\\no\\no\\nooo\\nooo oo\\noo o\\noo\\no\\no\\no\\noo\\no\\nooo\\no oo\\no\\no\\nooooo o\\no\\no\\noo\\no o\\noo\\no oooo\\no\\noo\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no o\\no\\no\\no\\noo\\no o\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\nooo oo\\no\\nooooo oo oooo\\no o\\no\\noo oo ooooo\\no\\nooo oo\\nooo\\no\\no\\noo\\no\\no\\nooo\\no o\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\no o\\no\\no o\\no\\no\\nooo\\noooo o o\\no\\no\\nooo o o\\noo\\noo\\no\\nooo\\no\\noo\\no\\no\\noooo\\no\\noo\\noo\\no\\no\\nooo o\\no oo oo\\no\\no\\no\\no\\no\\no\\no o\\no\\noo oo o\\noo oo ooo\\no\\no\\no ooooo\\no\\noo o\\no\\no o\\no\\no\\no\\no o oo oo\\noo o\\no o\\no\\noo\\noo\\noooooo o\\no\\no o oooo oo\\no\\noo\\nooo\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\noo\\noooo oo ooo o\\no\\no o\\noo\\no\\no\\no\\noo\\noo oo ooo o ooooo ooo oo\\noo o\\no oooo\\no\\nooo\\no\\no oo\\noo oo\\no\\nooo\\no\\nooo\\no\\noo\\noo\\nooo oo\\noooo\\noo\\noo\\noo\\no\\noo\\no\\noo oo\\no\\no\\noo\\no\\no\\no\\no oooo\\no\\noo\\no ooo oo\\no\\no ooo ooo o o oooo\\no\\no\\no o ooo o\\no\\no\\no o\\no\\no\\no o\\noooo\\no\\no\\no\\noo o\\no ooo\\no\\noo oo\\no\\no\\nooo\\no\\noooo\\noo o\\noo\\no\\no\\no\\no o\\no\\noo o\\nooo\\no\\no\\noooo oo\\no\\no\\noo\\no\\no\\noooo o\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo oo\\no\\nooooooooooo\\noo\\no\\no oo ooo ooo\\no\\nooo oo\\noo o\\no\\no\\noo\\no\\no\\nooo\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo o\\no\\noo o\\nooo ooo\\no\\no\\nooooo\\noo\\no o\\noo oo\\no\\no\\no\\no\\no\\noooo\\no\\noo\\noo\\noo\\nooo\\no\\nooooo\\no\\noo\\noo\\no\\no o\\no\\nooooo oooo oo\\no\\no\\no\\noooo oo\\no\\nooo\\no\\noo\\no\\no\\no\\no ooooo\\nooo\\no o\\no\\no o\\noo\\no oooooo\\no\\noo o\\nooooo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\noo\\noooooo\\noooo\\no\\noo\\no o\\no\\no\\no\\no o\\noooooooo ooo o\\nooooo\\no\\noo o\\nooooo\\no\\nooo\\no\\nooo\\noooo\\no\\noo o\\no\\noo o\\no\\noo\\noo\\noo ooo\\noooo\\no o\\noo\\no o\\no\\noo\\no\\noooo\\no\\no\\noo\\no\\no\\no\\nooooo\\no\\noo\\noo o o\\noo\\no\\noooo oooo oo ooo\\no\\noooooo o\\no\\no\\nooo\\no\\no o\\no\\noo o\\no\\no\\no\\no oo\\noooo\\no\\noo oo\\no\\no\\no oo\\no\\nooo o\\noo o\\noo\\no\\no\\no\\noo\\no\\no o o\\no oo\\no\\no\\nooo ooooo\\no o\\noo\\noo ooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\noo\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\no\\no o oo\\no\\nooooooooooo\\noo\\no\\no ooooo ooo\\nooo ooo\\nooo\\no\\no\\noo\\no\\no\\noo o\\no o\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no o o\\no\\nooo\\noo oo o o\\no\\no\\nooo oo\\no o\\noo\\no\\no oo\\no\\no\\no\\no\\no\\no oo o\\no\\noo\\noo\\no\\no\\noo o o\\no oo ooo\\no o\\noo\\no\\noo\\no\\noo oo oo o o oo o o\\no\\no\\no ooo oo\\no\\nooo\\no\\no o\\no\\no\\no\\no oo oo o\\no\\no o\\no o\\nooo\\noo\\noo oooo o\\no\\no o o\\nooo oo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\noo\\no\\noo o oo\\nooo o\\no\\noo\\noo\\no\\no\\no\\no o\\noo oooooo\\no ooo oo oooo\\nooo\\noo o oo\\no\\nooo\\no\\no o o\\no ooo\\no\\noo o\\no\\nooo\\no\\noo\\noo\\nooo\\no\\no\\noo\\no o\\noo\\no o\\noo\\no\\no o\\no\\nooo o\\no\\no\\noo\\no\\no\\no\\nooo o o\\no\\noo\\noo o o\\noo\\no\\noooo ooo o o\\no ooo\\no\\nooo o oo o\\no\\no\\noo\\no\\no\\noo\\no\\nooo\\no\\no o\\no oo\\noo oo\\no\\noo o\\no\\no\\no\\no oo\\no\\no o oo\\no oo\\no o\\no\\no\\no\\no o\\no\\nooo\\nooo\\no\\no o oooo o oo\\noo\\no o\\no oo oo oo\\no\\noo\\no\\noo\\no\\no\\no\\no o oo\\no\\no\\no\\no o\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\nooo o\\no\\nooooooooo o ooo\\no\\nooo ooo ooo\\no\\nooo ooooo\\no\\nooo\\no\\no\\no\\nooo\\no\\noo\\no ooo\\no\\no oo\\no o\\no\\noo\\no\\no\\nooo\\no\\nooo o\\no\\no\\no\\no\\noo\\no\\no\\noo o\\noo\\no\\no\\no\\no\\no\\no o\\no\\no oo o\\no\\no oo\\noooo\\no\\noo oooo\\no\\noo ooo\\noo\\no\\nooo oo\\nooo\\noo\\no\\nooo\\no\\no\\no\\noo oo\\noo oo\\no\\no o\\no\\no\\no\\no\\no oo\\no\\nooo\\no o\\no\\noo\\no\\noo\\noo\\no\\no\\no\\no ooo\\nooooo\\noo\\nooo o\\nooo\\no\\noo\\no\\no\\noo\\noo\\nooo\\no\\no\\no\\no o\\no\\no\\noo\\no\\noo\\no\\no o\\no o\\no\\no\\no\\noooo\\nooo o\\nooo o\\no\\no\\no\\noooo o ooo\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\nooo\\noo\\no\\noo o\\no\\no\\noo ooo oo\\no oo o\\no\\no\\noo o\\noo\\noo\\no\\no\\no\\noo\\noo\\no\\nooo oo\\no\\no oo\\no\\nooo\\no\\no\\no\\noo\\nooo o oo\\noooo\\noo\\no\\no\\no\\no\\noooo\\no oo\\no\\no\\no\\noo\\noooo\\no\\no o o\\no\\noo\\no oo oo ooo o\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\no oo o\\no\\noo\\noo\\noo o\\no\\no o o\\noo ooo o\\no\\nooo\\no\\noooo o\\no\\no\\no\\noo o\\no\\no\\no oo\\no oooo\\no\\no\\no\\nooo o o\\no\\no\\no ooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo o\\no ooo\\no\\noooo\\no\\noo ooo\\nooo\\no\\no\\no\\nooo\\no\\no\\no o\\noo o\\no\\no\\no o\\noo\\no\\no\\no o\\noo\\no\\no\\no\\nooo\\no\\no\\no\\noo oo\\no\\noo o\\no o\\no\\no o\\no\\no\\nooo\\no\\no ooo\\no\\no\\no\\no\\noo\\no\\nooo o\\noo\\no\\no\\no\\no\\no\\noo\\no\\no ooo\\no\\no oo\\no ooo\\no\\nooo o\\noo\\no\\nooooo\\noo\\nooo ooo\\nooo\\no o\\no\\nooo\\no\\no\\no\\noooo\\noo oo\\no\\no o\\no\\no\\no\\no\\noo o\\no\\nooo\\no o\\no\\noo\\no\\noooo\\no\\no\\no\\noooo\\nooooo\\noo\\noo o\\no\\nooo\\no\\noo\\no\\no\\noo\\noooo o\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\noo\\no\\no\\nooo o\\no\\no\\no\\noo\\noooo ooo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo o\\no\\no\\no\\nooo\\no\\noo\\noo oooo\\no ooo\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no o\\no\\nooo o o\\no\\no oo\\no\\nooo\\no\\no\\no\\no\\no\\nooooo o\\noooo\\noo\\no\\no\\no\\no\\noooo\\nooo\\no\\no\\no\\noo oooo\\no\\nooo\\no\\noo\\no ooo ooooo\\no\\no oo\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no o\\noo\\no\\noo\\noo\\nooo\\no\\no oo\\nooo\\no\\noo o\\noo o\\no\\no ooo o\\no\\no\\no\\no oo\\no\\no\\no\\nooooo oo\\no\\no\\no\\no oo o\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no oo oo oooo\\no\\no ooo\\no\\nooo\\noo\\nooo\\no\\no\\no\\nooo\\no\\no\\noo\\noo o\\no\\no\\noo\\no\\no\\no\\no\\no\\nldl\\noo o\\no\\no\\no\\no oo\\no\\no\\no\\nooo o\\no\\nooo\\no o\\no\\noo\\no\\no\\no oo\\no\\no ooo\\no\\no\\no o\\noo\\no\\no\\nooo\\noo\\noo\\no\\no\\no\\noo\\no\\nooo o\\no\\nooo\\no oo\\no\\no\\noooo\\noo\\no\\no oo oo\\no o\\no ooo oo\\nooo\\no\\no\\no\\no oo\\no\\no\\no\\no ooo\\noo oo\\no\\no o\\no\\no\\no\\no\\noo o\\no\\nooo\\noo\\no\\no o\\no\\noo\\noo\\no\\no\\no\\no oo o\\no oooo\\no o\\nooo\\no\\nooo\\nooo\\noo\\noo\\noooooo\\no\\no\\noo\\no\\no\\no o\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no ooo\\no ooo\\no\\no\\no\\noo\\no oo o\\noo\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no o\\noo\\no\\no oo\\no\\no\\no\\noooo oo\\noo oo\\no\\no\\no oo\\no o\\no\\no\\no\\no\\nooo\\noo\\no\\no oooo\\no\\noo o\\no\\noo o\\no\\no\\no\\noo\\noooooo\\noooo\\noo\\no\\no\\no\\no\\noooo\\no oo\\no\\no\\no\\no o\\no\\nooo\\no\\no oo\\no\\no o\\no oo ooo ooo\\no\\noo\\no\\no o\\no\\no\\no oo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noo\\noo\\no\\noo\\noo\\no o\\no\\no\\no oo\\nooo\\no\\noo\\no\\noo o\\no\\noo ooo\\no\\no\\no\\noo o\\no\\no\\no\\noo\\nooooo\\no\\no\\no\\nooo oo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\nooooo oooo\\no\\no\\nooo\\no\\noo o\\no o\\nooo\\no\\no\\no\\noo o\\no\\no\\noo\\noo o\\no\\no\\no o\\no\\no\\no\\no\\no o oo\\no\\no\\no\\nooo\\no\\noo\\noooo\\no\\noo\\no\\no o\\no\\noo\\no\\no\\no oo\\no\\no ooo\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no o\\no\\no\\no\\no\\no\\noo\\no\\noooo\\no\\nooo\\no o oo\\no\\nooo o\\no o\\no\\nooooo\\noo\\noo oo oo oo o\\noo\\no\\no oo\\no\\no\\no\\no ooo\\noo oo\\no\\noo\\no\\no\\no\\no\\nooo\\no\\no oo\\noo\\no\\no o\\no\\noo\\noo o\\no\\no\\nooo o\\noo o oo\\noo\\noooo\\noo\\no\\no\\noo\\no\\no\\noo\\nooo oo\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\noo\\noo\\no\\no\\no\\noooo\\no oo o\\noo\\noo\\no\\no\\no\\no ooo oooo o\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\nooo\\no o\\no\\nooo\\no\\noooooooo\\no ooo\\no\\no\\nooo\\no o\\no\\no\\no\\no\\noo o\\noo\\no\\no oo oo\\no\\no oo\\no\\no oo\\no\\no\\no\\no\\no\\no ooo oo\\noo oo\\no o\\no\\no\\no\\no\\nooo o\\no oo\\no\\no\\no\\no o\\noo o o\\no\\noo o\\no\\noo\\no ooooo oo o\\no\\noo o\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooo\\noo ooo o\\no\\no oo\\no\\nooo oo\\no o\\no\\no o o\\no\\no\\no\\noo\\noo o oo\\no\\no\\no\\noo oo o\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\no\\noooo oooo\\no\\noo oo\\no\\nooo o o\\nooo\\no\\no\\no\\no oo\\no\\no\\no o\\nooo\\no\\no\\no\\nooo\\no\\no\\no ooo\\no\\no\\no\\nooo\\no\\no\\no\\noooo\\no\\no oo\\noo\\no\\noo\\no\\no\\nooo\\no\\noooo\\no\\no\\noo\\noo\\no\\noooo\\noo\\no\\noo\\no\\no\\noo\\no\\noooo\\no\\no oo\\no ooo\\no\\nooo o\\noo\\no\\noo o oo\\noo\\noo ooooo\\no\\no\\no\\no\\no\\no oo\\no\\no\\no\\noooo\\no\\nooo\\no\\noo\\no\\no\\no\\no\\noo o\\no\\nooo\\noo\\no\\no o\\no\\no o\\noo\\no\\no\\no\\noooo\\no oo oo\\no o\\noooo\\nooo\\no\\noo\\no\\no\\noo\\noooo oo\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noo oo\\noooo\\nooo o\\no\\no\\nooo\\no ooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\noo\\no\\nooo\\no\\no\\noo ooo oo\\no oo o\\no\\no\\nooo\\noo\\no\\no\\no\\no\\nooo\\no\\no\\no\\nooo oo\\no\\nooo\\no\\nooo\\no\\no\\no\\no\\no\\noo o ooo\\noooo\\no o\\no\\no\\no\\no\\no ooo\\nooo\\no\\no\\no\\nooo o oo\\no\\noo o\\no\\noo\\nooo oo oooo\\no\\noo\\no\\no o\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\noo\\no\\noo\\no o\\no o\\no\\no\\no oo\\no oo\\no\\nooo ooo\\no\\no oo oo\\no\\no\\no\\no oo\\no\\no\\no\\noo\\nooooo\\no\\no\\no\\noo oo o\\no\\no\\no ooo\\no\\no\\no\\no\\no\\no\\no\\noo o oo ooo o\\no\\noooo\\no\\nooo\\noo\\nooo\\no\\no\\no\\no oo\\no\\no\\no ooo o\\no\\no\\noo\\no o\\no\\no\\no\\n2 6 10 14\\no oo\\no\\no\\no\\no oo\\no\\noo\\noo oo\\no\\no oo\\no o\\no\\noo\\no\\no\\noo o\\no\\no oo o\\no\\no\\no\\no\\noo\\no\\no oo o\\noo\\no\\no\\no\\no\\no\\no o\\no\\no oo o\\no\\no oo\\no ooo\\no\\noo o o\\no o\\no oooo o\\noo\\noo oooo\\noo oo\\no\\no\\noo o\\no\\no\\no\\no ooo\\noooo\\no\\no o\\no\\no\\no\\no\\no o o\\no\\noo o\\noo\\no\\no o\\no\\noo\\noo\\no\\no\\no\\nooo o\\noo o oo\\noo\\noo oo\\nooo\\no oo\\no\\no\\noo\\noo oo o\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\noo\\no o\\no\\no\\no\\noo oo\\no o\\no\\no\\nooo o\\no\\no\\no\\noo oooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\noo\\no\\no oo\\no\\no\\nooo o ooo\\no ooo\\no\\no\\no\\no o\\noo\\no\\no\\no\\no\\no oo\\no\\no\\no\\noo oo o\\no\\no oo\\no\\nooo\\no\\no\\no\\noo\\noo o o oo ooooo o\\no\\no\\no\\no\\no ooo\\no oo\\no\\no\\no\\no o\\no\\nooo\\no\\no oo\\no\\noo\\no oo oo oooo\\no\\no o\\no\\no o\\no\\no\\noo o\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no o\\noo\\no\\noo\\noo\\noo\\no\\no\\no o o\\nooo o oo\\no\\noo o\\no\\noo oo o\\no\\no\\no\\no o o\\no\\no\\no\\noo o oooo\\no\\no\\no\\nooo oo\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\no\\noooo oo ooo\\no\\noooo\\no\\nooo\\noo\\no o o\\no\\no\\no\\nooo o\\no\\no ooo o\\no\\no\\no o\\noo\\no\\no\\no\\n0.0 0.4 0.8\\no\\no\\no ooo\\no\\noo o\\no\\no\\no o\\noo\\no\\no oo\\no\\no o\\noo\\no\\no o\\no\\noo o\\no\\no oo oo\\nooo\\noo\\no\\noo\\noo\\no o\\no\\no o\\no\\nooooo\\no\\no\\no o\\noo\\noo\\no\\no\\noo oo\\no\\nooo\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\nooo oo oo\\nooo\\no\\no\\no\\no\\noo oo\\no\\nooo\\no\\no\\no\\no\\no\\no o\\no\\noooo o o\\no\\no\\no\\no\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\nooo o\\no o\\noo oooo\\no o\\no oo oo oooo oo\\noo o o\\noo o\\nooo\\nooo o\\no oooo\\noo\\noo\\no\\noo\\noo\\no\\no o\\no\\noo\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no oo o ooo oo\\no\\no\\no\\noo\\no\\no\\no oo\\nooo ooo\\no\\noo o\\no\\noo\\no\\no\\no o\\no\\no\\noo o\\no oooo\\no\\no\\no\\noooo\\noo o\\no oo\\no\\no\\noo\\no o\\noooo o oo\\no\\nooooooo o\\no\\nooooo\\noo\\no o\\no\\no\\no o\\noo\\no o\\no\\no\\no oo\\no\\no\\no\\no\\noo\\no\\no\\no o\\no\\noo oo\\noo\\noo\\noo\\no\\no\\noo\\no o\\noo\\noo\\no\\no\\no\\no o oooo\\noo\\no\\no\\no\\no\\no oo\\no o\\noo\\no\\noo\\no\\no o\\noo\\no\\no oo\\noo\\noo\\noo\\noo\\noo o\\noooo\\no\\no\\no\\nooo\\no o\\no\\no o o\\noo ooo\\nooo\\no\\noo\\noo o\\no o\\noo\\no\\no\\no\\nooo oo\\no\\no\\no\\noooo\\no\\nooo\\no\\no\\noo o\\noo\\noo\\noo\\no\\noooo\\no o\\no\\no o oo\\no\\nooo\\no\\no\\noo\\noo\\no\\nooo\\no\\noo\\no o\\no\\no o\\no\\nooo\\no\\nooooo\\no oo\\noo\\no\\no o\\noo\\no o\\no\\noo\\no\\no oo oo\\no\\no\\no o\\noo\\noo\\no\\no\\nooo o\\no\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no o\\noo\\noo oo ooo\\noo o\\no\\no\\no\\no\\noooo\\no\\nooo\\no\\no\\no\\no\\no\\noo\\no\\noooo o o\\no\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\noooo\\noo\\no ooooo\\no o\\no oo oo oooo o o\\noo oo\\nooo\\nooo\\noo oo\\no oo oo\\noo\\noo\\no\\noo\\noo\\no\\no o\\no\\noo\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\nooo oooo oo\\no\\no\\no\\noo\\no\\no\\nooo\\noooo oo\\no\\noo o\\no\\noo\\no\\no\\no o\\no\\no\\noo o\\noo ooo\\no\\no\\no\\noo o o\\noo o\\no oo\\no\\no\\noo\\noo\\noooo ooo\\no\\noooooooo\\no\\noooo o\\noo\\noo\\no\\no\\no o\\noo\\no o\\no\\no\\no oo\\no\\no\\no\\no\\no o\\no\\no\\noo\\no\\no ooo\\no o\\noo\\noo\\no\\no\\noo\\noo\\noo\\no o\\no\\no\\no\\nooo ooo\\no o\\no\\no\\no\\no\\nooo\\noo\\noo\\no\\noo\\no\\noo\\no o\\no\\no oo\\noo\\no o\\noo\\noo\\nooo\\noo oo\\no\\no\\no\\no oo\\noo\\no\\no oo\\noo oo o\\no oo\\no\\noo\\no oo\\noo\\noo\\no\\no\\no\\nooooo\\no\\no\\no\\noo oo\\no\\nooo\\no\\no\\nooo\\noo\\noo\\noo\\no\\no ooo\\no o\\no\\no oo o\\no\\noo o\\no\\no\\noo\\noo\\no\\nooo\\no\\no o\\noo\\no\\noo\\no\\noo o\\no\\nooo oo\\no oo\\noo\\no\\noo\\noo\\no o\\no\\no o\\no\\no ooo o\\no\\no\\noo\\noo\\no o\\no\\no\\noooo\\no\\noo o\\no\\no\\noo\\no\\no\\no\\no\\no o\\noo\\noo oo o oo\\noo o\\no\\no\\no\\no\\nooo o\\no\\noo o\\no\\no\\no\\no\\no\\no o\\no\\no ooooo\\no\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\noooo\\no o\\noo oooo\\noo\\no oo ooooooo o\\noo oo\\no oo\\no oo\\no ooo\\nooo oo\\noo\\noo\\no\\noo\\noo\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\nooo oo oo oo\\no\\no\\no\\no o\\no\\no\\nooo\\noooooo\\no\\nooo\\no\\no o\\no\\no\\noo\\no\\no\\nooo\\nooooo\\no\\no\\no\\noooo\\no oo\\no oo\\no\\no\\noo\\no o\\nooooooo\\no\\nooo ooo oo\\no\\noooo o\\noo\\noo\\no\\no\\no o\\noo\\no o\\no\\no\\no oo\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noo oo\\no o\\no o\\noo\\no\\no\\no o\\noo\\no o\\noo\\no\\no\\no\\noo oooo\\no o\\no\\no\\no\\no\\noo o\\noo\\no o\\no\\noo\\no\\noo\\noo\\no\\nooo\\no o\\noo\\noo\\noo\\noo o\\noooo\\no\\no\\no\\no oo\\no o\\no\\nooo\\noo oo o\\no oo\\no\\noo\\nooo\\noo\\noo\\no\\no\\no\\nooo oo\\no\\no\\no\\nooo o\\no\\nooo\\no\\no\\no o o\\noo\\no o\\noo\\no\\nooo o\\no\\nfamhist\\no\\no\\no oo o\\no\\noo o\\no\\no\\noo\\noo\\no\\nooo\\no\\noo\\noo\\no\\noo\\no\\nooo\\no\\nooo o o\\noo o\\noo\\no\\noo\\no o\\no o\\no\\noo\\no\\nooooo\\no\\no\\noo\\noo\\no o\\no\\no\\nooo o\\no\\noo o\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\noo ooo oo\\nooo\\no\\no\\no\\no\\noooo\\no\\noo o\\no\\no\\no\\no\\no\\noo\\no\\noooooo\\no\\no\\no\\no\\no\\no oo oo\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no o\\noooooo\\noo\\no ooooooooo o\\noo oo\\noo o\\no oo\\no ooo\\nooo oo\\noo\\noo\\no\\noo\\noo\\no\\no o\\no\\noo\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no oo oooo oo\\no\\no\\no\\no o\\no\\no\\nooo\\noooooo\\no\\noo o\\no\\no o\\no\\no\\noo\\no\\no\\nooo\\nooo oo\\no\\no\\no\\noo oo\\noo o\\no o o\\no\\no\\noo\\noo\\noo ooo oo\\no\\no ooo oooo\\no\\noooo o\\noo\\no o\\no\\no\\noo\\no o\\noo\\no\\no\\nooo\\no\\no\\no\\no\\noo\\no\\no\\no o\\no\\noo oo\\noo\\noo\\noo\\no\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noo oooo\\no o\\no\\no\\no\\no\\nooo\\no o\\noo\\no\\noo\\no\\no o\\noo\\no\\nooo\\noo\\no o\\noo\\noo\\noo o\\noo oo\\no\\no\\no\\noo o\\no o\\no\\no oo\\nooo oo\\no oo\\no\\noo\\nooo\\noo\\noo\\no\\no\\no\\nooooo\\no\\no\\no\\nooo o\\no\\noo o\\no\\no\\noo o\\no o\\no o\\noo\\no\\nooo o\\no o\\no\\no o oo\\no\\nooo\\no\\no\\noo\\no o\\no\\no oo\\no\\noo\\noo\\no\\no o\\no\\noo o\\no\\noo ooo\\no oo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\no oooo\\no\\no\\no o\\noo\\noo\\no\\no\\nooo o\\no\\nooo\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\nooooooo\\nooo\\no\\no\\no\\no\\noo oo\\no\\nooo\\no\\no\\no\\no\\no\\noo\\no\\noooooo\\no\\no\\no\\no\\no\\no ooo o\\no\\no\\no\\no\\no\\no\\no\\no ooo\\noo\\noooooo\\noo\\nooo oo oooo oo\\noooo\\nooo\\nooo\\noooo\\nooo oo\\noo\\noo\\no\\noo\\noo\\no\\noo\\no\\no o\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\nooooooooo\\no\\no\\no\\noo\\no\\no\\noo o\\nooo ooo\\no\\noo o\\no\\noo\\no\\no\\noo\\no\\no\\nooo\\no oooo\\no\\no\\no\\noo oo\\nooo\\nooo\\no\\no\\noo\\noo\\nooo o ooo\\no\\noooo oooo\\no\\no ooo o\\noo\\noo\\no\\no\\noo\\no o\\noo\\no\\no\\no oo\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\noooo\\noo\\noo\\no o\\no\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\nooo oo o\\noo\\no\\no\\no\\no\\nooo\\noo\\noo\\no\\no o\\no\\no o\\no o\\no\\nooo\\noo\\no o\\noo\\no o\\nooo\\noooo\\no\\no\\no\\noo o\\no o\\no\\noo o\\nooo oo\\noo o\\no\\noo\\no oo\\noo\\no o\\no\\no\\no\\nooooo\\no\\no\\no\\noooo\\no\\noo o\\no\\no\\noo o\\noo\\noo\\no o\\no\\no ooo\\no o\\no\\no ooo\\no\\noo o\\no\\no\\noo\\noo\\no\\no oo\\no\\no o\\noo\\no\\no o\\no\\nooo\\no\\no ooo o\\no oo\\noo\\no\\no o\\noo\\noo\\no\\noo\\no\\nooo oo\\no\\no\\no o\\noo\\noo\\no\\no\\nooo o\\no\\no o o\\no\\no\\no o\\no\\no\\no\\no\\no o\\noo\\noo oo o o o\\no o o\\no\\no\\no\\no\\noo oo\\no\\nooo\\no\\no\\no\\no\\no\\no o\\no\\no oo o oo\\no\\no\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no o\\no ooooo\\no o\\noo ooo oooo o o\\noo oo\\no oo\\nooo\\no oo o\\no oo oo\\noo\\noo\\no\\noo\\noo\\no\\no o\\no\\noo\\no\\no o\\no\\no\\no o\\no\\no\\no\\no\\nooo oooo oo\\no\\no\\no\\noo\\no\\no\\noo o\\noooo o o\\no\\noo o\\no\\no o\\no\\no\\noo\\no\\no\\noo o\\noo oo o\\no\\no\\no\\no oo o\\noo o\\no o o\\no\\no\\no o\\noo\\nooo o o oo\\no\\noooo ooo o\\no\\no ooo o\\noo\\no o\\no\\no\\noo\\noo\\no o\\no\\no\\noo o\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no o oo\\no o\\noo\\no o\\no\\no\\no o\\noo\\noo\\no o\\no\\no\\no\\noo oooo\\no o\\no\\no\\no\\no\\no oo\\noo\\no o\\no\\noo\\no\\noo\\noo\\no\\no oo\\noo\\no o\\noo\\noo\\noo o\\noooo\\no\\no\\no\\nooo\\noo\\no\\no oo\\noo ooo\\nooo\\no\\noo\\nooo\\no o\\noo\\no\\no\\no\\nooooo\\no\\no\\no\\no o oo\\no\\nooo\\no\\no\\noo o\\noo\\no o\\noo\\no\\noooo\\no\\nooo o\\no\\no\\nooo\\no o\\noooooo\\no\\noo\\no\\no o\\noo\\no\\no\\no\\noo\\no o\\noo o\\no\\no\\no\\nooo\\noo\\no\\no\\no o\\no\\no\\noo o o\\no ooooo o oo oo\\nooo\\no\\no\\noo o\\no\\no\\no\\no\\noo ooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\nooo oo\\noooo o\\no\\no\\nooo o\\no o\\no\\no\\no oo\\no\\nooo o o\\no\\no\\no\\no\\no\\no\\no\\no\\noooo oo\\no o\\no\\no\\no\\no oo\\noo\\no oooo o oo\\no\\no oo o\\noooo\\no oo\\no\\no\\noo\\no\\no\\noo\\no\\noo o\\no\\no\\no\\no\\no\\no\\nooo o\\no\\no\\no\\noo\\no\\noo\\noo o\\no\\noo\\nooo\\no\\no\\no\\no\\no o\\no\\no\\nooo\\no\\no\\no\\noo\\no\\noo oo oo oo\\no ooo o\\noo\\noo o\\no\\no\\noo oo\\no o\\no oo oo\\no\\noo\\no\\no\\noo\\no\\nooo\\no\\no\\no\\no\\no\\no oo\\no oo\\no\\noo o\\noo\\no\\no\\nooo\\no\\noo o\\no\\no\\nooo\\no o\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no o\\no\\no\\nooo oo oo o\\no\\no\\noo\\noo\\noo\\noo\\noo ooo\\noo\\noo o\\noo\\no\\no\\no\\no oo o\\no\\noo oo\\no\\no ooo o ooo\\no\\no\\no\\nooo\\no\\no\\nooo\\no\\no o\\no\\noo\\no\\no\\no\\nooo oo\\no\\noo\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no o\\noo o\\no\\no\\no\\no\\no o\\no\\noooo\\no\\nooo\\noo\\no\\no\\noo oo\\noooooo\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no o\\no o\\no\\no\\noo\\no\\no\\no\\nooo o\\no\\no\\nooo\\no\\no\\nooo oo o\\noo o\\no\\noo\\no oo\\no\\no\\noo\\noo\\nooo\\no\\noo\\no o\\no\\noo\\no\\no\\no o\\no\\no\\noooo\\noooo ooooo oo\\no o\\no\\no\\no\\nooo\\no\\no\\no\\no\\noo ooo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo o\\noo\\nooooo\\no\\no\\nooo o\\noo\\no\\no\\noo\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noooooo\\no o\\no\\no\\no\\nooo\\noo\\nooooo o oo\\no\\no oo o\\nooo o ooo\\no\\no\\noo\\no\\no\\noo\\no\\no oo\\no\\no\\no\\no\\no\\nooooo\\no\\no\\no\\nooo\\nooooo\\no\\noo\\noo oo\\noo\\no\\noo\\no\\no\\nooo\\no\\no\\no\\no o\\no\\noo ooo\\no\\noo\\noo ooo\\noo\\nooo\\no\\no\\noo oo\\noo\\noo oo o\\no\\no o\\no\\no\\noo\\no\\nooo\\no\\no\\no\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\no\\no\\noooo\\noo\\no\\no\\no\\nooo o\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\noo\\no\\no\\nooo\\nooo oo\\no\\no\\noo\\no o\\noo\\noo\\noo oooo o\\nooo\\noo\\no o\\no\\no o oo\\no\\nooo\\no\\no\\noo\\noo oooo\\no\\no\\no\\no oo\\no o\\noo\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\noo oo\\no\\no o\\no\\no\\no\\nooo o\\no\\noo\\no\\no\\noo\\noo o\\no\\no\\no\\no\\no o\\no\\noo oo\\no\\nooo\\noo\\no\\no\\noooooo\\noooo\\no\\no\\noo\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no o\\no\\no\\no\\nooo o\\no\\no\\nooo\\noo ooooo o\\no\\noo\\no\\no o\\noo o\\no\\no\\noo\\no o\\nooo\\no\\noo\\no oo\\noo\\no\\no\\no o\\no\\no\\no oo o\\noo oooooo ooo\\noo\\no\\no\\no\\nooo\\no\\no\\no\\no\\noooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo ooo\\no\\noooo\\no\\no\\noo oo\\no o\\no\\no\\nooo\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\nooo\\no o\\no\\no\\no\\nooo\\noo\\nooooo ooo\\no\\no ooo\\noooo o oo\\no\\no\\no o\\no\\no\\noo\\no\\nooo\\no\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\nooo\\no\\nooo o\\no\\noo\\noo oo\\no\\no\\no\\noo o\\no\\no oo\\no\\no\\no\\noo\\no\\no ooooo oo\\nooooo\\noo\\nooo\\no\\no\\nooo oo o\\noo oo o\\no\\no o\\no\\no\\noo\\no\\noo o\\no\\no\\noo\\no\\no oo\\no o o\\no\\nooo\\noo\\no\\no\\noo oo\\no oo\\no\\no\\nooo o\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\nooo\\no\\no\\noo oooooo\\no\\no\\no o\\noo\\no\\no\\no o\\no\\nooo o\\noo\\nooo\\no o\\noo\\no\\noooo\\no\\nooo\\no\\no\\noo oooo oo\\no\\no\\no\\no oo\\noo\\no o\\noo\\noo\\no\\no o\\no\\no\\no\\no\\noooo\\no\\noo\\no\\no\\no\\no ooo\\no\\no\\no\\no\\nooo\\noo o\\no\\no\\no\\no\\no o\\no\\noooo\\no\\nooo\\noo\\no\\no\\noo ooo\\noo ooo\\no\\no\\nooo\\no\\no\\no o\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no\\noo oo\\no\\no\\no oo\\noo\\nooo ooo\\nooo\\no\\noo\\noo o\\no\\no\\noo\\noo\\nooo\\no\\no\\no\\noo\\no\\noo\\no\\no\\no o\\no\\no\\no ooo\\noooooo oo ooo\\no oo\\no\\no\\nooo\\no\\no\\no\\no\\no oo o\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo o\\no oooo\\no\\no\\nooo o\\no\\no\\no\\no\\noo o\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo oo oo\\noo\\no\\no\\no\\noo o\\noo\\nooooo ooo\\no\\noooo\\noooo\\no oo\\no\\no\\nooo\\no\\noo\\no\\nooo\\no\\no\\no\\no\\no\\noo ooo\\no\\no\\no\\no o\\no\\no\\nooo o\\no\\no o\\no ooo\\no\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\no o\\no\\no oo ooooo\\noooo o\\noo\\no oo\\no\\no\\no ooo oo\\noo ooo\\no\\noo\\no\\no\\noo\\no\\no\\noo\\no\\no\\noo\\no\\noo\\no\\nooo\\no\\nooo\\noo\\no\\no\\noooo\\noo\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no o\\no\\no\\no oo oo ooo\\no\\no\\no o\\noo\\noo\\noo\\noo ooo oo\\no oo\\noo\\noo\\no\\noo oo\\no\\noooo\\no\\noo oo oooo\\no\\no\\no\\no\\noo\\no o\\nooo o\\noo\\no\\noo\\no\\no\\no\\no\\noo oo\\no\\noo\\no\\no\\no\\no ooo\\no\\no\\no\\no\\nooo\\nooo\\no\\no\\no\\no\\noo\\no\\no\\nooo\\no\\nooo\\noo\\no\\no\\noooo oo\\noooo\\no\\no\\nooo\\no\\no\\noo\\no\\no\\no o\\nooo\\no\\noo\\no\\no\\no\\nobesity\\nooo o\\no\\no\\nooo\\no\\nooooo oo\\no oo\\no\\noo\\noo\\no\\no\\no\\noo\\no o\\nooo\\no\\noo\\no oo\\noo\\no\\no\\noo\\no\\no\\noo oo\\noo ooooooo oo\\no o\\no\\no\\no\\nooo\\no\\no\\no\\no\\noo o oo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\nooo\\noo\\no\\nooo oo\\no\\noooo\\noo\\no\\no\\noo o\\no\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooooo\\noo\\no\\no\\no\\nooo\\noo\\nooooo ooo\\no\\no oo o\\nooo o\\nooo\\no\\no\\nooo\\no\\noo\\no\\nooo\\no\\no\\no\\no\\no\\noo ooo\\no\\no\\no\\nooo\\nooo oo\\no\\no o\\noo oo\\no\\no\\no\\noo\\no\\no\\nooo\\no\\no\\no\\noo\\no\\noo ooo o\\noo\\no oooooo\\noo o\\no\\no\\noooo\\no o\\nooo oo\\no\\no o\\no\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no o o\\noo\\no\\no\\nooo\\no\\noo\\no\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\nooo\\noo oo o\\no\\no\\noo\\noo\\no\\no\\noo\\noo o oo\\no o\\noo o\\noo\\noo\\no\\nooo o\\no\\noo o o\\no\\noo oo ooo o\\no\\no\\no\\no\\no o\\noo\\no\\no oo\\noo\\no\\noo\\no\\no\\no\\no\\no ooo\\no\\noo\\no\\no\\no\\noooo\\no\\no o\\nooo o\\nooo\\no\\no\\no\\no\\no o\\no\\no o oo\\no\\noo o\\noo\\no\\no\\noooooo\\noooo\\no\\no\\noo o\\no\\no\\noo\\no\\noo o\\no\\no\\no\\no\\no o\\no\\no\\no\\n15 25 35 45\\no\\noo o\\no\\no\\no oo\\no o\\nooo ooo\\no oo\\no\\no o\\nooo\\no\\no\\no o\\noo\\noo o\\no\\no o\\no oo\\noo\\no\\no\\no o\\no\\no\\no o oo\\no ooo oo oo o oo\\no oo\\no\\no\\noo o\\no\\no\\no\\no\\noooo o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no o oo o\\no ooo o\\no\\no\\noo\\no o\\no\\no\\no\\no\\no o\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\nooo\\noo\\no\\no\\no\\nooo\\noo\\noooo\\no o oo\\no\\no oo o\\no oo o ooo\\no\\no\\no o o\\no\\noo\\no\\no oo\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo\\no\\no o\\noo o\\no\\noo\\noo oo\\no\\no\\no\\no o\\no\\no\\nooo\\no\\no\\no\\noo\\no\\noo o oo o oo\\noo o oo\\noo\\nooo\\no\\no\\noooo\\noo\\no ooo o\\no\\no o\\no\\no\\no o\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\nooo\\no\\no o o\\noo\\no\\no\\nooo\\no\\noo o\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo o\\no\\no\\nooo oo oo o\\no\\no\\no o\\no o\\no\\no\\no o\\nooo oooo\\no oo\\no o\\no o\\no\\no ooo\\no\\noooo\\no\\noo oo o ooo\\no\\no\\no\\no oo\\no o\\no o\\no o\\no o\\no\\noo\\no\\no\\no\\nooo oo\\no\\noo\\no\\no\\no\\no ooo\\no\\no\\no\\no\\no\\noo\\noo o\\no\\no\\no\\no\\no o\\no\\nooo o\\no\\nooo\\no o\\no\\no\\noooo\\noo\\noo o o\\no\\no\\nooo\\no\\no\\noo\\no\\no\\no o\\noo\\no\\no\\noo\\no\\no\\no\\n0 50 100\\no\\noo\\no\\no\\noooo o\\no\\noo oo\\noo\\no\\no\\noo o\\nooooo\\no\\nooo\\nooo o\\no o\\no o\\no\\no ooooo oo\\no\\noo\\no\\no\\no o\\no\\nooo o oo\\nooo\\no\\noooooo\\no\\nooooo\\no\\no\\no oooo\\no\\no ooooo\\noo oo\\no\\noo\\noo oooo\\no\\no\\noooo oo o\\no\\noo\\no\\no\\noooo o o\\no o\\noo\\no\\no\\no\\noo\\no\\nooo\\noo o\\no\\no\\no\\no oo ooo oooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\noo oo o ooo ooooooo oo oo\\no\\no oo\\noo ooo o\\nooo ooo\\no\\no\\nooo\\noo\\no oo\\no\\noo oo o\\nooo oo oooo\\no\\no\\no\\no o\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\noooo ooo\\no\\no oo\\nooo\\no\\no o\\nooo\\no\\nooo oo o\\nooo oo o oooo\\no\\no\\noo\\no\\noooo\\no\\noo o\\no\\noo\\noo\\no\\nooo oooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo oooo oooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\nooo oo o\\no\\no\\no\\noo\\noo o\\noo\\no o\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no o\\no o oooo o\\no\\no\\nooo o\\nooo o\\no\\noo o\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo oooooooooo\\no\\no\\no\\noooo\\no\\no\\no\\no\\no o\\no\\no o\\noo o\\no\\no\\noo\\no\\no\\nooooo\\no\\nooo o\\no o\\no\\no\\noo o\\noo ooo\\no\\nooo\\no ooo\\noo\\noo\\no\\noooo\\no ooo\\no\\noo\\no\\no\\noo\\no\\no ooooo\\nooo\\no\\no\\nooooo\\no\\nooooo\\no\\no\\no oo oo\\no\\nooooooo ooo\\no\\no o\\no\\nooooo\\no\\no oooo ooo\\no\\no o\\no\\no\\noooo o o\\nooo o\\noo\\no\\no\\no\\no\\nooo\\noo o\\no\\no\\no\\nooooo ooooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no ooo oooooo oooo ooo o\\no o\\no oo\\nooooooooo\\nooo\\no\\no\\nooo\\noo\\no\\no\\no\\noo\\nooo oooo oooo oooo\\no\\noo\\no\\nooo\\no\\nooooo\\no\\no\\no\\no ooo oo o\\no\\no o o\\no oo\\no\\noo\\nooo\\no\\nooo oo o\\nooooo ooooo\\no\\no\\no o\\no\\noooo\\no\\nooo\\no\\noo\\noo\\no\\noo ooooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\nooooo oooo\\no oo\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\nooo o ooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\no\\noo oo o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo o\\no o\\noooo\\noo o\\no\\no\\no o\\no o\\nooo o\\no\\noo o\\no o\\no o\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\nooooooooooo ooo\\no\\no\\no\\noooo\\no\\no\\no\\no\\noo\\no\\noo ooo\\no\\no\\noo\\no\\no\\noooo o\\no\\noooo\\no o\\no\\no\\noo\\no\\nooo oo\\no\\no oo\\noooo\\no o\\noo\\no\\nooo ooo oo\\no\\no o\\no\\no\\no\\no\\no\\nooooo o\\nooo\\no\\noo oooo\\no\\nooo oo\\no\\no\\nooooo\\no\\nooooooo o oo\\no\\no o\\no\\nooooo\\no\\noooo ooo o\\no\\noo\\no\\no\\no ooooo\\nooo o\\no o\\no\\no o\\no\\nooo\\noo ooo\\no\\nooo oo\\no oooo\\no\\no\\no\\no\\no\\no\\no\\no\\no oo\\no o oo ooo ooo ooo ooooo\\no o\\no oo\\nooooooooo\\no oo\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\noo o oo oo oo oooo\\no\\no\\no\\noo\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\nooooo oo\\no\\noo o\\nooo\\no\\no o\\nooo\\no\\noo ooo o\\nooo oo o o ooo\\no\\no\\noo\\no\\nooo o\\no\\no oo\\no\\no\\no\\noo\\no\\noo oo ooo oo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo oooooo oo o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo oooo\\no\\no\\no\\noo\\nooo\\noo\\noo\\no\\no\\noooo o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo o\\noo\\no ooo\\noo o\\no\\no\\nooooo ooo\\no\\noo o\\no oo o\\no o\\no o\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo ooooo ooo oo\\no\\no\\no\\no oo o\\no\\no\\no\\no\\noo\\no\\no o\\noo o\\no\\no\\no o\\no\\no\\noo ooo\\no\\nooo o\\noo\\no\\no\\noo o\\nooo oo\\no\\nooo\\no ooo\\noo\\no o\\no\\nooo ooo oo\\no\\no o\\no\\no\\noo\\no\\nooo oo o\\nooo\\no\\noo oooo\\no\\noooo o\\no\\no\\noooo o\\no\\no ooooooooo\\no\\noo\\no oo ooo\\no\\no oooo oo o\\no\\noo\\no\\no\\noooooo\\noo oo\\noo\\no\\noo\\no\\noo o\\no oo o\\no\\no\\noo ooo\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\noo ooooooo oooo\\nooo oo\\noo\\nooo\\nooo oooo oo\\no oo\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\noooooooooo oo oo\\no\\no\\no\\noo\\no\\nooo\\no\\noo ooo\\no\\no\\no\\no oo ooo o\\no\\noo o\\nooo\\no\\no o\\no oo\\no\\noooo oo\\noo oooo ooooo\\no\\noo\\no\\noooo\\no\\nooo\\no\\no\\no\\noo\\no\\noooo oo o\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo ooooo oooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo oo ooo\\no\\no\\no\\no\\no oo\\noo\\noo\\no\\no\\no ooo o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\nooo\\no o\\noooo\\noo o\\no\\no\\noo\\no o\\noo oo\\no\\nooo\\noo oo\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\noooooo oo ooooo o\\no\\no\\no\\noo oo\\no\\no\\no\\no\\noo\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\noooo o\\no\\noooo\\noo\\no\\no\\noo o\\no ooo o\\no ooo\\no ooo\\no o\\noo\\no\\nooo o ooo o\\no\\noo\\no\\no\\noo\\no\\noooooo\\no\\noo\\no\\noo oooo\\no\\no oo oo\\no\\no\\no oooo\\no\\no oo oo ooo oo o\\noo\\noo o ooo\\no\\no\\nooo ooo o\\no\\noo\\no\\no\\noo oooo\\noo\\noo\\no\\no\\no\\no o\\no\\nooo\\no ooo o\\no\\nooo oo\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no ooo oooo oo ooo oo o oo\\no o\\no oo\\nooooo ooo o\\nooo\\no\\no\\nooo\\noo\\no\\no\\no\\no oooo oooo oo oooo\\no\\no\\no\\noo\\no\\nooo\\no\\nooooo\\no\\no\\no\\noooooo o\\no\\nooo\\noo o\\no\\no o\\no oo\\no\\nooo oo o\\no oooo oooo oo\\no\\noo\\no\\no ooo\\no\\noo o\\no\\no\\no\\noo\\no\\noo o ooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo oooo oo oooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo oooo o\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\nooo oo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo o\\noo\\no oo o\\noo o\\no\\no\\noo\\noo o oo o\\no\\nooo\\nooo o\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no ooooooo oooo oo\\no\\no\\no\\no ooo\\no\\no\\no\\no\\noo\\no\\noo\\noo o\\no\\nalcohol\\no\\noo\\no\\no\\noo oo o\\no\\nooo o\\noo\\no\\no\\noo\\no\\no ooo o\\no\\no oo\\no oo o\\no o\\noo\\no\\nooo\\noo o oo\\no\\no o\\no\\no\\no o\\no\\no oo oo o\\no\\noo\\no\\noo ooo o\\no\\no oo oo\\no\\no\\no oooo\\no\\nooo oo oo o o o\\no\\no o\\no oo ooo\\no\\nooooo oo o\\no\\noo\\no\\no\\no oo o oo\\noo\\no o\\noo\\no\\no o\\no\\nooo\\no ooo o\\no\\nooo oo\\nooooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no ooo ooo o ooooo oo oo o\\no o\\no oo\\nooo ooo\\nooo o oo\\no\\no\\no oo\\noo\\nooo\\nooo oo o\\nooo oo o oo o\\no\\no\\no\\noo\\no\\nooo\\no\\no o\\noo\\no\\no\\no\\no\\no oooooo\\no\\no oo\\no oo\\no\\noo\\noo o\\no\\nooo oo o\\nooo o o ooooo\\no\\no\\noo\\no\\noooo\\no\\noo o\\no\\no\\no\\noo\\no\\nooo ooo oo o\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo oo o oo o\\no oo\\no\\no\\no\\no\\no\\no o\\no\\no o\\no\\no oo ooo o\\no\\no\\noo\\nooo\\noo\\no o\\no\\no\\no o oo o\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no oo\\no o\\no o oo\\noo o\\no\\no\\noo\\no o\\nooo o\\no\\noo o\\noo\\noo\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\noooooooooo o ooo\\no\\no\\no\\no ooo\\no\\no\\no\\no\\noo\\no\\noo\\nooo\\no\\n100 160 220\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\noo o\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo\\noo o\\noo\\nooo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\no\\no o\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no o\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no ooo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no o\\no\\no o\\no\\no\\noo o\\noooo oo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\noooo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no oo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no oo\\no\\no o\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo\\no\\noo\\noo oooooo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no\\no\\no ooo\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no oo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\noo o\\no\\no o\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\nooo\\no\\no\\noo\\noo\\no\\no\\no\\no\\noooo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\no\\no\\nooo\\no\\nooo oo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noooo\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no oo\\nooo\\no\\noo\\no\\no\\noo o\\no\\no\\no\\no\\noo\\no o\\no\\noo\\no\\noo\\no\\noo\\noooooooo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\n2 6 10 14\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\noo oo\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo oo\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\nooo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no o\\no\\no\\no o\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no o\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo\\no\\no\\noo\\noo\\no\\no\\no\\no\\noooo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\noo\\no\\no\\noo\\no\\nooooo oo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\nooo o\\no\\no o\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no o\\noo\\no\\noo\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no oo\\no o o\\no\\noo\\no\\no\\noo o\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\no\\no o\\no\\noo\\noo ooooo o\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no o\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo o\\noo\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo oo\\no\\no\\no\\no\\no\\no\\no oo\\noo\\no oo\\no\\noo\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no oo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo oo\\no\\no\\noo\\noo\\no\\no\\no\\no\\noo oo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\noo\\no\\no o\\no\\no\\no o\\no\\no oo ooo o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noooo\\no\\noo\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\nooo\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\no\\no o\\no\\noo\\noooo oo oo\\no\\no\\no\\no\\no\\no\\no o\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no o\\n15 25 35 45\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo o\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no o\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no oo o\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\nooo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\nooo o\\no\\no\\noo\\no o\\noo\\no\\no\\noo oo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\noo\\no\\noo\\no\\no\\noo\\no\\noooooo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no ooo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\no\\noo\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no oo\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo o\\noo oo\\noo\\no\\no\\no oo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no o\\no\\no o\\noo oooo oo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no o\\no\\no\\no\\no\\noooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\nooo\\noo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no o\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\noo o\\noo\\noo o\\no\\noo\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\noo\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\noo o o\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\noo\\noo\\no\\no\\no\\no\\no o\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noooo\\no\\no\\no o\\noo\\no\\no\\no\\no\\noo oo\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nooo\\no o\\no\\no o\\no\\no\\noo\\no\\noooooo\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\noooo\\no\\noo\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no o\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no o\\no\\no o\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\nooo\\no oo\\no\\no o\\no\\no\\no oo\\no\\no\\no\\no\\noo\\no o\\no\\no\\no\\no\\no o\\no\\noo\\noooooooo\\no\\no\\no\\no\\no\\no\\noo\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\noo\\noo\\no\\no\\n20 40 60\\n20 40 60\\nage\\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\\nEach plot shows a pair of risk factors, and the cases and controls are color coded\\n(red is a case). The variablefamily history of heart disease (famhist) is binary\\n(yes or no).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d186f951-faba-4b17-a0cf-b52020751cc2', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 142, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='124 4. Linear Methods for Classiﬁcation\\nTABLE 4.3.Results from stepwise logistic regression ﬁt to South African heart\\ndisease data.\\nCoeﬃcient Std. Error Z score\\n(Intercept) −4.204 0 .498 −8.45\\ntobacco 0.081 0 .026 3 .16\\nldl 0.168 0 .054 3 .09\\nfamhist 0.924 0 .223 4 .14\\nage 0.044 0 .010 4 .52\\nother correlated variables, they are no longer needed (and can even get a\\nnegative sign).\\nAt this stage the analyst might do some model selection; ﬁnd a subset\\nof the variables that are suﬃcient for explaining their joint eﬀect on the\\nprevalence ofchd. One way to proceed by is to drop the least signiﬁcant co-\\neﬃcient, and reﬁt the model. This is done repeatedly until no further terms\\ncan be dropped from the model. This gave the model shown in Table 4.3.\\nA better but more time-consuming strategy is to reﬁt each of the models\\nwith one variable removed, and then perform ananalysis of deviance to\\ndecide which variable to exclude. The residual deviance of a ﬁtted model\\nis minus twice its log-likelihood, and the deviance between two models is\\nthe diﬀerence of their individual residual deviances (in analogy to sums-of-\\nsquares). This strategy gave the same ﬁnal model as above.\\nHow does one interpret a coeﬃcient of 0.081 (Std. Error = 0.026) for\\ntobacco, for example? Tobacco is measured in total lifetime usage in kilo-\\ngrams, with a median of 1.0kg for the controls and 4.1kg for the cases. Thus\\nan increase of 1kg in lifetime tobacco usage accounts for an increase in the\\nodds of coronary heart disease of exp(0.081) = 1.084 or 8.4%. Incorporat-\\ning the standard error we get an approximate 95% conﬁdence interval of\\nexp(0.081±2×0.026) = (1.03,1.14).\\nWe return to these data in Chapter 5, where we see that some of the\\nvariables have nonlinear eﬀects, and when modeled appropriately, are not\\nexcluded from the model.\\n4.4.3 Quadratic Approximations and Inference\\nThe maximum-likelihood parameter estimatesˆβsatisfy a self-consistency\\nrelationship: they are the coeﬃcients of a weighted least squares ﬁt, where\\nthe responses are\\nzi = xT\\ni ˆβ+ (yi −ˆpi)\\nˆpi(1−ˆpi), (4.29)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73bb0e21-d4c6-429e-a701-e47c52bac584', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 143, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Logistic Regression 125\\nand the weights arewi =ˆpi(1−ˆpi), both depending onˆβitself. Apart from\\nproviding a convenient algorithm, this connection with least squares has\\nmore to oﬀer:\\n•The weighted residual sum-of-squares is the familiar Pearson chi-\\nsquare statistic\\nN∑\\ni=1\\n(yi −ˆpi)2\\nˆpi(1−ˆpi), (4.30)\\na quadratic approximation to the deviance.\\n•Asymptotic likelihood theory says that if the model is correct, then\\nˆβis consistent (i.e., converges to thetrue β).\\n•A central limit theorem then shows that the distribution ofˆβcon-\\nverges toN(β,(XTWX)−1). This and other asymptotics can be de-\\nrived directly from the weighted least squares ﬁt by mimicking normal\\ntheory inference.\\n•Model building can be costly for logistic regression models, because\\neach model ﬁtted requires iteration. Popular shortcuts are theRao\\nscore testwhich tests for inclusion of a term, and theWald testwhich\\ncan be used to test for exclusion of a term. Neither of these require\\niterative ﬁtting, and are based on the maximum-likelihood ﬁt of the\\ncurrent model. It turns out that both of these amount to adding\\nor dropping a term from the weighted least squares ﬁt, using the\\nsame weights. Such computations can be done eﬃciently, without\\nrecomputing the entire weighted least squares ﬁt.\\nSoftware implementations can take advantage of these connections. For\\nexample, the generalized linear modeling software in R (which includes lo-\\ngistic regression as part of the binomial family of models) exploits them\\nfully.GLM(generalizedlinearmodel)objectscanbetreatedaslinearmodel\\nobjects, and all the tools available for linear models can be applied auto-\\nmatically.\\n4.4.4 L1 Regularized Logistic Regression\\nThe L1 penalty used in the lasso (Section 3.4.2) can be used for variable\\nselection and shrinkage with any linear regression model. For logistic re-\\ngression, we would maximize a penalized version of (4.20):\\nmax\\nβ0,β\\n⎧\\n⎨\\n⎩\\nN∑\\ni=1\\n[\\nyi(β0 +βTxi)−log(1+ eβ0+βT xi)\\n]\\n−λ\\np∑\\nj=1\\n|βj|\\n⎫\\n⎬\\n⎭. (4.31)\\nAs with the lasso, we typically do not penalize the intercept term, and stan-\\ndardize the predictors for the penalty to be meaningful. Criterion (4.31) is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d56936e8-9bc2-4dc7-8097-c91abf804252', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 144, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='126 4. Linear Methods for Classiﬁcation\\nconcave, and a solution can be found using nonlinear programming meth-\\nods (Koh et al., 2007, for example). Alternatively, using the same quadratic\\napproximations that were used in the Newton algorithm in Section 4.4.1,\\nwe can solve (4.31) by repeated application of a weighted lasso algorithm.\\nInterestingly, the score equations [see (4.24)] for the variables with non-zero\\ncoeﬃcients have the form\\nxT\\nj (y−p)= λ·sign(βj), (4.32)\\nwhich generalizes (3.58) in Section 3.4.4; the active variables are tied in\\ntheir generalizedcorrelation with the residuals.\\nPath algorithms such as LAR for lasso are more diﬃcult, because the\\ncoeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\\nprogress can be made using quadratic approximations.\\n*********************************************************************************************************************************************************************************************************************************************\\n0.0 0.5 1.0 1.5 2.0\\n0.0 0.2 0.4 0.6*********************************************************************************************************************************************************************************************************************************************\\n*********************************************************************************************************************************************************************************************************************************************\\n*********************************************************************************************************************************************************************************************************************************************\\n*********************************************************************************************************************************************************************************************************************************************\\n******************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\\nobesity\\nalcohol\\nsbp\\ntobaccoldl\\nfamhist\\nage\\n12 4 56 7\\nCoeﬃcientsβj(λ)\\n||β(λ)||1\\nFIGURE 4.13. L1 regularized logistic regression coeﬃcients for the South\\nAfrican heart disease data, plotted as a function of theL1 norm. The variables\\nwere all standardized to have unit variance. The proﬁles are computed exactly at\\neach of the plotted points.\\nFigure 4.13 shows the L1 regularization path for the South African\\nheart disease data of Section 4.4.2. This was produced using theR package\\nglmpath (Park and Hastie, 2007), which usespredictor–correctormethods\\nof convex optimization to identify the exact values ofλat which the active\\nset of non-zero coeﬃcients changes (vertical lines in the ﬁgure). Here the\\nproﬁles look almost linear; in other examples the curvature will be more\\nvisible.\\nCoordinate descent methods (Section 3.8.6) are very eﬃcient for comput-\\ning the coeﬃcient proﬁles on a grid of values forλ.T h eR packageglmnet', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89f17d77-c299-40e7-9871-e20655bb204c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 145, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4 Logistic Regression 127\\n(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\\ngression problems eﬃciently (large inN or p). Their algorithms can exploit\\nsparsity in the predictor matrixX, which allows for even larger problems.\\nSee Section 18.4 for more details, and a discussion ofL1-regularized multi-\\nnomial models.\\n4.4.5 Logistic Regression or LDA?\\nIn Section 4.3 we ﬁnd that the log-posterior odds between classk and K\\nare linear functions ofx (4.9):\\nlog Pr(G = k|X = x)\\nPr(G = K|X = x) =l o g πk\\nπK\\n−1\\n2(μk +μK)TΣ−1(μk −μK)\\n+xTΣ−1(μk −μK)\\n= αk0 +αT\\nk x. (4.33)\\nThis linearity is a consequence of the Gaussian assumption for the class\\ndensities, as well as the assumption of a common covariance matrix. The\\nlinear logistic model (4.17) by construction has linear logits:\\nlog Pr(G = k|X = x)\\nPr(G = K|X = x) = βk0 +βT\\nk x. (4.34)\\nItseemsthatthemodelsarethesame.Although theyhaveexactly thesame\\nform, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\\nlogistic regression model is more general, in that it makes less assumptions.\\nWe can write thejoint densityof X and G as\\nPr(X,G = k)=P r (X)Pr(G = k|X), (4.35)\\nwhere Pr(X) denotes the marginal density of the inputsX.F o rb o t hL D A\\nand logistic regression, the second term on the right has the logit-linear\\nform\\nPr(G = k|X = x)= eβk0+βT\\nk x\\n1+ ∑K−1\\nℓ=1 eβℓ0+βT\\nℓ x, (4.36)\\nwhere we have again arbitrarily chosen the last class as the reference.\\nThe logistic regression model leaves the marginal density ofX as an arbi-\\ntrary density function Pr(X), and ﬁts the parameters of Pr(G|X)b ym a x -\\nimizing theconditional likelihood—the multinomial likelihood with proba-\\nbilities the Pr(G = k|X). Although Pr(X) is totally ignored, we can think\\nof this marginal density as being estimated in a fully nonparametric and\\nunrestricted fashion, using the empirical distribution function which places\\nmass 1/N at each observation.\\nWith LDA we ﬁt the parameters by maximizing the full log-likelihood,\\nbased on the joint density\\nPr(X,G = k)= φ(X;μk,Σ)πk, (4.37)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='008d8aaf-49e5-40d2-b171-e3e10e8d462d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 146, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='128 4. Linear Methods for Classiﬁcation\\nwhere φis the Gaussian density function. Standard normal theory leads\\neasily to the estimates ˆμk,ˆΣ,a n dˆπk given in Section 4.3. Since the linear\\nparameters of the logistic form (4.33) are functions of the Gaussian param-\\neters, we get their maximum-likelihood estimates by plugging in the corre-\\nsponding estimates. However, unlike in the conditional case, the marginal\\ndensity Pr(X) does play a role here. It is a mixture density\\nPr(X)=\\nK∑\\nk=1\\nπkφ(X;μk,Σ), (4.38)\\nwhich also involves the parameters.\\nWhat role can this additional component/restriction play? By relying\\non the additional model assumptions, we have more information about the\\nparameters, and hence can estimate them more eﬃciently (lower variance).\\nIf in fact the truefk(x) are Gaussian, then in the worst case ignoring this\\nmarginal part of the likelihood constitutes a loss of eﬃciency of about 30%\\nasymptotically in the error rate (Efron, 1975). Paraphrasing: with 30%\\nmore data, the conditional likelihood will do as well.\\nFor example, observations far from the decision boundary (which are\\ndown-weighted by logistic regression) play a role in estimating the common\\ncovariance matrix. This is not all good news, because it also means that\\nLDA is not robust to gross outliers.\\nFrom the mixture formulation, it is clear that even observations without\\nclass labels have information about the parameters. Often it is expensive\\nto generate class labels, but unclassiﬁed observations come cheaply. By\\nrelying on strong model assumptions, such as here, we can use both types\\nof information.\\nThe marginal likelihood can be thought of as a regularizer, requiring\\nin some sense that class densities bevisible from this marginal view. For\\nexample, if the data in a two-class logistic regression model can be per-\\nfectly separated by a hyperplane, the maximum likelihood estimates of the\\nparameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\\ncients for the same data will be well deﬁned, since the marginal likelihood\\nwill not permit these degeneracies.\\nIn practice these assumptions are never correct, and often some of the\\ncomponents ofX are qualitative variables. It is generally felt that logistic\\nregression is a safer, more robust bet than the LDA model, relying on fewer\\nassumptions. It is our experience that the models give very similar results,\\neven when LDA is used inappropriately, such as with qualitative predictors.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e2aaa57f-833a-43d5-9a6e-fee1c9ac9d26', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 147, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Separating Hyperplanes 129\\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\\norange line is the least squares solution, which misclassiﬁes one of the training\\npoints. Also shown are two blue separating hyperplanes found by theperceptron\\nlearning algorithmwith diﬀerent random starts.\\n4.5 Separating Hyperplanes\\nWe have seen that linear discriminant analysis and logistic regression both\\nestimate linear decision boundaries in similar but slightly diﬀerent ways.\\nFor the rest of this chapter we describe separating hyperplane classiﬁers.\\nThese procedures construct linear decision boundaries that explicitly try\\nto separate the data into diﬀerent classes as well as possible. They provide\\nthe basis for support vector classiﬁers, discussed in Chapter 12. The math-\\nematical level of this section is somewhat higher than that of the previous\\nsections.\\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be\\nseparated by a linear boundary. Included in the ﬁgure (blue lines) are two\\nof the inﬁnitely many possibleseparating hyperplanes. The orange line is\\nthe least squares solution to the problem, obtained by regressing the−1/1\\nresponse Y on X (with intercept); the line is given by\\n{x : ˆβ0 + ˆβ1x1 + ˆβ2x2 =0}. (4.39)\\nThis least squares solution does not do a perfect job in separating the\\npoints, and makes one error. This is the same boundary found by LDA,\\nin light of its equivalence with linear regression in the two-class case (Sec-\\ntion 4.3 and Exercise 4.2).\\nClassiﬁers such as (4.39), that compute a linear combination of the input\\nfeatures and return the sign, were calledperceptronsin the engineering liter-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6d4d0e51-b137-4b93-9778-3e436bc7d95b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 148, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='130 4. Linear Methods for Classiﬁcation\\nx0 x\\nβ∗\\nβ0 +βTx =0\\nFIGURE 4.15.The linear algebra of a hyperplane (aﬃne set).\\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\\nfor the neural network models of the 1980s and 1990s.\\nBefore we continue, let us digress slightly and review some vector algebra.\\nFigure 4.15 depicts a hyperplane oraﬃne setL deﬁned by the equation\\nf(x)= β0 +βTx =0 ;s i n c ew ea r ei nI R2 this is a line.\\nHere we list some properties:\\n1. For any two pointsx1 and x2 lying inL, βT(x1 −x2) = 0, and hence\\nβ∗= β/||β|| is the vector normal to the surface ofL.\\n2. For any pointx0 in L, βTx0 =−β0.\\n3. The signed distance of any pointx to L is given by\\nβ∗T(x−x0)= 1\\n∥β∥(βTx+β0)\\n= 1\\n||f′(x)||f(x). (4.40)\\nHence f(x) is proportional to the signed distance fromx to the hyperplane\\ndeﬁned byf(x)=0 .\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm\\nThe perceptron learning algorithmtries to ﬁnd a separating hyperplane by\\nminimizing the distance of misclassiﬁed points to the decision boundary. If', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='792cef00-a3b9-4c6e-9c19-5557b2a1c312', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 149, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Separating Hyperplanes 131\\nar e s p o n s eyi = 1 is misclassiﬁed, thenxT\\ni β+β0 < 0, and the opposite for\\na misclassiﬁed response withyi =−1. The goal is to minimize\\nD(β,β0)= −\\n∑\\ni∈M\\nyi(xT\\ni β+β0), (4.41)\\nwhere M indexes the set of misclassiﬁed points. The quantity is non-\\nnegative and proportional to the distance of the misclassiﬁed points to\\nthe decision boundary deﬁned byβTx + β0 = 0. The gradient (assuming\\nM is ﬁxed) is given by\\n∂D(β,β0)\\n∂β = −\\n∑\\ni∈M\\nyixi, (4.42)\\n∂D(β,β0)\\n∂β0\\n= −\\n∑\\ni∈M\\nyi. (4.43)\\nThe algorithm in fact uses stochastic gradient descent to minimize this\\npiecewise linear criterion. This means that rather than computing the sum\\nof the gradient contributions of each observation followed by a step in the\\nnegative gradient direction, a step is taken after each observation is visited.\\nHence the misclassiﬁed observations are visited in some sequence, and the\\nparameters βare updated via\\n⎤\\nβ\\nβ0\\n⎦\\n←\\n⎤\\nβ\\nβ0\\n⎦\\n+ρ\\n⎤\\nyixi\\nyi\\n⎦\\n. (4.44)\\nHere ρis the learning rate, which in this case can be taken to be 1 without\\nloss in generality. If the classes are linearly separable, it can be shown that\\nthe algorithm converges to a separating hyperplane in a ﬁnite number of\\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, each\\nstarted at a diﬀerent random guess.\\nThere are a number of problems with this algorithm, summarized in\\nRipley (1996):\\n•When the data are separable, there are many solutions, and which\\none is found depends on the starting values.\\n•The “ﬁnite” number of steps can be very large. The smaller the gap,\\nthe longer the time to ﬁnd it.\\n•When the data are not separable, the algorithm will not converge,\\nand cycles develop. The cycles can be long and therefore hard to\\ndetect.\\nThe second problem can often be eliminated by seeking a hyperplane not\\nin the original space, but in a much enlarged space obtained by creating', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='92fe225b-79c2-46f6-be1a-b00f6fffe242', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 150, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='132 4. Linear Methods for Classiﬁcation\\nmany basis-function transformations of the original variables. This is anal-\\nogous to driving the residuals in a polynomial regression problem down\\nto zero by making the degree suﬃciently large. Perfect separation cannot\\nalways be achieved: for example, if observations from two diﬀerent classes\\nshare the same input. It may not be desirable either, since the resulting\\nmodel is likely to be overﬁt and will not generalize well. We return to this\\npoint at the end of the next section.\\nA rather elegant solution to the ﬁrst problem is to add additional con-\\nstraints to the separating hyperplane.\\n4.5.2 Optimal Separating Hyperplanes\\nThe optimal separating hyperplaneseparates the two classes and maximizes\\nthe distance to the closest point from either class (Vapnik, 1996). Not only\\ndoes this provide a unique solution to the separating hyperplane problem,\\nbut by maximizing the margin between the two classes on the training data,\\nthis leads to better classiﬁcation performance on test data.\\nWeneedtogeneralizecriterion(4.41).Considertheoptimizationproblem\\nmax\\nβ,β0,||β||=1\\nM\\nsubject toyi(xT\\ni β+β0)≥M, i =1 ,...,N.\\n(4.45)\\nThe set of conditions ensure that all the points are at least a signed\\ndistance M from the decision boundary deﬁned byβand β0, and we seek\\nthe largest suchM and associated parameters. We can get rid of the||β|| =\\n1 constraint by replacing the conditions with\\n1\\n||β||yi(xT\\ni β+β0)≥M, (4.46)\\n(which redeﬁnesβ0) or equivalently\\nyi(xT\\ni β+β0)≥M||β||. (4.47)\\nSince for anyβand β0 satisfying these inequalities, any positively scaled\\nmultiple satisﬁes them too, we can arbitrarily set||β|| =1 /M. Thus (4.45)\\nis equivalent to\\nmin\\nβ,β0\\n1\\n2||β||2\\nsubject toyi(xT\\ni β+β0)≥1,i =1 ,...,N.\\n(4.48)\\nIn light of (4.40), the constraints deﬁne an empty slab or margin around the\\nlinear decision boundary of thickness 1/||β||. Hence we chooseβand β0 to\\nmaximize its thickness. This is a convex optimization problem (quadratic', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='23ee96a1-ac60-455b-a80e-e5753854264b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 151, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5 Separating Hyperplanes 133\\ncriterion with linear inequality constraints). The Lagrange (primal) func-\\ntion, to be minimized w.r.t.βand β0,i s\\nLP = 1\\n2||β||2 −\\nN∑\\ni=1\\nαi[yi(xT\\ni β+β0)−1]. (4.49)\\nSetting the derivatives to zero, we obtain:\\nβ =\\nN∑\\ni=1\\nαiyixi, (4.50)\\n0=\\nN∑\\ni=1\\nαiyi, (4.51)\\nand substituting these in (4.49) we obtain the so-called Wolfe dual\\nLD =\\nN∑\\ni=1\\nαi −1\\n2\\nN∑\\ni=1\\nN∑\\nk=1\\nαiαkyiykxT\\ni xk\\nsubject toαi ≥0a n d\\nN∑\\ni=1\\nαiyi =0 . (4.52)\\nThe solution is obtained by maximizingLD in the positive orthant, a sim-\\npler convex optimization problem, for which standard software can be used.\\nIn addition the solution must satisfy the Karush–Kuhn–Tucker conditions,\\nwhich include (4.50), (4.51), (4.52) and\\nαi[yi(xT\\ni β+β0)−1] = 0∀i. (4.53)\\nFrom these we can see that\\n•if αi > 0, then yi(xT\\ni β+ β0) = 1, or in other words,xi is on the\\nboundary of the slab;\\n•if yi(xT\\ni β+β0) > 1, xi is not on the boundary of the slab, andαi =0 .\\nFrom (4.50) we see that the solution vectorβis deﬁned in terms of a linear\\ncombination of the support points xi—those points deﬁned to be on the\\nboundary of the slab viaαi > 0. Figure 4.16 shows the optimal separating\\nhyperplane for our toy example; there are three support points. Likewise,\\nβ0 is obtained by solving (4.53) for any of the support points.\\nThe optimal separating hyperplane produces a functionˆf(x)= xT ˆβ+ ˆβ0\\nfor classifying new observations:\\nˆG(x)=s i g nˆf(x). (4.54)\\nAlthough none of the training observations fall in the margin (by con-\\nstruction), this will not necessarily be the case for test observations. The', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='150e8927-9761-4128-83dc-e8e71b8aa278', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 152, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='134 4. Linear Methods for Classiﬁcation\\nFIGURE 4.16.The same data as in Figure 4.14. The shaded region delineates\\nthe maximum margin separating the two classes. There are three support points\\nindicated, which lie on the boundary of the margin, and the optimal separating\\nhyperplane (blue line) bisects the slab. Included in the ﬁgure is the boundary found\\nusing logistic regression (red line), which is very close to the optimal separating\\nhyperplane (see Section 12.3.3).\\nintuition is that a large margin on the training data will lead to good\\nseparation on the test data.\\nThe description of the solution in terms of support points seems to sug-\\ngest that the optimal hyperplane focuses more on the points that count,\\nand is more robust to model misspeciﬁcation. The LDA solution, on the\\nother hand, depends on all of the data, even points far away from the de-\\ncision boundary. Note, however, that the identiﬁcation of these support\\npoints required the use of all the data. Of course, if the classes are really\\nGaussian, then LDA is optimal, and separating hyperplanes will pay a price\\nfor focusing on the (noisier) data at the boundaries of the classes.\\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\\nlem, ﬁt by maximum likelihood. Both solutions are similar in this case.\\nWhen a separating hyperplane exists, logistic regression will always ﬁnd\\nit, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).\\nThe logistic regression solution shares some other qualitative features with\\nthe separating hyperplane solution. The coeﬃcient vector is deﬁned by a\\nweighted least squares ﬁt of a zero-mean linearized response on the input\\nfeatures, and the weights are larger for points near the decision boundary\\nthan for those further away.\\nWhen the data are not separable, there will be no feasible solution to\\nthis problem, and an alternative formulation is needed. Again one can en-\\nlarge the space using basis transformations, but this can lead to artiﬁcial', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='764f5c58-e31e-49af-b031-fd0f7a641570', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 153, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 135\\nseparation through over-ﬁtting. In Chapter 12 we discuss a more attractive\\nalternative known as thesupport vector machine, which allows for overlap,\\nbut minimizes a measure of the extent of this overlap.\\nBibliographic Notes\\nGood general texts on classiﬁcation include Duda et al. (2000), Hand\\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1979) have\\na concise discussion of linear discriminant analysis. Michie et al. (1994)\\ncompare a large number of popular classiﬁers on benchmark datasets. Lin-\\near separating hyperplanes are discussed in Vapnik (1996). Our account of\\nthe perceptron learning algorithm follows Ripley (1996).\\nExercises\\nEx. 4.1 Show how to solve the generalized eigenvalue problem maxaTBa\\nsubject toaTWa = 1 by transforming to a standard eigenvalue problem.\\nEx. 4.2Suppose we have featuresx∈IRp, a two-class response, with class\\nsizes N1,N2, and the target coded as−N/N1,N/N2.\\n(a) Show that the LDA rule classiﬁes to class 2 if\\nxT ˆΣ\\n−1\\n(ˆμ2 −ˆμ1) > 1\\n2(ˆμ2 +ˆμ1)T ˆΣ\\n−1\\n(ˆμ2 −ˆμ1)−log(N2/N1),\\nand class 1 otherwise.\\n(b) Consider minimization of the least squares criterion\\nN∑\\ni=1\\n(yi −β0 −xT\\ni β)2. (4.55)\\nShow that the solutionˆβsatisﬁes\\n[\\n(N −2)ˆΣ+N ˆΣB\\n]\\nβ= N(ˆμ2 −ˆμ1) (4.56)\\n(after simpliﬁcation), whereˆΣB = N1N2\\nN2 (ˆμ2 −ˆμ1)(ˆμ2 −ˆμ1)T.\\n(c) Hence show thatˆΣBβis in the direction (ˆμ2 −ˆμ1) and thus\\nˆβ∝ˆΣ\\n−1\\n(ˆμ2 −ˆμ1). (4.57)\\nTherefore the least-squares regression coeﬃcient is identical to the\\nLDA coeﬃcient, up to a scalar multiple.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60988011-d496-44b3-8f12-367829afeee1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 154, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='136 4. Linear Methods for Classiﬁcation\\n(d) Show that this result holds for any (distinct) coding of the two classes.\\n(e) Find the solution ˆβ0 (up to the same scalar multiple as in (c), and\\nhence the predicted valueˆf(x)= ˆβ0 + xT ˆβ. Consider the following\\nrule: classify to class 2 ifˆf(x) > 0 and class 1 otherwise. Show this is\\nnot the same as the LDA rule unless the classes have equal numbers\\nof observations.\\n(Fisher, 1936; Ripley, 1996)\\nEx. 4.3 Suppose we transform the original predictorsX to ˆY via linear\\nregression. In detail, let ˆY = X(XTX)−1XTY = XˆB,w h e r eY is the\\nindicator response matrix. Similarly for any inputx∈IRp,w eg e tat r a n s -\\nformed vector ˆy = ˆBTx ∈IRK. Show that LDA using ˆY is identical to\\nLDA in the original space.\\nEx. 4.4Consider the multilogit model withK classes (4.17). Letβbe the\\n(p +1 ) (K −1)-vector consisting of all the coeﬃcients. Deﬁne a suitably\\nenlarged version of the input vectorx to accommodate this vectorized co-\\neﬃcient matrix. Derive the Newton-Raphson algorithm for maximizing the\\nmultinomial log-likelihood, and describe how you would implement this\\nalgorithm.\\nEx. 4.5Consider a two-class logistic regression problem withx∈IR. Char-\\nacterize the maximum-likelihood estimates of the slope and intercept pa-\\nrameter if the samplexi for the two classes are separated by a pointx0 ∈IR.\\nGeneralize this result to (a)x ∈IRp (see Figure 4.16), and (b) more than\\ntwo classes.\\nEx. 4.6Suppose we haveN points xi in IRp in general position, with class\\nlabels yi ∈{−1,1}. Prove that the perceptron learning algorithm converges\\nto a separating hyperplane in a ﬁnite number of steps:\\n(a) Denote a hyperplane byf(x)= βT\\n1 x + β0 = 0, or in more compact\\nnotation βTx∗ =0 ,w h e r ex∗ =( x,1) and β=( β1,β0). Let zi =\\nx∗\\ni /||x∗\\ni||. Show that separability implies the existence of aβsep such\\nthat yiβT\\nsepzi ≥1 ∀i\\n(b)Givenacurrent βold,theperceptronalgorithmidentiﬁesapoint zi that\\nis misclassiﬁed, and produces the updateβnew ←βold + yizi. Show\\nthat||βnew−βsep||2 ≤||βold−βsep||2−1, and hence that the algorithm\\nconverges to a separating hyperplane in no more than||βstart−βsep||2\\nsteps (Ripley, 1996).\\nEx. 4.7Consider the criterion\\nD∗(β,β0)= −\\nN∑\\ni=1\\nyi(xT\\ni β+β0), (4.58)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d0fc05d0-405d-41d5-9b43-364160e2fd94', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 155, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 137\\na generalization of (4.41) where we sum over all the observations. Consider\\nminimizing D∗subject to||β|| = 1. Describe this criterion in words. Does\\nit solve the optimal separating hyperplane problem?\\nEx. 4.8 Consider the multivariate Gaussian modelX|G = k ∼N(μk,Σ),\\nwith the additional restriction that rank{μk}K\\n1 = L< max(K −1,p).\\nDerive the constrained MLEs for theμk and Σ. Show that the Bayes clas-\\nsiﬁcation rule is equivalent to classifying in the reduced subspace computed\\nby LDA (Hastie and Tibshirani, 1996b).\\nEx. 4.9 Write a computer program to perform a quadratic discriminant\\nanalysis by ﬁtting a separate Gaussian model per class. Try it out on the\\nvowel data, and compute the misclassiﬁcation error for the test data. The\\ndatacanbefoundinthebookwebsite www-stat.stanford.edu/ElemStatLearn.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='57572e62-c810-4606-b3b0-8303600858e0', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 156, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5\\nBasis Expansions and Regularization\\n5.1 Introduction\\nWe have already made use of models linear in the input features, both for\\nregression and classiﬁcation. Linear regression, linear discriminant analysis,\\nlogistic regression and separating hyperplanes all rely on a linear model.\\nIt is extremely unlikely that the true functionf(X) is actually linear in\\nX. In regression problems,f(X)=E (Y|X) will typically be nonlinear and\\nnonadditive inX, and representingf(X) by a linear model is usually a con-\\nvenient, and sometimes a necessary, approximation. Convenient because a\\nlinear model is easy to interpret, and is the ﬁrst-order Taylor approxima-\\ntion tof(X). Sometimes necessary, because withN small and/orp large,\\na linear model might be all we are able to ﬁt to the data without overﬁt-\\nting. Likewise in classiﬁcation, a linear, Bayes-optimal decision boundary\\nimplies that some monotone transformation of Pr(Y =1|X) is linear inX.\\nThis is inevitably an approximation.\\nIn this chapter and the next we discuss popular methods for moving\\nbeyond linearity. The core idea in this chapter is to augment/replace the\\nvector of inputsX with additional variables, which are transformations of\\nX, and then use linear models in this new space of derived input features.\\nDenote by hm(X):I R p ↦→ IR the mth transformation of X, m =\\n1,...,M .W et h e nm o d e l\\nf(X)=\\nM∑\\nm=1\\nβmhm(X), (5.1)\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 139\\nDOI: 10.1007/b94608_5,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44d38ae0-6a65-40d4-9df3-7f34dcc2abb5', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 157, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='140 5. Basis Expansions and Regularization\\na linear basis expansionin X. The beauty of this approach is that once the\\nbasis functions hm have been determined, the models are linear in these\\nnew variables, and the ﬁtting proceeds as before.\\nSome simple and widely used examples of thehm are the following:\\n•hm(X)= Xm,m =1 ,...,p recovers the original linear model.\\n•hm(X)= X2\\nj orhm(X)= XjXk allowsustoaugmenttheinputswith\\npolynomial terms to achieve higher-order Taylor expansions. Note,\\nhowever, that the number of variables grows exponentially in the de-\\ngree of the polynomial. A full quadratic model inp variables requires\\nO(p2) square and cross-product terms, or more generallyO(pd)f o ra\\ndegree-d polynomial.\\n•hm(X)=l o g (Xj),\\n√\\nXj,... permits other nonlinear transformations\\nof single inputs. More generally one can use similar functions involv-\\ning several inputs, such ashm(X)= ||X||.\\n•hm(X)= I(Lm ≤Xk <U m), an indicator for a region of Xk.B y\\nbreaking the range ofXk up into Mk such nonoverlapping regions\\nresults in a model with a piecewise constant contribution forXk.\\nSometimestheproblemathandwillcallforparticularbasisfunctions hm,\\nsuchaslogarithmsorpowerfunctions.Moreoften,however,weusethebasis\\nexpansions as a device to achieve more ﬂexible representations forf(X).\\nPolynomials are an example of the latter, although they are limited by\\ntheir global nature—tweaking the coeﬃcients to achieve a functional form\\nin one region can cause the function to ﬂap about madly in remote regions.\\nIn this chapter we consider more useful families ofpiecewise-polynomials\\nand splines that allow for local polynomial representations. We also discuss\\nthe wavelet bases, especially useful for modeling signals and images. These\\nmethods produce adictionaryD consisting of typically a very large number\\n|D| of basis functions, far more than we can aﬀord to ﬁt to our data. Along\\nwith the dictionary we require a method for controlling the complexity\\nof our model, using basis functions from the dictionary. There are three\\ncommon approaches:\\n•Restriction methods, where we decide before-hand to limit the class\\nof functions. Additivity is an example, where we assume that our\\nmodel has the form\\nf(X)=\\np∑\\nj=1\\nfj(Xj)\\n=\\np∑\\nj=1\\nMj∑\\nm=1\\nβjmhjm(Xj). (5.2)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9148ef55-63f3-4dac-a795-6d62e0e8da1f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 158, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Piecewise Polynomials and Splines 141\\nThe size of the model is limited by the number of basis functionsMj\\nused for each component functionfj.\\n•Selection methods, which adaptively scan the dictionary and include\\nonly those basis functionshm that contribute signiﬁcantly to the ﬁt of\\nthe model. Here the variable selection techniques discussed in Chap-\\nter 3 are useful. The stagewise greedy approaches such as CART,\\nMARS and boosting fall into this category as well.\\n•Regularization methods where we use the entire dictionary but re-\\nstrict the coeﬃcients. Ridge regression is a simple example of a regu-\\nlarization approach, while the lasso is both a regularization and selec-\\ntion method. Here we discuss these and more sophisticated methods\\nfor regularization.\\n5.2 Piecewise Polynomials and Splines\\nWe assume until Section 5.7 thatX is one-dimensional. A piecewise poly-\\nnomial functionf(X) is obtained by dividing the domain ofX into contigu-\\nous intervals, and representingf by a separate polynomial in each interval.\\nFigure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\\nconstant, with three basis functions:\\nh1(X)= I(X<ξ1),h 2(X)= I(ξ1 ≤X<ξ2),h 3(X)= I(ξ2 ≤X).\\nSince these are positive over disjoint regions, the least squares estimate of\\nthe modelf(X)= ∑3\\nm=1 βmhm(X) amounts toˆβm = ¯Ym, the mean ofY\\nin themth region.\\nThe top right panel shows a piecewise linear ﬁt. Three additional basis\\nfunctions are needed:hm+3 = hm(X)X, m =1 ,..., 3. Except in special\\ncases, we would typically prefer the third panel, which is also piecewise\\nlinear, but restricted to be continuous at the two knots. These continu-\\nity restrictions lead to linear constraints on the parameters; for example,\\nf(ξ−\\n1 )= f(ξ+\\n1 ) implies thatβ1 +ξ1β4 = β2 +ξ1β5. In this case, since there\\nare two restrictions, we expect toget backtwo parameters, leaving four free\\nparameters.\\nA more direct way to proceed in this case is to use a basis that incorpo-\\nrates the constraints:\\nh1(X)=1 ,h 2(X)= X, h 3(X)=( X−ξ1)+,h 4(X)=( X−ξ2)+,\\nwhere t+ denotes the positive part. The functionh3 i ss h o w ni nt h el o w e r\\nright panel of Figure 5.1. We often prefer smoother functions, and these\\ncan be achieved by increasing the order of the local polynomial. Figure 5.2\\nshows a series of piecewise-cubic polynomials ﬁt to the same data, with', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e4cfed8-7db2-4c9d-bd2d-cb30a537fd13', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 159, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='142 5. Basis Expansions and Regularization\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nPiecewise Constant\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nPiecewise Linear\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nContinuous Piecewise Linear Piecewise-linear Basis Function\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81 \\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nξ1ξ1\\nξ1ξ1\\nξ2ξ2\\nξ2ξ2\\n(X −ξ1)+\\nFIGURE 5.1.The top left panel shows a piecewise constant function ﬁt to some\\nartiﬁcial data. The broken vertical lines indicate the positions of the twoknots\\nξ1 and ξ2. The blue curve represents the true function, from which the data were\\ngenerated with Gaussian noise. The remaining two panels show piecewise lin-\\near functions ﬁt to the same data—the top right unrestricted, and the lower left\\nrestricted to be continuous at the knots. The lower right panel shows a piecewise–\\nlinear basis function, h3(X)=( X −ξ1)+, continuous at ξ1. The black points\\nindicate the sample evaluationsh3(xi),i =1 ,...,N .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f304306-b613-4d57-bc84-921bddc62a54', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 160, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Piecewise Polynomials and Splines 143\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO O\\nDiscontinuous\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO O\\nContinuous\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO O\\nContinuous First Derivative\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO O\\nContinuous Second Derivative\\nPiecewise Cubic Polynomials\\nξ1ξ1\\nξ1ξ1\\nξ2ξ2\\nξ2ξ2\\nFIGURE 5.2.A series of piecewise-cubic polynomials, with increasing orders of\\ncontinuity.\\nincreasing orders of continuity at the knots. The function in the lower\\nright panel is continuous, and has continuous ﬁrst and second derivatives\\nat the knots. It is known as acubic spline. Enforcing one more order of\\ncontinuity would lead to a global cubic polynomial. It is not hard to show\\n(Exercise 5.1) that the following basis represents a cubic spline with knots\\nat ξ1 and ξ2:\\nh1(X)=1 ,h 3(X)= X2,h 5(X)=( X−ξ1)3\\n+,\\nh2(X)= X, h 4(X)= X3,h 6(X)=( X−ξ2)3\\n+.\\n(5.3)\\nTherearesixbasisfunctionscorrespondingtoasix-dimensionallinearspace\\nof functions. A quick check conﬁrms the parameter count: (3 regions)×(4\\nparameters per region)−(2 knots)×(3 constraints per knot)= 6.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db9bacd6-ad1c-40bc-b3a1-d12117de24d3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 161, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='144 5. Basis Expansions and Regularization\\nMore generally, an order-M spline with knots ξj,j =1 ,...,K is a\\npiecewise-polynomial of order M, and has continuous derivatives up to\\norder M −2. A cubic spline hasM = 4. In fact the piecewise-constant\\nfunction in Figure 5.1 is an order-1 spline, while the continuous piece-\\nwise linear function is an order-2 spline. Likewise the general form for the\\ntruncated-power basis set would be\\nhj(X)= Xj−1,j =1 ,...,M,\\nhM+ℓ(X)=( X−ξℓ)M−1\\n+ ,ℓ =1 ,...,K.\\nIt is claimed that cubic splines are the lowest-order spline for which the\\nknot-discontinuity is not visible to the human eye. There is seldom any\\ngood reason to go beyond cubic-splines, unless one is interested in smooth\\nderivatives. In practice the most widely used orders areM =1 ,2a n d4 .\\nThese ﬁxed-knot splines are also known asregression splines. One needs\\nto select the order of the spline, the number of knots and their placement.\\nOne simple approach is to parameterize a family of splines by the number\\nof basis functions or degrees of freedom, and have the observationsxi de-\\ntermine the positions of the knots. For example, the expressionbs(x,df=7)\\nin R generates a basis matrix of cubic-spline functions evaluated at theN\\nobservations inx,w i t ht h e7−3=4 1 interior knots at the appropriate per-\\ncentilesof x (20, 40, 60and80th.)Onecanbemoreexplicit,however; bs(x,\\ndegree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\\nwith three interior knots, and returns anN ×4m a t r i x .\\nSincethe spaceofspline functions ofaparticular orderandknot sequence\\nis a vector space, there are many equivalent bases for representing them\\n(just as there are for ordinary polynomials.) While the truncated power\\nbasis is conceptually simple, it is not too attractive numerically: powers of\\nlarge numbers can lead to severe rounding problems. TheB-spline basis,\\ndescribed in the Appendix to this chapter, allows for eﬃcient computations\\neven when the number of knotsK is large.\\n5.2.1 Natural Cubic Splines\\nWe know that the behavior of polynomials ﬁt to data tends to be erratic\\nnear the boundaries, and extrapolation can be dangerous. These problems\\nare exacerbated with splines. The polynomials ﬁt beyond the boundary\\nknots behave even more wildly than the corresponding global polynomials\\nin that region. This can be conveniently summarized in terms of the point-\\nwise variance of spline functions ﬁt by least squares (see the example in the\\nnext section for details on these variance calculations). Figure 5.3 compares\\n1A cubic spline with four knots is eight-dimensional. Thebs() function omits by\\ndefault the constant term in the basis, since terms like this are typically included with\\nother terms in the model.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97c5909c-8146-42a5-ba2e-acb710b3bb02', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 162, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Piecewise Polynomials and Splines 145\\nX\\nPointwise Variances\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81 \\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81\\nGlobal Linear\\nGlobal Cubic Polynomial\\nCubic Spline - 2 knots\\nNatural Cubic Spline - 6 knots\\nFIGURE 5.3.Pointwise variance curves for four diﬀerent models, withX con-\\nsisting of 50 points drawn at random fromU[0,1], and an assumed error model\\nwith constant variance. The linear and cubic polynomial ﬁts have two and four\\ndegrees of freedom, respectively, while the cubic spline and natural cubic spline\\neach have six degrees of freedom. The cubic spline has two knots at0.33 and 0.66,\\nwhile the natural spline has boundary knots at0.1 and 0.9, and four interior knots\\nuniformly spaced between them.\\nthe pointwise variances for a variety of diﬀerent models. The explosion of\\nthe variance near the boundaries is clear, and inevitably is worst for cubic\\nsplines.\\nA natural cubic splineadds additional constraints, namely that the func-\\ntion is linear beyond the boundary knots. This frees up four degrees of\\nfreedom (two constraints each in both boundary regions), which can be\\nspent more proﬁtably by sprinkling more knots in the interior region. This\\ntradeoﬀ is illustrated in terms of variance in Figure 5.3. There will be a\\nprice paid in bias near the boundaries, but assuming the function is lin-\\near near the boundaries (where we have less information anyway) is often\\nconsidered reasonable.\\nA natural cubic spline withK knots is represented byK basis functions.\\nOne can start from a basis for cubic splines, and derive the reduced ba-\\nsis by imposing the boundary constraints. For example, starting from the\\ntruncated power series basis described in Section 5.2, we arrive at (Exer-\\ncise 5.4):\\nN1(X)=1 ,N 2(X)= X, N k+2(X)= dk(X)−dK−1(X), (5.4)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31cb1363-ad96-4ada-8d5d-ba58ebd371ac', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 163, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='146 5. Basis Expansions and Regularization\\nwhere\\ndk(X)= (X−ξk)3\\n+ −(X−ξK)3\\n+\\nξK −ξk\\n. (5.5)\\nEach of these basis functions can be seen to have zero second and third\\nderivative forX ≥ξK.\\n5.2.2 Example: South African Heart Disease (Continued)\\nIn Section 4.4.2 we ﬁt linear logistic regression models to the South African\\nheart disease data. Here we explore nonlinearities in the functions using\\nnatural splines. The functional form of the model is\\nlogit[Pr(chd|X)] =θ0 +h1(X1)Tθ1 +h2(X2)Tθ2 +··· +hp(Xp)Tθp, (5.6)\\nwhere each of theθj are vectors of coeﬃcients multiplying their associated\\nvector of natural spline basis functionshj.\\nWe use four natural spline bases for each term in the model. For example,\\nwith X1 representing sbp, h1(X1) is a basis consisting of four basis func-\\ntions. This actually implies three rather than two interior knots (chosen at\\nuniform quantiles ofsbp), plus two boundary knots at the extremes of the\\ndata, since we exclude the constant term from each of thehj.\\nSince famhist is a two-level factor, it is coded by a simple binary or\\ndummy variable, and is associated with a single coeﬃcient in the ﬁt of the\\nmodel.\\nMore compactly we can combine allp vectors of basis functions (and\\nthe constant term) into one big vectorh(X), and then the model is simply\\nh(X)Tθ, with total number of parameters df = 1 +∑p\\nj=1 dfj,t h es u mo f\\nthe parameters in each component term. Each basis function is evaluated\\nat each of theN samples, resulting in aN × df basis matrixH.A tt h i s\\npoint the model is like any other linear logistic model, and the algorithms\\ndescribed in Section 4.4.1 apply.\\nWe carried out a backward stepwise deletion process, dropping terms\\nfrom this model while preserving the group structure of each term, rather\\nthan dropping one coeﬃcient at a time. The AIC statistic (Section 7.5) was\\nused to drop terms, and all the terms remaining in the ﬁnal model would\\ncause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4\\nshows a plot of the ﬁnal model selected by the stepwise regression. The\\nfunctions displayed are ˆfj(Xj)= hj(Xj)T ˆθj for each variable Xj.T h e\\ncovariance matrix Cov(ˆθ)= Σis estimated byˆΣ=( HTWH)−1,w h e r eW\\nis the diagonal weight matrix from the logistic regression. Hencevj(Xj)=\\nVar[ˆfj(Xj)] =hj(Xj)T ˆΣjjhj(Xj) is the pointwise variance function ofˆfj,\\nwhereCov( ˆθj)= ˆΣjj istheappropriatesub-matrixof ˆΣ.Theshadedregion\\nin each panel is deﬁned byˆfj(Xj)±2\\n√\\nvj(Xj).\\nThe AIC statistic is slightly more generous than the likelihood-ratio test\\n(deviance test). Both sbp and obesity are included in this model, while', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99badb42-67af-49a2-930e-4451357bc99a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 164, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Piecewise Polynomials and Splines 147\\n100 120 140 160 180 200 220\\n-2 0 2 4\\n0 5 10 15 20 25 30\\n02468\\n2468 1 0 1 2 1 4\\n-4 -2 0 2 4\\n-4 -2 0 2 4\\nAbsent Present\\n15 20 25 30 35 40 45\\n- 2 0246\\n20 30 40 50 60\\n-6 -4 -2 0 2\\nˆf(sbp)\\nsbp\\nˆf(tobacco)\\ntobacco\\nˆf(ldl)\\nldl\\nˆf(obesity)\\nobesity\\nˆf(age)\\nage\\nˆf(famhist)\\nfamhist\\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁnal\\nmodel selected by the stepwise procedure. Included are pointwise standard-error\\nbands. Therug plotat the base of each ﬁgure indicates the location of each of the\\nsample values for that variable (jittered to break ties).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bf8bc67-8a8d-46ad-a02f-692cda08aead', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 165, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='148 5. Basis Expansions and Regularization\\nTABLE 5.1. Final logistic regression model, after stepwise deletion of natural\\nsplines terms. The column labeled “LRT” is the likelihood-ratio test statistic when\\nthat term is deleted from the model, and is the change in deviance from the full\\nmodel (labeled “none”).\\nTerms Df Deviance AIC LRT P-value\\nnone 458.09 502.09\\nsbp 4 467.16 503.16 9.076 0.059\\ntobacco 4 470.48 506.48 12.387 0.015\\nldl 4 472.39 508.39 14.307 0.006\\nfamhist 1 479.44 521.44 21.356 0.000\\nobesity 4 466.24 502.24 8.147 0.086\\nage 4 481.86 517.86 23.768 0.000\\nthey were not in the linear model. The ﬁgure explains why, since their\\ncontributions are inherently nonlinear. These eﬀects at ﬁrst may come as\\na surprise, but an explanation lies in the nature of the retrospective data.\\nThese measurements were made sometime after the patients suﬀered a\\nheart attack, and in many cases they had already beneﬁted from a healthier\\ndiet and lifestyle, hence the apparent increase in risk at low values for\\nobesity and sbp. Table 5.1 shows a summary of the selected model.\\n5.2.3 Example: Phoneme Recognition\\nIn this example we use splines to reduce ﬂexibility rather than increase it;\\nthe application comes under the general heading offunctional modeling. In\\nthe top panel of Figure 5.5 are displayed a sample of 15 log-periodograms\\nfor each of the two phonemes “aa” and “ao” measured at 256 frequencies.\\nThe goal is to use such data to classify a spoken phoneme. These two\\nphonemes were chosen because they are diﬃcult to separate.\\nThe input feature is a vectorx of length 256, which we can think of as\\na vector of evaluations of a functionX(f) over a grid of frequenciesf.I n\\nreality there is a continuous analog signal which is a function of frequency,\\nand we have a sampled version of it.\\nThe gray lines in the lower panel of Figure 5.5 show the coeﬃcients of\\na linear logistic regression model ﬁt by maximum likelihood to a training\\nsample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s. The\\ncoeﬃcients are also plotted as a function of frequency, and in fact we can\\nthink of the model in terms of its continuous counterpart\\nlog Pr(aa|X)\\nPr(ao|X) =\\n∫\\nX(f)β(f)df, (5.7)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7f4dfc78-eee8-4490-9ad8-9be17f1e9853', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 166, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Piecewise Polynomials and Splines 149\\nFrequency\\nLog-periodogram\\n0 50 100 150 200 250\\n0 5 10 15 20 25\\nPhoneme Examples\\naa\\nao\\nFrequency\\nLogistic Regression Coefficients\\n0 50 100 150 200 250\\n-0.4 -0.2 0.0 0.2 0.4\\nPhoneme Classification: Raw and Restricted Logistic Regression\\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre-\\nquency for15 examples each of the phonemes “aa” and “ao” sampled from a total\\nof 695 “aa”s and1022 “ao”s. Each log-periodogram is measured at256 uniformly\\nspaced frequencies. The lower panel shows the coeﬃcients (as a function of fre-\\nquency) of a logistic regression ﬁt to the data by maximum likelihood, using the\\n256 log-periodogram values as inputs. The coeﬃcients are restricted to be smooth\\nin the red curve, and are unrestricted in the jagged gray curve.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6cffc178-4a99-4d70-a7be-3c3f241a2bae', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 167, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='150 5. Basis Expansions and Regularization\\nwhich we approximate by\\n256∑\\nj=1\\nX(fj)β(fj)=\\n256∑\\nj=1\\nxjβj. (5.8)\\nThe coeﬃcients compute a contrast functional, and will have appreciable\\nvalues in regions of frequency where the log-periodograms diﬀer between\\nthe two classes.\\nThe gray curves are very rough. Since the input signals have fairly strong\\npositive autocorrelation, this results in negative autocorrelation in the co-\\neﬃcients. In addition the sample size eﬀectively provides only four obser-\\nvations per coeﬃcient.\\nApplications such as this permit a natural regularization. We force the\\ncoeﬃcientstovarysmoothlyasafunctionoffrequency.Theredcurveinthe\\nlower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\\ndata. Weseethat the lower frequencies oﬀerthe most discriminatory power.\\nNot only does the smoothing allow easier interpretation of the contrast, it\\nalso produces a more accurate classiﬁer:\\nRaw Regularized\\nTraining error 0.080 0.185\\nTest error 0.255 0.158\\nThe smooth red curve was obtained through a very simple use of natural\\ncubic splines. We can represent the coeﬃcient function as an expansion of\\nsplines β(f)= ∑M\\nm=1 hm(f)θm. In practice this means thatβ= Hθwhere,\\nH is a p× M basis matrix of natural cubic splines, deﬁned on the set of\\nfrequencies. Here we usedM = 12 basis functions, with knots uniformly\\nplaced over the integers 1,2,..., 256 representing the frequencies. Since\\nxTβ= xTHθ, we can simply replace the input featuresx by theirﬁltered\\nversions x∗= HTx, and ﬁtθby linear logistic regression on thex∗.T h e\\nred curve is thusˆβ(f)= h(f)T ˆθ.\\n5.3 Filtering and Feature Extraction\\nIn the previous example, we constructed ap×M basis matrixH, and then\\ntransformed our features x into new features x∗ = HTx. These ﬁltered\\nversions of the features were then used as inputs into a learning procedure:\\nin the previous example, this was linear logistic regression.\\nPreprocessing of high-dimensional features is a very general and pow-\\nerful method for improving the performance of a learning algorithm. The\\npreprocessing need not be linear as it was above, but can be a general', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f2df1c71-40ad-4b29-8e05-0b72e6da487b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 168, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4 Smoothing Splines 151\\n(nonlinear) function of the formx∗= g(x). The derived featuresx∗can\\nthen be used as inputs into any (linear or nonlinear) learning procedure.\\nFor example, for signal or image recognition a popular approach is to ﬁrst\\ntransform the raw features via a wavelet transformx∗= HTx (Section 5.9)\\nand then use the featuresx∗as inputs into a neural network (Chapter 11).\\nWavelets are eﬀective in capturing discrete jumps or edges, and the neural\\nnetwork is a powerful tool for constructing nonlinear functions of these\\nfeatures for predicting the target variable. By using domain knowledge\\nto construct appropriate features, one can often improve upon a learning\\nmethod that has only the raw featuresx at its disposal.\\n5.4 Smoothing Splines\\nHere we discuss a spline basis method that avoids the knot selection prob-\\nlem completely by using a maximal set of knots. The complexity of the ﬁt\\nis controlled by regularization. Consider the following problem: among all\\nfunctions f(x) with two continuous derivatives, ﬁnd one that minimizes the\\npenalized residual sum of squares\\nRSS(f,λ)=\\nN∑\\ni=1\\n{yi −f(xi)}2 +λ\\n∫\\n{f′′(t)}2dt, (5.9)\\nwhere λis a ﬁxedsmoothing parameter. The ﬁrst term measures closeness\\nto the data, while the second term penalizes curvature in the function, and\\nλestablishes a tradeoﬀ between the two. Two special cases are:\\nλ=0: f can be any function that interpolates the data.\\nλ= ∞: the simple least squares line ﬁt, since no second derivative can\\nbe tolerated.\\nThese vary from very rough to very smooth, and the hope is thatλ∈(0,∞)\\nindexes an interesting class of functions in between.\\nThe criterion (5.9) is deﬁned on an inﬁnite-dimensional function space—\\nin fact, a Sobolev space of functions for which the second term is deﬁned.\\nRemarkably, it can be shown that (5.9) has an explicit, ﬁnite-dimensional,\\nunique minimizer which is a natural cubic spline with knots at the unique\\nvalues of thexi,i =1 ,...,N (Exercise 5.7). At face value it seems that\\nthe family is still over-parametrized, since there are as many asN knots,\\nwhich impliesN degrees of freedom. However, the penalty term translates\\nto a penalty on the spline coeﬃcients, which are shrunk some of the way\\ntoward the linear ﬁt.\\nSince the solution is a natural spline, we can write it as\\nf(x)=\\nN∑\\nj=1\\nNj(x)θj, (5.10)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='833de8d1-4286-4042-9a89-0105c8561762', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 169, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='152 5. Basis Expansions and Regularization\\nAge\\nRelative Change in Spinal BMD\\n10 15 20 25\\n-0.05 0.0 0.05 0.10 0.15 0.20\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81 \\x81 \\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nMale\\nFemale\\nFIGURE 5.6.The response is the relative change in bone mineral density mea-\\nsured at the spine in adolescents, as a function of age. A separate smoothing spline\\nwas ﬁt to the males and females, withλ≈0.00022. This choice corresponds to\\nabout 12 degrees of freedom.\\nwhere the Nj(x)a r ea nN-dimensional set of basis functions for repre-\\nsenting this family of natural splines (Section 5.2.1 and Exercise 5.4). The\\ncriterion thus reduces to\\nRSS(θ,λ)=( y−Nθ)T(y−Nθ)+ λθTΩNθ, (5.11)\\nwhere {N}ij = Nj(xi)a n d{ΩN}jk =\\n∫\\nN′′\\nj (t)N′′\\nk (t)dt. The solution is\\neasily seen to be\\nˆθ=( NTN+λΩN)−1NTy, (5.12)\\na generalized ridge regression. The ﬁtted smoothing spline is given by\\nˆf(x)=\\nN∑\\nj=1\\nNj(x)ˆθj. (5.13)\\nEﬃcient computational techniques for smoothing splines are discussed in\\nthe Appendix to this chapter.\\nF i g u r e5 . 6s h o w sas m o o t h i n gs p l i n eﬁ tt os o m ed a t ao nb o n em i n e r a l\\ndensity (BMD) in adolescents. The response is relative change in spinal\\nBMD over two consecutive visits, typically about one year apart. The data\\nare color coded by gender, and two separate curves were ﬁt. This simple', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d0267b1-bc26-4654-bb33-b0326fa25350', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 170, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4 Smoothing Splines 153\\nsummary reinforces the evidence in the data that the growth spurt for\\nfemales precedes that for males by about two years. In both cases the\\nsmoothing parameterλwas approximately 0.00022; this choice is discussed\\nin the next section.\\n5.4.1 Degrees of Freedom and Smoother Matrices\\nWe have not yet indicated howλis chosen for the smoothing spline. Later\\nin this chapter we describe automatic methods using techniques such as\\ncross-validation. In this section we discuss intuitive ways of prespecifying\\nthe amount of smoothing.\\nA smoothing spline with prechosenλis an example of alinear smoother\\n(as in linear operator). This is because the estimated parameters in (5.12)\\nare a linear combination of theyi.D e n o t eb yˆf the N-vector of ﬁtted values\\nˆf(xi) at the training predictorsxi.T h e n\\nˆf = N(NTN+λΩN)−1NTy\\n= Sλy. (5.14)\\nAgain the ﬁt is linear iny, and the ﬁnite linear operatorSλ is known as\\nthe smoother matrix. One consequence of this linearity is that the recipe\\nfor producing ˆf from y does not depend ony itself; Sλ depends only on\\nthe xi and λ.\\nLinear operators are familiar in more traditional least squares ﬁtting as\\nwell. Suppose Bξ is a N × M matrix of M cubic-spline basis functions\\nevaluated at theN training pointsxi, with knot sequenceξ,a n dM ≪ N.\\nThen the vector of ﬁtted spline values is given by\\nˆf = Bξ(BT\\nξBξ)−1BT\\nξy\\n= Hξy. (5.15)\\nHere the linear operatorHξis a projection operator, also known as thehat\\nmatrix in statistics. There are some important similarities and diﬀerences\\nbetween Hξand Sλ:\\n•Both are symmetric, positive semideﬁnite matrices.\\n•HξHξ= Hξ(idempotent), whileSλSλ⪯ Sλ, meaning that the right-\\nhand side exceeds the left-hand side by a positive semideﬁnite matrix.\\nThis is a consequence of theshrinking nature ofSλ, which we discuss\\nfurther below.\\n•Hξhas rankM, whileSλhas rankN.\\nThe expressionM =t r a c e (Hξ) gives the dimension of the projection space,\\nwhich is also the number of basis functions, and hence the number of pa-\\nrameters involved in the ﬁt. By analogy we deﬁne theeﬀective degrees of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='048daf7f-a966-45b0-92b9-9b2fffd044c8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 171, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='154 5. Basis Expansions and Regularization\\nfreedomof a smoothing spline to be\\ndfλ= trace(Sλ), (5.16)\\nthe sum of the diagonal elements ofSλ. This very useful deﬁnition allows\\nus a more intuitive way to parameterize the smoothing spline, and indeed\\nmany other smoothers as well, in a consistent fashion. For example, in Fig-\\nure 5.6 we speciﬁed dfλ= 12 for each of the curves, and the corresponding\\nλ≈0.00022 was derived numerically by solving trace(Sλ) = 12. There are\\nmany arguments supporting this deﬁnition of degrees of freedom, and we\\ncover some of them here.\\nSince Sλ is symmetric (and positive semideﬁnite), it has a real eigen-\\ndecomposition. Before we proceed, it is convenient to rewriteSλ in the\\nReinsch form\\nSλ=( I+λK)−1, (5.17)\\nwhere K does not depend onλ(Exercise 5.9). Sinceˆf = Sλy solves\\nmin\\nf\\n(y−f)T(y−f)+ λfTKf, (5.18)\\nK is known as thepenalty matrix, and indeed a quadratic form inK has\\na representation in terms of a weighted sum of squared (divided) second\\ndiﬀerences. The eigen-decomposition ofSλis\\nSλ=\\nN∑\\nk=1\\nρk(λ)ukuT\\nk (5.19)\\nwith\\nρk(λ)= 1\\n1+ λdk\\n, (5.20)\\nand dk the corresponding eigenvalue ofK. Figure 5.7 (top) shows the re-\\nsults of applying a cubic smoothing spline to some air pollution data (128\\nobservations). Two ﬁts are given: asmoother ﬁt corresponding to a larger\\npenalty λand arougher ﬁt for a smaller penalty. The lower panels repre-\\nsent the eigenvalues (lower left) and some eigenvectors (lower right) of the\\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\\nresentation are the following:\\n•Theeigenvectorsarenotaﬀectedbychangesin λ,andhencethewhole\\nfamily of smoothing splines (for a particular sequencex) indexed by\\nλhave the same eigenvectors.\\n•Sλy =∑N\\nk=1 ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\\nates by decomposingy w.r.t. the (complete) basis{uk}, and diﬀer-\\nentially shrinking the contributions usingρk(λ). This is to be con-\\ntrasted with a basis-regression method, where the components are', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30ac5d22-04d7-4f5e-aa8f-e2cb5b6145a4', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 172, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4 Smoothing Splines 155\\nDaggot Pressure Gradient\\nOzone Concentration\\n-50 0 50 100\\n01 0 2 0 3 0\\n\\x81\\x81\\n\\x81\\n\\x81\\x81 \\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nOrder\\nEigenvalues\\n5 1 01 52 02 5\\n-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81 \\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\ndf=5\\ndf=11\\n-50 0 50 100 -50 0 50 100\\nFIGURE 5.7.(Top:) Smoothing spline ﬁt of ozone concentration versus Daggot\\npressure gradient. The two ﬁts correspond to diﬀerent values of the smoothing\\nparameter, chosen to achieve ﬁve and eleven eﬀective degrees of freedom, deﬁned\\nby dfλ= trace(Sλ).( L o w e rl e f t : )F i r s t25 eigenvalues for the two smoothing-spline\\nmatrices. The ﬁrst two are exactly1,a n da l la r e≥0. (Lower right:) Third to\\nsixth eigenvectors of the spline smoother matrices. In each case,uk is plotted\\nagainst x, and as such is viewed as a function ofx.T h erug at the base of the\\nplots indicate the occurrence of data points. The damped functions represent the\\nsmoothed versions of these functions (using the5 df smoother).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='025e6abb-8ab2-41db-9ecd-3530dd68395f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 173, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='156 5. Basis Expansions and Regularization\\neither left alone, or shrunk to zero—that is, a projection matrix such\\nas Hξabove has M eigenvalues equal to 1, and the rest are 0. For\\nthis reason smoothing splines are referred to asshrinking smoothers,\\nwhile regression splines areprojectionsmoothers (see Figure 3.17 on\\npage 80).\\n•The sequence ofuk, ordered by decreasingρk(λ), appear to increase\\nin complexity. Indeed, they have the zero-crossing behavior of polyno-\\nmials of increasing degree. SinceSλuk = ρk(λ)uk,w es e eh o we a c ho f\\nthe eigenvectors themselves are shrunk by the smoothing spline: the\\nhigher the complexity, the more they are shrunk. If the domain ofX\\nis periodic, then theuk are sines and cosines at diﬀerent frequencies.\\n•The ﬁrst two eigenvalues arealways one, and they correspond to the\\ntwo-dimensional eigenspace of functions linear inx (Exercise 5.11),\\nwhich are never shrunk.\\n•The eigenvaluesρk(λ)=1 /(1 +λdk) are an inverse function of the\\neigenvalues dk of the penalty matrixK, moderated byλ; λcontrols\\nt h er a t ea tw h i c ht h eρk(λ) decrease to zero.d1 = d2 = 0 and again\\nlinear functions are not penalized.\\n•One can reparametrize the smoothing spline using the basis vectors\\nuk (the Demmler–Reinsch basis). In this case the smoothing spline\\nsolves\\nmin\\nθ\\n∥y−Uθ∥2 +λθTDθ, (5.21)\\nwhere U has columnsuk and D is a diagonal matrix with elements\\ndk.\\n•dfλ = trace(Sλ)= ∑N\\nk=1 ρk(λ). For projection smoothers, all the\\neigenvalues are 1, each one corresponding to a dimension of the pro-\\njection subspace.\\nFigure 5.8 depicts a smoothing spline matrix, with the rows ordered with\\nx. The banded nature of this representation suggests that a smoothing\\nspline is a local ﬁtting method, much like the locally weighted regression\\nprocedures in Chapter 6. The right panel shows in detail selected rows of\\nS, which we call theequivalent kernels.A s λ→0, dfλ→N,a n dSλ→I,\\nthe N-dimensional identity matrix. Asλ→∞,d fλ→2, andSλ→H,t h e\\nhat matrix for linear regression onx.\\n5.5 Automatic Selection of the Smoothing\\nParameters\\nThe smoothing parameters for regression splines encompass the degree of\\nthe splines, and the number and placement of the knots. For smoothing', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5b2d5810-a5dc-4c8d-92dc-f31baad1ee26', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 174, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.5 Automatic Selection of the Smoothing Parameters 157\\n115\\n100\\n75\\n50\\n25\\n12\\nSmoother Matrix\\n\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\nRow 115\\n\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81\\n\\x81\\nRow 100\\n\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81 \\x81\\nRow 75\\n\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\nRow 50\\n\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81 \\x81\\nRow 25\\n\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81 \\x81 \\x81\\nRow 12\\nEquivalent Kernels\\nFIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\\nindicating an equivalent kernel with local support. The left panel represents the\\nelements ofS as an image. The right panel shows the equivalent kernel or weight-\\ning function in detail for the indicated rows.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2e322bf3-89cc-472f-a761-c44e539e6113', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 175, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='158 5. Basis Expansions and Regularization\\nsplines, we have only the penalty parameterλto select, since the knots are\\nat all the unique trainingX’s, and cubic degree is almost always used in\\npractice.\\nSelecting the placement and number of knots for regression splines can be\\na combinatorially complex task, unless some simpliﬁcations are enforced.\\nThe MARS procedure in Chapter 9 uses a greedy algorithm with some\\nadditional approximations to achieve a practical compromise. We will not\\ndiscuss this further here.\\n5.5.1 Fixing the Degrees of Freedom\\nSince dfλ= trace(Sλ) is monotone inλfor smoothing splines, we can in-\\nvert the relationship and specify λby ﬁxing df. In practice this can be\\nachieved by simple numerical methods. So, for example, inR one can use\\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\\nages a more traditional mode of model selection, where we might try a cou-\\nple of diﬀerent values of df, and select one based on approximateF-tests,\\nresidual plots and other more subjective criteria. Using df in this way pro-\\nvides a uniform approach to compare many diﬀerent smoothing methods.\\nIt is particularly useful ingeneralized additive models(Chapter 9), where\\nseveral smoothing methods can be simultaneously used in one model.\\n5.5.2 The Bias–Variance Tradeoﬀ\\nFigure 5.9 shows the eﬀect of the choice of dfλ when using a smoothing\\nspline on a simple example:\\nY = f(X)+ ε,\\nf(X)= sin(12(X +0.2))\\nX +0.2 ,\\n(5.22)\\nwith X ∼U[0,1] andε∼N(0,1). Our training sample consists ofN = 100\\npairs xi,yi drawn independently from this model.\\nThe ﬁtted splines for three diﬀerent values of dfλare shown. The yellow\\nshaded region in the ﬁgure represents the pointwise standard error ofˆfλ,\\nthat is, we have shaded the region between ˆfλ(x) ± 2 · se(ˆfλ(x)). Since\\nˆf = Sλy,\\nCov(ˆf)= SλCov(y)ST\\nλ\\n= SλST\\nλ. (5.23)\\nThe diagonal contains the pointwise variances at the trainingxi. The bias\\nis given by\\nBias(ˆf)= f −E(ˆf)\\n= f −Sλf, (5.24)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fd0a1fb-315c-4cf0-ac09-9725295f7e86', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 176, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.5 Automatic Selection of the Smoothing Parameters 159\\n6 8 10 12 14\\n1.0 1.1 1.2 1.3 1.4 1.5\\n0.0 0.2 0.4 0.6 0.8 1.0\\n−4 −2 0 2\\ny\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\n0.0 0.2 0.4 0.6 0.8 1.0\\n−4 −2 0 2\\ny\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\n0.0 0.2 0.4 0.6 0.8 1.0\\n−4 −2 0 2\\ny\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\nEPE\\nCV\\nXX\\nX\\ndfλ =5\\ndfλ =9 d f λ =1 5\\ndfλ\\nCross-Validation\\nEPE(λ)a n dC V (λ)\\nFIGURE 5.9. The top left panel shows theEPE(λ) and CV(λ) curves for a\\nrealization from a nonlinear additive error model (5.22). The remaining panels\\nshow the data, the true functions (in purple), and the ﬁtted curves (in green) with\\nyellow shaded±2× standard error bands, for three diﬀerent values of dfλ.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='130ebbb3-10fa-4267-a0fe-e220aafc475c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 177, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='160 5. Basis Expansions and Regularization\\nwhere f is the (unknown) vector of evaluations of the truef at the training\\nX’s. The expectations and variances are with respect to repeated draws\\nof samples of size N = 100 from the model (5.22). In a similar fashion\\nVar(ˆfλ(x0)) and Bias(ˆfλ(x0)) can be computed at any point x0 (Exer-\\ncise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\\nof the bias-variance tradeoﬀ associated with selecting the smoothing\\nparameter.\\ndfλ=5 :The spline under ﬁts, and clearlytrims down the hills and ﬁlls in\\nthe valleys. This leads to a bias that is most dramatic in regions of\\nhigh curvature. The standard error band is very narrow, so we esti-\\nmate a badly biased version of the true function with great reliability!\\ndfλ=9 :Here the ﬁtted function is close to the true function, although a\\nslight amount of bias seems evident. The variance has not increased\\nappreciably.\\ndfλ = 15: The ﬁtted function is somewhat wiggly, but close to the true\\nfunction. The wiggliness also accounts for the increased width of the\\nstandard error bands—the curve is starting to follow some individual\\npoints too closely.\\nNote that in these ﬁgures we are seeing a single realization of data and\\nhence ﬁtted spline ˆf in each case, while the bias involves an expectation\\nE(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the\\nbias is shown as well. The middle curve seems “just right,” in that it has\\nachieved a good compromise between bias and variance.\\nThe integrated squared prediction error (EPE) combines both bias and\\nvariance in a single summary:\\nEPE(ˆfλ)=E ( Y −ˆfλ(X))2\\n=V a r (Y)+E\\n[\\nBias2(ˆfλ(X))+Var( ˆfλ(X))\\n]\\n= σ2 +MSE( ˆfλ). (5.25)\\nNote that this is averaged both over the training sample (giving rise toˆfλ),\\nand the values of the (independently chosen) prediction points (X,Y ). EPE\\nis a natural quantity of interest, and does create a tradeoﬀ between bias\\nand variance. The blue points in the top left panel of Figure 5.9 suggest\\nthat dfλ= 9 is spot on!\\nSince we don’t know the true function, we do not have access to EPE, and\\nneed an estimate. This topic is discussed in some detail in Chapter 7, and\\ntechniques such as K-fold cross-validation, GCV andCp are all in common\\nuse. In Figure 5.9 we include theN-fold (leave-one-out) cross-validation\\ncurve:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c0b69c8-07c5-4753-b65b-fcad47ee72ef', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 178, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.6 Nonparametric Logistic Regression 161\\nCV(ˆfλ)= 1\\nN\\nN∑\\ni=1\\n(yi −ˆf(−i)\\nλ (xi))2 (5.26)\\n= 1\\nN\\nN∑\\ni=1\\n⎤\\nyi −ˆfλ(xi)\\n1−Sλ(i,i)\\n⎦2\\n, (5.27)\\nwhich can (remarkably) be computed for each value ofλfrom the original\\nﬁtted values and the diagonal elementsSλ(i,i)o fSλ(Exercise 5.13).\\nThe EPE and CV curves have a similar shape, but the entire CV curve\\nis above the EPE curve. For some realizations this is reversed, and overall\\nthe CV curve is approximately unbiased as an estimate of the EPE curve.\\n5.6 Nonparametric Logistic Regression\\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\\nsetting. It is typically straightforward to transfer this technology to other\\ndomains. Here we consider logistic regression with a single quantitative\\ninput X. The model is\\nlog Pr(Y =1|X = x)\\nPr(Y =0|X = x) = f(x), (5.28)\\nwhich implies\\nPr(Y =1|X = x)= ef(x)\\n1+ ef(x). (5.29)\\nFitting f(x) in a smooth fashion leads to a smooth estimate of the condi-\\ntional probability Pr(Y =1|x), which can be used for classiﬁcation or risk\\nscoring.\\nWe construct the penalized log-likelihood criterion\\nℓ(f;λ)=\\nN∑\\ni=1\\n[yi logp(xi)+(1 −yi)log(1 −p(xi))]−1\\n2λ\\n∫\\n{f′′(t)}2dt\\n=\\nN∑\\ni=1\\n[\\nyif(xi)−log(1+ ef(xi))\\n]\\n−1\\n2λ\\n∫\\n{f′′(t)}2dt, (5.30)\\nwhere we have abbreviatedp(x) = Pr(Y =1|x). The ﬁrst term in this ex-\\npression is the log-likelihood based on the binomial distribution (c.f. Chap-\\nter 4, page 120). Arguments similar to those used in Section 5.4 show that\\nthe optimalf is a ﬁnite-dimensional natural spline with knots at the unique', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf2685b0-547f-4afe-a4d5-72cdb15fa8bc', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 179, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='162 5. Basis Expansions and Regularization\\nvalues ofx. This means that we can representf(x)= ∑N\\nj=1 Nj(x)θj.W e\\ncompute the ﬁrst and second derivatives\\n∂ℓ(θ)\\n∂θ = NT(y−p)−λΩθ, (5.31)\\n∂2ℓ(θ)\\n∂θ∂θT = −NTWN−λΩ, (5.32)\\nwhere p is theN-vector with elementsp(xi), andW is a diagonal matrix\\nof weightsp(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear inθ,s o\\nwe need to use an iterative algorithm as in Section 4.4.1. Using Newton–\\nRaphson as in (4.23) and (4.26) for linear logistic regression, the update\\nequation can be written\\nθnew =( NTWN+λΩ)−1NTW\\n⎤\\nNθold +W−1(y−p)\\n⎦\\n=( NTWN+λΩ)−1NTWz. (5.33)\\nWe can also express this update in terms of the ﬁtted values\\nfnew = N(NTWN+λΩ)−1NTW\\n⎤\\nfold +W−1(y−p)\\n⎦\\n= Sλ,wz. (5.34)\\nReferring back to (5.12) and (5.14), we see that the update ﬁts a weighted\\nsmoothing spline to the working responsez (Exercise 5.12).\\nThe form of (5.34) is suggestive. It is tempting to replaceSλ,w by any\\nnonparametric (weighted) regression operator, and obtain general fami-\\nlies of nonparametric logistic regression models. Although herex is one-\\ndimensional, this procedure generalizes naturally to higher-dimensionalx.\\nThese extensions are at the heart ofgeneralized additive models,w h i c hw e\\npursue in Chapter 9.\\n5.7 Multidimensional Splines\\nSo far we have focused on one-dimensional spline models. Each of the ap-\\nproaches have multidimensional analogs. SupposeX ∈IR2, and we have\\na basis of functionsh1k(X1),k =1 ,...,M 1 for representing functions of\\ncoordinate X1, and likewise a set ofM2 functions h2k(X2) for coordinate\\nX2. Then theM1 ×M2 dimensional tensor product basisdeﬁned by\\ngjk(X)= h1j(X1)h2k(X2),j =1 ,...,M 1,k =1 ,...,M 2 (5.35)\\ncan be used for representing a two-dimensional function:\\ng(X)=\\nM1∑\\nj=1\\nM2∑\\nk=1\\nθjkgjk(X). (5.36)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1cef1ad7-94c3-4ba7-8e39-05ed52606142', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 180, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.7 Multidimensional Splines 163\\nFIGURE 5.10.A tensor product basis of B-splines, showing some selected pairs.\\nEach two-dimensional function is the tensor product of the corresponding one\\ndimensional marginals.\\nFigure 5.10 illustrates a tensor product basis using B-splines. The coeﬃ-\\ncients can be ﬁt by least squares, as before. This can be generalized tod\\ndimensions, but note that the dimension of the basis grows exponentially\\nfast—yet another manifestation of the curse of dimensionality. The MARS\\nprocedure discussed in Chapter 9 is a greedy forward algorithm for includ-\\ning only those tensor products that are deemed necessary by least squares.\\nFigure 5.11 illustrates the diﬀerence between additive and tensor product\\n(natural) splines on the simulated classiﬁcation example from Chapter 2.\\nA logistic regression model logit[Pr(T|x)] =h(x)Tθis ﬁt to the binary re-\\nsponse, and the estimated decision boundary is the contourh(x)T ˆθ=0 .\\nThe tensor product basis can achieve more ﬂexibility at the decision bound-\\nary, but introduces some spurious structure along the way.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b7559d72-589d-4948-88c3-bcb6aec28c36', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 181, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='164 5. Basis Expansions and Regularization\\nAdditive Natural Cubic Splines - 4 df each\\n............................................................................................................................................................................................................ ...................... ............................ .............................. ................................. .................................. ..................................... ...................................... ........................................ ......................................... ........................................... ............................................ .............................................. ................................................ ................................................. ................................................... ................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. ............................................. ........................................ ...................................... .................................... .................................. ............................... ............................... ............................ ............................ ......................... ......................... ...................... .................... .................... ................. .............. ........................................................................................................................................................................................................................................................................................\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nTraining Error: 0.23\\nTest Error:       0.28\\nBayes Error:    0.21\\nNatural Cubic Splines - Tensor Product - 4 df each\\n........................................................................................................................................................................................................................................................................................................................................................................................................................... .\\n..................................................... .............................. .................................. ................................... .................................... ....................................... ........................................ ........................................ ......................................... ......................................... .......................................... ............................................ ............................................ ............................................. ............................................... .................................................... ...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\n..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ............................................................. .......................................................... ......................................................... ........................................... ....................................... ................................... .................................. ................................ .............................. ............................. ............................. ............................ ............................ .......................... ......................... ......................... ....................... ..................... ............... .............. ..................................................................................................... ................................. ................................... ................................... ................................... .................................. ................................. .............................. ....................... ..................... ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\\no\\no\\nooo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo o\\noo\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no ooo\\no\\no\\no\\noo o\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\noo\\nooo\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\no\\no\\no o\\no\\no\\no\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no\\no\\no\\no\\noo\\no\\no\\no oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nTraining Error: 0.230\\nTest Error:       0.282\\nBayes Error:    0.210\\nFIGURE 5.11.The simulation example of Figure 2.1. The upper panel shows the\\ndecision boundary of an additive logistic regression model, using natural splines\\nin each of the two coordinates (total df=1+( 4 −1) + (4−1) = 7). The lower\\npanel shows the results of using a tensor product of natural spline bases in each\\ncoordinate (total df =4 × 4 = 16). The broken purple boundary is the Bayes\\ndecision boundary for this problem.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6768dbb8-abf7-4b37-b990-85af13288076', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 182, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.7 Multidimensional Splines 165\\nOne-dimensionalsmoothingsplines(viaregularization)generalizetohigh-\\ner dimensions as well. Suppose we have pairsyi,xi with xi ∈IRd,a n dw e\\nseek a d-dimensional regression function f(x). The idea is to set up the\\nproblem\\nmin\\nf\\nN∑\\ni=1\\n{yi −f(xi)}2 +λJ[f], (5.37)\\nwhere J is an appropriate penalty functional for stabilizing a functionf in\\nIRd. For example, a natural generalization of the one-dimensional roughness\\npenalty (5.9) for functions on IR2 is\\nJ[f]=\\n∫∫\\nIR2\\n[⎤∂2f(x)\\n∂x2\\n1\\n⎦2\\n+2\\n⎤∂2f(x)\\n∂x1∂x2\\n⎦2\\n+\\n⎤∂2f(x)\\n∂x2\\n2\\n⎦2]\\ndx1dx2. (5.38)\\nOptimizing (5.37) with this penalty leads to a smooth two-dimensional\\nsurface, known as a thin-plate spline. It shares many properties with the\\none-dimensional cubic smoothing spline:\\n•as λ→0, the solution approaches an interpolating function [the one\\nwith smallest penalty (5.38)];\\n•as λ→∞, the solution approaches the least squares plane;\\n•for intermediate values of λ, the solution can be represented as a\\nlinear expansion of basis functions, whose coeﬃcients are obtained\\nby a form of generalized ridge regression.\\nThe solution has the form\\nf(x)= β0 +βTx+\\nN∑\\nj=1\\nαjhj(x), (5.39)\\nwhere hj(x)= ||x−xj||2 log||x−xj||.T h e s ehj are examples of radial\\nbasis functions, which are discussed in more detail in the next section. The\\ncoeﬃcients are found by plugging (5.39) into (5.37), which reduces to a\\nﬁnite-dimensional penalized least squares problem. For the penalty to be\\nﬁnite, the coeﬃcients αj have to satisfy a set of linear constraints; see\\nExercise 5.14.\\nThin-plate splines are deﬁned more generally for arbitrary dimensiond,\\nfor which an appropriately more generalJ is used.\\nThere are a number of hybrid approaches that are popular in practice,\\nboth for computational and conceptual simplicity. Unlike one-dimensional\\nsmoothing splines, the computational complexity for thin-plate splines is\\nO(N3), since there is not in general any sparse structure that can be ex-\\nploited. However, as with univariate smoothing splines, we can get away\\nwith substantially less than theN knots prescribed by the solution (5.39).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e1383cf-d4b3-4402-99fc-ce0c466570d1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 183, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='166 5. Basis Expansions and Regularization\\n125\\n130\\n135\\n140\\n145\\n150\\n155\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n20 30 40 50 60\\nAge\\nObesity\\nSystolic Blood Pressure\\n120\\n125\\n130\\n135\\n140\\n145\\n150\\n155\\n160\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\nFIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a\\ncontour plot. The response issystolic blood pressure, modeled as a function\\nof age and obesity. The data points are indicated, as well as the lattice of points\\nused as knots. Care should be taken to use knots from the lattice inside the convex\\nhull of the data (red), and ignore those outside (green).\\nIn practice, it is usually suﬃcient to work with a lattice of knots covering\\nthe domain. The penalty is computed for the reduced expansion just as\\nbefore. Using K knots reduces the computations toO(NK2 + K3). Fig-\\nure 5.12 shows the result of ﬁtting a thin-plate spline to some heart disease\\nrisk factors, representing the surface as a contour plot. Indicated are the\\nlocation of the input features, as well as the knots used in the ﬁt. Note that\\nλwas speciﬁed via dfλ=t r a c e (Sλ) = 15.\\nMore generally one can representf ∈IRd as an expansion in any arbi-\\ntrarily large collection of basis functions, and control the complexity by ap-\\nplying a regularizer such as (5.38). For example, we could construct a basis\\nby forming the tensor products of all pairs of univariate smoothing-spline\\nbasis functions as in (5.35), using, for example, the univariateB-splines\\nrecommended in Section 5.9.2 as ingredients. This leads to an exponential', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0058150f-8c92-4721-a24c-2397ad24f29d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 184, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 167\\ngrowth in basis functions as the dimension increases, and typically we have\\nto reduce the number of functions per coordinate accordingly.\\nThe additive spline models discussed in Chapter 9 are a restricted class\\nof multidimensional splines. They can be represented in this general formu-\\nlation as well; that is, there exists a penaltyJ[f] that guarantees that the\\nsolution has the formf(X)= α+f1(X1)+ ··· +fd(Xd) and that each of\\nthe functionsfj are univariate splines. In this case the penalty is somewhat\\ndegenerate, and it is more natural toassume that f is additive, and then\\nsimply impose an additional penalty on each of the component functions:\\nJ[f]= J(f1 +f2 +··· +fd)\\n=\\nd∑\\nj=1\\n∫\\nf′′\\nj (tj)2dtj. (5.40)\\nThese are naturally extended to ANOVA spline decompositions,\\nf(X)= α+\\n∑\\nj\\nfj(Xj)+\\n∑\\nj<k\\nfjk(Xj,Xk)+ ··· , (5.41)\\nwhere each of the components are splines of the required dimension. There\\nare many choices to be made:\\n•The maximum order of interaction—we have shown up to order 2\\nabove.\\n•Which terms to include—not all main eﬀects and interactions are\\nnecessarily needed.\\n•What representation to use—some choices are:\\n– regression splines with a relatively small number of basis func-\\ntions per coordinate, and their tensor products for interactions;\\n– a complete basis as in smoothing splines, and include appropri-\\nate regularizers for each term in the expansion.\\nIn many cases when the number of potential dimensions (features) is large,\\nautomatic methods are more desirable. The MARS and MART procedures\\n(Chapters 9 and 10, respectively), both fall into this category.\\n5.8 Regularization and Reproducing Kernel\\nHilbert Spaces\\nIn this section we cast splines into the larger context of regularization meth-\\nods and reproducing kernel Hilbert spaces. This section is quite technical\\nand can be skipped by the disinterested or intimidated reader.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22d2a60f-4d60-4a55-b41a-9cc56dcfe9ff', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 185, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='168 5. Basis Expansions and Regularization\\nA general class of regularization problems has the form\\nmin\\nf∈H\\n[ N∑\\ni=1\\nL(yi,f (xi))+ λJ(f)\\n]\\n(5.42)\\nwhere L(y,f (x)) is a loss function,J(f) is a penalty functional, andH is\\na space of functions on whichJ(f) is deﬁned. Girosi et al. (1995) describe\\nquite general penalty functionals of the form\\nJ(f)=\\n∫\\nIRd\\n|˜f(s)|2\\n˜G(s)\\nds, (5.43)\\nwhere ˜f denotes the Fourier transform off,a n d˜G is some positive function\\nthat falls oﬀ to zero as||s||→∞. The idea is that 1/˜Gincreases the penalty\\nfor high-frequency components off. Under some additional assumptions\\nthey show that the solutions have the form\\nf(X)=\\nK∑\\nk=1\\nαkφk(X)+\\nN∑\\ni=1\\nθiG(X−xi), (5.44)\\nwhere theφk span the null space of the penalty functionalJ,a n dG is the\\ninverse Fourier transform of ˜G. Smoothing splines and thin-plate splines\\nfall into this framework. The remarkable feature of this solution is that\\nwhile the criterion (5.42) is deﬁned over an inﬁnite-dimensional space, the\\nsolution is ﬁnite-dimensional. In the next sections we look at some speciﬁc\\nexamples.\\n5.8.1 Spaces of Functions Generated by Kernels\\nAn important subclass of problems of the form (5.42) are generated by\\na positive deﬁnite kernel K(x,y), and the corresponding space of func-\\ntionsHK is called areproducing kernel Hilbert space(RKHS). The penalty\\nfunctional J is deﬁned in terms of the kernel as well. We give a brief and\\nsimpliﬁed introduction to this class of models, adapted from Wahba (1990)\\nand Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).\\nLet x,y ∈IRp. We consider the space of functions generated by the linear\\nspan of{K(·,y),y ∈IRp)}; i.e arbitrary linear combinations of the form\\nf(x)= ∑\\nm αmK(x,ym), where each kernel term is viewed as a function\\nof the ﬁrst argument, and indexed by the second. Suppose thatK has an\\neigen-expansion\\nK(x,y)=\\n∞∑\\ni=1\\nγiφi(x)φi(y) (5.45)\\nwith γi ≥0,∑∞\\ni=1 γ2\\ni <∞.E l e m e n t so fHK have an expansion in terms of\\nthese eigen-functions,\\nf(x)=\\n∞∑\\ni=1\\nciφi(x), (5.46)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='018b7d6a-3d2e-431b-b490-cac6cb542ac9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 186, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 169\\nwith the constraint that\\n||f||2\\nHK\\ndef\\n=\\n∞∑\\ni=1\\nc2\\ni/γi <∞, (5.47)\\nwhere ||f||HK is the norm induced byK. The penalty functional in (5.42)\\nfor the spaceHK is deﬁned to be the squared normJ(f)= ||f||2\\nHK .T h e\\nquantity J(f) can be interpreted as a generalized ridge penalty, where\\nfunctions with large eigenvalues in the expansion (5.45) get penalized less,\\nand vice versa.\\nRewriting (5.42) we have\\nmin\\nf∈HK\\n[ N∑\\ni=1\\nL(yi,f (xi))+ λ||f||2\\nHK\\n]\\n(5.48)\\nor equivalently\\nmin\\n{cj}∞\\n1\\n⎡\\n⎣\\nN∑\\ni=1\\nL(yi,\\n∞∑\\nj=1\\ncjφj(xi))+ λ\\n∞∑\\nj=1\\nc2\\nj/γj\\n⎤\\n⎦. (5.49)\\nIt can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\\nto (5.48) is ﬁnite-dimensional, and has the form\\nf(x)=\\nN∑\\ni=1\\nαiK(x,xi). (5.50)\\nThe basis functionhi(x)= K(x,xi) (as a function of the ﬁrst argument) is\\nknown as therepresenter of evaluationat xi inHK, since forf ∈HK,i ti s\\neasily seen that⟨K(·,xi),f⟩HK = f(xi). Similarly⟨K(·,xi),K(·,xj)⟩HK =\\nK(xi,xj) (thereproducingproperty ofHK), and hence\\nJ(f)=\\nN∑\\ni=1\\nN∑\\nj=1\\nK(xi,xj)αiαj (5.51)\\nfor f(x)= ∑N\\ni=1 αiK(x,xi).\\nIn light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dimensional crite-\\nrion\\nmin\\nα\\nL(y,Kα)+ λαTKα. (5.52)\\nWe are using a vector notation, in whichK is theN×N matrix withijth\\nentry K(xi,xj) and so on. Simple numerical algorithms can be used to\\noptimize (5.52). This phenomenon, whereby the inﬁnite-dimensional prob-\\nlem (5.48) or (5.49) reduces to a ﬁnite dimensional optimization problem,\\nhas been dubbed the kernel property in the literature on support-vector\\nmachines (see Chapter 12).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aedf6cb7-0759-4197-9992-aa88be5e1ff1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 187, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='170 5. Basis Expansions and Regularization\\nThere is a Bayesian interpretation of this class of models, in whichf\\nis interpreted as a realization of a zero-mean stationary Gaussian process,\\nwith prior covariance functionK. The eigen-decomposition produces a se-\\nries of orthogonal eigen-functionsφj(x) with associated variancesγj.T h e\\ntypical scenario is that “smooth” functionsφj have large prior variance,\\nwhile “rough” φj have small prior variances. The penalty in (5.48) is the\\ncontribution of the prior to the joint likelihood, and penalizes more those\\ncomponents with smaller prior variance (compare with (5.43)).\\nFor simplicity we have dealt with the case here where all members ofH\\nare penalized, as in (5.48). More generally, there may be some components\\nin H that we wish to leave alone, such as the linear functions for cubic\\nsmoothing splines in Section 5.4. The multidimensional thin-plate splines\\nof Section 5.7 and tensor product splines fall into this category as well.\\nIn these cases there is a more convenient representationH = H0 ⊕H1,\\nwith the null space H0 consisting of, for example, low degree polynomi-\\nals in x that do not get penalized. The penalty becomesJ(f)= ∥P1f∥,\\nwhere P1 is the orthogonal projection off onto H1. The solution has the\\nform f(x)= ∑M\\nj=1 βjhj(x)+ ∑N\\ni=1 αiK(x,xi), where the ﬁrst term repre-\\nsents an expansion inH0. From a Bayesian perspective, the coeﬃcients of\\ncomponents inH0 have improper priors, with inﬁnite variance.\\n5.8.2 Examples of RKHS\\nThe machinery above is driven by the choice of the kernelK and the loss\\nfunction L. We consider ﬁrst regression using squared-error loss. In this\\ncase (5.48) specializes to penalized least squares, and the solution can be\\ncharacterized in two equivalent ways corresponding to (5.49) or (5.52):\\nmin\\n{cj}∞\\n1\\nN∑\\ni=1\\n⎛\\n⎝yi −\\n∞∑\\nj=1\\ncjφj(xi)\\n⎞\\n⎠\\n2\\n+λ\\n∞∑\\nj=1\\nc2\\nj\\nγj\\n(5.53)\\nan inﬁnite-dimensional, generalized ridge regression problem, or\\nmin\\nα\\n(y−Kα)T(y−Kα)+ λαTKα. (5.54)\\nThe solution forαis obtained simply as\\nˆα=( K+λI)−1y, (5.55)\\nand\\nˆf(x)=\\nN∑\\nj=1\\nˆαjK(x,xj). (5.56)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c9b1613-a12e-40a1-8383-81fa97b18833', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 188, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 171\\nThe vector ofN ﬁtted values is given by\\nˆf = Kˆα\\n= K(K+λI)−1y (5.57)\\n=( I+λK−1)−1y. (5.58)\\nThe estimate (5.57) also arises as thekriging estimate of a Gaussian ran-\\ndom ﬁeld in spatial statistics (Cressie, 1993). Compare also (5.58) with the\\nsmoothing spline ﬁt (5.17) on page 154.\\nPenalized Polynomial Regression\\nThe kernel K(x,y)=( ⟨x,y⟩ +1 )d (Vapnik, 1996), for x,y ∈IRp,h a s\\nM =\\n⎤p+d\\nd\\n⎦\\neigen-functions that span the space of polynomials in IRp of\\ntotal degreed. For example, withp =2a n dd =2 ,M =6a n d\\nK(x,y)=1 + 2 x1y1 +2x2y2 +x2\\n1y2\\n1 +x2\\n2y2\\n2 +2x1x2y1y2 (5.59)\\n=\\nM∑\\nm=1\\nhm(x)hm(y) (5.60)\\nwith\\nh(x)T =( 1,\\n√\\n2x1,\\n√\\n2x2,x2\\n1,x2\\n2,\\n√\\n2x1x2). (5.61)\\nOne can represent h in terms of the M orthogonal eigen-functions and\\neigenvalues ofK,\\nh(x)= VD\\n1\\n2\\nγφ(x), (5.62)\\nwhere Dγ=d i a g (γ1,γ2,...,γ M), andV is M ×M and orthogonal.\\nSuppose we wish to solve the penalized polynomial regression problem\\nmin\\n{βm}M\\n1\\nN∑\\ni=1\\n⎤\\nyi −\\nM∑\\nm=1\\nβmhm(xi)\\n⎦2\\n+λ\\nM∑\\nm=1\\nβ2\\nm. (5.63)\\nSubstituting (5.62) into (5.63), we get an expression of the form (5.53) to\\noptimize (Exercise 5.16).\\nThe number of basis functionsM =\\n⎤p+d\\nd\\n⎦\\ncan be very large, often much\\nlarger thanN. Equation (5.55) tells us that if we use the kernel represen-\\ntation for the solution function, we have only to evaluate the kernelN2\\ntimes, and can compute the solution inO(N3)o p e r a t i o n s .\\nThis simplicity is not without implications. Each of the polynomialshm\\nin (5.61) inherits a scaling factor from the particular form ofK, which has\\na bearing on the impact of the penalty in (5.63). We elaborate on this in\\nthe next section.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='38a1d344-3d9b-4986-82c7-d3f21fd9d1b3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 189, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='172 5. Basis Expansions and Regularization\\n−2 −1 0 1 2 3 4\\n0.0 0.4 0.8\\nX\\nRadial Kernel in IR1\\nK(·,xm)\\nFIGURE 5.13.Radial kernelskk(x) for the mixture data, with scale parameter\\nν=1 . The kernels are centered at ﬁve pointsxm chosen at random from the 200.\\nGaussian Radial Basis Functions\\nIn the preceding example, the kernel is chosen because it represents an\\nexpansion of polynomials and can conveniently compute high-dimensional\\ninnerproducts.Inthisexample thekernelischosenbecauseofitsfunctional\\nform in the representation (5.50).\\nThe Gaussian kernelK(x,y)= e−ν||x−y||2\\nalong with squared-error loss,\\nfor example, leads to a regression model that is an expansion in Gaussian\\nradial basis functions,\\nkm(x)= e−ν||x−xm||2\\n,m =1 ,...,N, (5.64)\\neach one centered at one of the training feature vectorsxm. The coeﬃcients\\nare estimated using (5.54).\\nFigure 5.13 illustrates radial kernels in IR1 using the ﬁrst coordinate of\\nthe mixture example from Chapter 2. We show ﬁve of the 200 kernel basis\\nfunctions km(x)= K(x,xm).\\nFigure 5.14 illustrates the implicit feature space for the radial kernel\\nwith x∈IR1. We computed the 200×200 kernel matrixK, and its eigen-\\ndecomposition ΦDγΦT.W ec a nt h i n ko ft h ec o l u m n so fΦand the corre-\\nsponding eigenvalues inDγ as empirical estimates of the eigen expansion\\n(5.45)2. Although the eigenvectors are discrete, we can represent them as\\nfunctions on IR1 (Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\\nues ofK. The leading eigenfunctions are smooth, and they are successively\\nmore wiggly as the order increases. This brings to life the penalty in (5.49),\\nwhere we see the coeﬃcients of higher-order functions get penalized more\\nthan lower-order ones. The right panel in Figure 5.14 shows the correspond-\\n2The ℓth column ofΦis an estimate ofφℓ, evaluated at each of theN observations.\\nAlternatively, theith row ofΦis the estimated vector of basis functionsφ(xi), evaluated\\nat the pointxi. Although in principle, there can be inﬁnitely many elements inφ,o u r\\nestimate has at mostN elements.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d7076655-0bbd-4090-bf6c-33407481d3b1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 190, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.8 Regularization and Reproducing Kernel Hilbert Spaces 173\\n*\\n* *** *** **** ** * ** *\\n**\\n** *\\n*\\n*** ** * *\\n*** **\\n*\\n* ** ** * ** **\\n** *\\n*\\n* ** ** **\\n* *\\n** **\\n*\\n*\\n*\\n**\\n*\\n* ****** ** *\\n*\\n*** *\\n** ** *\\n* * *\\n**\\n*\\n**\\n* * *\\n**\\n* *** ****** *\\n**\\n** ***\\n*\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n* ** * ***** **\\n*\\n*\\n***\\n*\\n**** *\\n*\\n* ** * **** ** *\\n** ** ** * **** **\\n*\\n** **** **\\n****\\n*\\n*\\n** ****\\n*\\n*\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n* *\\n***\\n*\\n** * ***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n**\\n* *\\n*\\n*\\n* ** **\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n** *****\\n*\\n*\\n*\\n*\\n*\\n***\\n**\\n*\\n*\\n*\\n*\\n*\\n* **\\n* *\\n*** * *\\n**\\n*\\n*\\n**\\n*****\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n* ***\\n*\\n*\\n*\\n*\\n* *\\n**\\n***\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n* *\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n**\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*** *\\n* **\\n**\\n** *\\n* **\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n* *\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n* *\\n*** **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n** *\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n*** **\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*****\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n**\\n* *\\n* ***\\n*\\n**\\n*\\n***\\n** *\\n**\\n**\\n*\\n*\\n**\\n**\\n* **\\n*\\n*\\n* ** *\\n*\\n*\\n* **\\n***\\n*\\n*\\n*\\n*\\n*\\n*** *\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n***\\n* ** *\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n** *\\n*\\n**\\n**\\n****\\n**\\n**\\n*\\n*\\n**\\n** *** **\\n*\\n*\\n**\\n*\\n*\\n* *\\n**\\n*** *\\n*\\n* *\\n**\\n*\\n** *\\n*\\n**\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n**\\n**\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n** *** *\\n* *****\\n*\\n** ** **\\n*\\n*\\n** *\\n*\\n**\\n*\\n*\\n*\\n*\\n* *** * *\\n**\\n* *** *****\\n*\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* ** *\\n**\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n* **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n* *\\n*\\n*\\n** *\\n* **\\n* **** **\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n* *\\n***\\n*\\n*\\n**\\n**\\n**\\n** **\\n* **\\n* *\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n*** ** **\\n*\\n*\\n*\\n*\\n*\\n** *\\n* **\\n*\\n*\\n*\\n**\\n** **\\n* ***\\n**\\n*\\n* *\\n** **\\n*\\n*****\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n** ****\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n* ***** *\\n*\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n* **\\n*\\n** **\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n**\\n*\\n*\\n** *\\n*\\n****\\n*\\n*\\n**\\n*\\n*** **\\n***\\n*\\n*\\n*\\n**\\n*\\n*\\n** * ** *\\n** *\\n*\\n*\\n***\\n* *\\n*\\n*\\n*\\n**\\n*\\n*\\n* *\\n* ** ** *\\n*\\n* *\\n*\\n** *\\n*\\n* *\\n*\\n*\\n*\\n** *\\n*\\n** **\\n*\\n* *\\n** *\\n*\\n*\\n****\\n*\\n** **\\n*\\n**\\n*\\n*\\n* *\\n*\\n** * *\\n*\\n*\\n* **\\n* *\\n*\\n**\\n* *** *\\n***** *\\n**\\n*\\n* *\\n*** **\\n* *\\n*\\n*\\n*\\n*\\n** *\\n*\\n* *\\n*****\\n**\\n** *\\n**\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n* ***\\n*\\n*\\n*\\n*\\n** ** **\\n*\\n*\\n*** *** *\\n*\\n***\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n** *\\n*\\n**\\n*\\n* **\\n*\\n*\\n*\\n* *\\n**\\n* *\\n*\\n*\\n**\\n*** *\\n* *\\n*\\n*\\n*\\n* *\\n* * **** **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n* **\\n* ** ** *\\n*\\n**\\n*\\n*\\n** **\\n*\\n* *** **\\n*\\n***** *\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n* *\\n*\\n*\\n* * ***\\n* *\\n**\\n*\\n*\\n** ****\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *****\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n*\\n** *\\n*\\n*\\n**\\n*\\n* **\\n*\\n** **\\n*\\n*\\n* **** **\\n*\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n** ****\\n*\\n*\\n***\\n*\\n**\\n*\\n*\\n** ** *\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n** ** *\\n*\\n* *\\n*\\n*\\n* ** *\\n*\\n* *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n** *\\n*\\n*\\n* ***\\n**\\n*\\n****\\n* ** **\\n*\\n*\\n*\\n*\\n** *\\n* *\\n* *\\n***\\n* **\\n* *\\n*\\n**\\n* ***\\n*\\n**\\n*\\n*\\n*\\n*** *\\n* *\\n**\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* ** *\\n****\\n*\\n**\\n** *\\n**\\n*\\n***\\n*\\n*\\n*\\n*\\n**\\n* *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n** **\\n*\\n*\\n*** **\\n*\\n*\\n* ****\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n** *\\n*\\n*\\n*\\n**\\n**\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n**\\n*\\n** **\\n* **\\n*\\n* **\\n*\\n*\\n*** **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n*\\n* *\\n*\\n** *\\n*\\n** ** *\\n*\\n*** ** ******\\n*\\n*\\n*\\n* *\\n**\\n*\\n*\\n*\\n**\\n** * *\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n** ***\\n****\\n*\\n*\\n*** *\\n* *\\n*\\n** **\\n*\\n*\\n* **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n****\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n* *\\n*\\n* *\\n* *\\n**\\n** **\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n** *** ** *\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n*** *\\n*\\n*** **\\n*\\n*\\n*\\n*\\n*\\n** * **\\n***\\n**\\n*\\n**\\n**\\n*\\n* *\\n*\\n**\\n*\\n**\\n*\\n*\\n** *\\n*\\n*\\n**\\n**\\n** *\\n** *\\n*\\n** *\\n* *\\n** *\\n* *\\n*\\n*\\n**\\n**\\n*\\n****\\n*\\n**\\n*\\n* *\\n*\\n* *\\n**\\n**\\n**\\n*\\n*\\n***\\n*** *\\n*\\n*\\n*\\n* ***\\n*\\n**\\n*\\n*\\n*\\n***\\n*\\n* *\\n**\\n*\\n***\\n*\\n*\\n***\\n*\\n*\\n* **\\n*\\n**\\n*** *\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n**\\n*\\n**\\n*\\n*\\n*\\n** *\\n* *\\n*\\n*** ***\\n*\\n* *\\n**\\n* ** **** *\\n*\\n** *\\n*** **\\n**\\n*\\n*\\n** *\\n***\\n**\\n*\\n*\\n*\\n*\\n** *\\n*\\n* **\\n*\\n*\\n**\\n*\\n*\\n**\\n* ** *\\n*\\n*\\n*\\n*\\n*\\n*\\n** **\\n**\\n* **\\n**\\n*\\n*\\n**\\n*\\n** **\\n*\\n*\\n*** ** *****\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n** * **\\n* * ***\\n*\\n* **\\n* ***\\n****\\n** *\\n**\\n*\\n* **\\n*\\n*\\n*** *\\n*\\n*\\n*\\n* ** *\\n*\\n* * **\\n**\\n* **\\n** *\\n**\\n*\\n*\\n**\\n*\\n**\\n*\\n**\\n* **\\n** **\\n*\\n*\\n*\\n**\\n*\\n* *\\n*\\n*\\n**\\n**\\n*\\n** *\\n***\\n**\\n*\\n***\\n*\\n*\\n**\\n*\\n**\\n* *\\n* **\\n* **\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n**\\n*\\n* * **\\n**\\n*\\n* *\\n*\\n**\\n*\\n** *\\n* ** *\\n*\\n*\\n**\\n**\\n** *\\n*\\n*\\n** *\\n*\\n**\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n**\\n**\\n*\\n****\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n* *\\n*** *\\n*\\n**\\n* *\\n**\\n*\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n* **\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n** *\\n**\\n** * *\\n*\\n*\\n* *\\n*\\n*\\n** *\\n*\\n*\\n* *\\n*\\n*\\n*\\n* **\\n*\\n*\\n* ***\\n*\\n**\\n**\\n*\\n*\\n*\\n*\\n** *\\n***\\n**\\n**\\n* **\\n* *\\n**\\n* *\\n*\\n*\\n*\\n* *\\n**\\n*\\n*\\n* **\\n**\\n*\\n*\\n*\\n*\\n**\\n* ** *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n**\\n** *\\n* *\\n*\\n**\\n** *\\n* *\\n*\\n*\\n*\\n* *\\n**\\n**\\n*\\n*\\n* **\\n**\\n* *** *\\n* **\\n*\\n***\\n* **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n****\\n*\\n*\\n**\\n*\\n*\\n****\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n* * **\\n**\\n*\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n* * *\\n*\\n** *\\n* **\\n*\\n**\\n** *\\n*\\n*\\n**\\n**\\n*\\n** *\\n**\\n* ** *\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n** **\\n* **\\n*\\n*\\n*\\n** *\\n*\\n* ** *** **\\n*\\n**\\n**\\n*\\n*\\n* *\\n**\\n* **\\n*\\n*\\n** *\\n*\\n*\\n** *\\n* **\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n** *\\n*\\n*****\\n*\\n** *\\n* **\\n* ***\\n** *\\n*\\n* *** *\\n***\\n*\\n*\\n*\\n** *\\n** ***\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n****\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n** ***\\n* *\\n*\\n*\\n* *\\n*\\n*\\n*\\n* * ****\\n*\\n*\\n*\\n** *\\n*\\n*\\n* * *\\n*\\n*\\n* **\\n* **\\n*\\n*** *\\n* *\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n** *\\n*\\n**\\n*\\n*\\n*\\n*\\n***\\n*\\n** *\\n*\\n* *\\n** **\\n*\\n*\\n***\\n*\\n* * *\\n*** **\\n** ** ** *\\n*\\n* *\\n*\\n**\\n* ** *\\n*\\n**\\n*\\n*\\n*\\n***\\n**\\n*\\n* *\\n*\\n* **\\n*\\n**\\n***\\n*\\n*\\n*\\n*\\n*\\n**\\n*** *\\n*\\n*\\n*\\n* *\\n*\\n*\\n* **\\n*\\n*\\n*\\n*\\n*\\n*\\n*** *\\n**\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n***\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n* * *\\n*\\n***\\n*\\n*\\n*\\n*\\n**** **\\n**\\n**\\n*\\n*\\n* * *\\n*\\n** *\\n* *\\n** *\\n*\\n** * *\\n*\\n*\\n*\\n**\\n*\\n**\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n** **\\n*\\n*\\n*\\n** ** * *\\n*** *\\n*\\n*\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n* *\\n*\\n** **\\n*\\n**\\n* **\\n*\\n*\\n*** *\\n* *\\n*\\n***\\n*\\n*\\n*\\n**\\n*\\n*\\n* ***\\n*\\n*\\n** * *\\n*\\n*\\n* **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n****\\n*\\n*\\n**\\n*\\n**\\n*\\n**\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n* ****\\n* *\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n***\\n*\\n*\\n*\\n** *\\n*\\n*\\n* *\\n*\\n*\\n** **\\n*\\n** *\\n*\\n*\\n*\\n*\\n* **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\nOrthonormal BasisΦ\\n*\\n* **\\n*\\n*\\n*\\n* *\\n**\\n*\\n*\\n*\\n* *\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*** **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n**\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*****\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n**\\n* *\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n** *\\n**\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n**\\n*\\n* **\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n** *\\n*\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n** **\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n***\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n** *\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n****\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n* ***\\n*\\n**\\n*\\n**\\n*\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n*\\n* *\\n* *\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* ** *\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n* *\\n*\\n*\\n*\\n**\\n****\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*** **\\n*\\n*\\n**\\n*\\n*\\n* *\\n**\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n*\\n* *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n* *** *\\n*\\n***\\n**\\n*\\n*\\n* ** **\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n* *\\n**\\n* *\\n** ****\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n**\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n* *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n* *\\n*\\n*\\n** *\\n* **\\n* ***\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n***\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n** **\\n* **\\n* *\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*** **\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n*\\n**\\n*\\n*\\n*\\n**\\n** *\\n*\\n* *\\n**\\n**\\n*\\n*\\n*\\n** **\\n*\\n*****\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n** *\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*****\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n* **\\n*\\n** **\\n*\\n*\\n*\\n*\\n*\\n**\\n**\\n*\\n**\\n*\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*** **\\n***\\n*\\n*\\n*\\n**\\n*\\n*\\n** *\\n** *\\n** *\\n*\\n*\\n***\\n* *\\n*\\n*\\n*\\n**\\n*\\n*\\n* *\\n* ** ** *\\n*\\n*\\n*\\n*\\n** *\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n** **\\n*\\n* *\\n** *\\n*\\n*\\n****\\n*\\n*\\n* *\\n*\\n*\\n**\\n*\\n*\\n* *\\n*\\n** * *\\n*\\n*\\n* **\\n* *\\n*\\n**\\n*\\n***\\n*\\n****\\n*\\n*\\n**\\n*\\n* *\\n**\\n* **\\n* *\\n*\\n*\\n*\\n*\\n** *\\n*\\n* *\\n****\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n*\\n*\\n*\\n*\\n* **\\n*\\n*\\n*\\n*\\n*\\n** *\\n* **\\n*\\n*\\n*** *** *\\n*\\n***\\n*\\n*\\n*\\n*\\n***\\n*\\n*\\n** *\\n*\\n**\\n*\\n* **\\n*\\n*\\n*\\n* *\\n**\\n* *\\n*\\n*\\n**\\n*** *\\n* *\\n*\\n*\\n*\\n* *\\n* * **** **\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n* *\\n* **\\n* ** ** *\\n*\\n**\\n*\\n*\\n** **\\n*\\n* *** **\\n*\\n***** *\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n**\\n*\\n* *\\n*\\n*\\n* * ***\\n* *\\n**\\n*\\n*\\n** ****\\n** *\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n*\\n* *****\\n*\\n*\\n*\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n*\\n** *\\n*\\n*\\n**\\n*\\n* **\\n*\\n** **\\n*\\n*\\n* **** **\\n*\\n**\\n*\\n*\\n*\\n*\\n**\\n*\\n***\\n*\\n*\\n** ****\\n*\\n*\\n*** *\\n** *\\n*\\n** ** * ** *\\n**\\n**\\n*\\n*\\n**\\n* *\\n*\\n*\\n***\\n*\\n*\\n*\\n** ** ** * **\\n*\\n* ** *\\n*\\n* *\\n*\\n*\\n*\\n**\\n*\\n*\\n** **\\n*\\n* ***\\n**\\n*\\n****\\n* ** **\\n*\\n*\\n*\\n*\\n** *\\n* *\\n* *\\n***\\n* **\\n* *\\n* **\\n* ***\\n*\\n****\\n* *** *\\n* *\\n**\\n*\\n**\\n*\\n*\\n*\\n**\\n*\\n*\\n*\\n* ** *\\n*****\\n**\\n** *\\n**\\n*\\n***\\n*\\n*\\n*\\n*\\n**\\n* ***\\n*\\n*\\n*\\n*\\n*\\n*\\n** **\\n*\\n*\\n*** **\\n*\\n*\\n* **** ** **** *\\n*\\n** *\\n***\\n** **\\n*\\n*\\n*\\n* ***\\n* **\\n*\\n**\\n*** ** * **** ** *\\n*\\n*** ** **\\n*\\n*\\n*\\n* *\\n*\\n* ** ** * *\\n* ** *\\n*\\n** *\\n*\\n** ** ** *** ** ****** *\\n*\\n** ***\\n*\\n**\\n**\\n** * **\\n* * *** * * *** *** ****** *** ** **\\n** **\\n*\\n** **\\n*\\n*\\n*\\n*\\n*\\n* * ***** *\\n*\\n** ***\\n*\\n***\\n* **\\n* *\\n* * **\\n** ** *** **\\n*\\n* * **** *** ** *\\n**\\n*\\n** **** *\\n*\\n** **** ** *** **\\n*\\n**** ** * ** *** ** * **** ** * *\\n*** ** ** ** ** * ** ** ** * ** ** ** ** *\\n*\\n** ** ** *** ** ****** ** ** *** *** ** ** * **** *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** ***\\n*\\n***\\n*\\n**\\n*\\n*\\n* * **** ** *** ** ** * **** *** ** **** ** **** *\\n*\\n** **** ** *** *** **** ** * ** *** ** * **** ** * **** ** ** ** ** * ** ** ** * ** ** ** ** *\\n*\\n** ** ** *** ** ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** **** ****\\n*** ** * **** ** *\\n** ** ** * **** *** ** **** ** *\\n*** *\\n*\\n** **** ** *** *** **** ** * ** *** ** * **** ** * **** ** ** ** ** * ** ** ** * ** ** ** ** * *** ** ** *** ** ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** **** **** *** ** * **** ** *** ** ** * **** *** ** **** ** **** **\\n** ****\\n** *** *** **** ** * ** *** ** * **** ** * **** ** ** ** ** * ** ** ** * ** ** ** ** * *** ** ** *** ** ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** **** **** *** ** * **** ** *** ** *** **** *** ** **** ** **** ** ** **** ** *** *** **** ** * ** *** ** * **** ** * **** ** ** ** ** * ** ** ** * ** ** ** ** * *** ** ** **** * ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** **** **** *** ** * **** ** *** ** ** * **** *** ** **** ** **** ** ** **** ** *** *** **** ** * ** *** ** * **** ** * **** ** ** ** ** * ** ** ** *** ** ** ** * *** ** ** *** ** ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ***** ** ** **** **** *** ** * **** ** *** ** ** * **** *** ** **** ** **** ** ** **** ** *** *** **** ** * ** *** ** * ***** * * **** ** ** ** ** * ** ** ** * ** ** ** ** * *** ** ** *** ** ****** ** ** *** *** ** ** * *** * *** * * *** *** ****** *** ** **** *** ** *** ** * ** * ******* ** **** **** *** *** **** ** *** ** ** * **** *** ** **** ** **** ** ** ****\\nFeature SpaceH\\nFIGURE 5.14. (Left panel) The ﬁrst 16 normalized eigenvectors of K,t h e\\n200 × 200 kernel matrix for the ﬁrst coordinate of the mixture data. These are\\nviewed as estimates ˆφℓ of the eigenfunctions in (5.45), and are represented as\\nfunctions inIR1 with the observed values superimposed in color. They are arranged\\nin rows, starting at the top left. (Right panel) Rescaled versionshℓ = √ˆγℓ ˆφℓ of\\nthe functions in the left panel, for which the kernel computes the “inner product.”\\n0 1 02 03 04 05 0\\n1e−15 1e−11 1e−07 1e−03 1e+01\\nEigenvalue\\nFIGURE 5.15. The largest50 eigenvalues of K; all those beyond the30th are\\neﬀectively zero.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac9a22cc-322f-4a56-a89d-d920ff42b670', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 191, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='174 5. Basis Expansions and Regularization\\ning feature spacerepresentation of the eigenfunctions\\nhℓ(x)=\\n√\\nˆγℓ\\nˆφℓ(x),ℓ =1 ,...,N. (5.65)\\nNotethat ⟨h(xi),h(xi′)⟩ = K(xi,xi′).Thescalingbytheeigenvaluesquickly\\nshrinks most of the functions down to zero, leaving an eﬀective dimension\\nof about 12 in this case. The corresponding optimization problem is a stan-\\ndard ridge regression, as in (5.63). So although in principle the implicit\\nfeature space is inﬁnite dimensional, the eﬀective dimension is dramat-\\nically lower because of the relative amounts of shrinkage applied to each\\nbasis function. The kernel scale parameterνplays a role here as well; larger\\nνimplies more localkm functions, and increases the eﬀective dimension of\\nthe feature space. See Hastie and Zhu (2006) for more details.\\nIt is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7)\\nis an expansion in radial basis functions, generated by the kernel\\nK(x,y)= ∥x−y∥2 log(∥x−y∥). (5.66)\\nRadial basis functions are discussed in more detail in Section 6.7.\\nSupport Vector Classiﬁers\\nThe support vector machines of Chapter 12 for a two-class classiﬁcation\\nproblem have the formf(x)= α0+∑N\\ni=1 αiK(x,xi), where the parameters\\nare chosen to minimize\\nmin\\nα0,α\\n{ N∑\\ni=1\\n[1−yif(xi)]+ + λ\\n2αTKα\\n}\\n, (5.67)\\nwhere yi ∈{ −1,1},a n d[z]+ denotes the positive part ofz.T h i sc a nb e\\nviewed as a quadratic optimization problem with linear constraints, and\\nrequires a quadratic programming algorithm for its solution. The name\\nsupport vectorarises from the fact that typically many of the ˆαi = 0 [due\\nto the piecewise-zero nature of the loss function in (5.67)], and soˆf is an\\nexpansion in a subset of theK(·,xi). See Section 12.3.3 for more details.\\n5.9 Wavelet Smoothing\\nWe have seen two diﬀerent modes of operation with dictionaries of basis\\nfunctions. With regression splines, we select a subset of the bases, using\\neither subject-matter knowledge, or else automatically. The more adaptive\\nprocedures such as MARS (Chapter 9) can capture both smooth and non-\\nsmooth behavior. With smoothing splines, we use a complete basis, but\\nthen shrink the coeﬃcients toward smoothness.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a9d9c0e5-1a2b-475d-bd30-eb07e8f03bb6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 192, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.9 Wavelet Smoothing 175\\nTime\\n0.0 0.2 0.4 0.6 0.8 1.0\\nHaar Wavelets\\nTime\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSymmlet-8 Wavelets\\nψ1,0\\nψ2,1\\nψ2,3\\nψ3,2\\nψ3,5\\nψ4,4\\nψ4,9\\nψ5,1\\nψ5,15\\nψ6,15\\nψ6,35\\nFIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations\\nfor the Haar and symmlet families. The functions have been scaled to suit the\\ndisplay.\\nWavelets typically use a complete orthonormal basis to represent func-\\ntions, but then shrink and select the coeﬃcients toward asparse represen-\\ntation. Just as a smooth function can be represented by a few spline basis\\nfunctions, a mostly ﬂat function with a few isolated bumps can be repre-\\nsented with a few (bumpy) basis functions. Wavelets bases are very popular\\nin signal processing and compression, since they are able to represent both\\nsmooth and/or locally bumpy functions in an eﬃcient way—a phenomenon\\ndubbed time and frequency localization. In contrast, the traditional Fourier\\nbasis allows only frequency localization.\\nBefore we give details, let’s look at the Haar wavelets in the left panel\\nof Figure 5.16 to get an intuitive idea of how wavelet smoothing works.\\nThe vertical axis indicates the scale (frequency) of the wavelets, from low\\nscale at the bottom to high scale at the top. At each scale the wavelets are\\n“packed in” side-by-side to completely ﬁll the time axis: we have only shown', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fdba2b70-75bd-45ba-9d20-029ec12c2c2d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 193, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='176 5. Basis Expansions and Regularization\\na selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by\\nleast squares, and then thresholds (discards, ﬁlters) the smaller coeﬃcients.\\nSince there are many basis functions at each scale, it can use bases where\\nit needs them and discard the ones it does not need, to achieve time and\\nfrequencylocalization. TheHaarwaveletsaresimpletounderstand,butnot\\nsmooth enough for most purposes. Thesymmlet wavelets in the right panel\\nof Figure 5.16 have the same orthonormal properties, but are smoother.\\nFigure 5.17 displays an NMR (nuclear magnetic resonance) signal, which\\nappears to be composed of smooth components and isolated spikes, plus\\nsome noise. The wavelet transform, using a symmlet basis, is shown in the\\nlower left panel. The wavelet coeﬃcients are arranged in rows, from lowest\\nscale at the bottom, to highest scale at the top. The length of each line\\nsegment indicates the size of the coeﬃcient. The bottom right panel shows\\nthe wavelet coeﬃcients after they have been thresholded. The threshold\\nprocedure, given below in equation (5.69), is the same soft-thresholding\\nrule that arises in the lasso procedure for linear regression (Section 3.4.2).\\nNotice that many of the smaller coeﬃcients have been set to zero. The\\ngreen curve in the top panel shows the back-transform of the thresholded\\ncoeﬃcients: this is the smoothed version of the original signal. In the next\\nsection we give the details of this process, including the construction of\\nwavelets and the thresholding rule.\\n5.9.1 Wavelet Bases and the Wavelet Transform\\nIn this section we give details on the construction and ﬁltering of wavelets.\\nWavelet bases are generated by translations and dilations of a single scal-\\ning functionφ(x) (also known as thefather). The red curves in Figure 5.18\\nare the Haar and symmlet-8 scaling functions. The Haar basis is particu-\\nlarly easy to understand, especially for anyone with experience in analysis\\nof variance or trees, since it produces a piecewise-constant representation.\\nThus ifφ(x)= I(x∈[0,1]), thenφ0,k(x)= φ(x−k), k an integer, generates\\nan orthonormal basis for functions with jumps at the integers. Call thisref-\\nerencespace V0. The dilationsφ1,k(x)=\\n√\\n2φ(2x−k) form an orthonormal\\nbasis for a spaceV1 ⊃V0 of functions piecewise constant on intervals of\\nlength 1\\n2. In fact, more generally we have···⊃ V1 ⊃V0 ⊃V−1 ⊃··· where\\neach Vj is spanned byφj,k =2 j/2φ(2jx−k).\\nNow to the deﬁnition of wavelets. In analysis of variance, we often rep-\\nresent a pair of meansμ1 and μ2 by their grand meanμ= 1\\n2(μ1 +μ2), and\\nthen a contrastα= 1\\n2(μ1 −μ2). A simpliﬁcation occurs if the contrastαis\\nvery small, because then we can set it to zero. In a similar manner we might\\nrepresent a function inVj+1 by a component inVj plus the component in\\nthe orthogonal complementWj of Vj to Vj+1, written asVj+1 = Vj ⊕Wj.\\nThe component inWj represents detail, and we might wish to set some ele-\\nments of this component to zero. It is easy to see that the functionsψ(x−k)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='50986ab3-ad64-4920-8e09-4fb12031eb62', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 194, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.9 Wavelet Smoothing 177\\nNMR Signal\\n0 200 400 600 800 1000\\n0 2 04 06 0\\n0 200 400 600 800 1000\\nWavelet Transform - Original Signal\\n0 200 400 600 800 1000\\nWavelet Transform - WaveShrunk Signal\\nSignalSignal\\nW9W9\\nW8W8\\nW7W7\\nW6W6\\nW5W5\\nW4W4\\nV4V4\\nFIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\\nversion superimposed in green. The lower left panel represents the wavelet trans-\\nform of the original signal, down toV4,u s i n gt h esymmlet-8 basis. Each coeﬃ-\\ncient is represented by the height (positive or negative) of the vertical bar. The\\nlower right panel represents the wavelet coeﬃcients after being shrunken using\\nthe waveshrink function in S-PLUS, which implements theSureShrink method\\nof wavelet adaptation of Donoho and Johnstone.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12a407f3-fb37-470a-9722-a528765f77d1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 195, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='178 5. Basis Expansions and Regularization\\nHaar Basis Symmlet Basis\\nφ(x)φ(x)\\nψ(x)ψ(x)\\nFIGURE 5.18.The Haar and symmlet father (scaling) waveletφ(x) and mother\\nwavelet ψ(x).\\ngeneratedbythe mother waveletψ(x)= φ(2x)−φ(2x−1) formanorthonor-\\nmal basis forW0 for the Haar family. Likewiseψj,k =2 j/2ψ(2jx−k)f o r m\\na basis forWj.\\nNow Vj+1 = Vj ⊕Wj = Vj−1 ⊕Wj−1 ⊕Wj, so besides representing a\\nfunction by its level-j detail and level-j rough components, the latter can\\nbe broken down to level-(j−1) detail and rough, and so on. Finally we get\\na representation of the formVJ = V0 ⊕W0 ⊕W1 ···⊕ WJ−1. Figure 5.16\\non page 175 shows particular waveletsψj,k(x).\\nNotice that since these spaces are orthogonal, all the basis functions are\\northonormal. In fact, if the domain is discrete withN =2 J (time) points,\\nthis is as far as we can go. There are 2j basis elements at level j,a n d\\nadding up, we have a total of 2J −1e l e m e n t si nt h eWj,a n do n ei nV0.\\nThis structured orthonormal basis allows for a multiresolution analysis,\\nwhich we illustrate in the next section.\\nWhile helpful for understanding the construction above, the Haar basis\\nis often too coarse for practical purposes. Fortunately, many clever wavelet\\nbases have been invented. Figures 5.16 and 5.18 include theDaubechies\\nsymmlet-8 basis. This basis has smoother elements than the corresponding\\nHaar basis, but there is a tradeoﬀ:\\n•Each wavelet has a support covering 15 consecutive time intervals,\\nrather than one for the Haar basis. More generally, the symmlet-p\\nfamily has a support of 2p−1 consecutive intervals. The wider the\\nsupport, the more time the wavelet has to die to zero, and so it can', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c8e9caef-ebff-49df-998c-f6583d779deb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 196, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.9 Wavelet Smoothing 179\\nachieve this more smoothly. Note that the eﬀective support seems to\\nbe much narrower.\\n•The symmlet-p wavelet ψ(x)h a sp vanishing moments; that is,\\n∫\\nψ(x)xjdx =0 ,j =0 ,...,p −1.\\nOne implication is that any order-ppolynomial over theN =2 J times\\npoints is reproduced exactly inV0 (Exercise 5.18). In this senseV0\\nis equivalent to the null space of the smoothing-spline penalty. The\\nHaar wavelets have one vanishing moment, andV0 can reproduce any\\nconstant function.\\nThe symmlet-p scaling functions are one of many families of wavelet\\ngenerators. The operations are similar to those for the Haar basis:\\n•If V0 is spanned byφ(x−k), thenV1 ⊃V0 is spanned byφ1,k(x)=√\\n2φ(2x−k)a n dφ(x)= ∑\\nk∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients\\nh(k).\\n•W0 is spanned byψ(x)= ∑\\nk∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients\\ng(k)=( −1)1−kh(1−k).\\n5.9.2 Adaptive Wavelet Filtering\\nWavelets are particularly useful when the data are measured on a uniform\\nlattice, such as a discretized signal, image, or a time series. We will focus on\\nthe one-dimensional case, and havingN =2 J lattice-points is convenient.\\nSuppose y is the response vector, andW is theN×N orthonormal wavelet\\nbasis matrix evaluated at theN uniformly spaced observations. Theny∗=\\nWTy is called the wavelet transform of y (and is the full least squares\\nregression coeﬃcient). A popular method for adaptive wavelet ﬁtting is\\nknown asSURE shrinkage(Stein Unbiased Risk Estimation, Donoho and\\nJohnstone (1994)). We start with the criterion\\nmin\\nθ\\n||y−Wθ||2\\n2 +2λ||θ||1, (5.68)\\nwhich is the same as the lasso criterion in Chapter 3. BecauseW is or-\\nthonormal, this leads to the simple solution:\\nˆθj =s i g n (y∗\\nj)(|y∗\\nj|−λ)+. (5.69)\\nThe least squares coeﬃcients are translated toward zero, and truncated\\nat zero. The ﬁtted function (vector) is then given by theinverse wavelet\\ntransform ˆf = Wˆθ.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='af71870b-fc3b-4344-8e2d-c64dfcb1d341', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 197, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='180 5. Basis Expansions and Regularization\\nA simple choice forλis λ= σ√2log N,w h e r eσis an estimate of the\\nstandarddeviationofthenoise.Wecangivesomemotivationforthischoice.\\nSince W is an orthonormal transformation, if the elements ofy are white\\nnoise (independent Gaussian variates with mean 0 and varianceσ2), then\\nso arey∗. Furthermore if random variablesZ1,Z2,...,Z N are white noise,\\nthe expected maximum of|Zj|,j =1 ,...,N is approximatelyσ√2log N.\\nHence all coeﬃcients belowσ√2log N a r el i k e l yt ob en o i s ea n da r es e tt o\\nzero.\\nThe spaceW could be any basis of orthonormal functions: polynomials,\\nnaturalsplinesorcosinusoids.Whatmakeswaveletsspecialistheparticular\\nform of basis functions used, which allows for a representationlocalized in\\ntime and in frequency.\\nLet’s look again at the NMR signal of Figure 5.17. The wavelet transform\\nwas computed using asymmlet−8 basis. Notice that the coeﬃcients do not\\ndescend all the way toV0, but stop atV4 which has 16 basis functions.\\nAs we ascend to each level of detail, the coeﬃcients get smaller, except in\\nlocations where spiky behavior is present. The wavelet coeﬃcients represent\\ncharacteristics of the signal localized in time (the basis functions at each\\nlevelaretranslationsofeachother)andlocalizedinfrequency.Eachdilation\\nincreases the detail by a factor of two, and in this sense corresponds to\\ndoubling the frequency in a traditional Fourier representation. In fact, a\\nmore mathematical understanding of wavelets reveals that the wavelets at\\na particular scale have a Fourier transform that is restricted to a limited\\nrange or octave of frequencies.\\nTheshrinking/truncationintherightpanelwasachievedusingtheSURE\\napproach described in the introduction to this section. The orthonormal\\nN ×N basis matrixW has columns which are the wavelet basis functions\\nevaluated at theN time points. In particular, in this case there will be 16\\ncolumns corresponding to theφ4,k(x), and the remainder devoted to the\\nψj,k(x),j =4 ,..., 11. In practiceλdepends on the noise variance, and has\\nto be estimated from the data (such as the variance of the coeﬃcients at\\nthe highest level).\\nNotice the similarity between the SURE criterion (5.68) on page 179,\\nand the smoothing spline criterion (5.21) on page 156:\\n•Both are hierarchically structured from coarse to ﬁne detail, although\\nwavelets are also localized in time within each resolution level.\\n•The splines build in a bias toward smooth functions by imposing\\ndiﬀerential shrinking constantsdk. Early versions of SURE shrinkage\\ntreated all scales equally. TheS+wavelets function waveshrink() has\\nmany options, some of which allow for diﬀerential shrinkage.\\n•The spline L2 penalty cause pure shrinkage, while the SURE L1\\npenalty does shrinkage and selection.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a124d9ce-071d-4c56-87cb-2f553f766f47', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 198, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 181\\nMore generally smoothing splines achieve compression of the original signal\\nby imposing smoothness, while wavelets impose sparsity. Figure 5.19 com-\\npares a wavelet ﬁt (using SURE shrinkage) to a smoothing spline ﬁt (using\\ncross-validation) on two examples diﬀerent in nature. For the NMR data in\\nthe upper panel, the smoothing spline introduces detail everywhere in order\\nto capture the detail in the isolated spikes; the wavelet ﬁt nicely localizes\\nthe spikes. In the lower panel, the true function is smooth, and the noise is\\nrelatively high. The wavelet ﬁt has let in some additional and unnecessary\\nwiggles—a price it pays in variance for the additional adaptivity.\\nThe wavelet transform is not performed by matrix multiplication as in\\ny∗= WTy. In fact, using clever pyramidal schemesy∗can be obtained\\nin O(N) computations, which is even faster than theN log(N)o ft h ef a s t\\nFourier transform (FFT). While the general construction is beyond the\\nscope of this book, it is easy to see for the Haar basis (Exercise 5.19).\\nLikewise, the inverse wavelet transformWˆθis alsoO(N).\\nThis has been a very brief glimpse of this vast and growing ﬁeld. There is\\na very large mathematical and computational base built on wavelets. Mod-\\nern image compression is often performed using two-dimensional wavelet\\nrepresentations.\\nBibliographic Notes\\nSplines and B-splines are discussed in detail in de Boor (1978). Green\\nand Silverman (1994) and Wahba (1990) give a thorough treatment of\\nsmoothing splines and thin-plate splines; the latter also covers reproducing\\nkernel Hilbert spaces. See also Girosi et al. (1995) and Evgeniou et al.\\n(2000) for connections between many nonparametric regression techniques\\nusing RKHS approaches. Modeling functional data, as in Section 5.2.3, is\\ncovered in detail in Ramsay and Silverman (1997).\\nDaubechies (1992) is a classic and mathematical treatment of wavelets.\\nOther useful sources are Chui (1992) and Wickerhauser (1994). Donoho and\\nJohnstone (1994) developed the SURE shrinkage and selection technology\\nfrom a statistical estimation framework; see also Vidakovic (1999). Bruce\\nand Gao (1996) is a useful applied introduction, which also describes the\\nwavelet software in S-PLUS.\\nExercises\\nEx. 5.1Show that the truncated power basis functions in (5.3) represent a\\nbasis for a cubic spline with the two knots as indicated.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1e05dd33-ac84-4fc1-9eb9-7bb3041dd0b9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 199, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='182 5. Basis Expansions and Regularization\\nNMR Signal\\n0 200 400 600 800 1000\\n02 0 4 0 6 0\\nspline\\nwavelet\\nSmooth Function (Simulated)\\nn\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-4 -2 0 2 4\\nspline\\nwavelet\\ntrue\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\nFIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\\nexamples. Each panel compares the SURE-shrunk wavelet ﬁt to the cross-validated\\nsmoothing spline ﬁt.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='754baf5d-bd4c-43d0-abe6-1d02af47807f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 200, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 183\\nEx. 5.2 Suppose that Bi,M(x)i sa no r d e r -MB -spline deﬁned in the Ap-\\npendix on page 186 through the sequence (5.77)–(5.78).\\n(a) Show by induction thatBi,M(x)=0f o rx̸∈[τi,τi+M]. This shows, for\\nexample, that the support of cubicB-splines is at most 5 knots.\\n(b) Show by induction thatBi,M(x) > 0f o rx∈(τi,τi+M). TheB-splines\\nare positive in the interior of their support.\\n(c) Show by induction that∑K+M\\ni=1 Bi,M(x)=1 ∀x∈[ξ0,ξK+1].\\n(d) Show thatBi,M is a piecewise polynomial of orderM (degree M −1)\\non [ξ0,ξK+1], with breaks only at the knotsξ1,...,ξ K.\\n(e) Show that an order-MB -spline basis function is the density function\\nof a convolution ofM uniform random variables.\\nEx. 5.3Write a program to reproduce Figure 5.3 on page 145.\\nEx. 5.4Consider the truncated power series representation for cubic splines\\nwith K interior knots. Let\\nf(X)=\\n3∑\\nj=0\\nβjXj +\\nK∑\\nk=1\\nθk(X−ξk)3\\n+. (5.70)\\nProve that the natural boundary conditions for natural cubic splines (Sec-\\ntion 5.2.1) imply the following linear constraints on the coeﬃcients:\\nβ2 =0 , ∑K\\nk=1 θk =0 ,\\nβ3 =0 , ∑K\\nk=1 ξkθk =0 . (5.71)\\nHence derive the basis (5.4) and (5.5).\\nEx. 5.5Write a program to classify thephoneme data using a quadratic dis-\\ncriminant analysis (Section 4.3). Since there are many correlated features,\\nyou should ﬁlter them using a smooth basis of natural cubic splines (Sec-\\ntion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent choices for the\\nnumber and position of the knots, and use tenfold cross-validation to make\\nthe ﬁnal selection. Thephoneme data are available from the book website\\nwww-stat.stanford.edu/ElemStatLearn.\\nEx. 5.6Suppose you wish to ﬁt a periodic function, with a known periodT.\\nDescribe how you could modify the truncated power series basis to achieve\\nthis goal.\\nEx. 5.7Derivation of smoothing splines(Green and Silverman, 1994). Sup-\\npose thatN ≥2, and thatg is the natural cubic spline interpolant to the\\npairs {xi,zi}N\\n1 ,w i t ha<x 1 < ··· <x N <b . This is a natural spline', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2ddbfdff-57c3-4815-9b60-b842c521e636', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 201, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='184 5. Basis Expansions and Regularization\\nw i t hak n o ta te v e r yxi;b e i n ga nN-dimensional space of functions, we can\\ndetermine the coeﬃcients such that it interpolates the sequencezi exactly.\\nLet ˜g be any other diﬀerentiable function on [a,b] that interpolates theN\\npairs.\\n(a) Let h(x)=˜g(x)−g(x). Use integration by parts and the fact thatg is\\na natural cubic spline to show that\\n∫ b\\na\\ng′′(x)h′′(x)dx = −\\nN−1∑\\nj=1\\ng′′′(x+\\nj ){h(xj+1)−h(xj)} (5.72)\\n=0 .\\n(b) Hence show that ∫ b\\na\\n˜g′′(t)2dt≥\\n∫ b\\na\\ng′′(t)2dt,\\nand that equality can only hold ifh is identically zero in [a,b].\\n(c) Consider the penalized least squares problem\\nmin\\nf\\n[ N∑\\ni=1\\n(yi −f(xi))2 +λ\\n∫ b\\na\\nf′′(t)2dt\\n]\\n.\\nUse (b) to argue that the minimizer must be a cubic spline with knots\\nat each of thexi.\\nEx. 5.8In the appendix to this chapter we show how the smoothing spline\\ncomputations could be more eﬃciently carried out using a (N +4)dimen-\\nsional basis ofB-splines. Describe a slightly simpler scheme using a (N+2)\\ndimensional B-spline basis deﬁned on theN −2 interior knots.\\nEx. 5.9Derive the Reinsch formSλ=( I+λK)−1 for the smoothing spline.\\nEx. 5.10Derive an expression for Var(ˆfλ(x0)) and bias(ˆfλ(x0)). Using the\\nexample (5.22), create a version of Figure 5.9 where the mean and several\\n(pointwise) quantiles ofˆfλ(x)a r es h o w n .\\nEx. 5.11Prove that for a smoothing spline the null space ofK is spanned\\nby functions linear inX.\\nEx. 5.12Characterize the solution to the following problem,\\nmin\\nf\\nRSS(f,λ)=\\nN∑\\ni=1\\nwi{yi −f(xi)}2 +λ\\n∫\\n{f′′(t)}2dt, (5.73)\\nwhere thewi ≥0 are observation weights.\\nCharacterize the solution to the smoothing spline problem (5.9) when\\nt h et r a i n i n gd a t ah a v et i e si nX.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2520e41a-fe93-4cad-989d-8926d3fcabb3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 202, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 185\\nEx. 5.13 You have ﬁtted a smoothing spline ˆfλ t oas a m p l eo fN pairs\\n(xi,yi). Suppose you augment your original sample with the pairx0, ˆfλ(x0),\\nand reﬁt; describe the result. Use this to derive theN-fold cross-validation\\nformula (5.26).\\nEx. 5.14 Derive the constraints on theαj in the thin-plate spline expan-\\nsion (5.39) to guarantee that the penaltyJ(f) is ﬁnite. How else could one\\nensure that the penalty was ﬁnite?\\nEx. 5.15This exercise derives some of the results quoted in Section 5.8.1.\\nSuppose K(x,y) satisfying the conditions (5.45) and letf(x)∈HK. Show\\nthat\\n(a) ⟨K(·,xi),f⟩HK = f(xi).\\n(b) ⟨K(·,xi),K(·,xj)⟩HK = K(xi,xj).\\n(c) Ifg(x)= ∑N\\ni=1 αiK(x,xi), then\\nJ(g)=\\nN∑\\ni=1\\nN∑\\nj=1\\nK(xi,xj)αiαj.\\nSuppose that ˜g(x)= g(x)+ ρ(x), withρ(x)∈HK, and orthogonal inHK\\nto each ofK(x,xi),i =1 ,...,N . Show that\\n(d)\\nN∑\\ni=1\\nL(yi,˜g(xi))+ λJ(˜g)≥\\nN∑\\ni=1\\nL(yi,g(xi))+ λJ(g) (5.74)\\nwith equality iﬀρ(x)=0 .\\nEx. 5.16Consider the ridge regression problem (5.53), and assumeM ≥N.\\nAssume you have a kernelK that computes the inner productK(x,y)=∑M\\nm=1 hm(x)hm(y).\\n(a) Derive (5.62) on page 171 in the text. How would you compute the\\nmatrices V and Dγ,g i v e nK? Hence show that (5.63) is equivalent\\nto (5.53).\\n(b) Show that\\nˆf = Hˆβ\\n= K(K+λI)−1y, (5.75)\\nwhere H is theN×M matrix of evaluationshm(xi), andK = HHT\\nthe N ×N matrix of inner-productsh(xi)Th(xj).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed7a1865-0381-471b-9458-51a2cf2a1a89', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 203, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='186 5. Basis Expansions and Regularization\\n(c) Show that\\nˆf(x)= h(x)T ˆβ\\n=\\nN∑\\ni=1\\nK(x,xi)ˆαi (5.76)\\nand ˆα=( K+λI)−1y.\\n(d) How would you modify your solution ifM<N ?\\nEx. 5.17 Show how to convert the discrete eigen-decomposition ofK in\\nSection 5.8.2 to estimates of the eigenfunctions ofK.\\nEx. 5.18 The wavelet function ψ(x) of the symmlet-p wavelet basis has\\nvanishing moments up to orderp. Show that this implies that polynomials\\nof orderp are represented exactly inV0, deﬁned on page 176.\\nEx. 5.19Show that the Haar wavelet transform of a signal of lengthN =2 J\\ncan be computed inO(N) computations.\\nAppendix: Computations for Splines\\nIn this Appendix, we describe theB-spline basis for representing polyno-\\nmial splines. We also discuss their use in the computations of smoothing\\nsplines.\\nB-splines\\nBefore we can get started, we need to augment the knot sequence deﬁned\\nin Section 5.2. Letξ0 <ξ1 and ξK <ξK+1 be twoboundary knots, which\\ntypically deﬁne the domain over which we wish to evaluate our spline. We\\nnow deﬁne the augmented knot sequenceτsuch that\\n•τ1 ≤τ2 ≤···≤ τM ≤ξ0;\\n•τj+M = ξj,j =1 ,··· ,K;\\n•ξK+1 ≤τK+M+1 ≤τK+M+2 ≤···≤ τK+2M.\\nThe actual values of these additional knots beyond the boundary are arbi-\\ntrary, and it is customary to make them all the same and equal toξ0 and\\nξK+1, respectively.\\nDenote by Bi,m(x)t h eith B-spline basis function of orderm for the\\nknot-sequence τ, m≤M. They are deﬁned recursively in terms of divided', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6928e3ab-b020-4824-aad3-488f6478a5d8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 204, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix: Computations for Splines 187\\ndiﬀerences as follows:\\nBi,1(x)=\\n{\\n1i f τi ≤x<τi+1\\n0 otherwise (5.77)\\nfor i =1 ,...,K +2M −1. These are also known as Haar basis functions.\\nBi,m(x)= x−τi\\nτi+m−1 −τi\\nBi,m−1(x)+ τi+m −x\\nτi+m −τi+1\\nBi+1,m−1(x)\\nfor i =1 ,...,K +2M −m.\\n(5.78)\\nThus with M =4 , Bi,4,i =1 ,··· ,K +4a r et h eK + 4 cubicB-spline\\nbasis functions for the knot sequence ξ. This recursion can be contin-\\nued and will generate theB-spline basis for any order spline. Figure 5.20\\nshows the sequence ofB-splines up to order four with knots at the points\\n0.0,0.1,..., 1.0. Since we have created some duplicate knots, some care\\nhas to be taken to avoid division by zero. If we adopt the convention\\nthat Bi,1 =0i f τi = τi+1, then by induction Bi,m =0i f τi = τi+1 =\\n... = τi+m. Note also that in the construction above, only the subset\\nBi,m,i = M −m +1 ,...,M + K a r er e q u i r e df o rt h eB - s p l i n eb a s i s\\nof orderm<M with knotsξ.\\nTo fully understand the properties of these functions, and to show that\\nthey do indeed span the space of cubic splines for the knot sequence, re-\\nquires additional mathematical machinery, including the properties of di-\\nvided diﬀerences. Exercise 5.2 explores these issues.\\nThe scope ofB-splines is in fact bigger than advertised here, and has to\\ndo with knot duplication. If we duplicate an interior knot in the construc-\\ntion of theτsequence above, and then generate theB-spline sequence as\\nbefore, the resulting basis spans the space of piecewise polynomials with\\none less continuous derivative at the duplicated knot. In general, if in ad-\\ndition to the repeated boundary knots, we include the interior knot ξj\\n1 ≤rj ≤M times, then the lowest-order derivative to be discontinuous\\nat x = ξj will be order M −rj. Thus for cubic splines with no repeats,\\nrj =1 ,j =1 ,...,K , and at each interior knot the third derivatives (4−1)\\nare discontinuous. Repeating thejth knot three times leads to a discontin-\\nuous 1st derivative; repeating it four times leads to a discontinuous zeroth\\nderivative, i.e., the function is discontinuous atx = ξj. This is exactly what\\nhappens at the boundary knots; we repeat the knotsM times, so the spline\\nbecomes discontinuous at the boundary knots (i.e., undeﬁned beyond the\\nboundary).\\nThe local support of B-splines has important computational implica-\\ntions, especially when the number of knotsK is large. Least squares com-\\nputations withN observations andK+M variables (basis functions) take\\nO(N(K +M)2 +(K+M)3) ﬂops (ﬂoating point operations.) IfK is some\\nappreciable fraction ofN, this leads toO(N3) algorithms which becomes', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b296a7e-379b-4fa5-a803-515249f91569', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 205, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='188 5. Basis Expansions and Regularization\\nB-splines of Order 1\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.4 0.8 1.2\\nB-splines of Order 2\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.4 0.8 1.2\\nB-splines of Order 3\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.4 0.8 1.2\\nB-splines of Order 4\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.4 0.8 1.2\\nFIGURE 5.20.The sequence ofB-splines up to order four with ten knots evenly\\nspaced from0 to 1.T h eB-splines have local support; they are nonzero on an\\ninterval spanned byM +1 knots.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c981bae2-b16d-47de-ac4a-f1192a8578b6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 206, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix: Computations for Splines 189\\nunacceptable for largeN.I ft h eN observations are sorted, theN×(K+M)\\nregression matrix consisting of theK +MB -spline basis functions evalu-\\nated at theN points has many zeros, which can be exploited to reduce the\\ncomputational complexity back toO(N). We take this up further in the\\nnext section.\\nComputations for Smoothing Splines\\nAlthough natural splines (Section 5.2.1) provide a basis for smoothing\\nsplines, it is computationally more convenient to operate in the larger space\\nof unconstrainedB-splines. We writef(x)= ∑N+4\\n1 γjBj(x), whereγj are\\ncoeﬃcients and theBj are the cubicB-spline basis functions. The solution\\nlooks the same as before,\\nˆγ=( BTB+λΩB)−1BTy, (5.79)\\nexcept now theN × N matrix N is replaced by theN × (N +4 )m a t r i x\\nB, and similarly the (N +4 )× (N + 4) penalty matrixΩB replaces the\\nN × N dimensional ΩN. Although at face value it seems that there are\\nno boundary derivative constraints, it turns out that the penalty term\\nautomatically imposes them by giving eﬀectively inﬁnite weight to any non\\nzero derivative beyond the boundary. In practice, ˆγis restricted to a linear\\nsubspace for which the penalty is always ﬁnite.\\nSince the columns ofB are the evaluatedB-splines, in order from left\\nto right and evaluated at thesorted values ofX, and the cubicB-splines\\nhave local support, B is lower 4-banded. Consequently the matrixM =\\n(BTB+λΩ) is 4-banded and hence its Cholesky decompositionM = LLT\\ncanbecomputedeasily.Onethensolves LLTγ= BTybyback-substitution\\nto giveγand hence the solutionˆf in O(N)o p e r a t i o n s .\\nIn practice, whenN is large, it is unnecessary to use allN interior knots,\\nand any reasonablethinning strategy will save in computations and have\\nnegligible eﬀect on the ﬁt. For example, thesmooth.spline function in S-\\nPLUS uses an approximately logarithmic strategy: ifN< 50 all knots are\\nincluded, but even atN =5 ,000 only 204 knots are used.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0768cf51-f39a-48ae-9df3-75ef95386a2a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 207, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6\\nKernel Smoothing Methods\\nIn this chapter we describe a class of regression techniques that achieve\\nﬂexibility in estimating the regression functionf(X) over the domain IRp\\nby ﬁtting a diﬀerent but simple model separately at each query pointx0.\\nThis is done by using only those observations close to the target pointx0 to\\nﬁtthesimplemodel,andinsuchawaythattheresultingestimatedfunction\\nˆf(X)i ssmoothin IRp. This localization is achieved via a weighting function\\nor kernel Kλ(x0,xi), which assigns a weight toxi b a s e do ni t sd i s t a n c ef r o m\\nx0.T h ek e r n e l sKλ are typically indexed by a parameterλthat dictates\\nthe width of the neighborhood. Thesememory-based methods require in\\nprinciple little or no training; all the work gets done at evaluation time.\\nThe only parameter that needs to be determined from the training data is\\nλ. The model, however, is the entire training data set.\\nWe also discuss more general classes of kernel-based techniques , which\\ntie in with structured methods in other chapters, and are useful for density\\nestimation and classiﬁcation.\\nThe techniques in this chapter should not be confused with those asso-\\nciated with the more recent usage of the phrase “kernel methods”. In this\\nchapter kernels are mostly used as a device for localization. We discuss ker-\\nnel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in those contexts\\nthe kernel computes an inner product in a high-dimensional (implicit) fea-\\nture space, and is used for regularized nonlinear modeling. We make some\\nconnections to the methodology in this chapter at the end of Section 6.7.\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 191\\nDOI: 10.1007/b94608_6,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9249b8ed-29f0-49eb-a267-a8f2a4781339', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 208, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='192 6. Kernel Smoothing Methods\\nNearest-Neighbor Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\n\\x81\\nx0\\nˆf(x0)\\nEpanechnikov Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nOO\\n\\x81\\nx0\\nˆf(x0)\\nFIGURE 6.1.In each panel100 pairs xi,y i are generated at random from the\\nblue curve with Gaussian errors:Y =s i n ( 4X)+ε, X ∼U[0,1], ε∼N(0,1/3).I n\\nthe left panel the green curve is the result of a30-nearest-neighbor running-mean\\nsmoother. The red point is the ﬁtted constantˆf(x0), and the red circles indicate\\nthose observations contributing to the ﬁt atx0. The solid yellow region indicates\\nthe weights assigned to observations. In the right panel, the green curve is the\\nkernel-weighted average, using an Epanechnikov kernel with (half) window width\\nλ=0 .2.\\n6.1 One-Dimensional Kernel Smoothers\\nIn Chapter 2, we motivated thek–nearest-neighbor average\\nˆf(x)=A v e (yi|xi ∈Nk(x)) (6.1)\\nas an estimate of the regression function E(Y|X = x). HereNk(x)i st h es e t\\nof k points nearest tox in squared distance, and Ave denotes the average\\n(mean). The idea is to relax the deﬁnition of conditional expectation, as\\nillustrated in the left panel of Figure 6.1, and compute an average in a\\nneighborhood of the target point. In this case we have used the 30-nearest\\nneighborhood—the ﬁt at x0 is the average of the 30 pairs whosexi values\\nare closest tox0. The green curve is traced out as we apply this deﬁnition\\nat diﬀerent valuesx0. The green curve is bumpy, sinceˆf(x) is discontinuous\\nin x.A sw em o v ex0 from left to right, thek-nearest neighborhood remains\\nconstant, until a pointxi to the right ofx0 becomes closer than the furthest\\npoint xi′ in the neighborhood to the left ofx0,a tw h i c ht i m exi replaces xi′.\\nThe average in (6.1) changes in a discrete way, leading to a discontinuous\\nˆf(x).\\nThis discontinuity is ugly and unnecessary. Rather than give all the\\npoints in the neighborhood equal weight, we can assign weights that die\\noﬀ smoothly with distance from the target point. The right panel shows\\nan example of this, using the so-called Nadaraya–Watson kernel-weighted', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0946003a-bd66-4e25-95ce-7ba5275e72e0', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 209, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 One-Dimensional Kernel Smoothers 193\\naverage\\nˆf(x0)=\\n∑N\\ni=1 Kλ(x0,xi)yi\\n∑N\\ni=1 Kλ(x0,xi)\\n, (6.2)\\nwith theEpanechnikovquadratic kernel\\nKλ(x0,x)= D\\n⎤|x−x0|\\nλ\\n⎦\\n, (6.3)\\nwith\\nD(t)=\\n{ 3\\n4(1−t2)i f |t|≤1;\\n0 otherwise . (6.4)\\nThe ﬁtted function is now continuous, and quite smooth in the right panel\\nof Figure 6.1. As we move the target from left to right, points enter the\\nneighborhood initially with weight zero, and then their contribution slowly\\nincreases (see Exercise 6.1).\\nIn the right panel we used a metric window sizeλ=0 .2 for the kernel\\nﬁt, which does not change as we move the target pointx0, while the size\\nof the 30-nearest-neighbor smoothing window adapts to the local density\\nof the xi. One can, however, also use such adaptive neighborhoods with\\nkernels, but we need to use a more general notation. Lethλ(x0)b eaw i d t h\\nfunction (indexed byλ) that determines the width of the neighborhood at\\nx0. Then more generally we have\\nKλ(x0,x)= D\\n⎤|x−x0|\\nhλ(x0)\\n⎦\\n. (6.5)\\nIn (6.3), hλ(x0)= λis constant. Fork-nearest neighborhoods, the neigh-\\nborhood size k replaces λ, and we havehk(x0)= |x0 −x[k]| where x[k] is\\nthe kth closestxi to x0.\\nThere are a number of details that one has to attend to in practice:\\n•The smoothing parameterλ, which determines the width of the local\\nneighborhood, has to be determined. Largeλimplies lower variance\\n(averages over more observations) but higher bias (we essentially as-\\nsume the true function is constant within the window).\\n•Metric window widths (constanthλ(x)) tend to keep the bias of the\\nestimate constant, but the variance is inversely proportional to the\\nlocal density. Nearest-neighbor window widths exhibit the opposite\\nbehavior; the variance stays constant and the absolute bias varies\\ninversely with local density.\\n•Issues arise with nearest-neighbors when there are ties in thexi.W i t h\\nmost smoothing techniques one can simply reduce the data set by\\naveraging the yi at tied values ofX, and supplementing these new\\nobservations at the unique values ofxi with an additional weightwi\\n(which multiples the kernel weight).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed941adb-bf8b-455c-9cb0-41a6ab836496', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 210, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='194 6. Kernel Smoothing Methods\\n-3 -2 -1 0 1 2 3\\n0.0 0.4 0.8\\nEpanechnikov\\nTri-cube\\nGaussian\\nKλ(x0,x)\\nFIGURE 6.2.A comparison of three popular kernels for local smoothing. Each\\nhas been calibrated to integrate to1. The tri-cube kernel is compact and has two\\ncontinuous derivatives at the boundary of its support, while the Epanechnikov ker-\\nnel has none. The Gaussian kernel is continuously diﬀerentiable, but has inﬁnite\\nsupport.\\n•This leaves a more general problem to deal with: observation weights\\nwi. Operationally we simply multiply them by the kernel weights be-\\nfore computing the weighted average. With nearest neighborhoods, it\\nis now natural to insist on neighborhoods with a total weight content\\nk (relative to∑wi). In the event of overﬂow (the last observation\\nneeded in a neighborhood has a weightwj which causes the sum of\\nweights to exceed the budgetk), then fractional parts can be used.\\n•Boundary issues arise. The metric neighborhoods tend to contain less\\npoints on the boundaries, while the nearest-neighborhoods get wider.\\n•The Epanechnikov kernel has compact support (needed when used\\nwith nearest-neighbor window size). Another popular compact kernel\\nis based on the tri-cube function\\nD(t)=\\n{\\n(1−|t|3)3 if |t|≤1;\\n0 otherwise (6.6)\\nThis is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-\\nentiable at the boundary of its support. The Gaussian density func-\\ntion D(t)= φ(t) is a popular noncompact kernel, with the standard-\\ndeviation playing the role of the window size. Figure 6.2 compares\\nthe three.\\n6.1.1 Local Linear Regression\\nWe have progressed from the raw moving average to a smoothly varying\\nlocally weighted average by using kernel weighting. The smooth kernel ﬁt\\nstill has problems, however, as exhibited in Figure 6.3 (left panel). Locally-\\nweighted averages can be badly biased on the boundaries of the domain,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0acaa566-712a-411a-b853-4d99312e2762', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 211, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 One-Dimensional Kernel Smoothers 195\\nN-W Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\n\\x81\\nx0\\nˆf(x0)\\nLocal Linear Regression at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\n\\x81\\nx0\\nˆf(x0)\\nFIGURE 6.3. The locally weighted average has bias problems at or near the\\nboundaries of the domain. The true function is approximately linear here, but\\nmost of the observations in the neighborhood have a higher mean than the target\\npoint, so despite weighting, their mean will be biased upwards. By ﬁtting a locally\\nweighted linear regression (right panel), this bias is removed to ﬁrst order.\\nbecause of the asymmetry of the kernel in that region. By ﬁtting straight\\nlines rather than constants locally, we can remove this bias exactly to ﬁrst\\norder; see Figure 6.3 (right panel). Actually, this bias can be present in the\\ninterior of the domain as well, if theX values are not equally spaced (for\\nthe same reasons, but usually less severe). Again locally weighted linear\\nregression will make a ﬁrst-order correction.\\nLocallyweightedregressionsolvesaseparateweightedleastsquaresprob-\\nlem at each target pointx0:\\nmin\\nα(x0),β(x0)\\nN∑\\ni=1\\nKλ(x0,xi)[yi −α(x0)−β(x0)xi]2 . (6.7)\\nThe estimate is thenˆf(x0)=ˆα(x0)+ ˆβ(x0)x0. Notice that although we ﬁt\\nan entire linear model to the data in the region, we only use it to evaluate\\nt h eﬁ ta tt h es i n g l ep o i n tx0.\\nDeﬁne the vector-valued functionb(x)T =( 1,x). Let B be the N × 2\\nregression matrix with ith row b(xi)T,a n dW(x0)t h eN × N diagonal\\nmatrix withith diagonal elementKλ(x0,xi). Then\\nˆf(x0)= b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)\\n=\\nN∑\\ni=1\\nli(x0)yi. (6.9)\\nEquation (6.8) gives an explicit expression for the local linear regression\\nestimate, and (6.9) highlights the fact that the estimate islinear in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46583125-1376-4277-ba77-2121e5ce2070', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 212, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='196 6. Kernel Smoothing Methods\\nLocal Linear Equivalent Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO O\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\nx0\\nˆf(x0)\\nLocal Linear Equivalent Kernel in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nOOO\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO O\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\nx0\\nˆf(x0)\\nFIGURE 6.4. The green points show the equivalent kernelli(x0) for local re-\\ngression. These are the weights in ˆf(x0)= ∑N\\ni=1 li(x0)yi, plotted against their\\ncorresponding xi. For display purposes, these have been rescaled, since in fact\\nthey sum to1. Since the yellow shaded region is the (rescaled) equivalent kernel\\nfor the Nadaraya–Watson local average, we see how local regression automati-\\ncally modiﬁes the weighting kernel to correct for biases due to asymmetry in the\\nsmoothing window.\\nyi (the li(x0)d on o ti n v o l v ey). These weightsli(x0) combine the weight-\\ning kernel Kλ(x0,·) and the least squares operations, and are sometimes\\nreferred to as theequivalent kernel. Figure 6.4 illustrates the eﬀect of lo-\\ncal linear regression on the equivalent kernel. Historically, the bias in the\\nNadaraya–Watson and other local average kernel methods were corrected\\nby modifying the kernel. These modiﬁcations were based on theoretical\\nasymptotic mean-square-error considerations, and besides being tedious to\\nimplement, are only approximate for ﬁnite sample sizes. Local linear re-\\ngression automatically modiﬁes the kernel to correct the bias exactly to\\nﬁrst order, a phenomenon dubbed asautomatic kernel carpentry.C o n s i d e r\\nthe following expansion for Eˆf(x0), using the linearity of local regression\\nand a series expansion of the true functionf around x0,\\nEˆf(x0)=\\nN∑\\ni=1\\nli(x0)f(xi)\\n= f(x0)\\nN∑\\ni=1\\nli(x0)+ f′(x0)\\nN∑\\ni=1\\n(xi −x0)li(x0)\\n+f′′(x0)\\n2\\nN∑\\ni=1\\n(xi −x0)2li(x0)+ R, (6.10)\\nwhere the remainder termR involves third- and higher-order derivatives of\\nf, and is typically small under suitable smoothness assumptions. It can be', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b9347b4-240e-45ec-865f-569fb4674a33', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 213, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 One-Dimensional Kernel Smoothers 197\\nLocal Linear in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nOO\\nO\\nO\\nO\\n\\x81\\nˆf(x0)\\nLocal Quadratic in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 -0.5 0.0 0.5 1.0 1.5\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nOO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nOO\\nO\\nO\\nO\\n\\x81\\nˆf(x0)\\nFIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the true\\nfunction. Local quadratic ﬁts tend to eliminate this bias.\\nshown (Exercise 6.2) that for local linear regression,∑N\\ni=1 li(x0)=1a n d∑N\\ni=1(xi −x0)li(x0) = 0. Hence the middle term equalsf(x0), and since\\nthe bias is Eˆf(x0)−f(x0), we see that it depends only on quadratic and\\nhigher–order terms in the expansion off.\\n6.1.2 Local Polynomial Regression\\nWhy stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-\\ngree d,\\nmin\\nα(x0),βj(x0),j =1,...,d\\nN∑\\ni=1\\nKλ(x0,xi)\\n⎡\\n⎣yi −α(x0)−\\nd∑\\nj=1\\nβj(x0)xj\\ni\\n⎤\\n⎦\\n2\\n(6.11)\\nwith solution ˆf(x0)=ˆα(x0)+∑d\\nj=1\\nˆβj(x0)xj\\n0. In fact, an expansion such as\\n(6.10) will tell us that the bias will only have components of degreed+1and\\nhigher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local\\nlinear ﬁts tend to be biased in regions of curvature of the true function, a\\nphenomenon referred to astrimming the hillsand ﬁlling the valleys.L o c a l\\nquadratic regression is generally able to correct this bias.\\nThere is of course a price to be paid for this bias reduction, and that is\\nincreased variance. The ﬁt in the right panel of Figure 6.5 is slightly more\\nwiggly, especially in the tails. Assuming the modelyi = f(xi)+ εi,w i t h\\nεi independent and identically distributed with mean zero and variance\\nσ2,V a r (ˆf(x0)) =σ2||l(x0)||2,w h e r el(x0) is the vector of equivalent kernel\\nweights atx0. It can be shown (Exercise 6.3) that||l(x0)|| increases withd,\\nand so there is a bias–variance tradeoﬀ in selecting the polynomial degree.\\nFigure 6.6 illustrates these variance curves for degree zero, one and two', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c41fc17-a44d-4a57-9dab-75daaafeb949', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 214, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='198 6. Kernel Smoothing Methods\\nVariance\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0 0.1 0.2 0.3 0.4 0.5\\nConstant\\nLinear\\nQuadratic\\nFIGURE 6.6. The variances functions ||l(x)||2 for local constant, linear and\\nquadratic regression, for a metric bandwidth (λ=0 .2) tri-cube kernel.\\nlocal polynomials. To summarize some collected wisdom on this issue:\\n•Local linear ﬁts can help bias dramatically at the boundaries at a\\nmodest cost in variance. Local quadratic ﬁts do little at the bound-\\naries for bias, but increase the variance a lot.\\n•Local quadratic ﬁts tend to be most helpful in reducing bias due to\\ncurvature in the interior of the domain.\\n•Asymptotic analysis suggest that local polynomials of odd degree\\ndominate those of even degree. This is largely due to the fact that\\nasymptotically the MSE is dominated by boundary eﬀects.\\nWhile it may be helpful to tinker, and move from local linear ﬁts at the\\nboundary to local quadratic ﬁts in the interior, we do not recommend such\\nstrategies. Usually the application will dictate the degree of the ﬁt. For\\nexample, if we are interested in extrapolation, then the boundary is of\\nmore interest, and local linear ﬁts are probably more reliable.\\n6.2 Selecting the Width of the Kernel\\nIn each of the kernelsKλ, λis a parameter that controls its width:\\n•For the Epanechnikov or tri-cube kernel with metric width,λis the\\nradius of the support region.\\n•For the Gaussian kernel,λis the standard deviation.\\n•λis the numberk of nearest neighbors ink-nearest neighborhoods,\\noften expressed as a fraction orspank/N of the total training sample.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f3548b30-b01b-426f-a19b-02668846ca74', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 215, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2 Selecting the Width of the Kernel 199\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81 \\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\x81\\nFIGURE 6.7.Equivalent kernels for a local linear regression smoother (tri-cube\\nkernel; orange) and a smoothing spline (blue), with matching degrees of freedom.\\nThe vertical spikes indicates the target points.\\nThere is a natural bias–variance tradeoﬀ as we change the width of the\\naveraging window, which is most explicit for local averages:\\n•If the window is narrow,ˆf(x0) is an average of a small number ofyi\\nclose tox0, and its variance will be relatively large—close to that of\\nan individualyi. The bias will tend to be small, again because each\\nof theE(yi)= f(xi) should be close tof(x0).\\n•If the window is wide, the variance ofˆf(x0) will be small relative to\\nt h ev a r i a n c eo fa n yyi, because of the eﬀects of averaging. The bias\\nwill be higher, because we are now using observationsxi further from\\nx0, and there is no guarantee thatf(xi)w i l lb ec l o s et of(x0).\\nSimilar arguments apply to local regression estimates, say local linear: as\\nthe width goes to zero, the estimates approach a piecewise-linear function\\nthat interpolates the training data1; as the width gets inﬁnitely large, the\\nﬁt approaches the global linear least-squares ﬁt to the data.\\nThe discussion in Chapter 5 on selecting the regularization parameter for\\nsmoothing splines applies here, and will not be repeated. Local regression\\nsmoothers are linear estimators; the smoother matrix inˆf = Sλy is built up\\nfrom the equivalent kernels (6.8), and hasijth entry{Sλ}ij = li(xj). Leave-\\none-out cross-validation is particularly simple (Exercise 6.7), as is general-\\nized cross-validation,Cp (Exercise 6.10), andk-fold cross-validation. The\\neﬀective degrees of freedom is again deﬁned as trace(Sλ), and can be used\\nto calibrate the amount of smoothing. Figure 6.7 compares the equivalent\\nkernels for a smoothing spline and local linear regression. The local regres-\\nsion smoother has a span of 40%, which results in df = trace(Sλ)=5 .86.\\nThe smoothing spline was calibrated to have the same df, and their equiv-\\nalent kernels are qualitatively quite similar.\\n1With uniformly spacedxi; with irregularly spacedxi, the behavior can deteriorate.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='222d7612-8459-4ea0-b16c-eacce7c496a1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 216, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='200 6. Kernel Smoothing Methods\\n6.3 Local Regression in IRp\\nKernel smoothing and local regression generalize very naturally to two or\\nmore dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant\\nlocally with weights supplied by ap-dimensional kernel. Local linear re-\\ngression will ﬁt a hyperplane locally inX, by weighted least squares, with\\nweights supplied by ap-dimensional kernel. It is simple to implement and\\nis generally preferred to the local constant ﬁt for its superior performance\\non the boundaries.\\nLet b(X) be a vector of polynomial terms inX of maximum degreed.\\nFor example, withd =1a n dp =2w eg e tb(X)=( 1,X1,X2); withd =2\\nwe getb(X)=( 1,X1,X2,X2\\n1,X2\\n2,X1X2); and trivially withd =0w eg e t\\nb(X)=1 .A te a c hx0 ∈IRp solve\\nmin\\nβ(x0)\\nN∑\\ni=1\\nKλ(x0,xi)(yi −b(xi)Tβ(x0))2 (6.12)\\nto produce the ﬁtˆf(x0)= b(x0)T ˆβ(x0). Typically the kernel will be a radial\\nfunction, such as the radial Epanechnikov or tri-cube kernel\\nKλ(x0,x)= D\\n⎤||x−x0||\\nλ\\n⎦\\n, (6.13)\\nwhere||·|| is the Euclidean norm. Since the Euclidean norm depends on the\\nunits in each coordinate, it makes most sense to standardize each predictor,\\nfor example, to unit standard deviation, prior to smoothing.\\nWhile boundary eﬀects are a problem in one-dimensional smoothing,\\nthey are a much bigger problem in two or higher dimensions, since the\\nfraction of points on the boundary is larger. In fact, one of the manifesta-\\ntions of the curse of dimensionality is that the fraction of points close to the\\nboundary increases to one as the dimension grows. Directly modifying the\\nkernel to accommodate two-dimensional boundaries becomes very messy,\\nespecially for irregular boundaries. Local polynomial regression seamlessly\\nperforms boundary correction to the desired order in any dimensions. Fig-\\nure 6.8 illustrates local linear regression on some measurements from an\\nastronomical study with an unusual predictor design (star-shaped). Here\\nthe boundary is extremely irregular, and the ﬁtted surface must also inter-\\npolateoverregionsofincreasingdatasparsityasweapproachtheboundary.\\nLocal regression becomes less useful in dimensions much higher than two\\nor three. We have discussed in some detail the problems of dimensional-\\nity, for example, in Chapter 2. It is impossible to simultaneously main-\\ntain localness (⇒low bias) and a sizable sample in the neighborhood (⇒\\nlow variance) as the dimension increases, without the total sample size in-\\ncreasing exponentially inp. Visualization ofˆf(X) also becomes diﬃcult in\\nhigher dimensions, and this is often one of the primary goals of smoothing.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c1754988-02d6-4c4b-a0b7-f61e51ba5902', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 217, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Structured Local Regression Models in IRp 201\\nEast-West\\nSouth-North\\nVelocity\\nEast-West\\nSouth-North\\nVelocity\\nFIGURE 6.8. The left panel shows three-dimensional data, where the response\\nis the velocity measurements on a galaxy, and the two predictors record positions\\non the celestial sphere. The unusual “star”-shaped design indicates the way the\\nmeasurements were made, and results in an extremely irregular boundary. The\\nright panel shows the results of local linear regression smoothing inIR2,u s i n ga\\nnearest-neighbor window with15% of the data.\\nAlthough the scatter-cloud and wire-frame pictures in Figure 6.8 look at-\\ntractive, it is quite diﬃcult to interpret the results except at a gross level.\\nFrom a data analysis perspective, conditional plots are far more useful.\\nFigure 6.9 shows an analysis of some environmental data with three pre-\\ndictors. The trellis display here shows ozone as a function of radiation,\\nconditioned on the other two variables, temperature and wind speed. How-\\never, conditioning on the value of a variable really implies local to that\\nvalue (as in local regression). Above each of the panels in Figure 6.9 is an\\nindication of the range of values present in that panel for each of the condi-\\ntioning values. In the panel itself the data subsets are displayed (response\\nversus remaining variable), and a one-dimensional local linear regression is\\nﬁt to the data. Although this is not quite the same as looking at slices of\\na ﬁtted three-dimensional surface, it is probably more useful in terms of\\nunderstanding the joint behavior of the data.\\n6.4 Structured Local Regression Models in IRp\\nWhen the dimension to sample-size ratio is unfavorable, local regression\\ndoes not help us much, unless we are willing to make some structural as-\\nsumptions about the model. Much of this book is about structured regres-\\nsion and classiﬁcation models. Here we focus on some approaches directly\\nrelated to kernel methods.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1fcc68ba-0fc2-4f62-bd42-f4178ada5c12', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 218, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='202 6. Kernel Smoothing Methods\\n1\\n2\\n3\\n4\\n5\\nTemp\\nWind\\n0 50 150 250\\nTemp\\nWind\\nTemp\\nWind\\n0 50 150 250\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\n1\\n2\\n3\\n4\\n5\\nTemp\\nWind\\n1\\n2\\n3\\n4\\n5\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\nTemp\\nWind\\n0 50 150 250\\nTemp\\nWind\\n1\\n2\\n3\\n4\\n5\\nTemp\\nWind\\n0 50 150 250\\nSolar Radiation (langleys)\\nCube Root Ozone (cube root ppb)\\nFIGURE 6.9.Three-dimensional smoothing example. The response is (cube-root\\nof) ozone concentration, and the three predictors are temperature, wind speed and\\nradiation. Thetrellis display shows ozone as a function of radiation, conditioned\\non intervals of temperature and wind speed (indicated by darker green or orange\\nshaded bars). Each panel contains about40% of the range of each of the condi-\\ntioned variables. The curve in each panel is a univariate local linear regression,\\nﬁt to the data in the panel.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07209ac9-7454-4e11-ad0c-f3c6d761c501', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 219, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Structured Local Regression Models in IRp 203\\n6.4.1 Structured Kernels\\nOne line of approach is to modify the kernel. The default spherical ker-\\nnel (6.13) gives equal weight to each coordinate, and so a natural default\\nstrategy is to standardize each variable to unit standard deviation. A more\\ngeneral approach is to use a positive semideﬁnite matrixA to weigh the\\ndiﬀerent coordinates:\\nKλ,A(x0,x)= D\\n⎤(x−x0)TA(x−x0)\\nλ\\n⎦\\n. (6.14)\\nEntire coordinates or directions can be downgraded or omitted by imposing\\nappropriate restrictions onA. For example, ifA is diagonal, then we can\\nincrease or decrease the inﬂuence of individual predictorsXj by increasing\\nor decreasing Ajj. Often the predictors are many and highly correlated,\\nsuchasthosearisingfromdigitizedanalogsignalsorimages.Thecovariance\\nfunction of the predictors can be used to tailor a metricA that focuses less,\\nsay, on high-frequency contrasts (Exercise 6.4). Proposals have been made\\nfor learning the parameters for multidimensional kernels. For example, the\\nprojection-pursuit regression model discussed in Chapter 11 is of this ﬂavor,\\nwhere low-rank versions ofA imply ridge functions forˆf(X). More general\\nmodels forA are cumbersome, and we favor instead the structured forms\\nfor the regression function discussed next.\\n6.4.2 Structured Regression Functions\\nWe are trying to ﬁt a regression functionE(Y|X)= f(X1,X2,...,X p)i n\\nIRp, in which every level of interaction is potentially present. It is natural\\nto consider analysis-of-variance (ANOVA) decompositions of the form\\nf(X1,X2,...,X p)= α+\\n∑\\nj\\ngj(Xj)+\\n∑\\nk<ℓ\\ngkℓ(Xk,Xℓ)+ ··· (6.15)\\nand then introduce structure by eliminating some of the higher-order terms.\\nAdditive models assume only main eﬀect terms:f(X)= α+∑p\\nj=1 gj(Xj);\\nsecond-order models will have terms with interactions of order at most\\ntwo, and so on. In Chapter 9, we describe iterativebackﬁtting algorithms\\nfor ﬁtting such low-order interaction models. In the additive model, for\\nexample, if all but thekth term is assumed known, then we can estimategk\\nby local regression ofY−∑\\nj̸=k gj(Xj)o nXk. This is done for each function\\nin turn, repeatedly, until convergence. The important detail is that at any\\nstage, one-dimensional local regression is all that is needed. The same ideas\\ncan be used to ﬁt low-dimensional ANOVA decompositions.\\nAn important special case of these structured models are the class of\\nvarying coeﬃcient models. Suppose, for example, that we divide thep pre-\\ndictors inX i n t oas e t(X1,X2,...,X q)w i t hq<p , and the remainder of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='83d12ed8-7902-4ccc-809f-620d701baa4d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 220, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='204 6. Kernel Smoothing Methods\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\nDepth\\nFemale\\n20 30 40 50 60\\nDepth\\nFemale\\nDepth\\nFemale\\n20 30 40 50 60\\nDepth\\nFemale\\nDepth\\nFemale\\n20 30 40 50 60\\nDepth\\nFemale\\nDepth\\nMale\\nDepth\\nMale\\n20 30 40 50 60\\nDepth\\nMale\\nDepth\\nMale\\n20 30 40 50 60\\nDepth\\nMale\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\nDepth\\nMale\\n20 30 40 50 60\\nAge\\nDiameter\\nAortic Diameter vs Age\\nFIGURE 6.10.In each panel theaorta diameter is modeled as a linear func-\\ntion of age. The coeﬃcients of this model vary withgender and depth down\\nthe aorta (left is near the top, right is low down). There is a clear trend in the\\ncoeﬃcients of the linear model.\\nthe variables we collect in the vectorZ. We then assume the conditionally\\nlinear model\\nf(X)= α(Z)+ β1(Z)X1 +··· +βq(Z)Xq. (6.16)\\nFor givenZ, this is a linear model, but each of the coeﬃcients can vary\\nwith Z. It is natural to ﬁt such a model by locally weighted least squares:\\nmin\\nα(z0),β(z0)\\nN∑\\ni=1\\nKλ(z0,zi)(yi −α(z0)−x1iβ1(z0)−···−xqiβq(z0))2 .\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the human aorta.\\nA longstanding claim has been that the aorta thickens withage.H e r ew e\\nmodel thediameter of the aorta as a linear function ofage, but allow the\\ncoeﬃcients to vary withgender and depth down the aorta. We used a local\\nregression model separately for males and females. While the aorta clearly\\ndoes thicken with age at the higher regions of the aorta, the relationship\\nfades with distance down the aorta. Figure 6.11 shows the intercept and\\nslope as a function of depth.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8cb472cd-2978-43e1-93b1-3bfd6ae49eee', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 221, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Local Likelihood and Other Models 205\\nMale\\nAge Intercept\\nDistance Down Aorta\\nAge Slope\\n0.0 0.2 0.4 0.6 0.8 1. 0\\nFemale\\n14 16 18 20\\nDistance Down Aorta\\n0.0 0.4 0.8 1.2\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.11.The intercept and slope ofage as a function ofdistance down\\nthe aorta, separately for males and females. The yellow bands indicate one stan-\\ndard error.\\n6.5 Local Likelihood and Other Models\\nThe concept of local regression and varying coeﬃcient models is extremely\\nbroad: any parametric model can be made local if the ﬁtting method ac-\\ncommodates observation weights. Here are some examples:\\n•Associated with each observationyi is a parameterθi = θ(xi)= xT\\ni β\\nlinear in the covariate(s)xi, and inference forβis based on the log-\\nlikelihood l(β)= ∑N\\ni=1 l(yi,xT\\ni β). We can modelθ(X)m o r eﬂ e x i b l y\\nby using the likelihood local tox0 for inference ofθ(x0)= xT\\n0 β(x0):\\nl(β(x0)) =\\nN∑\\ni=1\\nKλ(x0,xi)l(yi,xT\\ni β(x0)).\\nMany likelihood models, in particular the family of generalized linear\\nmodels including logistic and log-linear models, involve the covariates\\nin a linear fashion. Local likelihood allows a relaxation from a globally\\nlinear model to one that is locally linear.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a1350b04-f142-4bc4-ac7c-e8f87eee921f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 222, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='206 6. Kernel Smoothing Methods\\n•As above, except diﬀerent variables are associated withθfrom those\\nused for deﬁning the local likelihood:\\nl(θ(z0)) =\\nN∑\\ni=1\\nKλ(z0,zi)l(yi,η(xi,θ(z0))).\\nFor example,η(x,θ)= xTθcould be a linear model inx. This will ﬁt\\na varying coeﬃcient modelθ(z) by maximizing the local likelihood.\\n•Autoregressive time series models of order k have the form yt =\\nβ0 + β1yt−1 + β2yt−2 +··· + βkyt−k + εt.D e n o t i n gt h elag set by\\nzt =( yt−1,yt−2,...,y t−k), the model looks like a standard linear\\nmodel yt = zT\\nt β+ εt, and is typically ﬁt by least squares. Fitting\\nby local least squares with a kernelK(z0,zt) allows the model to\\nvary according to the short-term history of the series. This is to be\\ndistinguished from the more traditional dynamic linear models that\\nvary by windowing time.\\nAs an illustration of local likelihood, we consider the local version of the\\nmulticlass linear logistic regression model (4.36) of Chapter 4. The data\\nconsistoffeatures xi andanassociatedcategoricalresponse gi∈{1,2,...,J},\\nand the linear model has the form\\nPr(G = j|X = x)= eβj0+βT\\nj x\\n1+ ∑J−1\\nk=1 eβk0+βT\\nk x. (6.18)\\nThe local log-likelihood for thisJ class model can be written\\nN∑\\ni=1\\nKλ(x0,xi)\\n{\\nβgi0(x0)+ βgi(x0)T(xi −x0)\\n−log\\n[\\n1+\\nJ−1∑\\nk=1\\nexp\\n⎤\\nβk0(x0)+ βk(x0)T(xi −x0)\\n⎦\\n]}\\n.\\n(6.19)\\nNotice that\\n•we have usedgi as a subscript in the ﬁrst line to pick out the appro-\\npriate numerator;\\n•βJ0 =0a n dβJ = 0 by the deﬁnition of the model;\\n•we have centered the local regressions atx0, so that the ﬁtted poste-\\nrior probabilities atx0 are simply\\nˆPr(G = j|X = x0)= eˆβj0(x0)\\n1+ ∑J−1\\nk=1 eˆβk0(x0)\\n. (6.20)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='79540a75-f42c-4c7f-be18-dc3ebcc6d783', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 223, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5 Local Likelihood and Other Models 207\\nSystolic Blood Pressure\\nPrevalence CHD\\n100 140 180 220\\n0.0 0.2 0.4 0.6 0.8 1.0\\nObesity\\nPrevalence CHD\\n15 25 35 45\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\\nease) as a function of a risk factor for the South African heart disease data.\\nFor each plot we have computed the ﬁtted prevalence of CHD using a local linear\\nlogistic regression model. The unexpected increase in the prevalence of CHD at\\nthe lower ends of the ranges is because these are retrospective data, and some of\\nthe subjects had already undergone treatment to reduce their blood pressure and\\nweight. The shaded region in the plot indicates an estimated pointwise standard\\nerror band.\\nThis model can be used for ﬂexible multiclass classiﬁcation in moderately\\nlow dimensions, although successes have been reported with the high-\\ndimensional ZIP-code classiﬁcation problem. Generalized additive models\\n(Chapter 9) using kernel smoothing methods are closely related, and avoid\\ndimensionality problems by assuming an additive structure for the regres-\\nsion function.\\nAs a simple illustration we ﬁt a two-class local linear logistic model to\\nthe heart disease data of Chapter 4. Figure 6.12 shows the univariate local\\nlogistic models ﬁt to two of the risk factors (separately). This is a useful\\nscreeningdevicefordetectingnonlinearities, whenthedatathemselveshave\\nlittle visual information to oﬀer. In this case an unexpected anomaly is\\nuncovered in the data, which may have gone unnoticed with traditional\\nmethods.\\nSince CHD is a binary indicator, we could estimate the conditional preva-\\nlence Pr(G = j|x0) by simply smoothing this binary response directly with-\\nout resorting to a likelihood formulation. This amounts to ﬁtting a locally\\nconstant logistic regression model (Exercise 6.5). In order to enjoy the bias-\\ncorrection of local-linear smoothing, it is more natural to operate on the\\nunrestricted logit scale.\\nTypically with logistic regression, we compute parameter estimates as\\nwell as their standard errors. This can be done locally as well, and so', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1003e64-0095-41fe-84ef-3aae7cc69583', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 224, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='208 6. Kernel Smoothing Methods\\nSystolic Blood Pressure (for CHD group)\\nDensity Estimate\\n100 120 140 160 180 200 220\\n0.0 0.005 0.010 0.015 0.020\\nFIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\\nCHD group). The density estimate at each point is the average contribution from\\neach of the kernels at that point. We have scaled the kernels down by a factor of\\n10 to make the graph readable.\\nwe can produce, as shown in the plot, estimated pointwise standard-error\\nbands about our ﬁtted prevalence.\\n6.6 Kernel Density Estimation and Classiﬁcation\\nKernel density estimation is an unsupervised learning procedure, which\\nhistorically precedes kernel regression. It also leads naturally to a simple\\nfamily of procedures for nonparametric classiﬁcation.\\n6.6.1 Kernel Density Estimation\\nSuppose we have a random samplex1,...,x N drawn from a probability\\ndensity fX(x) ,a n dw ew i s ht oe s t i m a t efX at a pointx0. For simplicity we\\nassume for now thatX ∈IR. Arguing as before, a natural local estimate\\nhas the form\\nˆfX(x0)= #xi ∈N(x0)\\nNλ , (6.21)\\nwhere N(x0) is a small metric neighborhood aroundx0 of width λ.T h i s\\nestimate is bumpy, and the smoothParzen estimate is preferred\\nˆfX(x0)= 1\\nNλ\\nN∑\\ni=1\\nKλ(x0,xi), (6.22)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ece8e8f6-0335-4df9-82c4-aa0af213a14e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 225, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Kernel Density Estimation and Classiﬁcation 209\\nSystolic Blood Pressure\\nDensity Estimates\\n100 140 180 220\\n0.0 0.010 0.020\\nCHD\\nno CHD\\nSystolic Blood Pressure\\nPosterior Estimate\\n100 140 180 220\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.14. The left panel shows the two separate density estimates for\\nsystolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\\nkernel density estimate in each. The right panel shows the estimated posterior\\nprobabilities for CHD, using (6.25).\\nbecause it counts observations close tox0 with weights that decrease with\\ndistance fromx0. In this case a popular choice forKλis the Gaussian kernel\\nKλ(x0,x)= φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt\\nto the sample values forsystolic blood pressure for theCHD group. Letting\\nφλdenote the Gaussian density with mean zero and standard-deviationλ,\\nthen (6.22) has the form\\nˆfX(x)= 1\\nN\\nN∑\\ni=1\\nφλ(x−xi)\\n=( ˆF⋆φλ)(x), (6.23)\\nthe convolution of the sample empirical distributionˆF with φλ. The dis-\\ntribution ˆF(x)p u t sm a s s1/N at each of the observedxi, and is jumpy; in\\nˆfX(x)w eh a v es m o o t h e dˆF by adding independent Gaussian noise to each\\nobservation xi.\\nThe Parzen density estimate is the equivalent of the local average, and\\nimprovements have been proposed along the lines of local regression [on the\\nlog scale for densities; see Loader (1999)]. We will not pursue these here.\\nIn IRp the natural generalization of the Gaussian density estimate amounts\\nto using the Gaussian product kernel in (6.23),\\nˆfX(x0)= 1\\nN(2λ2π)\\np\\n2\\nN∑\\ni=1\\ne−1\\n2(||xi−x0||/λ)2\\n. (6.24)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3a266bb0-7ffd-41c1-9906-3b574917d735', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 226, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='210 6. Kernel Smoothing Methods\\n0.0\\n0.5\\n1.0\\nFIGURE 6.15. The population class densities may have interesting structure\\n(left) that disappears when the posterior probabilities are formed (right).\\n6.6.2 Kernel Density Classiﬁcation\\nOne can use nonparametric density estimates for classiﬁcation in a straight-\\nforward fashion using Bayes’ theorem. Suppose for aJ class problem we ﬁt\\nnonparametric density estimatesˆfj(X),j =1 ,...,J s e p a r a t e l yi ne a c ho f\\nthe classes, and we also have estimates of the class priors ˆπj (usually the\\nsample proportions). Then\\nˆPr(G = j|X = x0)= ˆπj ˆfj(x0)∑J\\nk=1 ˆπk ˆfk(x0)\\n. (6.25)\\nFigure 6.14 uses this method to estimate the prevalence of CHD for the\\nheart risk factor study, and should be compared with the left panel of Fig-\\nure 6.12. The main diﬀerence occurs in the region of high SBP in the right\\npanel of Figure 6.14. In this region the data are sparse for both classes, and\\nsince the Gaussian kernel density estimates use metric kernels, the density\\nestimates are low and of poor quality (high variance) in these regions. The\\nlocal logistic regression method (6.20) uses the tri-cube kernel withk-NN\\nbandwidth; this eﬀectively widens the kernel in this region, and makes use\\nof the local linear assumption to smooth out the estimate (on the logit\\nscale).\\nIf classiﬁcation is the ultimate goal, then learning the separate class den-\\nsities well may be unnecessary, and can in fact be misleading. Figure 6.15\\nshows an example where the densities are both multimodal, but the pos-\\nterior ratio is quite smooth. In learning the separate densities from data,\\none might decide to settle for a rougher, high-variance ﬁt to capture these\\nfeatures, which are irrelevant for the purposes of estimating the posterior\\nprobabilities. In fact, if classiﬁcation is the ultimate goal, then we need only\\nto estimate the posterior well near the decision boundary (for two classes,\\nthis is the set{x|Pr(G =1|X = x)= 1\\n2}).\\n6.6.3 The Naive Bayes Classiﬁer\\nThis is a technique that has remained popular over the years, despite its\\nname (also known as “Idiot’s Bayes”!) It is especially appropriate when', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf61c630-4915-44b3-8486-e015e9020671', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 227, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6 Kernel Density Estimation and Classiﬁcation 211\\nthe dimension p of the feature space is high, making density estimation\\nunattractive. The naive Bayes model assumes that given a classG = j,t h e\\nfeatures Xk are independent:\\nfj(X)=\\np∏\\nk=1\\nfjk(Xk). (6.26)\\nWhile this assumption is generally not true, it does simplify the estimation\\ndramatically:\\n•The individual class-conditional marginal densitiesfjk can each be\\nestimated separately using one-dimensional kernel density estimates.\\nThis is in fact a generalization of the original naive Bayes procedures,\\nwhich used univariate Gaussians to represent these marginals.\\n•If a componentXj of X is discrete, then an appropriate histogram\\nestimate can be used. This provides a seamless way of mixing variable\\ntypes in a feature vector.\\nDespite these rather optimistic assumptions, naive Bayes classiﬁers often\\noutperform far more sophisticated alternatives. The reasons are related to\\nFigure 6.15: although the individual class density estimates may be biased,\\nthis bias might not hurt the posterior probabilities as much, especially\\nnear the decision regions. In fact, the problem may be able to withstand\\nconsiderable bias for the savings in variance such a “naive” assumption\\nearns.\\nStarting from (6.26) we can derive the logit-transform (using classJ as\\nthe base):\\nlog Pr(G = ℓ|X)\\nPr(G = J|X) =l o gπℓfℓ(X)\\nπJfJ(X)\\n=l o gπℓ\\n∏p\\nk=1 fℓk(Xk)\\nπJ\\n∏p\\nk=1 fJk(Xk)\\n=l o gπℓ\\nπJ\\n+\\np∑\\nk=1\\nlog fℓk(Xk)\\nfJk(Xk)\\n= αℓ +\\np∑\\nk=1\\ngℓk(Xk).\\n(6.27)\\nThishastheformofa generalized additive model,whichisdescribedinmore\\ndetail in Chapter 9. The models are ﬁt in quite diﬀerent ways though; their\\ndiﬀerences are explored in Exercise 6.9. The relationship between naive\\nBayes and generalized additive models is analogous to that between linear\\ndiscriminant analysis and logistic regression (Section 4.4.5).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85bab70e-9897-43c1-a5fc-aad688c36725', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 228, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='212 6. Kernel Smoothing Methods\\n6.7 Radial Basis Functions and Kernels\\nIn Chapter 5, functions are represented as expansions in basis functions:\\nf(x)= ∑M\\nj=1 βjhj(x). The art of ﬂexible modeling using basis expansions\\nconsists of picking an appropriate family of basis functions, and then con-\\ntrolling the complexity of the representation by selection, regularization, or\\nboth. Some of the families of basis functions have elements that are deﬁned\\nlocally; for example,B-splines are deﬁned locally in IR. If more ﬂexibility\\nis desired in a particular region, then that region needs to be represented\\nby more basis functions (which in the case ofB-splines translates to more\\nknots). Tensor products of IR-local basis functions deliver basis functions\\nlocal in IRp. Not all basis functions are local—for example, the truncated\\npower bases for splines, or the sigmoidal basis functionsσ(α0 + αx)u s e d\\nin neural-networks (see Chapter 11). The composed functionf(x) can nev-\\nertheless show local behavior, because of the particular signs and values\\nof the coeﬃcients causing cancellations of global eﬀects. For example, the\\ntruncated power basis has an equivalentB-spline basis for the same space\\nof functions; the cancellation is exact in this case.\\nKernel methods achieve ﬂexibility by ﬁtting simple models in a region\\nlocal to the target pointx0. Localization is achieved via a weighting kernel\\nKλ, and individual observations receive weightsKλ(x0,xi).\\nRadial basis functions combine these ideas, by treating the kernel func-\\ntions Kλ(ξ,x) as basis functions. This leads to the model\\nf(x)=\\nM∑\\nj=1\\nKλj (ξj,x)βj\\n=\\nM∑\\nj=1\\nD\\n⎤||x−ξj||\\nλj\\n⎦\\nβj, (6.28)\\nwhere each basis element is indexed by a location orprototypeparameter ξj\\nand a scale parameterλj. A popular choice forD is the standard Gaussian\\ndensity function. There are several approaches to learning the parameters\\n{λj,ξj,βj},j =1 ,...,M . For simplicity we will focus on least squares\\nmethods for regression, and use the Gaussian kernel.\\n•Optimize the sum-of-squares with respect to all the parameters:\\nmin\\n{λj,ξj,βj}M\\n1\\nN∑\\ni=1\\n⎛\\n⎝yi −β0 −\\nM∑\\nj=1\\nβj exp\\n{\\n−(xi −ξj)T(xi −ξj)\\nλ2\\nj\\n}⎞\\n⎠\\n2\\n.\\n(6.29)\\nThis model is commonly referred to as an RBF network, an alterna-\\ntive to the sigmoidal neural network discussed in Chapter 11; theξj\\nand λj playing the role of the weights. This criterion is nonconvex', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8c886665-57e8-432e-b5b5-f74cd649f178', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 229, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.7 Radial Basis Functions and Kernels 213\\n024680.0 0.4 0.8 1.2\\nFIGURE 6.16.Gaussian radial basis functions inIRwith ﬁxed width can leave\\nholes (top panel). Renormalized Gaussian radial basis functions avoid this prob-\\nlem, and produce basis functions similar in some respects toB-splines.\\nwith multiple local minima, and the algorithms for optimization are\\nsimilar to those used for neural networks.\\n•Estimate the{λj,ξj} separately from theβj. Given the former, the\\nestimation of the latter is a simple least squares problem. Often the\\nkernel parametersλj and ξj are chosen in an unsupervised way using\\nthe X distribution alone. One of the methods is to ﬁt a Gaussian\\nmixture density model to the trainingxi, which provides both the\\ncenters ξj and the scalesλj. Other even more adhoc approaches use\\nclustering methods to locate the prototypes ξj, and treat λj = λ\\nas a hyper-parameter. The obvious drawback of these approaches is\\nthat the conditional distribution Pr(Y|X) and in particularE(Y|X)\\nis having no say in where the action is concentrated. On the positive\\nside, they are much simpler to implement.\\nWhile it would seem attractive to reduce the parameter set and assume\\na constant value forλj = λ, this can have an undesirable side eﬀect of\\ncreating holes—regions of IRp where none of the kernels has appreciable\\nsupport, as illustrated in Figure 6.16 (upper panel).Renormalized radial\\nbasis functions,\\nhj(x)= D(||x−ξj||/λ)∑M\\nk=1 D(||x−ξk||/λ)\\n, (6.30)\\navoid this problem (lower panel).\\nThe Nadaraya–Watson kernel regression estimator (6.2) in IRp can be\\nviewed as an expansion in renormalized radial basis functions,\\nˆf(x0)= ∑N\\ni=1 yi\\nKλ(x0,xi)∑N\\ni=1 Kλ(x0,xi)\\n=∑N\\ni=1 yihi(x0) (6.31)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='09d6308b-935a-433c-b811-d650e666d30e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 230, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='214 6. Kernel Smoothing Methods\\nwith a basis functionhi located at every observation and coeﬃcientsyi;\\nthat is,ξi = xi, ˆβi = yi,i =1 ,...,N .\\nNote the similarity between the expansion (6.31) and the solution (5.50)\\non page 169 to the regularization problem induced by the kernelK.R a d i a l\\nbasis functions form the bridge between the modern “kernel methods” and\\nlocal ﬁtting technology.\\n6.8 Mixture Models for Density Estimation and\\nClassiﬁcation\\nThemixturemodelisausefultoolfordensityestimation,andcanbeviewed\\nas a kind of kernel method. The Gaussian mixture model has the form\\nf(x)=\\nM∑\\nm=1\\nαmφ(x;μm,Σm) (6.32)\\nwith mixing proportionsαm, ∑\\nm αm = 1, and each Gaussian density has\\nam e a nμm and covariance matrixΣm. In general, mixture models can use\\nany component densities in place of the Gaussian in (6.32): the Gaussian\\nmixture model is by far the most popular.\\nThe parameters are usually ﬁt by maximum likelihood, using the EM\\nalgorithm as described in Chapter 8. Some special cases arise:\\n•If the covariance matrices are constrained to be scalar:Σm = σmI,\\nthen (6.32) has the form of a radial basis expansion.\\n•If in addition σm = σ> 0 is ﬁxed, and M ↑N, then the max-\\nimum likelihood estimate for (6.32) approaches the kernel density\\nestimate (6.22) where ˆαm =1 /N and ˆμm = xm.\\nUsing Bayes’ theorem, separate mixture densities in each class lead to ﬂex-\\nible models for Pr(G|X); this is taken up in some detail in Chapter 12.\\nFigure 6.17 shows an application of mixtures to the heart disease risk-\\nfactor study. In the top row are histograms ofAge for theno CHD and CHD\\ngroups separately, and then combined on the right. Using the combined\\ndata, we ﬁt a two-component mixture of the form (6.32) with the (scalars)\\nΣ1 and Σ2 not constrained to be equal. Fitting was done via the EM\\nalgorithm (Chapter 8): note that the procedure does not use knowledge of\\nthe CHD labels. The resulting estimates were\\nˆμ1 =3 6.4, ˆΣ1 = 157.7, ˆα1 =0 .7,\\nˆμ2 =5 8.0, ˆΣ2 =1 5.6, ˆα2 =0 .3.\\nThe component densitiesφ(ˆμ1, ˆΣ1)a n dφ(ˆμ2, ˆΣ2) are shown in the lower-\\nleft and middle panels. The lower-right panel shows these component den-\\nsities (orange and blue) along with the estimated mixture density (green).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b7d1abad-b9a4-4584-917c-1a101218b9c9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 231, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.8 Mixture Models for Density Estimation and Classiﬁcation 215\\nNo CHD\\nAge\\nCount\\n20 30 40 50 60\\n0 5 10 15 20\\nCHD\\nAge\\nCount\\n20 30 40 50 60\\n0 5 10 15\\nCombined\\nAge\\nCount\\n20 30 40 50 60\\n0 5 10 15 20 25 30\\n20 30 40 50 60\\n0.00 0.02 0.04 0.06 0.08 0.10\\nAge\\nMixture Estimate\\n20 30 40 50 60\\n0.00 0.02 0.04 0.06 0.08 0.10\\nAge\\nMixture Estimate\\n20 30 40 50 60\\n0.00 0.02 0.04 0.06 0.08 0.10\\nAge\\nMixture Estimate\\nFIGURE 6.17. Application of mixtures to the heart disease risk-factor study.\\n(Top row:) Histograms of Age for the no CHD and CHD groups separately, and\\ncombined. (Bottom row:) estimated component densities from a Gaussian mix-\\nture model, (bottom left, bottom middle); (bottom right:) Estimated component\\ndensities (blue and orange) along with the estimated mixture density (green). The\\norange density has a very large standard deviation, and approximates a uniform\\ndensity.\\nThe mixture model also provides an estimate of the probability that\\nobservation i belongs to componentm,\\nˆrim = ˆαmφ(xi;ˆμm, ˆΣm)∑M\\nk=1 ˆαkφ(xi;ˆμk, ˆΣk)\\n, (6.33)\\nwhere xi is Age in our example. Suppose we threshold each value ˆri2 and\\nhence deﬁne ˆδi = I(ˆri2 > 0.5). Then we can compare the classiﬁcation of\\neach observation byCHD and the mixture model:\\nMixture model\\nˆδ=0 ˆδ=1\\nCHD No 232 70\\nYes 76 84\\nAlthough the mixture model did not use theCHD labels, it has done a fair\\njob in discovering the twoCHD subpopulations. Linear logistic regression,\\nusing theCHD as a response, achieves the same error rate (32%) when ﬁt to\\nthese data using maximum-likelihood (Section 4.4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='af02e97e-fdd8-41d2-ba08-5abc9d7d38ad', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 232, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='216 6. Kernel Smoothing Methods\\n6.9 Computational Considerations\\nKernel and local regression and density estimation arememory-basedmeth-\\nods: the model is the entire training data set, and the ﬁtting is done at\\nevaluation or prediction time. For many real-time applications, this can\\nmake this class of methods infeasible.\\nThe computational cost to ﬁt at a single observationx0 is O(N)ﬂ o p s ,\\nexcept in oversimpliﬁed cases (such as square kernels). By comparison,\\nan expansion in M basis functions costs O(M) for one evaluation, and\\ntypically M ∼O(logN). Basis function methods have an initial cost of at\\nleast O(NM2 +M3).\\nThe smoothing parameter(s) λfor kernel methods are typically deter-\\nmined oﬀ-line, for example using cross-validation, at a cost ofO(N2)ﬂ o p s .\\nPopular implementations of local regression, such as theloess function in\\nS-PLUS andR and thelocfit procedure (Loader, 1999), use triangulation\\nschemes to reduce the computations. They compute the ﬁt exactly atM\\ncarefully chosen locations (O(NM)), and then use blending techniques to\\ninterpolate the ﬁt elsewhere (O(M) per evaluation).\\nBibliographic Notes\\nThere is a vast literature on kernel methods which we will not attempt to\\nsummarize. Rather we will point to a few good references that themselves\\nhave extensive bibliographies. Loader (1999) gives excellent coverage of lo-\\ncal regression and likelihood, and also describes state-of-the-art software\\nfor ﬁtting these models. Fan and Gijbels (1996) cover these models from\\na more theoretical aspect. Hastie and Tibshirani (1990) discuss local re-\\ngression in the context of additive modeling. Silverman (1986) gives a good\\noverview of density estimation, as does Scott (1992).\\nExercises\\nEx. 6.1Show that the Nadaraya–Watson kernel smooth with ﬁxed metric\\nbandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for\\nthe Epanechnikov kernel? What can be said for the Epanechnikov kernel\\nwith adaptive nearest-neighbor bandwidthλ(x0)?\\nEx. 6.2Show that∑N\\ni=1(xi−x0)li(x0) = 0 for local linear regression. Deﬁne\\nbj(x0)= ∑N\\ni=1(xi −x0)jli(x0). Show thatb0(x0) = 1 for local polynomial\\nregression of any degree (including local constants). Show thatbj(x0)=0\\nfor all j ∈{1,2,...,k } for local polynomial regression of degreek.W h a t\\nare the implications of this on the bias?', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bb0876e-c8c5-46cb-9162-c5dac56baf6e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 233, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 217\\nEx. 6.3 Show that||l(x)|| (Section 6.1.2) increases with the degree of the\\nlocal polynomial.\\nEx. 6.4 Suppose that the p predictors X arise from sampling relatively\\nsmooth analog curves at p uniformly spaced abscissa values. Denote by\\nCov(X|Y)= Σ the conditional covariance matrix of the predictors, and\\nassume this does not change much withY. Discuss the nature ofMaha-\\nlanobis choice A = Σ−1 for the metric in (6.14). How does this compare\\nwith A = I? How might you construct a kernelA that (a) downweights\\nhigh-frequency components in the distance metric; (b) ignores them\\ncompletely?\\nEx. 6.5 Show that ﬁtting a locally constant multinomial logit model of\\nthe form (6.19) amounts to smoothing the binary response indicators for\\neachclassseparatelyusingaNadaraya–Watsonkernelsmootherwithkernel\\nweights Kλ(x0,xi).\\nEx. 6.6 Suppose that all you have is software for ﬁtting local regression,\\nbut you can specify exactly which monomials are included in the ﬁt. How\\ncould you use this software to ﬁt a varying-coeﬃcient model in some of the\\nvariables?\\nEx. 6.7Derive an expression for the leave-one-out cross-validated residual\\nsum-of-squares for local polynomial regression.\\nEx. 6.8Suppose that for continuous responseY and predictorX,w em o d e l\\nthe joint density ofX,Y using a multivariate Gaussian kernel estimator.\\nNote that the kernel in this case would be the product kernelφλ(X)φλ(Y).\\nShow that the conditional meanE(Y|X) derived from this estimate is a\\nNadaraya–Watson estimator. Extend this result to classiﬁcation by pro-\\nviding a suitable kernel for the estimation of the joint distribution of a\\ncontinuous X and discreteY.\\nEx. 6.9Explore the diﬀerences between the naive Bayes model (6.27) and\\na generalized additive logistic regression model, in terms of (a) model as-\\nsumptions and (b) estimation. If all the variablesXk are discrete, what can\\nyou say about the corresponding GAM?\\nEx. 6.10Supposewe haveN samplesgeneratedfromthemodel yi = f(xi)+\\nεi,w i t hεi independent and identically distributed with mean zero and\\nvariance σ2,t h exi assumed ﬁxed (non random). We estimatef using a\\nlinear smoother (local regression, smoothing spline, etc.) with smoothing\\nparameter λ. Thus the vector of ﬁtted values is given byˆf = Sλy.C o n s i d e r\\nthe in-sample prediction error\\nPE(λ)=E 1\\nN\\nN∑\\ni=1\\n(y∗\\ni −ˆfλ(xi))2 (6.34)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='19c3d10e-f25f-45b0-9f4c-f286fecda803', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 234, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='218 6. Kernel Smoothing Methods\\nfor predicting new responses at theN input values. Show that the aver-\\nage squared residual on the training data, ASR(λ), is a biased estimate\\n(optimistic) for PE(λ), while\\nCλ=A S R (λ)+ 2σ2\\nN trace(Sλ) (6.35)\\nis unbiased.\\nEx. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood\\nis maximized at +∞, and describe how.\\nEx. 6.12 Write a computer program to perform a local linear discrimi-\\nnant analysis. At each query pointx0, the training data receive weights\\nKλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\\nsion boundaries (see Section 4.3) are computed by weighted averages. Try\\nout your program on thezipcode data, and show the training and test er-\\nrors for a series of ﬁve pre-chosen values ofλ.T h ezipcode data are available\\nfrom the book websitewww-stat.stanford.edu/ElemStatLearn.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f9d4d0e5-f11a-4e18-99c3-ecd32d5b4cdb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 235, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7\\nModel Assessment and Selection\\n7.1 Introduction\\nThe generalization performance of a learning method relates to its predic-\\ntion capability on independent test data. Assessment of this performance\\nis extremely important in practice, since it guides the choice of learning\\nmethod or model, and gives us a measure of the quality of the ultimately\\nchosen model.\\nIn this chapter we describe and illustrate the key methods for perfor-\\nmance assessment, and show how they are used to select models. We begin\\nthe chapter with a discussion of the interplay between bias, variance and\\nmodel complexity.\\n7.2 Bias, Variance and Model Complexity\\nFigure 7.1 illustrates the important issue in assessing the ability of a learn-\\ningmethodtogeneralize.Considerﬁrstthecaseofaquantitative orinterval\\nscale response. We have a target variableY, a vector of inputsX,a n da\\nprediction model ˆf(X) that has been estimated from a training set T .\\nThe loss function for measuring errors betweenY and ˆf(X) is denoted by\\nL(Y, ˆf(X)). Typical choices are\\nL(Y, ˆf(X)) =\\n{\\n(Y −ˆf(X))2 squared error\\n|Y −ˆf(X)| absolute error. (7.1)\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 219\\nDOI: 10.1007/b94608_7,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5874e394-21d1-4d2d-91ce-7f7551667cd4', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 236, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='220 7. Model Assessment and Selection\\n0 5 10 15 20 25 30 35\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\\nModel Complexity (df)\\nPrediction Error\\nHigh Bias Low Bias\\nHigh VarianceLow Variance\\nFIGURE 7.1. Behavior of test sample and training sample error as the model\\ncomplexity is varied. The light blue curves show the training errorerr, while the\\nlight red curves show the conditional test errorErrT for 100 training sets of size\\n50 each, as the model complexity is increased. The solid curves show the expected\\ntest errorErr and the expected training errorE[err].\\nTest error,a l s or e f e r r e dt oa sgeneralization error, is the prediction error\\nover an independent test sample\\nErrT =E [L(Y, ˆf(X))|T ] (7.2)\\nwhere both X and Y are drawn randomly from their joint distribution\\n(population). Here the training setT is ﬁxed, and test error refers to the\\nerror for this speciﬁc training set. A related quantity is the expected pre-\\ndiction error (or expected test error)\\nErr = E[L(Y, ˆf(X))] = E[ErrT ]. (7.3)\\nNote that this expectation averages over everything that is random, includ-\\ning the randomness in the training set that producedˆf.\\nFigure 7.1 shows the prediction error (light red curves) ErrT for 100\\nsimulated training sets each of size 50. The lasso (Section 3.4.2) was used\\nto produce the sequence of ﬁts. The solid red curve is the average, and\\nhence an estimate of Err.\\nEstimation of ErrT will be our goal, although we will see that Err is\\nmore amenable to statistical analysis, and most methods eﬀectively esti-\\nmate the expected error. It does not seem possible to estimate conditional', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a463e4d-7e1e-44d5-8d4d-116b489f32df', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 237, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2 Bias, Variance and Model Complexity 221\\nerror eﬀectively, given only the information in the same training set. Some\\ndiscussion of this point is given in Section 7.12.\\nTraining erroris the average loss over the training sample\\nerr = 1\\nN\\nN∑\\ni=1\\nL(yi, ˆf(xi)). (7.4)\\nWe would like to know the expected test error of our estimated model\\nˆf. As the model becomes more and more complex, it uses the training\\ndata more and is able to adapt to more complicated underlying structures.\\nHence there is a decrease in bias but an increase in variance. There is some\\nintermediate model complexity that gives minimum expected test error.\\nUnfortunately training error is not a good estimate of the test error,\\nas seen in Figure 7.1. Training error consistently decreases with model\\ncomplexity, typically dropping to zero if we increase the model complexity\\nenough. However, a model with zero training error is overﬁt to the training\\ndata and will typically generalize poorly.\\nThe story is similar for a qualitative or categorical responseG taking\\none ofK values in a setG, labeled for convenience as 1,2,...,K . Typically\\nwe model the probabilities pk(X)=P r (G = k|X) (or some monotone\\ntransformations fk(X)), and thenˆG(X) = argmaxk ˆpk(X). In some cases,\\nsuch as 1-nearest neighbor classiﬁcation (Chapters 2 and 13) we produce\\nˆG(X) directly. Typical loss functions are\\nL(G, ˆG(X)) = I(G̸= ˆG(X)) (0–1 loss) , (7.5)\\nL(G, ˆp(X)) = −2\\nK∑\\nk=1\\nI(G = k)logˆpk(X)\\n= −2logˆpG(X)( −2× log-likelihood). (7.6)\\nThe quantity−2× the log-likelihood is sometimes referred to as the\\ndeviance.\\nAgain, test error here is ErrT =E [L(G, ˆG(X))|T ], the population mis-\\nclassiﬁcation error of the classiﬁer trained onT , and Err is the expected\\nmisclassiﬁcation error.\\nTraining error is the sample analogue, for example,\\nerr =−2\\nN\\nN∑\\ni=1\\nlog ˆpgi(xi), (7.7)\\nthe sample log-likelihood for the model.\\nThe log-likelihood can be used as a loss-function for general response\\ndensities, such as the Poisson, gamma, exponential, log-normal and others.\\nIf Prθ(X)(Y) is the density ofY, indexed by a parameterθ(X) that depends\\non the predictorX,t h e n\\nL(Y,θ(X)) =−2·logPrθ(X)(Y). (7.8)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22b80faf-06c6-482c-bd47-09da435da9bf', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 238, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='222 7. Model Assessment and Selection\\nThe “−2” in the deﬁnition makes the log-likelihood loss for the Gaussian\\ndistribution match squared-error loss.\\nFor ease of exposition, for the remainder of this chapter we will useY and\\nf(X) to represent all of the above situations, since we focus mainly on the\\nquantitative response (squared-error loss) setting. For the other situations,\\nthe appropriate translations are obvious.\\nIn this chapter we describe a number of methods for estimating the\\nexpected test error for a model. Typically our model will have a tuning\\nparameter or parametersαand so we can write our predictions asˆfα(x).\\nThe tuning parameter varies the complexity of our model, and we wish to\\nﬁnd the value ofαthat minimizes error, that is, produces the minimum of\\nthe average test error curve in Figure 7.1. Having said this, for brevity we\\nwill often suppress the dependence ofˆf(x)o nα.\\nIt is important to note that there are in fact two separate goals that we\\nmight have in mind:\\nModel selection: estimating the performance of diﬀerent models in order\\nto choose the best one.\\nModel assessment: having chosen a ﬁnal model, estimating its predic-\\ntion error (generalization error) on new data.\\nIf we are in a data-rich situation, the best approach for both problems is\\nto randomly divide the dataset into three parts: a training set, a validation\\nset, and a test set. The training set is used to ﬁt the models; the validation\\nset is used to estimate prediction error for model selection; the test set is\\nused for assessment of the generalization error of the ﬁnal chosen model.\\nIdeally, the test set should be kept in a “vault,” and be brought out only\\nat the end of the data analysis. Suppose instead that we use the test-set\\nrepeatedly, choosing the model with smallest test-set error. Then the test\\nset error of the ﬁnal chosen model will underestimate the true test error,\\nsometimes substantially.\\nIt is diﬃcult to give a general rule on how to choose the number of\\nobservations in each of the three parts, as this depends on the signal-to-\\nnoise ratio in the data and the training sample size. A typical split might\\nbe 50% for training, and 25% each for validation and testing:\\nTestTrain Validation TestTrain Validation TestValidationTrain Validation TestTrain\\nThe methods in this chapter are designed for situations where there is\\ninsuﬃcient data to split it into three parts. Again it is too diﬃcult to give\\na general rule on how much training data is enough; among other things,\\nthis depends on the signal-to-noise ratio of the underlying function, and\\nthe complexity of the models being ﬁt to the data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db82a5cc-c9db-44d2-81e8-ab57314267a1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 239, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 The Bias–Variance Decomposition 223\\nThe methods of this chapter approximate the validation step either an-\\nalytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use (cross-\\nvalidation and the bootstrap). Besides their use in model selection, we also\\nexamine to what extent each method provides a reliable estimate of test\\nerror of the ﬁnal chosen model.\\nBefore jumping into these topics, we ﬁrst explore in more detail the\\nnature of test error and the bias–variance tradeoﬀ.\\n7.3 The Bias–Variance Decomposition\\nAs in Chapter 2, if we assume thatY = f(X)+ εwhere E(ε)=0a n d\\nVar(ε)= σ2\\nε, we can derive an expression for the expected prediction error\\nof a regression ﬁtˆf(X) at an input pointX = x0, using squared-error loss:\\nErr(x0)= E[(Y −ˆf(x0))2|X = x0]\\n= σ2\\nε+[E ˆf(x0)−f(x0)]2 +E[ˆf(x0)−Eˆf(x0)]2\\n= σ2\\nε+Bias2(ˆf(x0))+Var( ˆf(x0))\\n= Irreducible Error+Bias 2 +V ariance. (7.9)\\nThe ﬁrst term is the variance of the target around its true meanf(x0), and\\ncannot be avoided no matter how well we estimatef(x0), unless σ2\\nε=0 .\\nThe second term is the squared bias, the amount by which the average of\\nour estimate diﬀers from the true mean; the last term is the variance; the\\nexpected squared deviation ofˆf(x0) around its mean. Typically the more\\ncomplex we make the modelˆf, the lower the (squared) bias but the higher\\nthe variance.\\nFor thek-nearest-neighbor regression ﬁt, these expressions have the sim-\\nple form\\nErr(x0)= E[(Y −ˆfk(x0))2|X = x0]\\n= σ2\\nε+\\n[\\nf(x0)−1\\nk\\nk∑\\nℓ=1\\nf(x(ℓ))\\n]2\\n+ σ2\\nε\\nk . (7.10)\\nHere we assume for simplicity that training inputsxi are ﬁxed, and the ran-\\ndomness arises from theyi. The number of neighborsk is inversely related\\nto the model complexity. For smallk, the estimate ˆfk(x) can potentially\\nadapt itself better to the underlyingf(x). As we increasek, the bias—the\\nsquared diﬀerence betweenf(x0) and the average off(x)a tt h ek-nearest\\nneighbors—will typically increase, while the variance decreases.\\nFor a linear model ﬁtˆfp(x)= xT ˆβ, where the parameter vectorβwith\\np components is ﬁt by least squares, we have\\nErr(x0)= E[(Y −ˆfp(x0))2|X = x0]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d613f9c0-7651-44cb-9db6-1271a16ce38e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 240, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='224 7. Model Assessment and Selection\\n= σ2\\nε+[f(x0)−Eˆfp(x0)]2 +||h(x0)||2σ2\\nε. (7.11)\\nHere h(x0)= X(XTX)−1x0,t h eN-vector of linear weights that produce\\nthe ﬁt ˆfp(x0)= x0T(XTX)−1XTy, and hence Var[ˆfp(x0)] =||h(x0)||2σ2\\nε.\\nWhile this variance changes withx0, its average (withx0 taken to be each\\nof the sample valuesxi)i s(p/N)σ2\\nε, and hence\\n1\\nN\\nN∑\\ni=1\\nErr(xi)= σ2\\nε+ 1\\nN\\nN∑\\ni=1\\n[f(xi)−Eˆf(xi)]2 + p\\nNσ2\\nε, (7.12)\\nthe in-sample error. Here model complexity is directly related to the num-\\nber of parametersp.\\nThe test error Err(x0) for a ridge regression ﬁt ˆfα(x0) is identical in\\nform to (7.11), except the linear weights in the variance term are diﬀerent:\\nh(x0)= X(XTX+αI)−1x0. The bias term will also be diﬀerent.\\nFor a linear model family such as ridge regression, we can break down\\nthe bias more ﬁnely. Letβ∗denote the parameters of the best-ﬁtting linear\\napproximation tof:\\nβ∗=a r gm i n\\nβ\\nE\\n⎤\\nf(X)−XTβ\\n⎦2\\n. (7.13)\\nHere the expectation is taken with respect to the distribution of the input\\nvariables X. Then we can write the average squared bias as\\nEx0\\n[\\nf(x0)−Eˆfα(x0)\\n]2\\n= Ex0\\n[\\nf(x0)−xT\\n0 β∗\\n]2\\n+Ex0\\n[\\nxT\\n0 β∗−ExT\\n0 ˆβα\\n]2\\n= Ave[Model Bias]2 +Ave[Estimation Bias]2\\n(7.14)\\nThe ﬁrst term on the right-hand side is the average squaredmodel bias,t h e\\nerror between the best-ﬁtting linear approximation and the true function.\\nThe second term is the average squaredestimation bias, the error between\\nthe average estimate E(xT\\n0 ˆβ) and the best-ﬁtting linear approximation.\\nFor linear models ﬁt by ordinary least squares, the estimation bias is zero.\\nFor restricted ﬁts, such as ridge regression, it is positive, and we trade it oﬀ\\nwith the beneﬁts of a reduced variance. The model bias can only be reduced\\nby enlarging the class of linear models to a richer collection of models, by\\nincluding interactions and transformations of the variables in the model.\\nFigure 7.2 shows the bias–variance tradeoﬀ schematically. In the case\\nof linear models, the model space is the set of all linear predictions from\\np inputs and the black dot labeled “closest ﬁt” isxTβ∗. The blue-shaded\\nregion indicates the errorσεwith which we see the truth in the training\\nsample.\\nAlso shown is the variance of the least squares ﬁt, indicated by the large\\nyellow circle centered at the black dot labeled “closest ﬁt in population,’', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2c6156e-d534-492a-bfd7-41fdb73150aa', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 241, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 The Bias–Variance Decomposition 225\\nRealization\\nClosest fit in population\\nEstimation Bias\\nSPACE\\nVariance\\nEstimation\\nClosest fit\\nTruth\\nModel bias\\nRESTRICTED\\nShrunken fit\\nMODEL SPACE\\nMODEL\\nFIGURE 7.2.Schematic of the behavior of bias and variance. The model space\\nis the set of all possible predictions from the model, with the “closest ﬁt” labeled\\nwith a black dot. The model bias from the truth is shown, along with the variance,\\nindicated by the large yellow circle centered at the black dot labeled “closest ﬁt\\nin population.” A shrunken or regularized ﬁt is also shown, having additional\\nestimation bias, but smaller prediction error due to its decreased variance.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d948bea-e76f-48d7-b2ec-ff6d81090139', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 242, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='226 7. Model Assessment and Selection\\nNow if we were to ﬁt a model with fewer predictors, or regularize the coef-\\nﬁcients by shrinking them toward zero (say), we would get the “shrunken\\nﬁt” shown in the ﬁgure. This ﬁt has an additional estimation bias, due to\\nthe fact that it is not the closest ﬁt in the model space. On the other hand,\\nit has smaller variance. If the decrease in variance exceeds the increase in\\n(squared) bias, then this is worthwhile.\\n7.3.1 Example: Bias–Variance Tradeoﬀ\\nFigure 7.3 shows the bias–variance tradeoﬀ for two simulated examples.\\nThere are 80 observations and 20 predictors, uniformly distributed in the\\nhypercube [0,1]20. The situations are as follows:\\nLeft panels:Y is 0 ifX1 ≤1/2a n d1i fX1 > 1/2, and we applyk-nearest\\nneighbors.\\nRight panels:Y is 1 if∑10\\nj=1 Xj is greater than 5 and 0 otherwise, and we\\nuse best subset linear regression of sizep.\\nThe top row is regression with squared error loss; the bottom row is classi-\\nﬁcation with 0–1 loss. The ﬁgures show the prediction error (red), squared\\nbias (green) and variance (blue), all computed for a large test sample.\\nIn the regression problems, bias and variance add to produce the predic-\\ntion error curves, with minima at aboutk =5f o rk-nearest neighbors, and\\np ≥10 for the linear model. For classiﬁcation loss (bottom ﬁgures), some\\ninteresting phenomena can be seen. The bias and variance curves are the\\nsame as in the top ﬁgures, and prediction error now refers to misclassiﬁ-\\ncation rate. We see that prediction error is no longer the sum of squared\\nbias and variance. For thek-nearest neighbor classiﬁer, prediction error\\ndecreases or stays the same as the number of neighbors is increased to 20,\\ndespite the fact that the squared bias is rising. For the linear model classi-\\nﬁer the minimum occurs forp≥10 as in regression, but the improvement\\nover thep = 1 model is more dramatic. We see that bias and variance seem\\nto interact in determining prediction error.\\nWhy does this happen? There is a simple explanation for the ﬁrst phe-\\nnomenon. Suppose at a given input point, the true probability of class 1 is\\n0.9 while the expected value of our estimate is 0.6. Then the squared bias—\\n(0.6−0.9)2—is considerable, but the prediction error is zero since we make\\nthe correct decision. In other words, estimation errors that leave us on the\\nright side of the decision boundary don’t hurt. Exercise 7.2 demonstrates\\nthis phenomenon analytically, and also shows the interaction eﬀect between\\nbias and variance.\\nThe overall point is that the bias–variance tradeoﬀ behaves diﬀerently\\nfor 0–1 loss than it does for squared error loss. This in turn means that\\nthe best choices of tuning parameters may diﬀer substantially in the two', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='797d13c1-a72d-4e5b-9a0a-dc613b9ff50b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 243, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 The Bias–Variance Decomposition 227\\n0.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k\\n50 40 30 20 10 0\\nk−NN − Regression\\n51 0 1 5 2 0\\n0.0 0.1 0.2 0.3 0.4\\nSubset Size p\\nLinear Model − Regression\\n0.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k\\n50 40 30 20 10 0\\nk−NN − Classification\\n51 0 1 5 2 0\\n0.0 0.1 0.2 0.3 0.4\\nSubset Size p\\nLinear Model − Classification\\nFIGURE 7.3.Expected prediction error (orange), squared bias (green) and vari-\\nance (blue) for a simulated example. The top row is regression with squared error\\nloss; the bottom row is classiﬁcation with 0–1 loss. The models are k-nearest\\nneighbors (left) and best subset regression of sizep (right). The variance and bias\\ncurves are the same in regression and classiﬁcation, but the prediction error curve\\nis diﬀerent.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0197ef21-228d-44e9-8bac-726dfb0a9a61', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 244, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='228 7. Model Assessment and Selection\\nsettings. One should base the choice of tuning parameter on an estimate of\\nprediction error, as described in the following sections.\\n7.4 Optimism of the Training Error Rate\\nDiscussions of error rate estimation can be confusing, because we have\\nto make clear which quantities are ﬁxed and which are random1.B e f o r e\\nwe continue, we need a few deﬁnitions, elaborating on the material of Sec-\\ntion 7.2. Given a training setT ={(x1,y1),(x2,y2),... (xN,yN)} the gen-\\neralization error of a modelˆf is\\nErrT =E X0,Y0[L(Y0, ˆf(X0))|T ]; (7.15)\\nNotethatthetrainingset T isﬁxedinexpression(7.15).Thepoint( X0,Y 0)\\nis a new test data point, drawn fromF, the joint distribution of the data.\\nAveraging over training setsT yields the expected error\\nErr = ET EX0,Y0[L(Y0, ˆf(X0))|T ], (7.16)\\nwhich is more amenable to statistical analysis. As mentioned earlier, it\\nturns out that most methods eﬀectively estimate the expected error rather\\nthan ET ; see Section 7.12 for more on this point.\\nNow typically, the training error\\nerr = 1\\nN\\nN∑\\ni=1\\nL(yi, ˆf(xi)) (7.17)\\nwill be less than the true error ErrT , because the same data is being used\\nto ﬁt the method and assess its error (see Exercise 2.9). A ﬁtting method\\ntypically adapts to the training data, and hence the apparent or training\\nerror err will be an overly optimistic estimate of the generalization error\\nErrT .\\nPart of the discrepancy is due to where the evaluation points occur. The\\nquantity ErrT can be thought of asextra-sample error, since the test input\\nvectors don’t need to coincide with the training input vectors. The nature\\nof the optimism inerr is easiest to understand when we focus instead on\\nthe in-sample error\\nErrin = 1\\nN\\nN∑\\ni=1\\nEY0[L(Y0\\ni , ˆf(xi))|T ] (7.18)\\nThe Y0 notation indicates that we observeN new response values at\\neach of the training pointsxi,i =1 ,2,...,N . We deﬁne theoptimism as\\n1Indeed, in the ﬁrst edition of our book, this section wasn’t suﬃciently clear.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a10f2e81-b4b9-4f56-acc5-7cb381d23140', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 245, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.4 Optimism of the Training Error Rate 229\\nthe diﬀerence between Errin and the training errorerr:\\nop≡Errin −err. (7.19)\\nThisistypicallypositivesince errisusuallybiaseddownwardasanestimate\\nof prediction error. Finally, the average optimism is the expectation of the\\noptimism over training sets\\nω≡Ey(op). (7.20)\\nHere the predictors in the training set are ﬁxed, and the expectation is\\nover the training set outcome values; hence we have used the notation Ey\\ninstead of ET . We can usually estimate only the expected errorωrather\\nthan op, in the same way that we can estimate the expected error Err\\nrather than the conditional error ErrT .\\nFor squared error, 0–1, and other loss functions, one can show quite\\ngenerally that\\nω= 2\\nN\\nN∑\\ni=1\\nCov(ˆyi,yi), (7.21)\\nwhere Cov indicates covariance. Thus the amount by whicherr underesti-\\nmates the true error depends on how stronglyyi aﬀects its own prediction.\\nThe harder we ﬁt the data, the greater Cov(ˆyi,yi) will be, thereby increas-\\ningtheoptimism.Exercise7.4provesthisresultforsquarederrorlosswhere\\nˆyi is the ﬁtted value from the regression. For 0–1 loss, ˆyi ∈{0,1} is the\\nclassiﬁcation atxi, and for entropy loss, ˆyi ∈[0,1] is the ﬁtted probability\\nof class 1 atxi.\\nI ns u m m a r y ,w eh a v et h ei m p o r t a n tr e l a t i o n\\nEy(Errin)=E y(err)+ 2\\nN\\nN∑\\ni=1\\nCov(ˆyi,yi). (7.22)\\nThis expression simpliﬁes if ˆyi is obtained by a linear ﬁt withd inputs\\nor basis functions. For example,\\nN∑\\ni=1\\nCov(ˆyi,yi)= dσ2\\nε (7.23)\\nfor the additive error modelY = f(X)+ ε,a n ds o\\nEy(Errin)=E y(err)+2 · d\\nNσ2\\nε. (7.24)\\nExpression (7.23) is the basis for the deﬁnition of theeﬀective number of\\nparameters discussed in Section 7.6 The optimism increases linearly with', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f0761863-b5cd-423c-a78d-0cd9487bd234', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 246, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='230 7. Model Assessment and Selection\\nthe number d of inputs or basis functions we use, but decreases as the\\ntraining sample size increases. Versions of (7.24) hold approximately for\\nother error models, such as binary data and entropy loss.\\nAn obvious way to estimate prediction error is to estimate the optimism\\nand then add it to the training errorerr. The methods described in the\\nnext section— Cp, AIC, BIC and others—work in this way, for a special\\nclass of estimates that are linear in their parameters.\\nIn contrast, cross-validation and bootstrap methods, described later in\\nthe chapter, are direct estimates of the extra-sample error Err. These gen-\\neral tools can be used with any loss function, and with nonlinear, adaptive\\nﬁtting techniques.\\nIn-sample error is not usually of direct interest since future values of the\\nfeatures are not likely to coincide with their training set values. But for\\ncomparison between models, in-sample error is convenient and often leads\\nto eﬀective model selection. The reason is that the relative (rather than\\nabsolute) size of the error is what matters.\\n7.5 Estimates of In-Sample Prediction Error\\nThe general form of the in-sample estimates is\\nˆErrin = err+ ˆω, (7.25)\\nwhere ˆωis an estimate of the average optimism.\\nUsing expression (7.24), applicable when d parameters are ﬁt under\\nsquared error loss, leads to a version of the so-calledCp statistic,\\nCp = err+2 · d\\nN ˆσε\\n2. (7.26)\\nHere ˆσε\\n2 is an estimate of the noise variance, obtained from the mean-\\nsquarederrorofalow-biasmodel.Usingthiscriterionweadjustthetraining\\nerror by a factor proportional to the number of basis functions used.\\nThe Akaike information criterionis a similar but more generally appli-\\nc a b l ee s t i m a t eo fE r rin when a log-likelihood loss function is used. It relies\\non a relationship similar to (7.24) that holds asymptotically asN →∞:\\n−2·E[logPr ˆθ(Y)]≈−2\\nN ·E[loglik]+2 · d\\nN. (7.27)\\nHere Prθ(Y) is a family of densities forY (containing the “true” density),\\nˆθis the maximum-likelihood estimate ofθ, and “loglik” is the maximized\\nlog-likelihood:\\nloglik =\\nN∑\\ni=1\\nlogPrˆθ(yi). (7.28)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6a1f386b-27ac-4af2-b17b-cabd24282ce7', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 247, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.5 Estimates of In-Sample Prediction Error 231\\nFor example, for the logistic regression model, using the binomial log-\\nlikelihood, we have\\nAIC =−2\\nN ·loglik+2 · d\\nN. (7.29)\\nFor the Gaussian model (with varianceσ2\\nε=ˆσε\\n2 assumed known), the AIC\\nstatistic is equivalent toCp, and so we refer to them collectively as AIC.\\nTo use AIC for model selection, we simply choose the model giving small-\\nest AIC over the set of models considered. For nonlinear and other complex\\nmodels, we need to replaced by some measure of model complexity. We\\ndiscuss this in Section 7.6.\\nGiven a set of modelsfα(x) indexed by a tuning parameterα,d e n o t e\\nby err(α)a n dd(α) the training error and number of parameters for each\\nmodel. Then for this set of models we deﬁne\\nAIC(α)= err(α)+2 · d(α)\\nN ˆσε\\n2. (7.30)\\nThe function AIC(α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆαthat minimizes it. Our ﬁnal chosen model\\nis fˆα(x). Note that if the basis functions are chosen adaptively, (7.23) no\\nlonger holds. For example, if we have a total ofp inputs, and we choose\\nthe best-ﬁtting linear model withd<p inputs, the optimism will exceed\\n(2d/N)σ2\\nε. Put another way, by choosing the best-ﬁtting model with d\\ninputs, theeﬀective number of parametersﬁt is more thand.\\nFigure 7.4 shows AIC in action for the phoneme recognition example\\nof Section 5.2.3 on page 148. The input vector is the log-periodogram of\\nthe spoken vowel, quantized to 256 uniformly spaced frequencies. A lin-\\near logistic regression model is used to predict the phoneme class, with\\ncoeﬃcient functionβ(f)= ∑M\\nm=1 hm(f)θm, an expansion inM spline ba-\\nsis functions. For any givenM, a basis of natural cubic splines is used\\nfor thehm, with knots chosen uniformly over the range of frequencies (so\\nd(α)= d(M)= M). Using AIC to select the number of basis functions will\\napproximately minimize Err(M) for both entropy and 0–1 loss.\\nT h es i m p l ef o r m u l a\\n(2/N)\\nN∑\\ni=1\\nCov(ˆyi,yi)=( 2d/N)σ2\\nε\\nholds exactly for linear models with additive errors and squared error loss,\\nand approximately for linear models and log-likelihoods. In particular, the\\nformula does not hold in general for 0–1 loss (Efron, 1986), although many\\nauthors nevertheless use it in that context (right panel of Figure 7.4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='87fddd7e-8651-4ad9-8763-411436ab71ae', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 248, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='232 7. Model Assessment and Selection\\nNumber of Basis Functions\\nLog-likelihood\\n0.5 1.0 1.5 2.0 2.5\\nLog-likelihood Loss\\n2 4 8 16 32 64 128\\nO\\nO\\nO\\nO O O\\nO\\nO\\nO\\nO\\nO\\nO O O\\nO\\nO\\nO\\nO\\nO\\nO O O O O\\nTrain\\nTest\\nAIC\\nNumber of Basis Functions\\nMisclassification Error\\n0.10 0.15 0.20 0.25 0.30 0.35\\n0-1 Loss\\n2 4 8 16 32 64 128\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nO\\nO\\nO\\nO O\\nO\\nO\\nO\\nFIGURE 7.4. AIC used for model selection for the phoneme recogni-\\ntion example of Section 5.2.3. The logistic regression coeﬃcient function\\nβ(f)= ∑M\\nm=1 hm(f)θm is modeled as an expansion inM spline basis functions.\\nIn the left panel we see the AIC statistic used to estimateErrin using log-likeli-\\nhood loss. Included is an estimate ofErr based on an independent test sample. It\\ndoes well except for the extremely over-parametrized case (M = 256 parameters\\nfor N = 1000 observations). In the right panel the same is done for 0–1 loss.\\nAlthough the AIC formula does not strictly apply here, it does a reasonable job in\\nthis case.\\n7.6 The Eﬀective Number of Parameters\\nThe concept of “number of parameters” can be generalized, especially to\\nmodels where regularization is used in the ﬁtting. Suppose we stack the\\noutcomes y1,y2,...,y N into a vectory, and similarly for the predictions\\nˆy. Then a linear ﬁtting method is one for which we can write\\nˆy = Sy, (7.31)\\nwhere S is anN×N matrix depending on the input vectorsxi but not on\\nthe yi. Linear ﬁtting methods include linear regression on the original fea-\\ntures or on a derived basis set, and smoothing methods that use quadratic\\nshrinkage, such as ridge regression and cubic smoothing splines. Then the\\neﬀective number of parametersis deﬁned as\\ndf(S)=t r a c e (S), (7.32)\\nthe sum of the diagonal elements ofS (also known as theeﬀective degrees-\\nof-freedom). Note that ifS is an orthogonal-projection matrix onto a basis', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c6b71e29-b487-4f33-98bb-5bab3999ccfb', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 249, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.7 The Bayesian Approach and BIC 233\\nset spanned byM features, then trace(S)= M. It turns out that trace(S)i s\\nexactly the correct quantity to replaced as the number of parameters in the\\nCp statistic (7.26). Ify arises from an additive-error modelY = f(X)+ ε\\nwith Var(ε)= σ2\\nε, then one can show that∑N\\ni=1 Cov(ˆyi,yi) = trace(S)σ2\\nε,\\nwhich motivates the more general deﬁnition\\ndf(ˆy)=\\n∑N\\ni=1 Cov(ˆyi,yi)\\nσ2ε\\n(7.33)\\n(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuition\\nfor the deﬁnition df = trace(S) in the context of smoothing splines.\\nFor models like neural networks, in which we minimize an error function\\nR(w) with weight decay penalty (regularization)α∑\\nm w2\\nm, the eﬀective\\nnumber of parameters has the form\\ndf(α)=\\nM∑\\nm=1\\nθm\\nθm +α, (7.34)\\nwhere the θm are the eigenvalues of the Hessian matrix∂2R(w)/∂w∂wT.\\nExpression (7.34) follows from (7.32) if we make a quadratic approximation\\nto the error function at the solution (Bishop, 1995).\\n7.7 The Bayesian Approach and BIC\\nThe Bayesian information criterion (BIC), like AIC, isapplicable insettings\\nwhere the ﬁtting is carried out by maximization of a log-likelihood. The\\ngeneric form of BIC is\\nBIC =−2·loglik+(log N)·d. (7.35)\\nTheBICstatistic(times1/2)isalsoknownastheSchwarzcriterion(Schwarz,\\n1978).\\nUnder the Gaussian model, assuming the varianceσ2\\nεis known,−2·loglik\\nequals (up to a constant)∑\\ni(yi−ˆf(xi))2/σ2\\nε,w h i c hi sN·err/σ2\\nεfor squared\\nerror loss. Hence we can write\\nBIC =N\\nσ2ε\\n[\\nerr+(log N)· d\\nNσ2\\nε\\n]\\n. (7.36)\\nTherefore BIC is proportional to AIC (Cp), with the factor 2 replaced\\nby logN. AssumingN>e 2 ≈7.4, BIC tends to penalize complex models\\nmore heavily, giving preference to simpler models in selection. As with AIC,\\nσ2\\nεis typically estimated by the mean squared error of a low-bias model.\\nFor classiﬁcation problems, use of the multinomial log-likelihood leads to a\\nsimilar relationship with the AIC, using cross-entropy as the error measure.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='116728a0-d0fe-4c9f-85ff-be7272e62364', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 250, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='234 7. Model Assessment and Selection\\nNote however that the misclassiﬁcation error measure does not arise in the\\nBIC context, since it does not correspond to the log-likelihood of the data\\nunder any probability model.\\nDespite its similarity with AIC, BIC is motivated in quite a diﬀerent\\nway. It arises in the Bayesian approach to model selection, which we now\\ndescribe.\\nSuppose we have a set of candidate models Mm,m =1 ,...,M and\\ncorresponding model parametersθm, and we wish to choose a best model\\nfrom among them. Assuming we have a prior distribution Pr(θm|Mm)f o r\\nthe parameters of each modelMm, the posterior probability of a given\\nmodel is\\nPr(Mm|Z) ∝ Pr(Mm)·Pr(Z|Mm) (7.37)\\n∝ Pr(Mm)·\\n∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm,\\nwhere Z represents the training data{xi,yi}N\\n1 .T oc o m p a r et w om o d e l s\\nMm and Mℓ, we form the posterior odds\\nPr(Mm|Z)\\nPr(Mℓ|Z) = Pr(Mm)\\nPr(Mℓ) · Pr(Z|Mm)\\nPr(Z|Mℓ) . (7.38)\\nIf the odds are greater than one we choose modelm, otherwise we choose\\nmodel ℓ. The rightmost quantity\\nBF(Z)= Pr(Z|Mm)\\nPr(Z|Mℓ) (7.39)\\nis called theBayes factor, the contribution of the data toward the posterior\\nodds.\\nTypically we assume that the prior over models is uniform, so that\\nPr(Mm) is constant. We need some way of approximating Pr(Z|Mm).\\nA so-called Laplace approximation to the integral followed by some other\\nsimpliﬁcations (Ripley, 1996, page 64) to (7.37) gives\\nlogPr(Z|Mm)=l o gP r (Z|ˆθm,Mm)−dm\\n2 ·logN +O(1). (7.40)\\nHere ˆθm is a maximum likelihood estimate anddm is the number of free\\nparameters in modelMm. If we deﬁne our loss function to be\\n−2logPr( Z|ˆθm,Mm),\\nthis is equivalent to the BIC criterion of equation (7.35).\\nTherefore, choosing the model with minimum BIC is equivalent to choos-\\ning the model with largest (approximate) posterior probability. But this\\nframework gives us more. If we compute the BIC criterion for a set ofM,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='feec815d-b7bc-44b6-b748-485916ec9ef8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 251, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.8 Minimum Description Length 235\\nmodels, giving BICm, m =1 ,2,...,M , then we can estimate the posterior\\nprobability of each modelMm as\\ne−1\\n2·BICm\\n∑M\\nℓ=1 e−1\\n2·BICℓ\\n. (7.41)\\nThus we can estimate not only the best model, but also assess the relative\\nmerits of the models considered.\\nFor model selection purposes, there is no clear choice between AIC and\\nBIC. BIC is asymptotically consistent as a selection criterion. What this\\nmeans is that given a family of models, including the true model, the prob-\\nability that BIC will select the correct model approaches one as the sample\\nsize N →∞. This is not the case for AIC, which tends to choose models\\nwhich are too complex asN →∞. On the other hand, for ﬁnite samples,\\nBIC often chooses models that are too simple, because of its heavy penalty\\non complexity.\\n7.8 Minimum Description Length\\nThe minimum description length (MDL) approach gives a selection cri-\\nterion formally identical to the BIC approach, but is motivated from an\\noptimal coding viewpoint. We ﬁrst review the theory of coding for data\\ncompression, and then apply it to model selection.\\nWe think of our datum z as a message that we want to encode and\\nsend to someone else (the “receiver”). We think of our model as a way of\\nencoding the datum, and will choose the most parsimonious model, that is\\nthe shortest code, for the transmission.\\nSuppose ﬁrst that the possible messages we might want to transmit are\\nz1,z2,...,z m. Our code uses a ﬁnite alphabet of lengthA: for example, we\\nmight use a binary code{0,1} of length A = 2. Here is an example with\\nfour possible messages and a binary coding:\\nMessage z1 z2 z3 z4\\nCode 0 10 110 111 (7.42)\\nThis code is known as an instantaneous preﬁx code: no code is the pre-\\nﬁx of any other, and the receiver (who knows all of the possible codes),\\nknows exactly when the message has been completely sent. We restrict our\\ndiscussion to such instantaneous preﬁx codes.\\nOne could use the coding in (7.42) or we could permute the codes, for\\nexample use codes 110,10,111,0f o rz1,z2,z3,z4. How do we decide which\\nto use? It depends on how often we will be sending each of the messages.\\nIf, for example, we will be sendingz1 most often, it makes sense to use the\\nshortest code 0 forz1. Using this kind of strategy—shorter codes for more\\nfrequent messages—the average message length will be shorter.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='df967c24-d2d6-4152-ad11-8a54845cb58e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 252, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='236 7. Model Assessment and Selection\\nIn general, if messages are sent with probabilities Pr(zi),i =1 ,2,..., 4,\\na famous theorem due to Shannon says we should use code lengthsli =\\n−log2 Pr(zi) and the average message length satisﬁes\\nE(length)≥−\\n∑\\nPr(zi)log2 (Pr(zi)). (7.43)\\nThe right-hand side above is also called the entropy of the distribution\\nPr(zi). The inequality is an equality when the probabilities satisfypi =\\nA−li. In our example, if Pr(zi)=1 /2,1/4,1/8,1/8, respectively, then the\\ncoding shown in (7.42) is optimal and achieves the entropy lower bound.\\nIn general the lower bound cannot be achieved, but procedures like the\\nHuﬀman coding scheme can get close to the bound. Note that with an\\ninﬁnite set of messages, the entropy is replaced by−\\n∫\\nPr(z)log2 Pr(z)dz.\\nFrom this result we glean the following:\\nTo transmit a random variablez having probability density func-\\ntion Pr(z), we require about−log2 Pr(z) bits of information.\\nWe henceforth change notation from log2 Pr(z)t ol o gP r (z)=l o ge Pr(z);\\nthis is for convenience, and just introduces an unimportant multiplicative\\nconstant.\\nNow we apply this result to the problem of model selection. We have\\nam o d e lM with parameters θ, and data Z =( X,y) consisting of both\\ninputs and outputs. Let the (conditional) probability of the outputs under\\nthe model be Pr(y|θ,M,X), assume the receiver knows all of the inputs,\\nand we wish to transmit the outputs. Then the message length required to\\ntransmit the outputs is\\nlength =−logPr(y|θ,M,X)−logPr(θ|M), (7.44)\\nthe log-probability of the target values given the inputs. The second term\\nis the average code length for transmitting the model parametersθ, while\\nthe ﬁrst term is the average code length for transmitting the discrepancy\\nbetween the model and actual target values. For example suppose we have\\na single targety with y ∼N(θ,σ2), parameter θ∼N(0,1) and no input\\n(for simplicity). Then the message length is\\nlength = constant+logσ+ (y−θ)2\\n2σ2 + θ2\\n2 . (7.45)\\nNote that the smallerσis, the shorter on average is the message length,\\nsince y is more concentrated aroundθ.\\nThe MDL principle says that we should choose the model that mini-\\nmizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-\\ntion, and hence minimizing description length is equivalent to maximizing\\nposterior probability. Hence the BIC criterion, derived as approximation to\\nlog-posterior probability, can also be viewed as a device for (approximate)\\nmodel choice by minimum description length.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b90cc23c-e6eb-44b5-a53d-e68cf367a6c8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 253, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.9 Vapnik–Chervonenkis Dimension 237\\n0.0 0.2 0.4 0.6 0.8 1.0\\n-1.0 0.0 1.0\\nx\\nsin(50·x)\\nFIGURE 7.5.The solid curve is the functionsin(50x) for x ∈[0,1].T h eg r e e n\\n(solid) and blue (hollow) points illustrate how the associated indicator function\\nI(sin(αx) > 0) can shatter (separate) an arbitrarily large number of points by\\nchoosing an appropriately high frequencyα.\\nNote that we have ignored the precision with which a random variable\\nz is coded. With a ﬁnite code length we cannot code a continuous variable\\nexactly. However, if we codez within a toleranceδz, the message length\\nneeded is the log of the probability in the interval [z,z +δz] which is well ap-\\nproximated byδzPr(z)i fδzis small. Since logδzPr(z)=l o gδz+logPr( z),\\nthis means we can just ignore the constant logδzand use logPr(z)a so u r\\nmeasure of message length, as we did above.\\nThe preceding view of MDL for model selection says that we should\\nchoose the model with highest posterior probability. However, many Bayes-\\nians would instead do inference by sampling from the posterior distribution.\\n7.9 Vapnik–Chervonenkis Dimension\\nA diﬃculty in using estimates of in-sample error is the need to specify the\\nnumber of parameters (or the complexity)d used in the ﬁt. Although the\\neﬀective number of parameters introduced in Section 7.6 is useful for some\\nnonlinear models, it is not fully general. The Vapnik–Chervonenkis (VC)\\ntheory provides such a general measure of complexity, and gives associated\\nbounds on the optimism. Here we give a brief review of this theory.\\nSuppose we have a class of functions{f(x,α)} indexed by a parameter\\nvector α,w i t hx ∈IRp. Assume for now thatf is an indicator function,\\nthat is, takes the values 0 or 1. Ifα=( α0,α1)a n df is the linear indi-\\ncator functionI(α0 +αT\\n1 x> 0), then it seems reasonable to say that the\\ncomplexity of the class f is the number of parametersp +1 .B u tw h a t\\nabout f(x,α)= I(sinα·x)w h e r eαis any real number andx ∈IR? The\\nfunction sin(50· x) is shown in Figure 7.5. This is a very wiggly function\\nthat gets even rougher as the frequencyαincreases, but it has only one\\nparameter: despite this, it doesn’t seem reasonable to conclude that it has\\nless complexity than the linear indicator functionI(α0 + α1x)i n p =1\\ndimension.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='39e05282-d5eb-437b-a82e-4b5d8343d0d8', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 254, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='238 7. Model Assessment and Selection\\nFIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane\\ncan shatter three points. The last panel shows that this class cannot shatter four\\npoints, as no line will put the hollow points on one side and the solid points on\\nthe other. Hence the VC dimension of the class of straight lines in the plane is\\nthree. Note that a class of nonlinear curves could shatter four points, and hence\\nhas VC dimension greater than three.\\nThe Vapnik–Chervonenkis dimension is a way of measuring the com-\\nplexity of a class of functions by assessing how wiggly its members can\\nbe.\\nThe VC dimension of the class {f(x,α)} is deﬁned to be the\\nlargest number of points (in some conﬁguration) that can be\\nshattered by members of{f(x,α)}.\\nA set of points is said to be shattered by a class of functions if, no matter\\nhow we assign a binary label to each point, a member of the class can\\nperfectly separate them.\\nFigure 7.6 shows that the VC dimension of linear indicator functions\\nin the plane is 3 but not 4, since no four points can be shattered by a\\nset of lines. In general, a linear indicator function inp dimensions has VC\\ndimension p+1, which is also the number of free parameters. On the other\\nhand, it can be shown that the family sin(αx) has inﬁnite VC dimension,\\nas Figure 7.5 suggests. By appropriate choice ofα,a n ys e to fp o i n t sc a nb e\\nshattered by this class (Exercise 7.8).\\nSo far we have discussed the VC dimension only of indicator functions,\\nbut this can be extended to real-valued functions. The VC dimension of a\\nclass of real-valued functions{g(x,α)} is deﬁned to be the VC dimension\\nof the indicator class{I(g(x,α)−β> 0)},w h e r eβtakes values over the\\nrange ofg.\\nOne can use the VC dimension in constructing an estimate of (extra-\\nsample) prediction error; diﬀerent types of results are available. Using the\\nconcept of VC dimension, one can prove results about the optimism of the\\ntraining error when using a class of functions. An example of such a result is\\nthe following. If we ﬁtN training points using a class of functions{f(x,α)}\\nhaving VC dimensionh, then with probability at least 1−ηover training', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cdd7a801-2ab3-490f-82c2-8be4a26f5ea5', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 255, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.9 Vapnik–Chervonenkis Dimension 239\\nsets:\\nErrT ≤ err+ ϵ\\n2\\n⎤\\n1+\\n√\\n1+ 4·err\\nϵ\\n⎦\\n(binary classiﬁcation)\\nErrT ≤ err\\n(1−c√ϵ)+\\n(regression) (7.46)\\nwhere ϵ = a1\\nh[log(a2N/h)+1] −log(η/4)\\nN ,\\nand 0<a 1 ≤4, 0 <a 2 ≤2\\nThese bounds hold simultaneously for all membersf(x,α), and are taken\\nfrom Cherkassky and Mulier (2007, pages 116–118). They recommend the\\nvalue c = 1. For regression they suggesta1 = a2 = 1, and for classiﬁcation\\nthey make no recommendation, witha1 =4a n d a2 = 2 corresponding\\nto worst-case scenarios. They also give an alternativepractical bound for\\nregression\\nErrT ≤err\\n⎤\\n1−\\n√\\nρ−ρlogρ+ logN\\n2N\\n⎦−1\\n+\\n(7.47)\\nwith ρ= h\\nN , which is free of tuning constants. The bounds suggest that the\\noptimism increases withh and decreases withN in qualitative agreement\\nwith the AIC correctiond/N given in (7.24). However, the results in (7.46)\\nare stronger: rather than giving the expected optimism for each ﬁxed func-\\ntion f(x,α), they give probabilistic upper bounds for all functionsf(x,α),\\nand hence allow for searching over the class.\\nVapnik’sstructural risk minimization(SRM) approach ﬁts a nested se-\\nquence of models of increasing VC dimensionsh1 <h 2 < ··· , and then\\nchooses the model with the smallest value of the upper bound.\\nWe note that upper bounds like the ones in (7.46) are often very loose,\\nbut that doesn’t rule them out as good criteria for model selection, where\\nthe relative (not absolute) size of the test error is important. The main\\ndrawback of this approach is the diﬃculty in calculating the VC dimension\\nof a class of functions. Often only a crude upper bound for VC dimension\\nis obtainable, and this may not be adequate. An example in which the\\nstructural risk minimization program can be successfully carried out is the\\nsupport vector classiﬁer, discussed in Section 12.2.\\n7.9.1 Example (Continued)\\nFigure 7.7 shows the results when AIC, BIC and SRM are used to select\\nthe model size for the examples of Figure 7.3. For the examples labeledKNN,\\nthe model indexαrefers to neighborhood size, while for those labeledREG α\\nrefers to subset size. Using each selection method (e.g., AIC) we estimated\\nthe best model ˆαand found its true prediction error ErrT (ˆα) on a test\\nset. For the same training set we computed the prediction error of the best', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2e9bb5e2-ada8-4e5f-917f-9a03a55ddef6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 256, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='240 7. Model Assessment and Selection\\nreg/KNN reg/linear class/KNN class/linear\\n0 2 04 06 08 0 1 0 0\\n% Increase Over Best\\nAIC\\nreg/KNN reg/linear class/KNN class/linear\\n0 2 04 06 08 0 1 0 0\\n% Increase Over Best\\nBIC\\nreg/KNN reg/linear class/KNN class/linear\\n0 2 04 06 08 0 1 0 0\\n% Increase Over Best\\nSRM\\nFIGURE 7.7. Boxplots show the distribution of the relative error\\n100 × [ErrT (ˆα) −minαErrT (α)]/[maxαErrT (α) −minαErrT (α)] over the four\\nscenarios of Figure 7.3. This is the error in using the chosen model relative to\\nthe best model. There are100 training sets each of size80 represented in each\\nboxplot, with the errors computed on test sets of size10,000.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e610883b-f7f4-439f-9a98-e2794f799dca', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 257, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.10 Cross-Validation 241\\nand worst possible model choices: minαErrT (α)a n dm a xαErrT (α). The\\nboxplots show the distribution of the quantity\\n100× ErrT (ˆα)−minαErrT (α)\\nmaxαErrT (α)−minαErrT (α),\\nwhich represents the error in using the chosen model relative to the best\\nmodel. For linear regression the model complexity was measured by the\\nnumber of features; as mentioned in Section 7.5, this underestimates the\\ndf, since it does notcharge for the search for the best model of that size.\\nThis was also used for the VC dimension of the linear classiﬁer. Fork-\\nnearest neighbors, we used the quantityN/k. Under an additive-error re-\\ngression model, this can be justiﬁed as the exact eﬀective degrees of free-\\ndom (Exercise 7.6); we do not know if it corresponds to the VC dimen-\\nsion. We useda1 = a2 = 1 for the constants in (7.46); the results for SRM\\nchangedwithdiﬀerentconstants,andthischoicegavethemostfavorablere-\\nsults. We repeated the SRM selection using the alternative practical bound\\n(7.47), and got almost identical results. For misclassiﬁcation error we used\\nˆσε\\n2 =[ N/(N −d)]·err(α) for the least restrictive model (k = 5 for KNN,\\nsince k = 1 results in zero training error). The AIC criterion seems to work\\nwell in all four scenarios, despite the lack of theoretical support with 0–1\\nloss. BIC does nearly as well, while the performance of SRM is mixed.\\n7.10 Cross-Validation\\nProbably the simplest and most widely used method for estimating predic-\\ntion error is cross-validation. This method directly estimates the expected\\nextra-sample error Err = E[L(Y, ˆf(X))], the average generalization error\\nwhen the methodˆf(X) is applied to an independent test sample from the\\njoint distribution ofX and Y. As mentioned earlier, we might hope that\\ncross-validation estimates the conditional error, with the training setT\\nheld ﬁxed. But as we will see in Section 7.12, cross-validation typically\\nestimates well only the expected prediction error.\\n7.10.1 K-Fold Cross-Validation\\nIdeally, if we had enough data, we would set aside a validation set and use\\nit to assess the performance of our prediction model. Since data are often\\nscarce, this is usually not possible. To ﬁnesse the problem,K-fold cross-\\nvalidation uses part of the available data to ﬁt the model, and a diﬀerent\\npart to test it. We split the data intoK roughly equal-sized parts; for\\nexample, whenK = 5, the scenario looks like this:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='018a60e4-7ec4-4ae5-9847-8449720bff65', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 258, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='242 7. Model Assessment and Selection\\nValidationTrain\\n12 3 4 5\\nTrain Train Train\\nFor thekth part (third above), we ﬁt the model to the otherK−1p a r t s\\nof the data, and calculate the prediction error of the ﬁtted model when\\npredicting the kth part of the data. We do this fork =1 ,2,...,K and\\ncombine theK estimates of prediction error.\\nHere are more details. Letκ: {1,...,N } ↦→{1,...,K } be an indexing\\nfunction that indicates the partition to which observationi is allocated by\\nthe randomization. Denote byˆf−k(x) the ﬁtted function, computed with\\nthe kth part of the data removed. Then the cross-validation estimate of\\nprediction error is\\nCV(ˆf)= 1\\nN\\nN∑\\ni=1\\nL(yi, ˆf−κ(i)(xi)). (7.48)\\nTypical choices ofK are 5 or 10 (see below). The caseK = N is known\\nas leave-one-out cross-validation. In this case κ(i)= i, and for the ith\\nobservation the ﬁt is computed using all the data except theith.\\nGiven a set of modelsf(x,α) indexed by a tuning parameterα,d e n o t e\\nby ˆf−k(x,α)t h eαth model ﬁt with thekth part of the data removed. Then\\nfor this set of models we deﬁne\\nCV(ˆf,α)= 1\\nN\\nN∑\\ni=1\\nL(yi, ˆf−κ(i)(xi,α)). (7.49)\\nThe function CV(ˆf,α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆαthat minimizes it. Our ﬁnal chosen model is\\nf(x, ˆα) ,w h i c hw et h e nﬁ tt oa l lt h ed a t a .\\nIt is interesting to wonder about what quantityK-fold cross-validation\\nestimates. With K = 5 or 10, we might guess that it estimates the ex-\\npected error Err, since the training sets in each fold are quite diﬀerent\\nfrom the original training set. On the other hand, ifK = N we might\\nguess that cross-validation estimates the conditional error ErrT . It turns\\nout that cross-validation only estimates eﬀectively the average error Err,\\nas discussed in Section 7.12.\\nWhat value should we choose forK?W i t hK = N, the cross-validation\\nestimator is approximately unbiased for the true (expected) prediction er-\\nror, but can have high variance because theN “training sets” are so similar\\nto one another. The computational burden is also considerable, requiring\\nN applications of the learning method. In certain special problems, this\\ncomputation can be done quickly—see Exercises 7.3 and 5.13.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='101dff2a-7998-4634-a6f5-af23f0ad0ec9', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 259, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.10 Cross-Validation 243\\nSize of Training Set\\n1-Err\\n0 50 100 150 200\\n0.0 0.2 0.4 0.6 0.8\\nFIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a\\nplot of 1 −Err versus the size of the training set N. With a dataset of 200\\nobservations, 5-fold cross-validation would use training sets of size160,w h i c h\\nwould behave much like the full set. However, with a dataset of50 observations\\nﬁvefold cross-validation would use training sets of size40, and this would result\\nin a considerable overestimate of prediction error.\\nOn the other hand, withK = 5 say, cross-validation has lower variance.\\nBut bias could be a problem, depending on how the performance of the\\nlearning method varies with the size of the training set. Figure 7.8 shows\\na hypothetical “learning curve” for a classiﬁer on a given task, a plot of\\n1 −Err versus the size of the training set N. The performance of the\\nclassiﬁer improves as the training set size increases to 100 observations;\\nincreasing the number further to 200 brings only a small beneﬁt. If our\\ntraining set had 200 observations, ﬁvefold cross-validation would estimate\\nthe performance of our classiﬁer over training sets of size 160, which from\\nFigure 7.8 is virtually the same as the performance for training set size\\n200. Thus cross-validation would not suﬀer from much bias. However if the\\ntraining set had 50 observations, ﬁvefold cross-validation would estimate\\nthe performance of our classiﬁer over training sets of size 40, and from the\\nﬁgure that would be an underestimate of 1−Err. Hence as an estimate of\\nErr, cross-validation would be biased upward.\\nTo summarize, if the learning curve has a considerable slope at the given\\ntraining set size, ﬁve- or tenfold cross-validation will overestimate the true\\nprediction error. Whether this bias is a drawback in practice depends on\\nthe objective. On the other hand, leave-one-out cross-validation has low\\nbias but can have high variance. Overall, ﬁve- or tenfold cross-validation\\nare recommended as a good compromise: see Breiman and Spector (1992)\\nand Kohavi (1995).\\nFigure 7.9 shows the prediction error and tenfold cross-validation curve\\nestimated from a single training set, from the scenario in the bottom right\\npanel of Figure 7.3. This is a two-class classiﬁcation problem, using a lin-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eb4e864b-a904-488a-a3c6-a1e12e6bb849', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 260, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='244 7. Model Assessment and Selection\\nSubset Size p\\nMisclassification Error\\n5 1 01 52 0\\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\\n\\x81 \\x81 \\x81\\n\\x81 \\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81 \\x81 \\x81 \\x81 \\x81\\x81\\x81 \\x81 \\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81 \\x81 \\x81\\x81\\x81 \\x81\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curve\\n(blue) estimated from a single training set, from the scenario in the bottom right\\npanel of Figure 7.3.\\near model with best subsets regression of subset sizep. Standard error bars\\nare shown, which are the standard errors of the individual misclassiﬁcation\\nerror rates for each of the ten parts. Both curves have minima atp = 10,\\nalthough the CV curve is rather ﬂat beyond 10. Often a “one-standard\\nerror” rule is used with cross-validation, in which we choose the most par-\\nsimonious model whose error is no more than one standard error above\\nthe error of the best model. Here it looks like a model with aboutp =9\\npredictors would be chosen, while the true model usesp = 10.\\nGeneralized cross-validationprovidesaconvenientapproximationtoleave-\\none out cross-validation, for linear ﬁtting under squared-error loss. As de-\\nﬁned in Section 7.6, a linear ﬁtting method is one for which we can write\\nˆy = Sy. (7.50)\\nNow for many linear ﬁtting methods,\\n1\\nN\\nN∑\\ni=1\\n[yi −ˆf−i(xi)]2 = 1\\nN\\nN∑\\ni=1\\n[yi −ˆf(xi)\\n1−Sii\\n]2\\n, (7.51)\\nwhere Sii is the ith diagonal element ofS (see Exercise 7.3). The GCV\\napproximation is\\nGCV(ˆf)= 1\\nN\\nN∑\\ni=1\\n[\\nyi −ˆf(xi)\\n1−trace(S)/N\\n]2\\n. (7.52)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b42c1e1-8177-42b1-be55-7c87368ee395', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 261, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.10 Cross-Validation 245\\nThe quantity trace(S) is the eﬀective number of parameters, as deﬁned in\\nSection 7.6.\\nGCV can have a computational advantage in some settings, where the\\ntrace ofS can be computed more easily than the individual elementsSii.\\nIn smoothing problems, GCV can also alleviate the tendency of cross-\\nvalidation to undersmooth. The similarity between GCV and AIC can be\\nseen from the approximation 1/(1−x)2 ≈1+2 x (Exercise 7.7).\\n7.10.2 The Wrong and Right Way to Do Cross-validation\\nConsider a classiﬁcation problem with a large number of predictors, as may\\narise, for example, in genomic or proteomic applications. A typical strategy\\nfor analysis might be as follows:\\n1. Screen the predictors: ﬁnd a subset of “good” predictors that show\\nfairly strong (univariate) correlation with the class labels\\n2. Using just this subset of predictors, build a multivariate classiﬁer.\\n3. Use cross-validation to estimate the unknown tuning parameters and\\nto estimate the prediction error of the ﬁnal model.\\nIs this a correct application of cross-validation? Consider a scenario with\\nN = 50 samples in two equal-sized classes, and p = 5000 quantitative\\npredictors (standard Gaussian) that are independent of the class labels.\\nThe true (test) error rate of any classiﬁer is 50%. We carried out the above\\nrecipe, choosing in step (1) the 100 predictors having highest correlation\\nwith the class labels, and then using a 1-nearest neighbor classiﬁer, based\\non just these 100 predictors, in step (2). Over 50 simulations from this\\nsetting, the average CV error rate was 3%. This is far lower than the true\\nerror rate of 50%.\\nWhat has happened? The problem is that the predictors have an unfair\\nadvantage, as they were chosen in step (1) on the basis ofall of the samples.\\nLeaving samples outafter the variables have been selected does not cor-\\nrectly mimic the application of the classiﬁer to a completely independent\\ntest set, since these predictors “have already seen” the left out samples.\\nFigure 7.10 (top panel) illustrates the problem. We selected the 100 pre-\\ndictors having largest correlation with the class labels over all 50 samples.\\nThen we chose a random set of 10 samples, as we would do in ﬁve-fold cross-\\nvalidation, and computed the correlations of the pre-selected 100 predictors\\nwith the class labels over just these 10 samples (top panel). We see that\\nthe correlations average about 0.28, rather than 0, as one might expect.\\nHere is the correct way to carry out cross-validation in this example:\\n1. Divide the samples intoK cross-validation folds (groups) at random.\\n2. For each foldk =1 ,2,...,K', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='66290c1f-48dc-4d18-87e4-b5729d4dd5ba', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 262, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='246 7. Model Assessment and Selection\\nCorrelations of Selected Predictors with Outcome\\nFrequency\\n−1.0 −0.5 0.0 0.5 1.0\\n0 1 02 03 0\\nWrong way\\nCorrelations of Selected Predictors with Outcome\\nFrequency\\n−1.0 −0.5 0.0 0.5 1.0\\n0 1 02 03 0\\nRight way\\nFIGURE 7.10.Cross-validation the wrong and right way: histograms shows the\\ncorrelation of class labels, in10 randomly chosen samples, with the100 predic-\\ntors chosen using the incorrect (upper red) and correct (lower green) versions of\\ncross-validation.\\n(a) Find a subset of “good” predictors that show fairly strong (uni-\\nvariate) correlation with the class labels, using all of the samples\\nexcept those in foldk.\\n(b) Using just this subset of predictors, build a multivariate classi-\\nﬁer, using all of the samples except those in foldk.\\n(c) Use the classiﬁer to predict the class labels for the samples in\\nfold k.\\nThe error estimates from step 2(c) are then accumulated over allK folds, to\\nproduce the cross-validation estimate of prediction error. The lower panel\\nof Figure 7.10 shows the correlations of class labels with the 100 predictors\\nchosen in step 2(a) of the correct procedure, over the samples in a typical\\nfold k. We see that they average about zero, as they should.\\nIn general, with a multistep modeling procedure, cross-validation must\\nbe applied to the entire sequence of modeling steps. In particular, samples\\nmust be “left out” before any selection or ﬁltering steps are applied. There\\nis one qualiﬁcation: initial unsupervised screening steps can be done be-\\nfore samples are left out. For example, we could select the 1000 predictors', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e1ccb11-0340-496e-a3b3-b55522d927de', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 263, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.10 Cross-Validation 247\\nwith highest variance across all 50 samples, before starting cross-validation.\\nSince this ﬁltering does not involve the class labels, it does not give the\\npredictors an unfair advantage.\\nWhile this point may seem obvious to the reader, we have seen this\\nblunder committed many times in published papers in top rank journals.\\nWith the large numbers of predictors that are so common in genomic and\\nother areas, the potential consequences of this error have also increased\\ndramatically; see Ambroise and McLachlan (2002) for a detailed discussion\\nof this issue.\\n7.10.3 Does Cross-Validation Really Work?\\nWeonceagainexaminethebehaviorofcross-validationinahigh-dimensional\\nclassiﬁcation problem. Consider a scenario withN =2 0s a m p l e si nt w o\\nequal-sized classes, andp = 500 quantitative predictors that are indepen-\\ndent of the class labels. Once again, the true error rate of any classiﬁer is\\n50%. Consider a simple univariate classiﬁer: a single split that minimizes\\nthe misclassiﬁcation error (a “stump”). Stumps are trees with a single split,\\nand are used in boosting methods (Chapter 10). A simple argument sug-\\ngests that cross-validation will not work properly in this setting2:\\nFitting to the entire training set, we will ﬁnd a predictor that\\nsplits the data very well. If we do 5-fold cross-validation, this\\nsame predictor should split any4/5ths and 1/5th of the data\\nwell too, and hence its cross-validation error will be small (much\\nless than 50%.) Thus CV does not give an accurate estimate of\\nerror.\\nTo investigate whether this argument is correct, Figure 7.11 shows the\\nresult of a simulation from this setting. There are 500 predictors and 20\\nsamples, in each of two equal-sized classes, with all predictors having a\\nstandard Gaussian distribution. The panel in the top left shows the number\\nof training errors for each of the 500 stumps ﬁt to the training data. We\\nhave marked in color the six predictors yielding the fewest errors. In the top\\nright panel, the training errors are shown for stumps ﬁt to a random 4/5ths\\npartition of the data (16 samples), and tested on the remaining 1/5th (four\\nsamples). The colored points indicate the same predictors marked in the\\ntop left panel. We see that the stump for the blue predictor (whose stump\\nwas the best in the top left panel), makes two out of four test errors (50%),\\nand is no better than random.\\nWhat has happened? The preceding argument has ignored the fact that\\nin cross-validation, the model must be completely retrained for each fold\\n2This argument was made to us by a scientist at a proteomics lab meeting, and led\\nto material in this section.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b79f9a3a-2f98-451a-94b2-38777ecbf9ef', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 264, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='248 7. Model Assessment and Selection\\n0 100 200 300 400 500\\n23456789\\nPredictor\\nError on Full Training Set\\n12345678\\n01234\\nError on 4/5\\nError on 1/5\\n−1 0 1 2\\nPredictor 436 (blue)\\nClass Label\\n01\\nfull\\n4/5\\n0.0 0.2 0.4 0.6 0.8 1.0\\nCV Errors\\nFIGURE 7.11. Simulation study to investigate the performance of cross vali-\\ndation in a high-dimensional problem where the predictors are independent of the\\nclass labels. The top-left panel shows the number of errors made by individual\\nstump classiﬁers on the full training set (20 observations). The top right panel\\nshows the errors made by individual stumps trained on a random split of the\\ndataset into 4/5ths (16 observations) and tested on the remaining1/5th (4 ob-\\nservations). The best performers are depicted by colored dots in each panel. The\\nbottom left panel shows the eﬀect of re-estimating the split point in each fold: the\\ncolored points correspond to the four samples in the1/5th validation set. The split\\npoint derived from the full dataset classiﬁes all four samples correctly, but when\\nthe split point is re-estimated on the4/5ths data (as it should be), it commits\\ntwo errors on the four validation samples. In the bottom right we see the overall\\nresult of ﬁve-fold cross-validation applied to50 simulated datasets. The average\\nerror rate is about50%, as it should be.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='618cd108-181b-459a-b106-4e1e65d16be6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 265, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.11 Bootstrap Methods 249\\nof the process. In the present example, this means that the best predictor\\nand corresponding split point are found from 4/5ths of the data. The eﬀect\\nof predictor choice is seen in the top right panel. Since the class labels are\\nindependent of the predictors, the performance of a stump on the 4/5ths\\ntraining data contains no information about its performance in the remain-\\ning 1/5th. The eﬀect of the choice of split point is shown in the bottom left\\npanel. Here we see the data for predictor 436, corresponding to the blue\\ndot in the top left plot. The colored points indicate the 1/5th data, while\\nthe remaining points belong to the 4/5ths. The optimal split points for this\\npredictor based on both the full training set and 4/5ths data are indicated.\\nThe split based on the full data makes no errors on the 1/5ths data. But\\ncross-validation must base its split on the 4/5ths data, and this incurs two\\nerrors out of four samples.\\nThe results of applying ﬁve-fold cross-validation to each of 50 simulated\\ndatasets is shown in the bottom right panel. As we would hope, the average\\ncross-validation error is around 50%, which is the true expected prediction\\nerror for this classiﬁer. Hence cross-validation has behaved as it should.\\nOn the other hand, there is considerable variability in the error, underscor-\\ning the importance of reporting the estimated standard error of the CV\\nestimate. See Exercise 7.10 for another variation of this problem.\\n7.11 Bootstrap Methods\\nThe bootstrap is a general tool for assessing statistical accuracy. First we\\ndescribe the bootstrap in general, and then show how it can be used to\\nestimate extra-sample prediction error. As with cross-validation, the boot-\\nstrap seeks to estimate the conditional error ErrT , but typically estimates\\nwell only the expected prediction error Err.\\nSuppose we have a model ﬁt to a set of training data. We denote the\\ntraining set byZ =( z1,z2,...,z N)w h e r ezi =( xi,yi). The basic idea is\\nto randomly draw datasets with replacement from the training data, each\\nsample the same size as the original training set. This is doneB times\\n(B = 100 say), producingB bootstrap datasets, as shown in Figure 7.12.\\nThen we reﬁt the model to each of the bootstrap datasets, and examine\\nthe behavior of the ﬁts over theB replications.\\nIn the ﬁgure, S(Z) is any quantity computed from the dataZ,f o re x -\\nample, the prediction at some input point. From the bootstrap sampling\\nwe can estimate any aspect of the distribution ofS(Z), for example, its\\nvariance,\\nˆVar[S(Z)] = 1\\nB−1\\nB∑\\nb=1\\n(S(Z∗b)−¯S∗)2, (7.53)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c34a876f-6311-4ea9-b63a-79c66608031c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 266, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='250 7. Model Assessment and Selection\\n  \\nBootstrap\\nBootstrap\\nreplications\\nsamples\\nsampleTrainingZ =( z1,z2,...,z N)\\nZ∗1 Z∗2 Z∗B\\nS(Z∗1) S(Z∗2) S(Z∗B)\\nFIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta-\\ntistical accuracy of a quantityS(Z) computed from our dataset.B training sets\\nZ∗b,b =1 ,...,B each of sizeN are drawn with replacement from the original\\ndataset. The quantity of interestS(Z) is computed from each bootstrap training\\nset, and the valuesS(Z∗1),...,S (Z∗B) are used to assess the statistical accuracy\\nof S(Z).\\nwhere ¯S∗ = ∑\\nb S(Z∗b)/B.N o t et h a tˆVar[S(Z)] can be thought of as a\\nMonte-Carlo estimate of the variance ofS(Z) under sampling from the\\nempirical distribution functionˆF for the data (z1,z2,...,z N).\\nHow can we apply the bootstrap to estimate prediction error? One ap-\\nproach would be to ﬁt the model in question on a set of bootstrap samples,\\nand then keep track of how well it predicts the original training set. If\\nˆf∗b(xi) is the predicted value atxi, from the model ﬁtted to thebth boot-\\nstrap dataset, our estimate is\\nˆErrboot = 1\\nB\\n1\\nN\\nB∑\\nb=1\\nN∑\\ni=1\\nL(yi, ˆf∗b(xi)). (7.54)\\nHowever, it is easy to see thatˆErrboot does not provide a good estimate in\\ngeneral. The reason is that the bootstrap datasets are acting as the training\\nsamples, while the original training set is acting as the test sample, and\\nthese two samples have observations in common. This overlap can make\\noverﬁt predictions look unrealistically good, and is the reason that cross-\\nvalidation explicitly uses non-overlapping data for the training and test\\nsamples. Consider for example a 1-nearest neighbor classiﬁer applied to a\\ntwo-class classiﬁcation problem with the same number of observations in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='88247600-bba7-48a0-9df7-9c9df913bd90', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 267, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.11 Bootstrap Methods 251\\neach class, in which the predictors and class labels are in fact independent.\\nThen the true error rate is 0.5. But the contributions to the bootstrap\\nestimateˆErrboot will be zero unless the observationi does not appear in the\\nbootstrap sampleb. In this latter case it will have the correct expectation\\n0.5. Now\\nPr{observation i∈bootstrap sampleb} =1 −\\n⎤\\n1−1\\nN\\n⎦N\\n≈ 1−e−1\\n=0 .632. (7.55)\\nHence the expectation ofˆErrboot is about 0.5× 0.368 = 0.184, far below\\nthe correct error rate 0.5.\\nBy mimicking cross-validation, a better bootstrap estimate can be ob-\\ntained. For each observation, we only keep track of predictions from boot-\\nstrap samples not containing that observation. The leave-one-out bootstrap\\nestimate of prediction error is deﬁned by\\nˆErr\\n(1)\\n= 1\\nN\\nN∑\\ni=1\\n1\\n|C−i|\\n∑\\nb∈C−i\\nL(yi, ˆf∗b(xi)). (7.56)\\nHere C−i is the set of indices of the bootstrap samplesb that donot contain\\nobservationi,and |C−i|isthenumberofsuchsamples.Incomputing ˆErr\\n(1)\\n,\\nwe either have to chooseB large enough to ensure that all of the|C−i| are\\ngreater than zero, or we can just leave out the terms in (7.56) corresponding\\nto |C−i|’s that are zero.\\nThe leave-one out bootstrap solves the overﬁtting problem suﬀered by\\nˆErrboot, but has the training-set-size bias mentioned in the discussion of\\ncross-validation. The average number of distinct observations in each boot-\\nstrap sample is about 0.632·N, so its bias will roughly behave like that of\\ntwofold cross-validation. Thus if the learning curve has considerable slope\\nat sample sizeN/2, the leave-one out bootstrap will be biased upward as\\nan estimate of the true error.\\nThe “.632 estimator” is designed to alleviate this bias. It is deﬁned by\\nˆErr\\n(.632)\\n= .368·err+ .632·ˆErr\\n(1)\\n. (7.57)\\nThe derivation of the .632 estimator is complex; intuitively it pulls the\\nleave-one out bootstrap estimate down toward the training error rate, and\\nhencereducesits upward bias. The useof the constant .632 relates to(7.55).\\nThe .632 estimator works well in “light ﬁtting” situations, but can break\\ndown in overﬁt ones. Here is an example due to Breiman et al. (1984).\\nSuppose we have two equal-size classes, with the targets independent of\\nthe class labels, and we apply a one-nearest neighbor rule. Thenerr = 0,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3dc3c121-5a98-4740-a4e9-3af22b22e732', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 268, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='252 7. Model Assessment and Selection\\nˆErr\\n(1)\\n=0 .5a n ds oˆErr\\n(.632)\\n= .632×0.5= .316. However, the true error\\nrate is 0.5.\\nOne can improve the .632 estimator by taking into account the amount\\nof overﬁtting. First we deﬁneγto be theno-information error rate:t h i s\\nis the error rate of our prediction rule if the inputs and class labels were\\nindependent. An estimate ofγis obtained by evaluating the prediction rule\\non all possible combinations of targetsyi and predictorsxi′\\nˆγ= 1\\nN2\\nN∑\\ni=1\\nN∑\\ni′=1\\nL(yi, ˆf(xi′)). (7.58)\\nFor example, consider the dichotomous classiﬁcation problem: let ˆp1 be\\nthe observed proportion of responsesyi equaling 1, and let ˆq1 be the ob-\\nserved proportion of predictionsˆf(xi′) equaling 1. Then\\nˆγ=ˆp1(1−ˆq1)+(1 −ˆp1)ˆq1. (7.59)\\nWith a rule like 1-nearest neighbors for which ˆq1 =ˆp1 the value of ˆγis\\n2ˆp1(1−ˆp1). The multi-category generalization of (7.59) is ˆγ=∑\\nℓ ˆpℓ(1−ˆqℓ).\\nUsing this, therelative overﬁtting rateis deﬁned to be\\nˆR =\\nˆErr\\n(1)\\n−err\\nˆγ−err , (7.60)\\na quantity that ranges from 0 if there is no overﬁtting (ˆErr\\n(1)\\n= err) to 1\\nif the overﬁtting equals the no-information value ˆγ−err. Finally, we deﬁne\\nthe “.632+” estimator by\\nˆErr\\n(.632+)\\n=( 1 −ˆw)·err+ ˆw·ˆErr\\n(1)\\n(7.61)\\nwith ˆw = .632\\n1−.368ˆR\\n.\\nThe weight w ranges from .632 if ˆR =0t o1i f ˆR =1 ,s o ˆErr\\n(.632+)\\nranges fromˆErr\\n(.632)\\ntoˆErr\\n(1)\\n. Again, the derivation of (7.61) is compli-\\ncated: roughly speaking, it produces a compromise between the leave-one-\\nout bootstrap and the training error rate that depends on the amount of\\noverﬁtting. For the 1-nearest-neighbor problem with class labels indepen-\\ndent of the inputs, ˆw = ˆR =1 ,s oˆErr\\n(.632+)\\n=ˆErr\\n(1)\\n, which has the correct\\nexpectation of 0.5. In other problems with less overﬁtting,ˆErr\\n(.632+)\\nwill\\nlie somewhere betweenerr andˆErr\\n(1)\\n.\\n7.11.1 Example (Continued)\\nFigure7.13showstheresultsoftenfoldcross-validationandthe.632+boot-\\nstrap estimate in the same four problems of Figures 7.7. As in that ﬁgure,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e725f1f9-9e0b-44d9-b071-98fddda0d91b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 269, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.11 Bootstrap Methods 253\\nreg/KNN reg/linear class/KNN class/linear\\n0 2 04 06 08 0 1 0 0\\n% Increase Over Best\\nCross−validation\\nreg/KNN reg/linear class/KNN class/linear\\n0 2 04 06 08 0 1 0 0\\n% Increase Over Best\\nBootstrap\\nFIGURE 7.13. Boxplots show the distribution of the relative error\\n100 · [Errˆα −minαErr(α)]/[maxαErr(α) −minαErr(α)] over the four scenar-\\nios of Figure 7.3. This is the error in using the chosen model relative to the best\\nmodel. There are100 training sets represented in each boxplot.\\nFigure 7.13 shows boxplots of 100· [Errˆα−minαErr(α)]/[maxαErr(α)−\\nminαErr(α)],theerrorinusingthechosenmodelrelativetothebestmodel.\\nThere are 100 diﬀerent training sets represented in each boxplot. Both mea-\\nsures perform well overall, perhaps the same or slightly worse than the AIC\\nin Figure 7.7.\\nOur conclusion is that for these particular problems and ﬁtting methods,\\nminimization of either AIC, cross-validation or bootstrap yields a model\\nfairly close to the best available. Note that for the purpose of model selec-\\ntion, any of the measures could be biased and it wouldn’t aﬀect things, as\\nlong as the bias did not change the relative performance of the methods.\\nFor example, the addition of a constant to any of the measures would not\\nchange the resulting chosen model. However, for many adaptive, nonlinear\\ntechniques (like trees), estimation of the eﬀective number of parameters is\\nvery diﬃcult. This makes methods like AIC impractical and leaves us with\\ncross-validation or bootstrap as the methods of choice.\\nA diﬀerent question is: how well does each method estimate test error?\\nOn the average the AIC criterion overestimated prediction error of its cho-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='261e481c-482f-4489-899b-b75f3d92d74d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 270, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='254 7. Model Assessment and Selection\\nsenmodelby38%,37%,51%,and30%,respectively,overthefourscenarios,\\nwith BIC performing similarly. In contrast, cross-validation overestimated\\nthe error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\\nsame. Hence the extra work involved in computing a cross-validation or\\nbootstrap measure is worthwhile, if an accurate estimate of test error is\\nrequired. With other ﬁtting methods like trees, cross-validation and boot-\\nstrap can underestimate the true error by 10%, because the search for best\\ntree is strongly aﬀected by the validation set. In these situations only a\\nseparate test set will provide an unbiased estimate of test error.\\n7.12 Conditional or Expected Test Error?\\nFigures 7.14 and 7.15 examine the question of whether cross-validation does\\na good job in estimating ErrT , the error conditional on a given training set\\nT (expression (7.15) on page 228), as opposed to the expected test error.\\nFor each of 100 training sets generated from the “reg/linear” setting in\\nthe top-right panel of Figure 7.3, Figure 7.14 shows the conditional error\\ncurves ErrT as a function of subset size (top left). The next two panels show\\n10-fold andN-fold cross-validation, the latter also known as leave-one-out\\n(LOO). The thick red curve in each plot is the expected error Err, while\\nthe thick black curves are the expected cross-validation curves. The lower\\nright panel shows how well cross-validation approximates the conditional\\nand expected error.\\nOne might have expectedN-fold CV to approximate ErrT well, since it\\nalmost u s e st h ef u l lt r a i n i n gs a m p l et oﬁ tan e wt e s tp o i n t .1 0 - f o l dC V ,o n\\nthe other hand, might be expected to estimate Err well, since it averages\\nover somewhat diﬀerent training sets. From the ﬁgure it appears 10-fold\\ndoes a better job thanN-fold in estimating ErrT , and estimates Err even\\nbetter. Indeed, the similarity of the two black curves with the red curve\\nsuggests both CV curves are approximately unbiased for Err, with 10-fold\\nhaving less variance. Similar trends were reported by Efron (1983).\\nFigure7.15showsscatterplots of both10-fold and N-fold cross-validation\\nerror estimates versus the true conditional error for the 100 simulations.\\nAlthough the scatterplots do not indicate much correlation, the lower right\\npanel shows that for the most part the correlations are negative, a curi-\\nous phenomenon that has been observed before. This negative correlation\\nexplains why neither form of CV estimates ErrT well. The broken lines in\\neach plot are drawn at Err(p), the expected error for the best subset of\\nsize p. We see again that both forms of CV are approximately unbiased for\\nexpected error, but the variation in test error for diﬀerent training sets is\\nquite substantial.\\nAmong the four experimental conditions in 7.3, this “reg/linear” scenario\\nshowedthehighestcorrelationbetweenactualandpredictedtesterror.This', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d16f286-a671-49de-aa41-e5103b8ab4a3', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 271, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.12 Conditional or Expected Test Error? 255\\n51 0 1 5 2 0\\n0.1 0.2 0.3 0.4\\nPrediction Error\\nSubset Size p\\nError\\n51 0 1 5 2 0\\n0.1 0.2 0.3 0.4\\n10−Fold CV Error\\nSubset Size p\\nError\\n51 0 1 5 2 0\\n0.1 0.2 0.3 0.4\\nLeave−One−Out CV Error\\nSubset Size p\\nError\\n51 0 1 5 2 0\\n0.015 0.025 0.035 0.045\\nApproximation Error\\nSubset Size p\\nMean Absolute Deviation\\nET |CV10−Err|\\nET |CV10−ErrT |\\nET |CVN −ErrT |\\nFIGURE 7.14.Conditional prediction-errorErrT , 10-fold cross-validation, and\\nleave-one-out cross-validation curves for a 100 simulations from the top-right\\npanel in Figure 7.3. The thick red curve is the expected prediction errorErr,\\nwhile the thick black curves are the expected CV curvesET CV10 and ET CVN.\\nThe lower-right panel shows the mean absolute deviation of the CV curves from\\nthe conditional error,ET |CVK −ErrT | for K =1 0(blue) and K = N (green),\\nas well as from the expected errorET |CV10 −Err| (orange).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e4146e32-2668-4561-99a1-7dd350548f48', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 272, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='256 7. Model Assessment and Selection\\n0.10 0.15 0.20 0.25 0.30 0.35 0.40\\n0.10 0.20 0.30 0.40\\nSubset Size 1\\nPrediction Error\\nCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.40\\n0.10 0.20 0.30 0.40\\nSubset Size 5\\nPrediction Error\\nCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.40\\n0.10 0.20 0.30 0.40\\nSubset Size 10\\nPrediction Error\\nCV Error\\n51 0 1 5 2 0\\n−0.6 −0.4 −0.2 0.0 0.2\\nSubset Size\\nCorrelation\\nLeave−one−out\\n10−Fold\\nFIGURE 7.15. Plots of the CV estimates of error versus the true conditional\\nerror for each of the100 training sets, for the simulation setup in the top right\\npanel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent\\ncolors. The ﬁrst three panels correspond to diﬀerent subset sizesp, and vertical\\nand horizontal lines are drawn atErr(p). Although there appears to be little cor-\\nrelation in these plots, we see in the lower right panel that for the most part the\\ncorrelation isnegative.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7b874a55-a7f0-4a6f-aa1a-5fc692231aec', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 273, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 257\\nphenomenon also occurs for bootstrap estimates of error, and we would\\nguess, for any other estimate of conditional prediction error.\\nWe conclude that estimation of test error for a particular training set is\\nnot easy in general, given just the data from that same training set. Instead,\\ncross-validation and related methods may provide reasonable estimates of\\nthe expectederror Err.\\nBibliographic Notes\\nKey references for cross-validation are Stone (1974), Stone (1977) and\\nAllen (1974). The AIC was proposed by Akaike (1973), while the BIC\\nwas introduced by Schwarz (1978). Madigan and Raftery (1994) give an\\noverview of Bayesian model selection. The MDL criterion is due to Rissa-\\nnen (1983). Cover and Thomas (1991) contains a good description of coding\\ntheory and complexity. VC dimension is described in Vapnik (1996). Stone\\n(1977) showed that the AIC and leave-one out cross-validation are asymp-\\ntotically equivalent. Generalized cross-validation is described by Golub et\\nal. (1979) and Wahba (1980); a further discussion of the topic may be found\\nin the monograph by Wahba (1990). See also Hastie and Tibshirani (1990),\\nChapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\\nrani (1993) for an overview. Efron (1983) proposes a number of bootstrap\\nestimates of prediction error, including the optimism and .632 estimates.\\nEfron (1986) compares CV, GCV and bootstrap estimates of error rates.\\nThe use of cross-validation and the bootstrap for model selection is stud-\\nied by Breiman and Spector (1992), Breiman (1992), Shao (1996), Zhang\\n(1993) and Kohavi (1995). The.632+ estimator was proposed by Efron\\nand Tibshirani (1997).\\nCherkassky and Ma (2003) published a study on the performance of\\nSRM for model selection in regression, in response to our study of section\\n7.9.1. They complained that we had been unfair to SRM because had not\\napplied it properly. Our response can be found in the same issue of the\\njournal (Hastie et al. (2003)).\\nExercises\\nEx. 7.1Derive the estimate of in-sample error (7.24).\\nEx. 7.2For 0–1 loss withY ∈{0,1} and Pr(Y =1|x0)= f(x0), show that\\nErr(x0)=P r ( Y ̸= ˆG(x0)|X = x0)\\n=E r rB(x0)+ |2f(x0)−1|Pr(ˆG(x0)̸= G(x0)|X = x0),\\n(7.62)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f22165d5-2bb9-44aa-9464-6e2574e9c616', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 274, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='258 7. Model Assessment and Selection\\nwhere ˆG(x)= I(ˆf(x) > 1\\n2), G(x)= I(f(x) > 1\\n2) is the Bayes classiﬁer,\\nand ErrB(x0)=P r (Y ̸= G(x0)|X = x0), the irreducibleBayes errorat x0.\\nUsing the approximationˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that\\nPr(ˆG(x0)̸= G(x0)|X = x0)≈Φ\\n⎤\\nsign(1\\n2 −f(x0))(Eˆf(x0)−1\\n2)√\\nVar(ˆf(x0))\\n⎦\\n. (7.63)\\nIn the above,\\nΦ(t)= 1√\\n2π\\n∫ t\\n−∞\\nexp(−t2/2)dt,\\nthe cumulative Gaussian distribution function. This is an increasing func-\\ntion, with value 0 att =−∞and value 1 att =+∞.\\nWe can think of sign(1\\n2 −f(x0))(Eˆf(x0) −1\\n2) as a kind of boundary-\\nbias term, as it depends on the truef(x0) only through which side of the\\nboundary (1\\n2) that it lies. Notice also that the bias and variance combine\\nin a multiplicative rather than additive fashion. If Eˆf(x0)i so nt h es a m e\\nside of 1\\n2 as f(x0), then the bias is negative, and decreasing the variance\\nwill decrease the misclassiﬁcation error. On the other hand, if Eˆf(x0)i s\\non the opposite side of1\\n2 to f(x0), then the bias is positive and it pays to\\nincrease the variance! Such an increase will improve the chance thatˆf(x0)\\nfalls on the correct side of1\\n2 (Friedman, 1997).\\nEx. 7.3Let ˆf = Sy be a linear smoothing ofy.\\n(a) IfSii is theith diagonal element ofS, show that forSarising from least\\nsquares projections and cubic smoothing splines, the cross-validated\\nresidual can be written as\\nyi −ˆf−i(xi)= yi −ˆf(xi)\\n1−Sii\\n. (7.64)\\n(b) Use this result to show that|yi −ˆf−i(xi)|≥|yi −ˆf(xi)|.\\n(c) Find general conditions on any smootherS to make result (7.64) hold.\\nEx. 7.4 Consider the in-sample prediction error (7.18) and the training\\nerror err in the case of squared-error loss:\\nErrin = 1\\nN\\nN∑\\ni=1\\nEY0(Y0\\ni −ˆf(xi))2\\nerr = 1\\nN\\nN∑\\ni=1\\n(yi −ˆf(xi))2.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='112ccf61-18f2-4f00-892a-bfa1ebf039ad', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 275, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Exercises 259\\nAdd and subtractf(xi)a n dEˆf(xi) in each expression and expand. Hence\\nestablish that the average optimism in the training error is\\n2\\nN\\nN∑\\ni=1\\nCov(ˆyi,yi),\\nas given in (7.21).\\nEx. 7.5For a linear smootherˆy = Sy, show that\\nN∑\\ni=1\\nCov(ˆyi,yi)=t r a c e (S)σ2\\nε, (7.65)\\nwhich justiﬁes its use as the eﬀective number of parameters.\\nEx. 7.6 Show that for an additive-error model, the eﬀective degrees-of-\\nfreedom for thek-nearest-neighbors regression ﬁt isN/k.\\nEx. 7.7Use the approximation 1/(1−x)2 ≈1+2xto expose the relationship\\nbetween Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the\\nmodel used to estimate the noise varianceσ2\\nε.\\nEx. 7.8 Show that the set of functions{I(sin(αx) > 0)} can shatter the\\nfollowing points on the line:\\nz1 =1 0−1,...,z ℓ =1 0−ℓ, (7.66)\\nfor anyℓ. Hence the VC dimension of the class{I(sin(αx) > 0)} is inﬁnite.\\nEx. 7.9For the prostate data of Chapter 3, carry out a best-subset linear\\nregression analysis, as in Table 3.3 (third column from left). Compute the\\nAIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimates\\nof prediction error. Discuss the results.\\nEx. 7.10Referring to the example in Section 7.10.3, suppose instead that\\nall of thep predictors are binary, and hence there is no need to estimate\\nsplit points. The predictors are independent of the class labels as before.\\nThen if p is very large, we can probably ﬁnd a predictor that splits the\\nentire training data perfectly, and hence would split the validation data\\n(one-ﬁfth of data) perfectly as well. This predictor would therefore have\\nzero cross-validation error. Does this mean that cross-validation does not\\nprovide a good estimate of test error in this situation? [This question was\\nsuggested by Li Ma.]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='753736f6-628f-483b-9384-218eea87afde', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 276, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8\\nModel Inference and Averaging\\n8.1 Introduction\\nFor most of this book, the ﬁtting (learning) of models has been achieved by\\nminimizing a sum of squares for regression, or by minimizing cross-entropy\\nfor classiﬁcation. In fact, both of these minimizations are instances of the\\nmaximum likelihood approach to ﬁtting.\\nIn this chapter we provide a general exposition of the maximum likeli-\\nhood approach, as well as the Bayesian method for inference. The boot-\\nstrap, introduced in Chapter 7, is discussed in this context, and its relation\\nto maximum likelihood and Bayes is described. Finally, we present some\\nrelated techniques for model averaging and improvement, including com-\\nmittee methods, bagging, stacking and bumping.\\n8.2 The Bootstrap and Maximum Likelihood\\nMethods\\n8.2.1 A Smoothing Example\\nThe bootstrap method provides a direct computational way of assessing\\nuncertainty, by sampling from the training data. Here we illustrate the\\nbootstrap in a simple one-dimensional smoothing problem, and show its\\nconnection to maximum likelihood.\\n © Springer Science+Business Media, LLC 2009 \\nT. Hastie et al., The Elements of Statistical Learning, Second Edition, 261\\nDOI: 10.1007/b94608_8,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='730c064c-4054-4010-b42e-c179ebb729ea', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 277, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='262 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\nx\\ny\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx\\nB-spline Basis\\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\\nseven B-spline basis functions. The broken vertical lines indicate the placement\\nof the three knots.\\nDenote the training data by Z = {z1,z2,...,z N},w i t hzi =( xi,yi),\\ni =1 ,2,...,N .H e r exi is a one-dimensional input, andyi the outcome,\\neither continuous or categorical. As an example, consider theN =5 0d a t a\\npoints shown in the left panel of Figure 8.1.\\nSuppose we decide to ﬁt a cubic spline to the data, with three knots\\nplaced at the quartiles of theX values. This is a seven-dimensional lin-\\near space of functions, and can be represented, for example, by a linear\\nexpansion ofB-spline basis functions (see Section 5.9.2):\\nμ(x)=\\n7∑\\nj=1\\nβjhj(x). (8.1)\\nHere the hj(x), j =1 ,2,..., 7 are the seven functions shown in the right\\npanel of Figure 8.1. We can think ofμ(x) as representing the conditional\\nmean E(Y|X = x).\\nLet H be theN×7m a t r i xw i t hijth elementhj(xi). The usual estimate\\nof β, obtained by minimizing the squared error over the training set, is\\ngiven by\\nˆβ=( HTH)−1HTy. (8.2)\\nThe corresponding ﬁt ˆμ(x)= ∑7\\nj=1\\nˆβjhj(x) is shown in the top left panel\\nof Figure 8.2.\\nThe estimated covariance matrix ofˆβis\\nˆVar(ˆβ)=( HTH)−1ˆσ2, (8.3)\\nwhere we have estimated the noise variance by ˆσ2 =∑N\\ni=1(yi −ˆμ(xi))2/N.\\nLetting h(x)T =( h1(x),h2(x),...,h 7(x)), the standard error of a predic-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1bcba60d-8d32-4c3c-85b4-286e16b027ed', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 278, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2 The Bootstrap and Maximum Likelihood Methods 263\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\nx\\ny\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\nx\\ny\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\nx\\ny\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\nx\\ny\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\nFIGURE 8.2.(Top left:)B-spline smooth of data. (Top right:)B-spline smooth\\nplus and minus 1.96× standard error bands. (Bottom left:) Ten bootstrap repli-\\ncates of theB-spline smooth. (Bottom right:)B-spline smooth with95% standard\\nerror bands computed from the bootstrap distribution.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c4e87db7-eb64-4fa1-bcb7-2ea2712d0ca1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 279, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='264 8. Model Inference and Averaging\\ntion ˆμ(x)= h(x)T ˆβis\\nˆse[ˆμ(x)] = [h(x)T(HTH)−1h(x)]\\n1\\n2 ˆσ. (8.4)\\nIn the top right panel of Figure 8.2 we have plotted ˆμ(x)±1.96·ˆse[ˆμ(x)].\\nSince 1.96 is the 97.5% point of the standard normal distribution, these\\nrepresent approximate 100−2×2.5% = 95% pointwise conﬁdence bands\\nfor μ(x).\\nHere is how we could apply the bootstrap in this example. We drawB\\ndatasets each of sizeN = 50 with replacement from our training data, the\\nsampling unit being the pairzi =( xi,yi). To each bootstrap datasetZ∗\\nwe ﬁt a cubic spline ˆμ∗(x); the ﬁts from ten such samples are shown in the\\nbottom left panel of Figure 8.2. UsingB = 200 bootstrap samples, we can\\nform a 95% pointwise conﬁdence band from the percentiles at eachx:w e\\nﬁnd the 2.5%×200 = ﬁfth largest and smallest values at eachx.T h e s ea r e\\nplotted in the bottom right panel of Figure 8.2. The bands look similar to\\nthose in the top right, being a little wider at the endpoints.\\nThere is actually a close connection between the least squares estimates\\n(8.2)and(8.3),thebootstrap,andmaximumlikelihood.Supposewefurther\\nassume that the model errors are Gaussian,\\nY = μ(X)+ ε; ε∼N(0,σ2),\\nμ(x)=\\n7∑\\nj=1\\nβjhj(x). (8.5)\\nThe bootstrap method described above, in which we sample with re-\\nplacement from the training data, is called thenonparametric bootstrap.\\nThis really means that the method is “model-free,” since it uses the raw\\ndata, not a speciﬁc parametric model, to generate new datasets. Consider\\na variation of the bootstrap, called theparametric bootstrap,i nw h i c hw e\\nsimulate new responses by adding Gaussian noise to the predicted values:\\ny∗\\ni =ˆμ(xi)+ ε∗\\ni ; ε∗\\ni ∼N(0,ˆσ2); i =1 ,2,...,N. (8.6)\\nThis process is repeatedB times, whereB = 200 say. The resulting boot-\\nstrap datasets have the form (x1,y∗\\n1),..., (xN,y∗\\nN) and we recompute the\\nB-spline smooth on each. The conﬁdence bands from this method will ex-\\nactly equal the least squares bands in the top right panel, as the number of\\nbootstrap samples goes to inﬁnity. A function estimated from a bootstrap\\nsample y∗is given by ˆμ∗(x)= h(x)T(HTH)−1HTy∗, and has distribution\\nˆμ∗(x)∼N(ˆμ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)\\nNotice that the mean of this distribution is the least squares estimate, and\\nthe standard deviation is the same as the approximate formula (8.4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='34558b6f-190a-4f35-bd3f-72c4666d6339', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 280, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2 The Bootstrap and Maximum Likelihood Methods 265\\n8.2.2 Maximum Likelihood Inference\\nIt turns out that the parametric bootstrap agrees with least squares in the\\nprevious example because the model (8.5) has additive Gaussian errors. In\\ngeneral, the parametric bootstrap agrees not with least squares but with\\nmaximum likelihood, which we now review.\\nWe begin by specifying a probability density or probability mass function\\nfor our observations\\nzi ∼gθ(z). (8.8)\\nIn this expressionθrepresents one or more unknown parameters that gov-\\nern the distribution ofZ.T h i si sc a l l e daparametric modelfor Z.A sa n\\nexample, ifZ has a normal distribution with meanμand varianceσ2,t h e n\\nθ=( μ,σ2), (8.9)\\nand\\ngθ(z)= 1√\\n2πσe−1\\n2(z−μ)2/σ2\\n. (8.10)\\nMaximum likelihood is based on thelikelihood function,g i v e nb y\\nL(θ;Z)=\\nN∏\\ni=1\\ngθ(zi), (8.11)\\nthe probability of the observed data under the modelgθ. The likelihood is\\ndeﬁned only up to a positive multiplier, which we have taken to be one.\\nWe think ofL(θ;Z) as a function ofθ, with our dataZ ﬁxed.\\nDenote the logarithm ofL(θ;Z)b y\\nℓ(θ;Z)=\\nN∑\\ni=1\\nℓ(θ;zi)\\n=\\nN∑\\ni=1\\nloggθ(zi), (8.12)\\nwhich we will sometimes abbreviate asℓ(θ). This expression is called the\\nlog-likelihood, and each valueℓ(θ;zi)=l o ggθ(zi) is called a log-likelihood\\ncomponent. The method of maximum likelihood chooses the valueθ= ˆθ\\nto maximizeℓ(θ;Z).\\nThe likelihood function can be used to assess the precision ofˆθ. We need\\na few more deﬁnitions. Thescore functionis deﬁned by\\n˙ℓ(θ;Z)=\\nN∑\\ni=1\\n˙ℓ(θ;zi), (8.13)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec4866c5-6bee-420a-8382-1f10ec8b9f7f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 281, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='266 8. Model Inference and Averaging\\nwhere ˙ℓ(θ;zi)= ∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-\\nmum in the interior of the parameter space,˙ℓ(ˆθ;Z)=0 .T h einformation\\nmatrix is\\nI(θ)= −\\nN∑\\ni=1\\n∂2ℓ(θ;zi)\\n∂θ∂θT . (8.14)\\nWhen I(θ) is evaluated atθ= ˆθ, it is often called theobserved information.\\nThe Fisher information(or expected information) is\\ni(θ)=E θ[I(θ)]. (8.15)\\nFinally, letθ0 denote the true value ofθ.\\nA standard result says that the sampling distribution of the maximum\\nlikelihood estimator has a limiting normal distribution\\nˆθ→N(θ0,i(θ0)−1), (8.16)\\nas N →∞. Here we are independently sampling fromgθ0(z). This suggests\\nthat the sampling distribution ofˆθmay be approximated by\\nN(ˆθ,i(ˆθ)−1)o rN(ˆθ,I(ˆθ)−1), (8.17)\\nwhere ˆθrepresents the maximum likelihood estimate from the observed\\ndata.\\nThe corresponding estimates for the standard errors ofˆθj are obtained\\nfrom\\n√\\ni(ˆθ)−1\\njj and\\n√\\nI(ˆθ)−1\\njj . (8.18)\\nConﬁdence points forθj can be constructed from either approximation\\nin (8.17). Such a conﬁdence point has the form\\nˆθj −z(1−α) ·\\n√\\ni(ˆθ)−1\\njj or ˆθj −z(1−α) ·\\n√\\nI(ˆθ)−1\\njj ,\\nrespectively, wherez(1−α) is the 1−αpercentile of the standard normal\\ndistribution. More accurate conﬁdence intervals can be derived from the\\nlikelihood function, by using the chi-squared approximation\\n2[ℓ(ˆθ)−ℓ(θ0)]∼χ2\\np, (8.19)\\nwhere p is the number of components inθ. The resulting 1−2αconﬁ-\\ndence interval is the set of all θ0 such that 2[ℓ(ˆθ) −ℓ(θ0)] ≤χ2\\np\\n(1−2α)\\n,\\nwhere χ2\\np\\n(1−2α)\\nis the 1−2αpercentile of the chi-squared distribution with\\np degrees of freedom.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ddd8b1c-6ca3-4f5e-81ae-b20ff24fab27', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 282, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Bayesian Methods 267\\nLet’s return to our smoothing example to see what maximum likelihood\\nyields. The parameters areθ=( β,σ2). The log-likelihood is\\nℓ(θ)= −N\\n2 logσ22π− 1\\n2σ2\\nN∑\\ni=1\\n(yi −h(xi)Tβ)2. (8.20)\\nThe maximum likelihood estimate is obtained by setting∂ℓ/∂β=0a n d\\n∂ℓ/∂σ2 = 0, giving\\nˆβ=( HTH)−1HTy,\\nˆσ2 = 1\\nN\\n∑\\n(yi −ˆμ(xi))2,\\n(8.21)\\nwhich are the same as the usual estimates given in (8.2) and below (8.3).\\nThe information matrix forθ=( β,σ2) is block-diagonal, and the block\\ncorresponding toβis\\nI(β)=( HTH)/σ2, (8.22)\\nso that the estimated variance (HTH)−1ˆσ2 agrees with the least squares\\nestimate (8.3).\\n8.2.3 Bootstrap versus Maximum Likelihood\\nIn essence the bootstrap is a computer implementation of nonparametric or\\nparametric maximum likelihood. The advantage of the bootstrap over the\\nmaximum likelihood formula is that it allows us to compute maximum like-\\nlihood estimates of standard errors and other quantities in settings where\\nno formulas are available.\\nIn our example, suppose that we adaptively choose by cross-validation\\nthe number and position of the knots that deﬁne the B-splines, rather\\nthan ﬁx them in advance. Denote byλthe collection of knots and their\\npositions. Then the standard errors and conﬁdence bands should account\\nfor the adaptive choice ofλ, but there is no way to do this analytically.\\nWith the bootstrap, we compute theB-spline smooth with an adaptive\\nchoice of knots for each bootstrap sample. The percentiles of the resulting\\ncurves capture the variability from both the noise in the targets as well as\\nthat from ˆλ. In this particular example the conﬁdence bands (not shown)\\ndon’t look much diﬀerent than the ﬁxedλbands. But in other problems,\\nwhere more adaptation is used, this can be an important eﬀect to capture.\\n8.3 Bayesian Methods\\nIn the Bayesian approach to inference, we specify a sampling model Pr(Z|θ)\\n(density or probability mass function) for our data given the parameters,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7067322d-67d1-402d-bf26-c6cd1b001281', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 283, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='268 8. Model Inference and Averaging\\nand a prior distribution for the parameters Pr(θ) reﬂecting our knowledge\\nabout θbefore we see the data. We then compute the posterior distribution\\nPr(θ|Z)= Pr(Z|θ)·Pr(θ)∫\\nPr(Z|θ)·Pr(θ)dθ, (8.23)\\nwhich represents our updated knowledge aboutθafter we see the data. To\\nunderstand this posterior distribution, one might draw samples from it or\\nsummarize by computing its mean or mode. The Bayesian approach diﬀers\\nfrom the standard (“frequentist”) method for inference in its use of a prior\\ndistribution to express the uncertainty present before seeing the data, and\\nto allow the uncertainty remaining after seeing the data to be expressed in\\nthe form of a posterior distribution.\\nTheposteriordistributionalsoprovidesthebasisforpredictingthevalues\\nof a future observationznew,v i at h epredictive distribution:\\nPr(znew|Z)=\\n∫\\nPr(znew|θ)·Pr(θ|Z)dθ. (8.24)\\nIn contrast, the maximum likelihood approach would use Pr(znew|ˆθ),\\nthe data density evaluated at the maximum likelihood estimate, to predict\\nfuture data. Unlike the predictive distribution (8.24), this does not account\\nfor the uncertainty in estimatingθ.\\nLet’s walk through the Bayesian approach in our smoothing example.\\nWe start with the parametric model given by equation (8.5), and assume\\nfor the moment thatσ2 is known. We assume that the observed feature\\nvalues x1,x2,...,x N are ﬁxed, so that the randomness in the data comes\\nsolely fromy varying around its meanμ(x).\\nThe second ingredient we need is a prior distribution. Distributions on\\nfunctions are fairly complex entities: one approach is to use a Gaussian\\nprocess prior in which we specify the prior covariance between any two\\nfunction valuesμ(x)a n dμ(x′) (Wahba, 1990; Neal, 1996).\\nHere we take a simpler route: by considering a ﬁniteB-spline basis for\\nμ(x),wecaninsteadprovideapriorforthecoeﬃcients β,andthisimplicitly\\ndeﬁnes a prior forμ(x). We choose a Gaussian prior centered at zero\\nβ∼N(0,τΣ) (8.25)\\nwith the choices of the prior correlation matrixΣ and variance τto be\\ndiscussed below. The implicit process prior for μ(x) is hence Gaussian,\\nwith covariance kernel\\nK(x,x′)=c o v [ μ(x),μ(x′)]\\n= τ·h(x)TΣh(x′). (8.26)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='76187860-cf9a-4dfa-86a2-a0dc403a6f53', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 284, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.3 Bayesian Methods 269\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 3 - 2 - 1 0123\\nμ(x)\\nx\\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\\nbution for the functionμ(x).\\nThe posterior distribution forβis also Gaussian, with mean and covariance\\nE(β|Z)=\\n⎤\\nHTH+ σ2\\nτΣ−1\\n⎦−1\\nHTy,\\ncov(β|Z)=\\n⎤\\nHTH+ σ2\\nτΣ−1\\n⎦−1\\nσ2,\\n(8.27)\\nwith the corresponding posterior values forμ(x),\\nE(μ(x)|Z)= h(x)T\\n⎤\\nHTH+ σ2\\nτΣ−1\\n⎦−1\\nHTy,\\ncov[μ(x),μ(x′)|Z]= h(x)T\\n⎤\\nHTH+ σ2\\nτΣ−1\\n⎦−1\\nh(x′)σ2.\\n(8.28)\\nHow do we choose the prior correlation matrixΣ? In some settings the\\nprior can be chosen from subject matter knowledge about the parameters.\\nHere we are willing to say the functionμ(x) should be smooth, and have\\nguaranteed this by expressingμin a smooth low-dimensional basis ofB-\\nsplines. Hence we can take the prior correlation matrix to be the identity\\nΣ= I. When the number of basis functions is large, this might not be suf-\\nﬁcient, and additional smoothness can be enforced by imposing restrictions\\non Σ; this is exactly the case with smoothing splines (Section 5.8.1).\\nFigure 8.3 shows ten draws from the corresponding prior forμ(x). To\\ngenerateposteriorvaluesofthefunction μ(x),wegeneratevalues β′ fromits\\nposterior (8.27), giving corresponding posterior valueμ′(x)= ∑7\\n1 β′\\njhj(x).\\nTen such posterior curves are shown in Figure 8.4. Two diﬀerent values\\nwere used for the prior varianceτ, 1 and 1000. Notice how similar the\\nright panel looks to the bootstrap distribution in the bottom left panel', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a726a7e6-89f4-4ee9-a26f-41fd7600418c', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 285, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='270 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\n- 1 012345\\n\\x81\\x81\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\x81 \\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\nμ(x)\\nμ(x)\\nxx\\nτ=1 τ= 1000\\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distribution\\nfor the functionμ(x), for two diﬀerent values of the prior varianceτ. The purple\\ncurves are the posterior means.\\nof Figure 8.2 on page 263. This similarity is no accident. Asτ→∞,t h e\\nposterior distribution (8.27) and the bootstrap distribution (8.7) coincide.\\nOn the other hand, forτ= 1, the posterior curvesμ(x) in the left panel\\nof Figure 8.4 are smoother than the bootstrap curves, because we have\\nimposed more prior weight on smoothness.\\nThe distribution (8.25) withτ→∞is called anoninformative priorfor\\nθ. In Gaussian models, maximum likelihood and parametric bootstrap anal-\\nyses tend to agree with Bayesian analyses that use a noninformative prior\\nfor the free parameters. These tend to agree, because with a constant prior,\\nthe posterior distribution is proportional to the likelihood. This correspon-\\ndence also extends to the nonparametric case, where the nonparametric\\nbootstrap approximates a noninformative Bayes analysis; Section 8.4 has\\nthe details.\\nWe have, however, done some things that are not proper from a Bayesian\\npoint of view. We have used a noninformative (constant) prior forσ2 and\\nreplaced it with the maximum likelihood estimate ˆσ2 in the posterior. A\\nmore standard Bayesian analysis would also put a prior onσ(typically\\ng(σ)∝1/σ), calculate a joint posterior forμ(x)a n dσ, and then integrate\\nout σ, rather than just extract the maximum of the posterior distribution\\n(“MAP” estimate).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4bbd7098-71d5-4d51-b4e8-dda45f860d8d', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 286, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.4 Relationship Between the Bootstrap and Bayesian Inference 271\\n8.4 Relationship Between the Bootstrap and\\nBayesian Inference\\nConsider ﬁrst a very simple example, in which we observe a single obser-\\nvation z from a normal distribution\\nz ∼N(θ,1). (8.29)\\nTo carry out a Bayesian analysis forθ, we need to specify a prior. The\\nmost convenient and common choice would beθ∼N(0,τ) giving posterior\\ndistribution\\nθ|z ∼N\\n⎤ z\\n1+1 /τ, 1\\n1+1 /τ\\n⎦\\n. (8.30)\\nNow the larger we takeτ, the more concentrated the posterior becomes\\naround the maximum likelihood estimateˆθ= z. In the limit asτ→∞we\\nobtain a noninformative (constant) prior, and the posterior distribution is\\nθ|z ∼N(z,1). (8.31)\\nThis is the same as a parametric bootstrap distribution in which we gen-\\nerate bootstrap values z∗ from the maximum likelihood estimate of the\\nsampling densityN(z,1).\\nThere are three ingredients that make this correspondence work:\\n1. The choice of noninformative prior forθ.\\n2. The dependence of the log-likelihood ℓ(θ;Z) on the data Z only\\nthrough the maximum likelihood estimateˆθ. Hence we can write the\\nlog-likelihood asℓ(θ;ˆθ).\\n3. The symmetry of the log-likelihood in θand ˆθ,t h a ti s ,ℓ(θ;ˆθ)=\\nℓ(ˆθ;θ)+constant.\\nProperties (2) and (3) essentially only hold for the Gaussian distribu-\\ntion. However, they also hold approximately for the multinomial distribu-\\ntion, leading to a correspondence between the nonparametric bootstrap\\nand Bayes inference, which we outline next.\\nAssume that we have adiscrete sample spacewithLcategories. Letwj be\\nthe probability that a sample point falls in categoryj,a n dˆwj the observed\\nproportion in categoryj.L e tw =( w1,w2,...,w L), ˆw =(ˆw1, ˆw2,..., ˆwL).\\nDenote our estimator byS(ˆw); take as a prior distribution forw as y m -\\nmetric Dirichlet distribution with parametera:\\nw ∼DiL(a1), (8.32)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2eb79fd5-e599-4122-999b-4761dbeeaf07', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 287, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='272 8. Model Inference and Averaging\\nthat is, the prior probability mass function is proportional to∏L\\nℓ=1 wa−1\\nℓ .\\nThen the posterior density ofw is\\nw ∼DiL(a1+ N ˆw), (8.33)\\nwhere N is the sample size. Lettinga→0 to obtain a noninformative prior\\ngives\\nw∼DiL(N ˆw). (8.34)\\nNow the bootstrap distribution, obtained by sampling with replacement\\nfrom the data, can be expressed as sampling the category proportions from\\na multinomial distribution. Speciﬁcally,\\nN ˆw∗∼Mult(N, ˆw), (8.35)\\nwhere Mult(N, ˆw) denotes a multinomial distribution, having probability\\nmass function\\n⎤ N\\nN ˆw∗\\n1,...,N ˆw∗\\nL\\n⎦∏ ˆwN ˆw∗\\nℓ\\nℓ . This distribution is similar to the pos-\\nterior distribution above, having the same support, same mean, and nearly\\nthe same covariance matrix. Hence the bootstrap distribution ofS(ˆw∗) will\\nclosely approximate the posterior distribution ofS(w).\\nIn this sense, the bootstrap distribution represents an (approximate)\\nnonparametric, noninformative posterior distribution for our parameter.\\nBut this bootstrap distribution is obtained painlessly—without having to\\nformally specify a prior and without having to sample from the posterior\\ndistribution. Hence we might think of the bootstrap distribution as a “poor\\nman’s” Bayes posterior. By perturbing the data, the bootstrap approxi-\\nmates the Bayesian eﬀect of perturbing the parameters, and is typically\\nmuch simpler to carry out.\\n8.5 The EM Algorithm\\nThe EM algorithm is a popular tool for simplifying diﬃcult maximum\\nlikelihood problems. We ﬁrst describe it in the context of a simple mixture\\nmodel.\\n8.5.1 Two-Component Mixture Model\\nIn this section we describe a simple mixture model for density estimation,\\nand the associated EM algorithm for carrying out maximum likelihood\\nestimation. This has a natural connection to Gibbs sampling methods for\\nBayesian inference. Mixture models are discussed and demonstrated in sev-\\neral other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.\\nThe left panel of Figure 8.5 shows a histogram of the 20 ﬁctitious data\\npoints in Table 8.1.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a181bb7-e8f0-4f57-9dc3-fa868d59ebdf', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 288, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.5 The EM Algorithm 273\\n0246\\n0.0 0.2 0.4 0.6 0.8 1.0\\nyy\\ndensity\\n0246\\n0.0 0.2 0.4 0.6 0.8 1.0\\n\\x81\\x81 \\x81 \\x81 \\x81\\x81 \\x81\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81 \\x81\\x81\\x81 \\x81 \\x81\\nFIGURE 8.5.Mixture example. (Left panel:) Histogram of data. (Right panel:)\\nMaximum likelihood ﬁt of Gaussian densities (solid red) and responsibility (dotted\\ngreen) of the left component density for observationy, as a function ofy.\\nTABLE 8.1. Twenty ﬁctitious data points used in the two-component mixture\\nexample in Figure 8.5.\\n-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\\n0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\\nWe would like to model the density of the data points, and due to the\\napparent bi-modality, a Gaussian distribution would not be appropriate.\\nThere seems to be two separate underlying regimes, so instead we model\\nY as a mixture of two normal distributions:\\nY1 ∼ N(μ1,σ2\\n1),\\nY2 ∼ N(μ2,σ2\\n2), (8.36)\\nY =( 1 −Δ)·Y1 +Δ·Y2,\\nwhere Δ∈{0,1} with Pr(Δ = 1) =π.T h i sgenerative representation is\\nexplicit: generate a Δ∈{0,1} with probabilityπ, and then depending on\\nthe outcome, deliver eitherY1 or Y2.L e tφθ(x) denote the normal density\\nwith parametersθ=( μ,σ2). Then the density ofY is\\ngY (y)=( 1−π)φθ1(y)+ πφθ2(y). (8.37)\\nNow suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-\\nmum likelihood. The parameters are\\nθ=( π,θ1,θ2)=( π,μ1,σ2\\n1,μ2,σ2\\n2). (8.38)\\nThe log-likelihood based on theN training cases is\\nℓ(θ;Z)=\\nN∑\\ni=1\\nlog[(1−π)φθ1(yi)+ πφθ2(yi)]. (8.39)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a5f5518-6c15-4976-8162-98ea4dfe4a9b', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 289, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='274 8. Model Inference and Averaging\\nDirect maximization ofℓ(θ;Z) is quite diﬃcult numerically, because of\\nthe sum of terms inside the logarithm. There is, however, a simpler ap-\\nproach. We consider unobserved latent variables Δi taking values 0 or 1 as\\nin (8.36): if Δi = 1 thenYi comes from model 2, otherwise it comes from\\nmodel 1. Suppose we knew the values of the Δi’s. Then the log-likelihood\\nwould be\\nℓ0(θ;Z,Δ)=\\nN∑\\ni=1\\n[(1−Δi)log φθ1(yi)+Δ i logφθ2(yi)]\\n+\\nN∑\\ni=1\\n[(1−Δi)log(1 −π)+Δ i logπ], (8.40)\\nand the maximum likelihood estimates ofμ1 and σ2\\n1 would be the sample\\nmean and variance for those data with Δi = 0, and similarly those forμ2\\nand σ2\\n2 would be the sample mean and variance of the data with Δi =1 .\\nThe estimate ofπwould be the proportion of Δi =1 .\\nSince the values of the Δi’s are actually unknown, we proceed in an\\niterative fashion, substituting for each Δi in (8.40) its expected value\\nγi(θ)=E ( Δi|θ,Z)=P r ( Δi =1|θ,Z), (8.41)\\nalso called theresponsibilityof model 2 for observationi.W eu s eap r o c e -\\ndure called the EM algorithm, given in Algorithm 8.1 for the special case of\\nGaussian mixtures. In theexpectationstep, we do a soft assignment of each\\nobservation to each model: the current estimates of the parameters are used\\nto assign responsibilities according to the relative density of the training\\npoints under each model. In themaximization step, these responsibilities\\nare used in weighted maximum-likelihood ﬁts to update the estimates of\\nthe parameters.\\nA good way to construct initial guesses for ˆμ1 and ˆμ2 is simply to choose\\ntwo of theyi at random. Both ˆσ2\\n1 and ˆσ2\\n2 can be set equal to the overall\\nsample variance∑N\\ni=1(yi−¯y)2/N. The mixing proportion ˆπcan be started\\nat the value 0.5.\\nNote that the actual maximizer of the likelihood occurs when we put a\\nspike of inﬁnite height at any one data point, that is, ˆμ1 = yi for some\\ni and ˆσ2\\n1 = 0. This gives inﬁnite likelihood, but is not a useful solution.\\nHence we are actually looking for a good local maximum of the likelihood,\\none for which ˆσ2\\n1,ˆσ2\\n2 > 0. To further complicate matters, there can be\\nmore than one local maximum having ˆσ2\\n1,ˆσ2\\n2 > 0. In our example, we\\nran the EM algorithm with a number of diﬀerent initial guesses for the\\nparameters, all having ˆσ2\\nk > 0.5, and chose the run that gave us the highest\\nmaximized likelihood. Figure 8.6 shows the progress of the EM algorithm in\\nmaximizing the log-likelihood. Table 8.2 shows ˆπ=∑\\ni ˆγi/N, the maximum\\nlikelihood estimate of the proportion of observations in class 2, at selected\\niterations of the EM procedure.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db5b6723-4461-437c-b277-ac504bbfcea6', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 290, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.5 The EM Algorithm 275\\nAlgorithm 8.1EM Algorithm for Two-component Gaussian Mixture.\\n1. Take initial guesses for the parameters ˆμ1,ˆσ2\\n1,ˆμ2,ˆσ2\\n2,ˆπ(see text).\\n2. Expectation Step: compute the responsibilities\\nˆγi =\\nˆπφˆθ2\\n(yi)\\n(1−ˆπ)φˆθ1\\n(yi)+ˆπφˆθ2\\n(yi),i =1 ,2,...,N. (8.42)\\n3. Maximization Step: compute the weighted means and variances:\\nˆμ1 =\\n∑N\\ni=1(1−ˆγi)yi\\n∑N\\ni=1(1−ˆγi)\\n, ˆσ2\\n1 =\\n∑N\\ni=1(1−ˆγi)(yi −ˆμ1)2\\n∑N\\ni=1(1−ˆγi)\\n,\\nˆμ2 =\\n∑N\\ni=1 ˆγiyi\\n∑N\\ni=1 ˆγi\\n, ˆσ2\\n2 =\\n∑N\\ni=1 ˆγi(yi −ˆμ2)2\\n∑N\\ni=1 ˆγi\\n,\\nand the mixing probability ˆπ=∑N\\ni=1 ˆγi/N.\\n4. Iterate steps 2 and 3 until convergence.\\nTABLE 8.2.Selected iterations of the EM algorithm for mixture example.\\nIteration ˆ π\\n1 0.485\\n5 0.493\\n10 0.523\\n15 0.544\\n20 0.546\\nThe ﬁnal maximum likelihood estimates are\\nˆμ1 =4 .62, ˆσ2\\n1 =0 .87,\\nˆμ2 =1 .06, ˆσ2\\n2 =0 .77,\\nˆπ=0 .546.\\nThe right panel of Figure 8.5 shows the estimated Gaussian mixture density\\nfromthisprocedure(solidredcurve),alongwiththeresponsibilities(dotted\\ngreen curve). Note that mixtures are also useful for supervised learning; in\\nSection 6.7 we show how the Gaussian mixture model leads to a version of\\nradial basis functions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='53669891-4072-4d42-877e-8862174ac0ea', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 291, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='276 8. Model Inference and Averaging\\nIteration\\nObserved Data Log-likelihood\\n5 1 01 52 0\\n-44 -43 -42 -41 -40 -39\\no\\no o o o\\no\\no\\no\\no o o o o o oooooo\\nFIGURE 8.6. EM algorithm: observed data log-likelihood as a function of the\\niteration number.\\n8.5.2 The EM Algorithm in General\\nThe above procedure is an example of the EM (or Baum–Welch) algorithm\\nfor maximizing likelihoods in certain classes of problems. These problems\\nare ones for which maximization of the likelihood is diﬃcult, but made\\neasier by enlarging the sample with latent (unobserved) data. This is called\\ndata augmentation. Here the latent data are the model memberships Δi.\\nIn other problems, the latent data are actual data that should have been\\nobserved but are missing.\\nAlgorithm 8.2 gives the general formulation of the EM algorithm. Our\\nobserved data isZ, having log-likelihoodℓ(θ;Z) depending on parameters\\nθ. The latent or missing data isZm, so that the complete data isT =\\n(Z,Zm) with log-likelihoodℓ0(θ;T), ℓ0 based on the complete density. In\\nthe mixture problem (Z,Zm)=( y,Δ), andℓ0(θ;T) is given in (8.40).\\nIn our mixture example, E(ℓ0(θ′;T)|Z, ˆθ(j)) is simply (8.40) with the Δi\\nreplaced by the responsibilities ˆγi(ˆθ), and the maximizers in step 3 are just\\nweighted means and variances.\\nWe now give an explanation of why the EM algorithm works in general.\\nSince\\nPr(Zm|Z,θ′)= Pr(Zm,Z|θ′)\\nPr(Z|θ′) , (8.44)\\nwe can write\\nPr(Z|θ′)= Pr(T|θ′)\\nPr(Zm|Z,θ′). (8.45)\\nIntermsoflog-likelihoods,wehave ℓ(θ′;Z)= ℓ0(θ′;T)−ℓ1(θ′;Zm|Z),where\\nℓ1 is based on the conditional density Pr(Zm|Z,θ′). Taking conditional\\nexpectations with respect to the distribution ofT|Z governed by parameter\\nθgives\\nℓ(θ′;Z)=E [ ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='021630fd-8123-4028-945e-e3b0f4c34f10', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 292, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.5 The EM Algorithm 277\\nAlgorithm 8.2The EM Algorithm.\\n1. Start with initial guesses for the parametersˆθ(0).\\n2. Expectation Step:a tt h ejth step, compute\\nQ(θ′,ˆθ(j))=E (ℓ0(θ′;T)|Z, ˆθ(j)) (8.43)\\nas a function of the dummy argumentθ′.\\n3. Maximization Step: determine the new estimateˆθ(j+1) as the maxi-\\nmizer ofQ(θ′,ˆθ(j))o v e rθ′.\\n4. Iterate steps 2 and 3 until convergence.\\n≡ Q(θ′,θ)−R(θ′,θ). (8.46)\\nIn theM step, the EM algorithm maximizesQ(θ′,θ)o v e rθ′, rather than\\nthe actual objective functionℓ(θ′;Z). Why does it succeed in maximizing\\nℓ(θ′;Z)?Notethat R(θ∗,θ)istheexpectationofalog-likelihoodofadensity\\n(indexed byθ∗), with respect to the same density indexed byθ, and hence\\n(by Jensen’s inequality) is maximized as a function ofθ∗,w h e nθ∗= θ(see\\nExercise 8.1). So ifθ′ maximizes Q(θ′,θ), we see that\\nℓ(θ′;Z)−ℓ(θ;Z)=[ Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]\\n≥ 0. (8.47)\\nHence the EM iteration never decreases the log-likelihood.\\nThis argument also makes it clear that a full maximization in theM\\nstep is not necessary: we need only to ﬁnd a valueˆθ(j+1) so thatQ(θ′,ˆθ(j))\\nincreases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j)) >\\nQ(ˆθ(j),ˆθ(j)). Such procedures are calledGEM (generalized EM)algorithms.\\nThe EM algorithm can also be viewed as a minorization procedure: see\\nExercise 8.7.\\n8.5.3 EM as a Maximization–Maximization Procedure\\nHere is a diﬀerent view of the EM procedure, as a joint maximization\\nalgorithm. Consider the function\\nF(θ′, ˜P)=E ˜P[ℓ0(θ′;T)]−E˜P[log ˜P(Zm)]. (8.48)\\nHere ˜P(Zm) is any distribution over the latent dataZm. In the mixture\\nexample, ˜P(Zm) comprises the set of probabilitiesγi =P r ( Δi =1 |θ,Z).\\nNote that F evaluated at ˜P(Zm)=P r (Zm|Z,θ′), is the log-likelihood of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='72b48c9e-d70d-457d-9016-075c9d6fb9ea', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 293, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='278 8. Model Inference and Averaging\\n12345\\n01234\\n0.10.3\\n0.50.7\\n0.9\\nModel Parameters\\nLatent Data Parameters\\nE\\nM\\nE M\\nFIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown\\nare the contours of the (augmented) observed data log-likelihoodF(θ′, ˜P).T h e\\nE step is equivalent to maximizing the log-likelihood over the parameters of the\\nlatent data distribution. The M step maximizes it over the parameters of the\\nlog-likelihood. The red curve corresponds to the observed data log-likelihood, a\\nproﬁle obtained by maximizingF(θ′, ˜P) for each value ofθ′.\\nthe observed data, from (8.46)1. The function F expands the domain of\\nthe log-likelihood, to facilitate its maximization.\\nThe EM algorithm can be viewed as a joint maximization method forF\\noverθ′ and ˜P(Zm), by ﬁxing one argument and maximizing over the other.\\nThe maximizer over˜P(Zm) for ﬁxedθ′ can be shown to be\\n˜P(Zm)=P r (Zm|Z,θ′) (8.49)\\n(Exercise8.2).Thisisthedistributioncomputedbythe E step,forexample,\\n(8.42) in the mixture example. In theM step, we maximizeF(θ′, ˜P)o v e rθ′\\nwith ˜P ﬁxed: this is the same as maximizing the ﬁrst term E˜P[ℓ0(θ′;T)|Z,θ]\\nsince the second term does not involveθ′.\\nFinally, sinceF(θ′, ˜P) and the observed data log-likelihood agree when\\n˜P(Zm)=P r (Zm|Z,θ′), maximization of the former accomplishes maxi-\\nmization of the latter. Figure 8.7 shows a schematic view of this process.\\nThis view of the EM algorithm leads to alternative maximization proce-\\n1 (8.46) holds for allθ, includingθ= θ′.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac4a2f28-9667-4f99-beca-56cc9b9608f1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 294, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.6 MCMC for Sampling from the Posterior 279\\nAlgorithm 8.3Gibbs Sampler.\\n1. Take some initial valuesU(0)\\nk ,k =1 ,2,...,K .\\n2. Repeat fort =1 ,2,...,. :\\nFor k =1 ,2,...,K generate U(t)\\nk from\\nPr(U(t)\\nk |U(t)\\n1 ,...,U (t)\\nk−1,U (t−1)\\nk+1 ,...,U (t−1)\\nK ).\\n3. Continue step 2 until the joint distribution of (U(t)\\n1 ,U (t)\\n2 ,...,U (t)\\nK )\\ndoes not change.\\ndures. For example, one does not need to maximize with respect to all of\\nthe latent data parameters at once, but could instead maximize over one\\nof them at a time, alternating with theM step.\\n8.6 MCMC for Sampling from the Posterior\\nHaving deﬁned a Bayesian model, one would like to draw samples from\\nthe resulting posterior distribution, in order to make inferences about the\\nparameters. Except for simple models, this is often a diﬃcult computa-\\ntional problem. In this section we discuss theMarkov chain Monte Carlo\\n(MCMC) approach to posterior sampling. We will see that Gibbs sampling,\\nan MCMC procedure, is closely related to the EM algorithm: the main dif-\\nference is that it samples from the conditional distributions rather than\\nmaximizing over them.\\nConsider ﬁrst the following abstract problem. We have random variables\\nU1,U2,...,U K and we wish to draw a sample from their joint distribution.\\nSuppose this is diﬃcult to do, but it is easy to simulate from the conditional\\ndistributions Pr(Uj|U1,U2,...,U j−1,Uj+1,...,U K),j =1 ,2,...,K .T h e\\nGibbs sampling procedure alternatively simulates from each of these distri-\\nbutions and when the process stabilizes, provides a sample from the desired\\njoint distribution. The procedure is deﬁned in Algorithm 8.3.\\nUnder regularity conditions it can be shown that this procedure even-\\ntually stabilizes, and the resulting random variables are indeed a sample\\nfrom the joint distribution ofU1,U2,...,U K. This occurs despite the fact\\nthat the samples (U(t)\\n1 ,U (t)\\n2 ,...,U (t)\\nK ) are clearly not independent for dif-\\nferent t. More formally, Gibbs sampling produces a Markov chain whose\\nstationary distribution is the true joint distribution, and hence the term\\n“Markov chain Monte Carlo.” It is not surprising that the true joint dis-\\ntribution is stationary under this process, as the successive steps leave the\\nmarginal distributions of theUk’s unchanged.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7439fc59-ca88-4a02-b5a2-146cb99bf986', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 295, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='280 8. Model Inference and Averaging\\nNote that we don’t need to know the explicit form of the conditional\\ndensities, but just need to be able to sample from them. After the procedure\\nreaches stationarity, the marginal density of any subset of the variables\\ncan be approximated by a density estimate applied to the sample values.\\nHowever if the explicit form of the conditional density Pr(Uk,|Uℓ,ℓ ̸= k)\\nis available, a better estimate of say the marginal density ofUk can be\\nobtained from (Exercise 8.3):\\nˆPrUk(u)= 1\\n(M −m+1)\\nM∑\\nt=m\\nPr(u|U(t)\\nℓ ,ℓ ̸= k). (8.50)\\nHere we have averaged over the lastM −m+1 members of the sequence,\\nto allow for an initial “burn-in” period before stationarity is reached.\\nNowgettingbacktoBayesianinference,ourgoalistodrawasamplefrom\\nthe joint posterior of the parameters given the dataZ. Gibbs sampling will\\nbe helpful if it is easy to sample from the conditional distribution of each\\nparameter given the other parameters andZ. An example—the Gaussian\\nmixture problem—is detailed next.\\nThere is a close connection between Gibbs sampling from a posterior and\\nthe EM algorithm in exponential family models. The key is to consider the\\nlatent data Zm from the EM procedure to be another parameter for the\\nGibbs sampler. To make this explicit for the Gaussian mixture problem,\\nwe take our parameters to be (θ,Zm). For simplicity we ﬁx the variances\\nσ2\\n1,σ2\\n2 and mixing proportionπat their maximum likelihood values so that\\nthe only unknown parameters inθare the meansμ1 and μ2. The Gibbs\\nsampler for the mixture problem is given in Algorithm 8.4. We see that\\nsteps 2(a) and 2(b) are the same as theE and M steps of the EM pro-\\ncedure, except that we sample rather than maximize. In step 2(a), rather\\nthan compute the maximum likelihood responsibilities γi =E ( Δi|θ,Z),\\nthe Gibbs sampling procedure simulates the latent data Δi from the distri-\\nbutions Pr(Δi|θ,Z). In step 2(b), rather than compute the maximizers of\\nthe posterior Pr(μ1,μ2,Δ|Z) we simulate from the conditional distribution\\nPr(μ1,μ2|Δ,Z).\\nFigure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-\\neters μ1 (lower) andμ2 (upper) shown in the left panel, and the proportion\\nof class 2 observations∑\\ni Δi/N on the right. Horizontal broken lines have\\nbeen drawn at the maximum likelihood estimate values ˆμ1,ˆμ2 and∑\\ni ˆγi/N\\nin each case. The values seem to stabilize quite quickly, and are distributed\\nevenly around the maximum likelihood values.\\nThe above mixture model was simpliﬁed, in order to make the clear\\nconnection between Gibbs sampling and the EM algorithm. More realisti-\\ncally, one would put a prior distribution on the variancesσ2\\n1,σ2\\n2 and mixing\\nproportion π, and include separate Gibbs sampling steps in which we sam-\\nple from their posterior distributions, conditional on the other parameters.\\nOne can also incorporate proper (informative) priors for the mean param-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5d9bb008-cfe3-44dd-ab57-fceff0d9b438', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 296, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.6 MCMC for Sampling from the Posterior 281\\nAlgorithm 8.4Gibbs sampling for mixtures.\\n1. Take some initial valuesθ(0) =( μ(0)\\n1 ,μ(0)\\n2 ).\\n2. Repeat fort =1 ,2,...,.\\n(a) Fori =1 ,2,...,N generate Δ(t)\\ni ∈{0,1} with Pr(Δ(t)\\ni =1 )=\\nˆγi(θ(t)), from equation (8.42).\\n(b) Set\\nˆμ1 =\\n∑N\\ni=1(1−Δ(t)\\ni )·yi\\n∑N\\ni=1(1−Δ(t)\\ni )\\n,\\nˆμ2 =\\n∑N\\ni=1 Δ(t)\\ni ·yi\\n∑N\\ni=1 Δ(t)\\ni\\n,\\nand generateμ(t)\\n1 ∼N(ˆμ1,ˆσ2\\n1)a n dμ(t)\\n2 ∼N(ˆμ2,ˆσ2\\n2).\\n3. Continue step 2 until the joint distribution of (Δ(t),μ(t)\\n1 ,μ(t)\\n2 )d o e s n ’ t\\nchange\\nGibbs Iteration\\nMean Parameters\\n0 50 100 150 200\\n02468\\nGibbs Iteration\\nMixing Proportion\\n0 50 100 150 200\\n0.3 0.4 0.5 0.6 0.7\\nFIGURE 8.8.Mixture example. (Left panel:)200 values of the two mean param-\\neters from Gibbs sampling; horizontal lines are drawn at the maximum likelihood\\nestimates ˆμ1, ˆμ2. (Right panel:) Proportion of values withΔi =1 ,f o re a c ho ft h e\\n200 Gibbs sampling iterations; a horizontal line is drawn at∑\\ni ˆγi/N.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d025649-da19-42af-9744-ad1d79463455', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 297, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='282 8. Model Inference and Averaging\\neters. These priors must not be improper as this will lead to a degenerate\\nposterior, with all the mixing weight on one component.\\nGibbs sampling is just one of a number of recently developed procedures\\nfor sampling from posterior distributions. It uses conditional sampling of\\neach parameter given the rest, and is useful when the structure of the prob-\\nlem makes this sampling easy to carry out. Other methods do not require\\nsuch structure, for example theMetropolis–Hastingsalgorithm. These and\\nother computational Bayesian methods have been applied to sophisticated\\nlearning algorithms such as Gaussian process models and neural networks.\\nDetails may be found in the references given in the Bibliographic Notes at\\nthe end of this chapter.\\n8.7 Bagging\\nEarlier we introduced the bootstrap as a way of assessing the accuracy of a\\nparameter estimate or a prediction. Here we show how to use the bootstrap\\nto improve the estimate or prediction itself. In Section 8.4 we investigated\\nthe relationship between the bootstrap and Bayes approaches, and found\\nthat the bootstrap mean is approximately a posterior average. Bagging\\nfurther exploits this connection.\\nConsider ﬁrst the regression problem. Suppose we ﬁt a model to our\\ntraining data Z = {(x1,y1),(x2,y2),..., (xN,yN)}, obtaining the predic-\\ntion ˆf(x) at inputx. Bootstrap aggregation orbagginga v e r a g e st h i sp r e d i c -\\ntion over a collection of bootstrap samples, thereby reducing its variance.\\nFor each bootstrap sampleZ∗b, b =1 ,2,...,B , we ﬁt our model, giving\\nprediction ˆf∗b(x). The bagging estimate is deﬁned by\\nˆfbag(x)= 1\\nB\\nB∑\\nb=1\\nˆf∗b(x). (8.51)\\nDenote by ˆP the empirical distribution putting equal probability 1/N on\\neach of the data points (xi,yi). In fact the “true” bagging estimate is\\ndeﬁned by EˆP\\nˆf∗(x), whereZ∗={(x∗\\n1,y∗\\n1),(x∗\\n2,y∗\\n2),..., (x∗\\nN,y∗\\nN)} and each\\n(x∗\\ni ,y∗\\ni ) ∼ ˆP. Expression (8.51) is a Monte Carlo estimate of the true\\nbagging estimate, approaching it asB →∞.\\nThe bagged estimate (8.51) will diﬀer from the original estimateˆf(x)\\nonly when the latter is a nonlinear or adaptive function of the data. For\\nexample, to bag theB-spline smooth of Section 8.2.1, we average the curves\\nin the bottom left panel of Figure 8.2 at each value ofx.T h eB-spline\\nsmoother is linear in the data if we ﬁx the inputs; hence if we sample using\\nthe parametric bootstrap in equation (8.6), thenˆfbag(x)→ ˆf(x)a sB →∞\\n(Exercise 8.4). Hence bagging just reproduces the original smooth in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='43c7e2a3-a2c8-4894-9db2-886ba811889e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 298, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.7 Bagging 283\\ntop left panel of Figure 8.2. The same is approximately true if we were to\\nbag using the nonparametric bootstrap.\\nA more interesting example is a regression tree, whereˆf(x) denotes the\\ntree’s prediction at input vectorx (regression trees are described in Chap-\\nter 9). Each bootstrap tree will typically involve diﬀerent features than the\\noriginal, and might have a diﬀerent number of terminal nodes. The bagged\\nestimate is the average prediction atx from theseB trees.\\nNow suppose our tree produces a classiﬁerˆG(x)f o raK-class response.\\nHere it is useful to consider an underlying indicator-vector functionˆf(x),\\nwith value a single one andK−1 zeroes, such thatˆG(x) = argmaxk ˆf(x).\\nThen the bagged estimate ˆfbag(x) (8.51) is aK-vector [p1(x),p2(x),...,\\npK(x)], withpk(x) equal to the proportion of trees predicting classk at x.\\nThe bagged classiﬁer selects the class with the most “votes” from theB\\ntrees, ˆGbag(x) = argmaxk ˆfbag(x).\\nOften we require the class-probability estimates atx, rather than the\\nclassiﬁcations themselves. It is tempting to treat the voting proportions\\npk(x) as estimates of these probabilities. A simple two-class example shows\\nthat they fail in this regard. Suppose the true probability of class 1 atx is\\n0.75, and each of the bagged classiﬁers accurately predict a 1. Thenp1(x)=\\n1, which is incorrect. For many classiﬁersˆG(x), however, there is already\\nan underlying functionˆf(x) that estimates the class probabilities atx (for\\ntrees, the class proportions in the terminal node). An alternative bagging\\nstrategy is to average these instead, rather than the vote indicator vectors.\\nNot only does this produce improved estimates of the class probabilities,\\nbutitalsotendstoproducebaggedclassiﬁerswithlowervariance,especially\\nfor smallB (see Figure 8.10 in the next example).\\n8.7.1 Example: Trees with Simulated Data\\nWe generated a sample of sizeN = 30, with two classes andp =5f e a t u r e s ,\\neach having a standard Gaussian distribution with pairwise correlation\\n0.95. The responseY was generated according to Pr(Y =1|x1 ≤0.5) = 0.2,\\nPr(Y =1|x1 > 0.5) = 0.8. The Bayes error is 0.2. A test sample of size 2000\\nwas also generated from the same population. We ﬁt classiﬁcation trees to\\nthe training sample and to each of 200 bootstrap samples (classiﬁcation\\ntrees are described in Chapter 9). No pruning was used. Figure 8.9 shows\\nthe original tree and eleven bootstrap trees. Notice how the trees are all\\ndiﬀerent, with diﬀerent splitting features and cutpoints. The test error for\\nthe original tree and the bagged tree is shown in Figure 8.10. In this ex-\\nample the trees have high variance due to the correlation in the predictors.\\nBagging succeeds in smoothing out this variance and hence reducing the\\ntest error.\\nBagging can dramatically reduce the variance of unstable procedures\\nlike trees, leading to improved prediction. A simple argument shows why', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='349a8f18-1616-4037-9821-e5156bc6997a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 299, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='284 8. Model Inference and Averaging\\n|\\nx.1 < 0.395\\n0 1\\n0\\n1 0\\n1\\n1 0\\nOriginal Tree\\n|\\nx.1 < 0.555\\n0\\n1 0\\n0\\n1\\nb = 1\\n|\\nx.2 < 0.205\\n0 1\\n0 1\\n0 1\\nb = 2\\n|\\nx.2 < 0.285\\n11\\n0\\n1 0\\nb = 3\\n|\\nx.3 < 0.985\\n0\\n1\\n0 1\\n11\\nb = 4\\n|\\nx.4 < −1.36\\n0\\n1\\n1 0\\n1\\n0\\n1 0\\nb = 5\\n|\\nx.1 < 0.395\\n11 00\\n1\\nb = 6\\n|\\nx.1 < 0.395\\n0 1\\n0 1\\n1\\nb = 7\\n|\\nx.3 < 0.985\\n0 1\\n00\\n1 0\\nb = 8\\n|\\nx.1 < 0.395\\n0\\n1\\n0 1\\n1 0\\nb = 9\\n|\\nx.1 < 0.555\\n1 0\\n1\\n0 1\\nb = 10\\n|\\nx.1 < 0.555\\n0 1\\n0\\n1\\nb = 11\\nFIGURE 8.9.Bagging trees on simulated dataset. The top left panel shows the\\noriginal tree. Eleven trees grown on bootstrap samples are shown. For each tree,\\nthe top split is annotated.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fd52de43-27c4-49da-bf4a-d5f206d7f5e1', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 300, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.7 Bagging 285\\n0 50 100 150 200\\n0.20 0.25 0.30 0.35 0.40 0.45 0.50\\nNumber of Bootstrap Samples\\nTest Error\\nBagged Trees\\nOriginal Tree\\nBayes\\nConsensus\\nProbability\\nFIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\\nthe test error of the original tree and bagged trees as a function of the number of\\nbootstrap samples. The orange points correspond to the consensus vote, while the\\ngreen points average the probabilities.\\nbagging helps under squared-error loss, in short because averaging reduces\\nvariance and leaves bias unchanged.\\nAssume our training observations (xi,yi),i =1 ,...,N are indepen-\\ndently drawn from a distributionP, and consider the ideal aggregate es-\\ntimator fag(x)=E P ˆf∗(x). Here x is ﬁxed and the bootstrap datasetZ∗\\nconsists of observationsx∗\\ni ,y∗\\ni , i =1 ,2,...,N sampled fromP.N o t et h a t\\nfag(x) is a bagging estimate, drawing bootstrap samples from the actual\\npopulation P rather than the data. It is not an estimate that we can use\\nin practice, but is convenient for analysis. We can write\\nEP[Y −ˆf∗(x)]2 =E P[Y −fag(x)+ fag(x)−ˆf∗(x)]2\\n=E P[Y −fag(x)]2 +EP[ˆf∗(x)−fag(x)]2\\n≥ EP[Y −fag(x)]2. (8.52)\\nThe extra error on the right-hand side comes from the variance ofˆf∗(x)\\naround its mean fag(x). Therefore true population aggregation never in-\\ncreases mean squared error. This suggests that bagging—drawing samples\\nfrom the training data— will often decrease mean-squared error.\\nThe above argument does not hold for classiﬁcation under 0-1 loss, be-\\ncause of the nonadditivity of bias and variance. In that setting, bagging a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec01a68d-d6bb-4704-8ae3-8b4477f905cc', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 301, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='286 8. Model Inference and Averaging\\ngood classiﬁer can make it better, but bagging a bad classiﬁer can make it\\nworse. Here is a simple example, using a randomized rule. SupposeY =1\\nfor all x, and the classiﬁer ˆG(x) predicts Y = 1 (for all x)w i t hp r o b a -\\nbility 0.4 and predictsY = 0 (for all x) with probability 0.6. Then the\\nmisclassiﬁcation error ofˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.\\nFor classiﬁcation we can understand the bagging eﬀect in terms of a\\nconsensus of independentweak learners(Dietterich, 2000a). Let the Bayes\\noptimal decision atx be G(x) = 1 in a two-class example. Suppose each\\nof the weak learnersG∗\\nb have an error-rateeb = e< 0.5, and letS1(x)=∑B\\nb=1 I(G∗\\nb(x) = 1) be the consensus vote for class 1. Since the weak learn-\\ners are assumed to be independent,S1(x) ∼Bin(B,1−e), and Pr(S1 >\\nB/2) →1a s B gets large. This concept has been popularized outside of\\nstatistics as the “Wisdom of Crowds” (Surowiecki, 2004) — the collective\\nknowledge of a diverse and independent body of people typically exceeds\\nthe knowledge of any single individual, and can be harnessed by voting.\\nOf course, the main caveat here is “independent,” and bagged trees are\\nnot. Figure 8.11 illustrates the power of a consensus vote in a simulated\\nexample, where only 30% of the voters have some knowledge.\\nIn Chapter 15 we see how random forests improve on bagging by reducing\\nthe correlation between the sampled trees.\\nNote that when we bag a model, any simple structure in the model is\\nlost. As an example, a bagged tree is no longer a tree. For interpretation\\nof the model this is clearly a drawback. More stable procedures like near-\\nest neighbors are typically not aﬀected much by bagging. Unfortunately,\\nthe unstable models most helped by bagging are unstable because of the\\nemphasis on interpretability, and this is lost in the bagging process.\\nFigure 8.12 shows an example where bagging doesn’t help. The 100 data\\npoints shown have two features and two classes, separated by the gray\\nlinear boundary x1 + x2 = 1. We choose as our classiﬁer ˆG(x)as i n g l e\\naxis-oriented split, choosing the split along eitherx1 or x2 that produces\\nthe largest decrease in training misclassiﬁcation error.\\nThe decision boundary obtained from bagging the 0-1 decision rule over\\nB = 50 bootstrap samples is shown by the blue curve in the left panel.\\nIt does a poor job of capturing the true boundary. The single split rule,\\nderived from the training data, splits near 0 (the middle of the range ofx1\\nor x2), and hence has little contribution away from the center. Averaging\\nthe probabilities rather than the classiﬁcations does not help here. Bagging\\nestimates the expected class probabilities from the single split rule, that is,\\naveraged over many replications. Note that the expected class probabilities\\ncomputed by bagging cannot be realized on any single replication, in the\\nsame way that a woman cannot have 2.4 children. In this sense, bagging\\nincreases somewhat the space of models of the individual base classiﬁer.\\nHowever, it doesn’t help in this and many other examples where a greater\\nenlargement of the model class is needed. “Boosting” is a way of doing this', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99669cd9-ce7a-482e-850c-73b9eae2362a', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 302, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.7 Bagging 287\\n02468 1 0\\nP −  Probability of Informed Person Being Correct\\nExpected Correct out of 10\\nWisdom of Crowds\\nConsensus\\nIndividual\\n0.25 0.50 0.75 1.00\\nFIGURE 8.11. Simulated academy awards voting.50 members vote in 10 cat-\\negories, each with 4 nominations. For any category, only15 voters have some\\nknowledge, represented by their probability of selecting the “correct” candidate in\\nthat category (soP =0 .25 means they have no knowledge). For each category, the\\n15 experts are chosen at random from the50. Results show the expected correct\\n(based on50 simulations) for the consensus, as well as for the individuals. The\\nerror bars indicate one standard deviation. We see, for example, that if the15\\ninformed for a category have a50% chance of selecting the correct candidate, the\\nconsensus doubles the expected performance of an individual.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2e5c29cb-82a7-4383-a3ee-64f37aa1124e', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 303, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='288 8. Model Inference and Averaging\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\nBagged Decision Rule\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\nBoosted Decision Rule\\nFIGURE 8.12. Data with two features and two classes, separated by a linear\\nboundary. (Left panel:) Decision boundary estimated from bagging the decision\\nrule from a single split, axis-oriented classiﬁer. (Right panel:) Decision boundary\\nfrom boosting the decision rule of the same classiﬁer. The test error rates are\\n0.166,a n d0.065, respectively. Boosting is described in Chapter 10.\\nand is described in Chapter 10. The decision boundary in the right panel is\\nthe result of the boosting procedure, and it roughly captures the diagonal\\nboundary.\\n8.8 Model Averaging and Stacking\\nIn Section 8.4 we viewed bootstrap values of an estimator as approximate\\nposterior values of a corresponding parameter, from a kind of nonparamet-\\nric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) is\\nan approximate posterior Bayesian mean. In contrast, the training sample\\nestimate ˆf(x) corresponds to the mode of the posterior. Since the posterior\\nmean (not mode) minimizes squared-error loss, it is not surprising that\\nbagging can often reduce mean squared-error.\\nHere we discuss Bayesian model averaging more generally. We have a\\nset of candidate modelsMm,m =1 ,...,M for our training setZ.T h e s e\\nmodels may be of the same type with diﬀerent parameter values (e.g.,\\nsubsets in linear regression), or diﬀerent models for the same task (e.g.,\\nneural networks and regression trees).\\nSuppose ζis some quantity of interest, for example, a predictionf(x)a t\\nsome ﬁxed feature valuex. The posterior distribution ofζis\\nPr(ζ|Z)=\\nM∑\\nm=1\\nPr(ζ|Mm,Z)Pr(Mm|Z), (8.53)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d28cb3e5-c995-4b67-a1eb-6b176179e76f', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 304, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.8 Model Averaging and Stacking 289\\nwith posterior mean\\nE(ζ|Z)=\\nM∑\\nm=1\\nE(ζ|Mm,Z)Pr(Mm|Z). (8.54)\\nThisBayesianpredictionisaweightedaverageoftheindividualpredictions,\\nwith weights proportional to the posterior probability of each model.\\nThis formulation leads to a number of diﬀerent model-averaging strate-\\ngies. Committee methodstake a simple unweighted average of the predic-\\ntions from each model, essentially giving equal probability to each model.\\nMore ambitiously, the development in Section 7.7 shows the BIC criterion\\ncan be used to estimate posterior model probabilities. This is applicable\\nin cases where the diﬀerent models arise from the same parametric model,\\nwith diﬀerent parameter values. The BIC gives weight to each model de-\\npending on how well it ﬁts and how many parameters it uses. One can also\\ncarry out the Bayesian recipe in full. If each modelMm has parameters\\nθm,w ew r i t e\\nPr(Mm|Z) ∝ Pr(Mm)·Pr(Z|Mm)\\n∝ Pr(Mm)·\\n∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm.\\n(8.55)\\nIn principle one can specify priors Pr(θm|Mm) and numerically com-\\npute the posterior probabilities from (8.55), to be used as model-averaging\\nweights. However, we have seen no real evidence that this is worth all of\\nthe eﬀort, relative to the much simpler BIC approximation.\\nHow can we approach model averaging from a frequentist viewpoint?\\nGiven predictions ˆf1(x), ˆf2(x),..., ˆfM(x), under squared-error loss, we can\\nseek the weightsw =( w1,w2,...,w M) such that\\nˆw =a r g m i n\\nw\\nEP\\n[\\nY −\\nM∑\\nm=1\\nwm ˆfm(x)\\n]2\\n. (8.56)\\nHere the input valuexis ﬁxed and theN observations in the datasetZ(and\\nthe targetY) are distributed according toP. The solution is the population\\nlinear regression ofY on ˆF(x)T ≡[ˆf1(x), ˆf2(x),..., ˆfM(x)]:\\nˆw =E P[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)\\nNow the full regression has smaller error than any single model\\nEP\\n[\\nY −\\nM∑\\nm=1\\nˆwm ˆfm(x)\\n]2\\n≤EP\\n[\\nY −ˆfm(x)\\n]2\\n∀m (8.58)\\nso combining models never makes things worse, at the population level.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9f162f4b-b179-43f0-bcc8-2f672c210bda', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 305, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='290 8. Model Inference and Averaging\\nOf course the population linear regression (8.57) is not available, and it\\nis natural to replace it with the linear regression over the training set. But\\nthere are simple examples where this does not work well. For example, if\\nˆfm(x),m =1 ,2,...,M represent the prediction from the best subset of\\ninputs of sizem among M total inputs, then linear regression would put all\\nof the weight on the largest model, that is, ˆwM =1 , ˆwm =0 ,m<M .T h e\\nproblem is that we have not put each of the models on the same footing\\nby taking into account their complexity (the number of inputsm in this\\nexample).\\nStacked generalization,o r stacking,i saw a yo fd o i n gt h i s .L e tˆf−i\\nm (x)\\nbe the prediction at x, using model m, applied to the dataset with the\\nith training observation removed. The stacking estimate of the weights is\\nobtained from the least squares linear regression ofyi on ˆf−i\\nm (xi),m =\\n1,2,...,M . In detail the stacking weights are given by\\nˆwst =a r g m i n\\nw\\nN∑\\ni=1\\n[\\nyi −\\nM∑\\nm=1\\nwm ˆf−i\\nm (xi)\\n]2\\n. (8.59)\\nThe ﬁnal prediction is ∑\\nm ˆwst\\nm ˆfm(x). By using the cross-validated pre-\\ndictions ˆf−i\\nm (x), stacking avoids giving unfairly high weight to models with\\nhigher complexity. Better results can be obtained by restricting the weights\\nto be nonnegative, and to sum to 1. This seems like a reasonable restriction\\nif we interpret the weights as posterior model probabilities as in equation\\n(8.54), and it leads to a tractable quadratic programming problem.\\nThere is a close connection between stacking and model selection via\\nleave-one-outcross-validation(Section7.10).Ifwerestricttheminimization\\nin (8.59) to weight vectorsw that have one unit weight and the rest zero,\\nthis leads to a model choice ˆm with smallest leave-one-out cross-validation\\nerror. Rather than choose a single model, stacking combines them with\\nestimated optimal weights. This will often lead to better prediction, but\\nless interpretability than the choice of only one of theM models.\\nThe stacking idea is actually more general than described above. One\\ncan use any learning method, not just linear regression, to combine the\\nmodels as in (8.59); the weights could also depend on the input location\\nx. In this way, learning methods are “stacked” on top of one another, to\\nimprove prediction performance.\\n8.9 Stochastic Search: Bumping\\nThe ﬁnal method described in this chapter does not involve averaging or\\ncombining models, but rather is a technique for ﬁnding a better single\\nmodel. Bumping uses bootstrap sampling to move randomly through model\\nspace. For problems where ﬁtting method ﬁnds many local minima, bump-\\ning can help the method to avoid getting stuck in poor solutions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0bcc5630-f300-4f0c-a521-daed392d0315', embedding=None, metadata={'file_name': 'elements_of_statistical_learning.pdf', 'page_num': 306, 'doc_type': 'textbook', 'course': 'Machine Learning'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.9 Stochastic Search: Bumping 291\\nRegular 4-Node Tree\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nBumped 4-Node Tree\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81 \\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81 \\x81\\n\\x81\\n\\x81\\x81\\n\\x81\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\n\\x81\\nFIGURE 8.13. Data with two features and two classes (blue and orange), dis-\\nplaying a pure interaction. The left panel shows the partition found by three splits\\nof a standard, greedy, tree-growing algorithm. The vertical grey line near the left\\nedge is the ﬁrst split, and the broken lines are the two subsequent splits. The al-\\ngorithm has no idea where to make a good initial split, and makes a poor choice.\\nThe right panel shows the near-optimal splits found by bumping the tree-growing\\nalgorithm 20 times.\\nAs in bagging, we draw bootstrap samples and ﬁt a model to each. But\\nrather than average the predictions, we choose the model estimated from a\\nbootstrap sample that best ﬁts the training data. In detail, we draw boot-\\nstrap samples Z∗1,..., Z∗B and ﬁt our model to each, giving predictions\\nˆf∗b(x),b =1 ,2,...,B at input pointx. We then choose the model that\\nproduces the smallest prediction error, averaged over theoriginal training\\nset. For squared error, for example, we choose the model obtained from\\nbootstrap sampleˆb,w h e r e\\nˆb =a r gm i n\\nb\\nN∑\\ni=1\\n[yi −ˆf∗b(xi)]2. (8.60)\\nThe corresponding model predictions are ˆf∗ˆb(x). By convention we also\\ninclude the original training sample in the set of bootstrap samples, so that\\nthe method is free to pick the original model if it has the lowest training\\nerror.\\nBy perturbing the data, bumping tries to move the ﬁtting procedure\\naround to good areas of model space. For example, if a few data points are\\ncausing the procedure to ﬁnd a poor solution, any bootstrap sample that\\nomits those data points should procedure a better solution.\\nFor another example, consider the classiﬁcation data in Figure 8.13, the\\nnotorious exclusive or (XOR) problem. There are two classes (blue and\\norange) and two input features, with the features exhibiting a pure inter-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required_exts = ['.pdf', '.tex']\n",
    "#reader = SimpleDirectoryReader(input_dir = \"../data\", required_exts = required_exts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1789 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length: 147 characters\n",
      "Preview: Gareth James • Daniela Witten •\n",
      "Trevor Hastie • Robert Tibshirani\n",
      "An Introduction to Statistical\n",
      "Learning\n",
      "with Applications in R\n",
      "Second Edition\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# See what one chunk looks like\n",
    "print(f\"Chunk length: {len(docs[0].text)} characters\")\n",
    "print(f\"Preview: {docs[0].text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the default embedding model from the OpenAI one to the sentence transformer model from HuggingFace as it is open-source and free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 00:18:30,989 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-V2\n",
      "2026-02-13 00:18:31,145 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,245 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,260 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:31,363 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,493 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,507 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:31,622 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,729 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,748 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:31,854 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,954 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:31,970 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:32,081 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,186 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,203 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:32,311 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,411 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,426 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:32,530 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/adapter_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,639 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-13 00:18:32,738 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,837 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:32,850 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1660.73it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-V2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-13 00:18:33,112 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,214 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,226 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:33,326 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,424 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,438 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:33,541 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-V2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,655 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-13 00:18:33,758 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-V2/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:33,869 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:34,037 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-V2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:34,138 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:34,152 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 00:18:34,252 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-V2 \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-13 00:18:34,365 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name = 'sentence-transformers/all-MiniLM-L6-V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path = './chroma')\n",
    "chroma_collection = chroma_client.create_collection('ml_textbooks') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection= chroma_collection,)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text chunks after repacking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 00:19:07,880 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode = 'tree_summarize', verbose = True,  similarity_top_k = 5)\n",
    "\n",
    "response = query_engine.query(\"What is gradient descent? Return answers only based off of the books given to you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : Gradient descent is a first-order optimization algorithm used to find a local minimum of a function by taking steps proportional to the negative of the gradient of the function at the current point. The algorithm involves iteratively updating the parameters based on the negative gradient direction to converge towards a local minimum.\n",
      "\n",
      "Sources: \n",
      "\n",
      "1. Score: 0.534\n",
      "   File: mml-book.pdf\n",
      "   Text: 228 Continuous Optimization\n",
      "where f : Rd → R is an objective function that captures the machine\n",
      "learning problem at hand. We assume that our functionf is differentiable,\n",
      "and we are unable to analytica...\n",
      "\n",
      "2. Score: 0.485\n",
      "   File: mml-book.pdf\n",
      "   Text: 228 Continuous Optimization\n",
      "where f : Rd → R is an objective function that captures the machine\n",
      "learning problem at hand. We assume that our functionf is differentiable,\n",
      "and we are unable to analytica...\n",
      "\n",
      "3. Score: 0.450\n",
      "   File: mml-book.pdf\n",
      "   Text: 230 Continuous Optimization\n",
      "Although the “undo” step seems to be a waste of resources, using this\n",
      "heuristic guarantees monotonic convergence.\n",
      "Example 7.2 (Solving a Linear Equation System)\n",
      "When we sol...\n",
      "\n",
      "4. Score: 0.426\n",
      "   File: ST443_Lecture_3.pdf\n",
      "   Text: Gradient descent\n",
      "▶ Gradient descent is a popular method for minimising a smooth function\n",
      "𝑓 ∶ ℝ𝑝 → ℝ (e.g. the empirical risk function).\n",
      "Algorithm:\n",
      "1. Initialise 𝑤(0) ∈ ℝ𝑝.\n",
      "2. Update 𝑤(𝑡) ← 𝑤(𝑡−1) − 𝛼𝑡...\n",
      "\n",
      "5. Score: 0.425\n",
      "   File: mml-book.pdf\n",
      "   Text: 5.6 Backpropagation and Automatic Differentiation 159\n",
      "“transpose” a tensor, we mean swapping the first two dimensions. Specif-\n",
      "ically , in (5.99) through (5.102), we require tensor-related computation...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer : {response.response}\")\n",
    "print(\"\\nSources: \")\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    print(f\"\\n{i}. Score: {node.score:.3f}\")\n",
    "    print(f\"   File: {node.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"   Text: {node.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
